An Evaluation of Pedagogical Tutorial Tactics for a Natural Language Tutoring System: a RL Approach.	RL	Reinforcement Learning	1
An Evaluation of Pedagogical Tutorial Tactics for a Natural Language Tutoring System: a RL Approach.	RL	rel loc	0
An Evaluation of Pedagogical Tutorial Tactics for a Natural Language Tutoring System: a RL Approach.	RL	Reduce Left	0
An Evaluation of Pedagogical Tutorial Tactics for a Natural Language Tutoring System: a RL Approach.	RL	reinforcement learning	0
An Evaluation of Pedagogical Tutorial Tactics for a Natural Language Tutoring System: a RL Approach.	RL	Relevant Links	0
RL: An Introduction.	RL	Reinforcement Learning	1
RL: An Introduction.	RL	rel loc	0
RL: An Introduction.	RL	Reduce Left	0
RL: An Introduction.	RL	reinforcement learning	0
RL: An Introduction.	RL	Relevant Links	0
RL.	RL	Reinforcement Learning	1
RL.	RL	rel loc	0
RL.	RL	Reduce Left	0
RL.	RL	reinforcement learning	0
RL.	RL	Relevant Links	0
2016 Association for Computational Linguistics Deep RL with a Natural Language Action Space Ji He ?	RL	Reinforcement Learning	1
2016 Association for Computational Linguistics Deep RL with a Natural Language Action Space Ji He ?	RL	rel loc	0
2016 Association for Computational Linguistics Deep RL with a Natural Language Action Space Ji He ?	RL	Reduce Left	0
2016 Association for Computational Linguistics Deep RL with a Natural Language Action Space Ji He ?	RL	reinforcement learning	0
2016 Association for Computational Linguistics Deep RL with a Natural Language Action Space Ji He ?	RL	Relevant Links	0
RL for Dialog Management Using Least-Squares Policy Iteration and Fast Fea-ture Selection.	RL	Reinforcement Learning	1
RL for Dialog Management Using Least-Squares Policy Iteration and Fast Fea-ture Selection.	RL	rel loc	0
RL for Dialog Management Using Least-Squares Policy Iteration and Fast Fea-ture Selection.	RL	Reduce Left	0
RL for Dialog Management Using Least-Squares Policy Iteration and Fast Fea-ture Selection.	RL	reinforcement learning	0
RL for Dialog Management Using Least-Squares Policy Iteration and Fast Fea-ture Selection.	RL	Relevant Links	0
Optimizing Dialogue Management with RL: Experiments with the NJFun System.	RL	Reinforcement Learning	1
Optimizing Dialogue Management with RL: Experiments with the NJFun System.	RL	rel loc	0
Optimizing Dialogue Management with RL: Experiments with the NJFun System.	RL	Reduce Left	0
Optimizing Dialogue Management with RL: Experiments with the NJFun System.	RL	reinforcement learning	0
Optimizing Dialogue Management with RL: Experiments with the NJFun System.	RL	Relevant Links	0
prep of(S, object word) relations, recovering RL word ?	RL	Reinforcement Learning	0
prep of(S, object word) relations, recovering RL word ?	RL	rel loc	1
prep of(S, object word) relations, recovering RL word ?	RL	Reduce Left	0
prep of(S, object word) relations, recovering RL word ?	RL	reinforcement learning	0
prep of(S, object word) relations, recovering RL word ?	RL	Relevant Links	0
252 J?rgensen and L?nning Minimal Recursion Semantic Analysis of Locatives prep relation mode RL rel id rel . . .	RL	Reinforcement Learning	0
252 J?rgensen and L?nning Minimal Recursion Semantic Analysis of Locatives prep relation mode RL rel id rel . . .	RL	rel loc	1
252 J?rgensen and L?nning Minimal Recursion Semantic Analysis of Locatives prep relation mode RL rel id rel . . .	RL	Reduce Left	0
252 J?rgensen and L?nning Minimal Recursion Semantic Analysis of Locatives prep relation mode RL rel id rel . . .	RL	reinforcement learning	0
252 J?rgensen and L?nning Minimal Recursion Semantic Analysis of Locatives prep relation mode RL rel id rel . . .	RL	Relevant Links	0
We pre-define a dictionary of attribute-values (color word, size word, abs location word, RLation word) for each of the attributes based on the observed data using a combination of POS-tagging and manual labeling.	RL	Reinforcement Learning	0
We pre-define a dictionary of attribute-values (color word, size word, abs location word, RLation word) for each of the attributes based on the observed data using a combination of POS-tagging and manual labeling.	RL	rel loc	1
We pre-define a dictionary of attribute-values (color word, size word, abs location word, RLation word) for each of the attributes based on the observed data using a combination of POS-tagging and manual labeling.	RL	Reduce Left	0
We pre-define a dictionary of attribute-values (color word, size word, abs location word, RLation word) for each of the attributes based on the observed data using a combination of POS-tagging and manual labeling.	RL	reinforcement learning	0
We pre-define a dictionary of attribute-values (color word, size word, abs location word, RLation word) for each of the attributes based on the observed data using a combination of POS-tagging and manual labeling.	RL	Relevant Links	0
Re- cently, inspired by advances in deep learning (Le- Cun et al, 2015; Hinton et al, 2012; Krizhevsky et al, 2012; Dahl et al, 2012), significant progress has been made by combining deep learning with RL.	RL	Reinforcement Learning	0
Re- cently, inspired by advances in deep learning (Le- Cun et al, 2015; Hinton et al, 2012; Krizhevsky et al, 2012; Dahl et al, 2012), significant progress has been made by combining deep learning with RL.	RL	rel loc	0
Re- cently, inspired by advances in deep learning (Le- Cun et al, 2015; Hinton et al, 2012; Krizhevsky et al, 2012; Dahl et al, 2012), significant progress has been made by combining deep learning with RL.	RL	Reduce Left	0
Re- cently, inspired by advances in deep learning (Le- Cun et al, 2015; Hinton et al, 2012; Krizhevsky et al, 2012; Dahl et al, 2012), significant progress has been made by combining deep learning with RL.	RL	reinforcement learning	1
Re- cently, inspired by advances in deep learning (Le- Cun et al, 2015; Hinton et al, 2012; Krizhevsky et al, 2012; Dahl et al, 2012), significant progress has been made by combining deep learning with RL.	RL	Relevant Links	0
In some applications, it is possible to manually design fea- tures for state-action pairs, which are then used in RL to learn a near-optimal policy (Li et al, 2009).	RL	Reinforcement Learning	0
In some applications, it is possible to manually design fea- tures for state-action pairs, which are then used in RL to learn a near-optimal policy (Li et al, 2009).	RL	rel loc	0
In some applications, it is possible to manually design fea- tures for state-action pairs, which are then used in RL to learn a near-optimal policy (Li et al, 2009).	RL	Reduce Left	0
In some applications, it is possible to manually design fea- tures for state-action pairs, which are then used in RL to learn a near-optimal policy (Li et al, 2009).	RL	reinforcement learning	1
In some applications, it is possible to manually design fea- tures for state-action pairs, which are then used in RL to learn a near-optimal policy (Li et al, 2009).	RL	Relevant Links	0
Because a player?s action changes the environment, RL (Sutton and Barto, 1998) is appropriate for modeling long- term dependency in text games.	RL	Reinforcement Learning	0
Because a player?s action changes the environment, RL (Sutton and Barto, 1998) is appropriate for modeling long- term dependency in text games.	RL	rel loc	0
Because a player?s action changes the environment, RL (Sutton and Barto, 1998) is appropriate for modeling long- term dependency in text games.	RL	Reduce Left	0
Because a player?s action changes the environment, RL (Sutton and Barto, 1998) is appropriate for modeling long- term dependency in text games.	RL	reinforcement learning	1
Because a player?s action changes the environment, RL (Sutton and Barto, 1998) is appropriate for modeling long- term dependency in text games.	RL	Relevant Links	0
In language processing, RL has been ap- plied to a dialogue management system that con- verses with a human user by taking actions that generate natural language (Scheffler and Young, 2002; Young et al, 2013).	RL	Reinforcement Learning	0
In language processing, RL has been ap- plied to a dialogue management system that con- verses with a human user by taking actions that generate natural language (Scheffler and Young, 2002; Young et al, 2013).	RL	rel loc	0
In language processing, RL has been ap- plied to a dialogue management system that con- verses with a human user by taking actions that generate natural language (Scheffler and Young, 2002; Young et al, 2013).	RL	Reduce Left	0
In language processing, RL has been ap- plied to a dialogue management system that con- verses with a human user by taking actions that generate natural language (Scheffler and Young, 2002; Young et al, 2013).	RL	reinforcement learning	1
In language processing, RL has been ap- plied to a dialogue management system that con- verses with a human user by taking actions that generate natural language (Scheffler and Young, 2002; Young et al, 2013).	RL	Relevant Links	0
4 Related Work There has been increasing interest in applying deep RL to a variety problems, but only a few studies address problems with nat- ural language state or action spaces.	RL	Reinforcement Learning	0
4 Related Work There has been increasing interest in applying deep RL to a variety problems, but only a few studies address problems with nat- ural language state or action spaces.	RL	rel loc	0
4 Related Work There has been increasing interest in applying deep RL to a variety problems, but only a few studies address problems with nat- ural language state or action spaces.	RL	Reduce Left	0
4 Related Work There has been increasing interest in applying deep RL to a variety problems, but only a few studies address problems with nat- ural language state or action spaces.	RL	reinforcement learning	1
4 Related Work There has been increasing interest in applying deep RL to a variety problems, but only a few studies address problems with nat- ural language state or action spaces.	RL	Relevant Links	0
Microsoft Research, Redmond, WA 98052, USA {jianshuc, xiaohe, jfgao, lihongli, deng}@microsoft.com Abstract This paper introduces a novel architec- ture for RL with deep neural networks designed to handle state and action spaces characterized by natural language, as found in text-based games.	RL	Reinforcement Learning	0
Microsoft Research, Redmond, WA 98052, USA {jianshuc, xiaohe, jfgao, lihongli, deng}@microsoft.com Abstract This paper introduces a novel architec- ture for RL with deep neural networks designed to handle state and action spaces characterized by natural language, as found in text-based games.	RL	rel loc	0
Microsoft Research, Redmond, WA 98052, USA {jianshuc, xiaohe, jfgao, lihongli, deng}@microsoft.com Abstract This paper introduces a novel architec- ture for RL with deep neural networks designed to handle state and action spaces characterized by natural language, as found in text-based games.	RL	Reduce Left	0
Microsoft Research, Redmond, WA 98052, USA {jianshuc, xiaohe, jfgao, lihongli, deng}@microsoft.com Abstract This paper introduces a novel architec- ture for RL with deep neural networks designed to handle state and action spaces characterized by natural language, as found in text-based games.	RL	reinforcement learning	1
Microsoft Research, Redmond, WA 98052, USA {jianshuc, xiaohe, jfgao, lihongli, deng}@microsoft.com Abstract This paper introduces a novel architec- ture for RL with deep neural networks designed to handle state and action spaces characterized by natural language, as found in text-based games.	RL	Relevant Links	0
Thus the Dative DOt construc-  tion is stated as a construction i the grammar whose combination with  specific lexieal items comes up only in use.	DO	Direct Objec	1
Thus the Dative DOt construc-  tion is stated as a construction i the grammar whose combination with  specific lexieal items comes up only in use.	DO	Direct Object	0
So, in relation to the verb afelippen (to cut off) we find -  among others- DOt and Means.	DO	Direct Objec	1
So, in relation to the verb afelippen (to cut off) we find -  among others- DOt and Means.	DO	Direct Object	0
Clause  Head: Send  Indire~-I Object: Noun Phrase  Head: Him  DOt Noun Phrase  Head: Message  Article: The  Relative: Clause  Head: Arrive  Subject: That  Time: Yesterday  Figure 1: Syntactic Analysis of "Send him the message that  arrived yesterday.".	DO	Direct Objec	1
Clause  Head: Send  Indire~-I Object: Noun Phrase  Head: Him  DOt Noun Phrase  Head: Message  Article: The  Relative: Clause  Head: Arrive  Subject: That  Time: Yesterday  Figure 1: Syntactic Analysis of "Send him the message that  arrived yesterday.".	DO	Direct Object	0
The semantic onstraints on when a Dative  DOt can be used can not be expressed in the lexicon - the  difference between (12) and (13) depends on a constmal of the entire  utterance conceptualization.	DO	Direct Objec	1
The semantic onstraints on when a Dative  DOt can be used can not be expressed in the lexicon - the  difference between (12) and (13) depends on a constmal of the entire  utterance conceptualization.	DO	Direct Object	0
DOt NPs and Indirect Object NPs are all untagged? (	DO	Direct Objec	1
DOt NPs and Indirect Object NPs are all untagged? (	DO	Direct Object	0
In the above example, the preposition met (with) is an  indicator for Means and it is the absence of a preposition  which points to the DOt.	DO	Direct Objec	1
In the above example, the preposition met (with) is an  indicator for Means and it is the absence of a preposition  which points to the DOt.	DO	Direct Object	0
SYNTACTIC FORM:  - Realize the Goal as the DOt  - Realize the Theme as the Second Object.	DO	Direct Objec	1
SYNTACTIC FORM:  - Realize the Goal as the DOt  - Realize the Theme as the Second Object.	DO	Direct Object	0
Thus the Dative DO construc-  tion is stated as a construction i the grammar whose combination with  specific lexieal items comes up only in use.	DO	Direct Objec	0
Thus the Dative DO construc-  tion is stated as a construction i the grammar whose combination with  specific lexieal items comes up only in use.	DO	Direct Object	1
So, in relation to the verb afelippen (to cut off) we find -  among others- DO and Means.	DO	Direct Objec	0
So, in relation to the verb afelippen (to cut off) we find -  among others- DO and Means.	DO	Direct Object	1
Clause  Head: Send  Indire~-I Object: Noun Phrase  Head: Him  DO Noun Phrase  Head: Message  Article: The  Relative: Clause  Head: Arrive  Subject: That  Time: Yesterday  Figure 1: Syntactic Analysis of "Send him the message that  arrived yesterday.".	DO	Direct Objec	0
Clause  Head: Send  Indire~-I Object: Noun Phrase  Head: Him  DO Noun Phrase  Head: Message  Article: The  Relative: Clause  Head: Arrive  Subject: That  Time: Yesterday  Figure 1: Syntactic Analysis of "Send him the message that  arrived yesterday.".	DO	Direct Object	1
The semantic onstraints on when a Dative  DO can be used can not be expressed in the lexicon - the  difference between (12) and (13) depends on a constmal of the entire  utterance conceptualization.	DO	Direct Objec	0
The semantic onstraints on when a Dative  DO can be used can not be expressed in the lexicon - the  difference between (12) and (13) depends on a constmal of the entire  utterance conceptualization.	DO	Direct Object	1
DO NPs and Indirect Object NPs are all untagged? (	DO	Direct Objec	0
DO NPs and Indirect Object NPs are all untagged? (	DO	Direct Object	1
In the above example, the preposition met (with) is an  indicator for Means and it is the absence of a preposition  which points to the DO.	DO	Direct Objec	0
In the above example, the preposition met (with) is an  indicator for Means and it is the absence of a preposition  which points to the DO.	DO	Direct Object	1
SYNTACTIC FORM:  - Realize the Goal as the DO  - Realize the Theme as the Second Object.	DO	Direct Objec	0
SYNTACTIC FORM:  - Realize the Goal as the DO  - Realize the Theme as the Second Object.	DO	Direct Object	1
SVMs have been used for text classification (Tong and Koller, 2002), using properties of the support vector ma- chine algorithm for determining what unlabelled data to select for classification.	SVMs	Support vector machines	1
SVMs have been used for text classification (Tong and Koller, 2002), using properties of the support vector ma- chine algorithm for determining what unlabelled data to select for classification.	SVMs	Support  Vector Machines	0
SVMs (SVM, Cortes and Vapnik (1995)) have gained a lot of popularity in the past decade and very often are state-of-the-art approach for text mining challenges.	SVMs	Support vector machines	1
SVMs (SVM, Cortes and Vapnik (1995)) have gained a lot of popularity in the past decade and very often are state-of-the-art approach for text mining challenges.	SVMs	Support  Vector Machines	0
SVMs are powerful statisti- cal classifiers, as employed in the ?	SVMs	Support vector machines	1
SVMs are powerful statisti- cal classifiers, as employed in the ?	SVMs	Support  Vector Machines	0
2.6 Support Vector Machine SVMs (SVM) are statistical classifiers which use labelled training data to pre- dict the class of unseen inputs.	SVMs	Support vector machines	1
2.6 Support Vector Machine SVMs (SVM) are statistical classifiers which use labelled training data to pre- dict the class of unseen inputs.	SVMs	Support  Vector Machines	0
At  this  point  the  focus switches over to the tool itself, which learns  regular  patterns  using  SVMs  and then uses the information gathered to tag any  possible list of words  (Figure 1, Line 5).	SVMs	Support vector machines	0
At  this  point  the  focus switches over to the tool itself, which learns  regular  patterns  using  SVMs  and then uses the information gathered to tag any  possible list of words  (Figure 1, Line 5).	SVMs	Support  Vector Machines	1
Decisions were  made  by  an  annotator  with  a  well-grounded  knowledge of SVMs and their  behaviour,  which  turned  out  to  be  quite  useful  when deciding which output should be classified as  ?	SVMs	Support vector machines	0
Decisions were  made  by  an  annotator  with  a  well-grounded  knowledge of SVMs and their  behaviour,  which  turned  out  to  be  quite  useful  when deciding which output should be classified as  ?	SVMs	Support  Vector Machines	1
The fragments  also consisted of parsed ones: terse question,  TR and definitions, and nonparsed ones:  false starts and phatics.	TR	terse reply	1
The fragments  also consisted of parsed ones: terse question,  TR and definitions, and nonparsed ones:  false starts and phatics.	TR	Template Relation	0
The fragments  also consisted of parsed ones: terse question,  TR and definitions, and nonparsed ones:  false starts and phatics.	TR	Translation retrieval	0
The fragments  also consisted of parsed ones: terse question,  TR and definitions, and nonparsed ones:  false starts and phatics.	TR	Template Relatio	0
Of particular interest in the  analysis is the notion of a terse question, which is a  type of telegraphic ellipsis, and a TR, which is  a type of contextual ellipsis.	TR	terse reply	1
Of particular interest in the  analysis is the notion of a terse question, which is a  type of telegraphic ellipsis, and a TR, which is  a type of contextual ellipsis.	TR	Template Relation	0
Of particular interest in the  analysis is the notion of a terse question, which is a  type of telegraphic ellipsis, and a TR, which is  a type of contextual ellipsis.	TR	Translation retrieval	0
Of particular interest in the  analysis is the notion of a terse question, which is a  type of telegraphic ellipsis, and a TR, which is  a type of contextual ellipsis.	TR	Template Relatio	0
To illustrate concretely, suppose our search state is: (some felines have tails, valid) 538 Transition TR WordNet hypernym v WordNet hyponym w WordNet antonym ?	TR	terse reply	0
To illustrate concretely, suppose our search state is: (some felines have tails, valid) 538 Transition TR WordNet hypernym v WordNet hyponym w WordNet antonym ?	TR	Template Relation	1
To illustrate concretely, suppose our search state is: (some felines have tails, valid) 538 Transition TR WordNet hypernym v WordNet hyponym w WordNet antonym ?	TR	Translation retrieval	0
To illustrate concretely, suppose our search state is: (some felines have tails, valid) 538 Transition TR WordNet hypernym v WordNet hyponym w WordNet antonym ?	TR	Template Relatio	0
2 Related Work  The relation extraction task was first introduced as  part of the Template Element task in MUC6 and then  formulated as the TR task in MUC7.	TR	terse reply	0
2 Related Work  The relation extraction task was first introduced as  part of the Template Element task in MUC6 and then  formulated as the TR task in MUC7.	TR	Template Relation	1
2 Related Work  The relation extraction task was first introduced as  part of the Template Element task in MUC6 and then  formulated as the TR task in MUC7.	TR	Translation retrieval	0
2 Related Work  The relation extraction task was first introduced as  part of the Template Element task in MUC6 and then  formulated as the TR task in MUC7.	TR	Template Relatio	0
The reports are ordered by task (Template Element, TR, Scenario Template, Named Entity, and Coreference) and secondarily by site and language (in alphabetical order).	TR	terse reply	0
The reports are ordered by task (Template Element, TR, Scenario Template, Named Entity, and Coreference) and secondarily by site and language (in alphabetical order).	TR	Template Relation	1
The reports are ordered by task (Template Element, TR, Scenario Template, Named Entity, and Coreference) and secondarily by site and language (in alphabetical order).	TR	Translation retrieval	0
The reports are ordered by task (Template Element, TR, Scenario Template, Named Entity, and Coreference) and secondarily by site and language (in alphabetical order).	TR	Template Relatio	0
While the  MUC TR and Scenario Template  tasks targeted relations and events plus their attrib- utes, the focus of these tasks was domain specific.	TR	terse reply	0
While the  MUC TR and Scenario Template  tasks targeted relations and events plus their attrib- utes, the focus of these tasks was domain specific.	TR	Template Relation	1
While the  MUC TR and Scenario Template  tasks targeted relations and events plus their attrib- utes, the focus of these tasks was domain specific.	TR	Translation retrieval	0
While the  MUC TR and Scenario Template  tasks targeted relations and events plus their attrib- utes, the focus of these tasks was domain specific.	TR	Template Relatio	0
4.5 Base Phrase Chunking  It is well known that chunking plays a critical role  in the TR task of the 7th Message  Understanding Conference (MUC-7 1998).	TR	terse reply	0
4.5 Base Phrase Chunking  It is well known that chunking plays a critical role  in the TR task of the 7th Message  Understanding Conference (MUC-7 1998).	TR	Template Relation	1
4.5 Base Phrase Chunking  It is well known that chunking plays a critical role  in the TR task of the 7th Message  Understanding Conference (MUC-7 1998).	TR	Translation retrieval	0
4.5 Base Phrase Chunking  It is well known that chunking plays a critical role  in the TR task of the 7th Message  Understanding Conference (MUC-7 1998).	TR	Template Relatio	0
2 Related Work  The task of relation extraction was introduced as a  part of the Template Element task in MUC6 and  formulated as the TR task in MUC7  (MUC, 1987-1998).	TR	terse reply	0
2 Related Work  The task of relation extraction was introduced as a  part of the Template Element task in MUC6 and  formulated as the TR task in MUC7  (MUC, 1987-1998).	TR	Template Relation	1
2 Related Work  The task of relation extraction was introduced as a  part of the Template Element task in MUC6 and  formulated as the TR task in MUC7  (MUC, 1987-1998).	TR	Translation retrieval	0
2 Related Work  The task of relation extraction was introduced as a  part of the Template Element task in MUC6 and  formulated as the TR task in MUC7  (MUC, 1987-1998).	TR	Template Relatio	0
1 Introduction TR aims to search for the most probable translation candidate from a set of target- language strings for a given source-language string.	TR	terse reply	0
1 Introduction TR aims to search for the most probable translation candidate from a set of target- language strings for a given source-language string.	TR	Template Relation	0
1 Introduction TR aims to search for the most probable translation candidate from a set of target- language strings for a given source-language string.	TR	Translation retrieval	1
1 Introduction TR aims to search for the most probable translation candidate from a set of target- language strings for a given source-language string.	TR	Template Relatio	0
Institute for Interdisciplinary Information Sciences Tsinghua University, Beijing, China chengyong3001@gmail.com, xu@tsinghua.edu.cn  Toshiba Corporation Corporate Research & Development Center tatsuya.izuha@toshiba.co.jp # Toshiba (China) R&D Center haojie@toshiba.com.cn Abstract TR aims to find the most likely translation among a set of target-language strings for a given source-language string.	TR	terse reply	0
Institute for Interdisciplinary Information Sciences Tsinghua University, Beijing, China chengyong3001@gmail.com, xu@tsinghua.edu.cn  Toshiba Corporation Corporate Research & Development Center tatsuya.izuha@toshiba.co.jp # Toshiba (China) R&D Center haojie@toshiba.com.cn Abstract TR aims to find the most likely translation among a set of target-language strings for a given source-language string.	TR	Template Relation	0
Institute for Interdisciplinary Information Sciences Tsinghua University, Beijing, China chengyong3001@gmail.com, xu@tsinghua.edu.cn  Toshiba Corporation Corporate Research & Development Center tatsuya.izuha@toshiba.co.jp # Toshiba (China) R&D Center haojie@toshiba.com.cn Abstract TR aims to find the most likely translation among a set of target-language strings for a given source-language string.	TR	Translation retrieval	1
Institute for Interdisciplinary Information Sciences Tsinghua University, Beijing, China chengyong3001@gmail.com, xu@tsinghua.edu.cn  Toshiba Corporation Corporate Research & Development Center tatsuya.izuha@toshiba.co.jp # Toshiba (China) R&D Center haojie@toshiba.com.cn Abstract TR aims to find the most likely translation among a set of target-language strings for a given source-language string.	TR	Template Relatio	0
To illustrate concretely, suppose our search state is: (some felines have tails, valid) 538 Transition TRn WordNet hypernym v WordNet hyponym w WordNet antonym ?	TR	terse reply	0
To illustrate concretely, suppose our search state is: (some felines have tails, valid) 538 Transition TRn WordNet hypernym v WordNet hyponym w WordNet antonym ?	TR	Template Relation	0
To illustrate concretely, suppose our search state is: (some felines have tails, valid) 538 Transition TRn WordNet hypernym v WordNet hyponym w WordNet antonym ?	TR	Translation retrieval	0
To illustrate concretely, suppose our search state is: (some felines have tails, valid) 538 Transition TRn WordNet hypernym v WordNet hyponym w WordNet antonym ?	TR	Template Relatio	1
2 Related Work  The relation extraction task was first introduced as  part of the Template Element task in MUC6 and then  formulated as the TRn task in MUC7.	TR	terse reply	0
2 Related Work  The relation extraction task was first introduced as  part of the Template Element task in MUC6 and then  formulated as the TRn task in MUC7.	TR	Template Relation	0
2 Related Work  The relation extraction task was first introduced as  part of the Template Element task in MUC6 and then  formulated as the TRn task in MUC7.	TR	Translation retrieval	0
2 Related Work  The relation extraction task was first introduced as  part of the Template Element task in MUC6 and then  formulated as the TRn task in MUC7.	TR	Template Relatio	1
The reports are ordered by task (Template Element, TRn, Scenario Template, Named Entity, and Coreference) and secondarily by site and language (in alphabetical order).	TR	terse reply	0
The reports are ordered by task (Template Element, TRn, Scenario Template, Named Entity, and Coreference) and secondarily by site and language (in alphabetical order).	TR	Template Relation	0
The reports are ordered by task (Template Element, TRn, Scenario Template, Named Entity, and Coreference) and secondarily by site and language (in alphabetical order).	TR	Translation retrieval	0
The reports are ordered by task (Template Element, TRn, Scenario Template, Named Entity, and Coreference) and secondarily by site and language (in alphabetical order).	TR	Template Relatio	1
While the  MUC TRn and Scenario Template  tasks targeted relations and events plus their attrib- utes, the focus of these tasks was domain specific.	TR	terse reply	0
While the  MUC TRn and Scenario Template  tasks targeted relations and events plus their attrib- utes, the focus of these tasks was domain specific.	TR	Template Relation	0
While the  MUC TRn and Scenario Template  tasks targeted relations and events plus their attrib- utes, the focus of these tasks was domain specific.	TR	Translation retrieval	0
While the  MUC TRn and Scenario Template  tasks targeted relations and events plus their attrib- utes, the focus of these tasks was domain specific.	TR	Template Relatio	1
4.5 Base Phrase Chunking  It is well known that chunking plays a critical role  in the TRn task of the 7th Message  Understanding Conference (MUC-7 1998).	TR	terse reply	0
4.5 Base Phrase Chunking  It is well known that chunking plays a critical role  in the TRn task of the 7th Message  Understanding Conference (MUC-7 1998).	TR	Template Relation	0
4.5 Base Phrase Chunking  It is well known that chunking plays a critical role  in the TRn task of the 7th Message  Understanding Conference (MUC-7 1998).	TR	Translation retrieval	0
4.5 Base Phrase Chunking  It is well known that chunking plays a critical role  in the TRn task of the 7th Message  Understanding Conference (MUC-7 1998).	TR	Template Relatio	1
2 Related Work  The task of relation extraction was introduced as a  part of the Template Element task in MUC6 and  formulated as the TRn task in MUC7  (MUC, 1987-1998).	TR	terse reply	0
2 Related Work  The task of relation extraction was introduced as a  part of the Template Element task in MUC6 and  formulated as the TRn task in MUC7  (MUC, 1987-1998).	TR	Template Relation	0
2 Related Work  The task of relation extraction was introduced as a  part of the Template Element task in MUC6 and  formulated as the TRn task in MUC7  (MUC, 1987-1998).	TR	Translation retrieval	0
2 Related Work  The task of relation extraction was introduced as a  part of the Template Element task in MUC6 and  formulated as the TRn task in MUC7  (MUC, 1987-1998).	TR	Template Relatio	1
In this paper, we use Chinese  Propbank 1.0 provided by Linguistic Data Consor- tium (LDC), which is based on CTB.	CTB	Chinese Treebank	1
In this paper, we use Chinese  Propbank 1.0 provided by Linguistic Data Consor- tium (LDC), which is based on CTB.	CTB	Chinese TreeBank	0
This was effective  because of the properties of Chinese: First, there is  no multi-root in CTB.	CTB	Chinese Treebank	1
This was effective  because of the properties of Chinese: First, there is  no multi-root in CTB.	CTB	Chinese TreeBank	0
5 Experiment and discussion  This section will describe the experiment on the  SRL in CTB, compare TSVM with  regular SVM, and evaluate the effect of the pro- posed argument-specific heuristics.	CTB	Chinese Treebank	1
5 Experiment and discussion  This section will describe the experiment on the  SRL in CTB, compare TSVM with  regular SVM, and evaluate the effect of the pro- posed argument-specific heuristics.	CTB	Chinese TreeBank	0
It consists of 37,183 propositions indexed to the                                                    1 F1 measure computes the harmonic mean of precision  and recall of SRL systems in CoNLL-2005  first 250k words in CTB 5.1, includ- ing 4,865 verb types and 5,298 framesets.	CTB	Chinese Treebank	1
It consists of 37,183 propositions indexed to the                                                    1 F1 measure computes the harmonic mean of precision  and recall of SRL systems in CoNLL-2005  first 250k words in CTB 5.1, includ- ing 4,865 verb types and 5,298 framesets.	CTB	Chinese TreeBank	0
We incorporated Nivre?s method with these  preprocessing methods for Chinese dependency  analysis with Penn CTB and Sinica  Treebank (Chen   et al, 2003).	CTB	Chinese Treebank	1
We incorporated Nivre?s method with these  preprocessing methods for Chinese dependency  analysis with Penn CTB and Sinica  Treebank (Chen   et al, 2003).	CTB	Chinese TreeBank	0
Adding seman- tic roles to the CTB.	CTB	Chinese Treebank	1
Adding seman- tic roles to the CTB.	CTB	Chinese TreeBank	0
The Penn CTB: Phrase structure annotation of a large corpus.	CTB	Chinese Treebank	0
The Penn CTB: Phrase structure annotation of a large corpus.	CTB	Chinese TreeBank	1
The Penn CTB: phrase structure an- notation of a large corpus.	CTB	Chinese Treebank	0
The Penn CTB: phrase structure an- notation of a large corpus.	CTB	Chinese TreeBank	1
The Penn CTB: Phrase Structure Annotation of a Large Corpus.	CTB	Chinese Treebank	0
The Penn CTB: Phrase Structure Annotation of a Large Corpus.	CTB	Chinese TreeBank	1
k+njk (7) where the second product is over pairs (kl) where k is a parent of l on the path from the root to x. The HDT model we pro- posed has a large number of parameters and hy- perparameters (even after integrating out the ?	HDT	hierarchical Dirichlet tree	1
k+njk (7) where the second product is over pairs (kl) where k is a parent of l on the path from the root to x. The HDT model we pro- posed has a large number of parameters and hy- perparameters (even after integrating out the ?	HDT	Hindi Dependency Treebank	0
6 Conclusion and Future Work We presented a HDT model for information retrieval which can inject (semantical or syntactical) word relationships as the domain knowl- edge into a probabilistic model for information re- trieval.	HDT	hierarchical Dirichlet tree	1
6 Conclusion and Future Work We presented a HDT model for information retrieval which can inject (semantical or syntactical) word relationships as the domain knowl- edge into a probabilistic model for information re- trieval.	HDT	Hindi Dependency Treebank	0
We give encour- aging experimental evidence of the superiority of the HDT compared to standard baselines.	HDT	hierarchical Dirichlet tree	1
We give encour- aging experimental evidence of the superiority of the HDT compared to standard baselines.	HDT	Hindi Dependency Treebank	0
We generalize the model of (Cowans, 2004) by re- placing the Dirichlet distributions with Dirichlet tree distributions (Minka, 2003), thus we call our model the HDT.	HDT	hierarchical Dirichlet tree	1
We generalize the model of (Cowans, 2004) by re- placing the Dirichlet distributions with Dirichlet tree distributions (Minka, 2003), thus we call our model the HDT.	HDT	Hindi Dependency Treebank	0
b) The global tree and local trees in HDT docu- ment model.	HDT	hierarchical Dirichlet tree	1
b) The global tree and local trees in HDT docu- ment model.	HDT	Hindi Dependency Treebank	0
c?2011 Association for Computational Linguistics Empty Categories in HDT: Analysis and Recovery Chaitanya GSK Intl Institute of Info.	HDT	hierarchical Dirichlet tree	0
c?2011 Association for Computational Linguistics Empty Categories in HDT: Analysis and Recovery Chaitanya GSK Intl Institute of Info.	HDT	Hindi Dependency Treebank	1
Unlike PropBanks in most other languages, the Hind PropBank is annotated on top of dependency structure, the HDT.	HDT	hierarchical Dirichlet tree	0
Unlike PropBanks in most other languages, the Hind PropBank is annotated on top of dependency structure, the HDT.	HDT	Hindi Dependency Treebank	1
3 HDT (HTB) A multi layered and multi representational tree- bank for Hindi is developed by annotating with morpho-syntactic (morphological analyses, POS tags, chunk) and syntacto-semantic (dependency re- lations labeled in the computational paninian frame- work) information.	HDT	hierarchical Dirichlet tree	0
3 HDT (HTB) A multi layered and multi representational tree- bank for Hindi is developed by annotating with morpho-syntactic (morphological analyses, POS tags, chunk) and syntacto-semantic (dependency re- lations labeled in the computational paninian frame- work) information.	HDT	Hindi Dependency Treebank	1
(2012) describe a graphical tool that was used in the annotation of the HDT.	HDT	hierarchical Dirichlet tree	0
(2012) describe a graphical tool that was used in the annotation of the HDT.	HDT	Hindi Dependency Treebank	1
94 2 The HDT  Hindi is a free word order language with SOV as  the default order.	HDT	hierarchical Dirichlet tree	0
94 2 The HDT  Hindi is a free word order language with SOV as  the default order.	HDT	Hindi Dependency Treebank	1
A GUI to Detect and Correct Errors in HDT.	HDT	hierarchical Dirichlet tree	0
A GUI to Detect and Correct Errors in HDT.	HDT	Hindi Dependency Treebank	1
If bx corresponds to empirical expectations and p?(z|x) is uniform, then Equation (10) would be a log-likelihood and Equation (14) (fol- lowing) would be a maxiMaxEnt problem.	MaxEnt	mum entropy	1
If bx corresponds to empirical expectations and p?(z|x) is uniform, then Equation (10) would be a log-likelihood and Equation (14) (fol- lowing) would be a maxiMaxEnt problem.	MaxEnt	Maximum entropy	0
If bx corresponds to empirical expectations and p?(z|x) is uniform, then Equation (10) would be a log-likelihood and Equation (14) (fol- lowing) would be a maxiMaxEnt problem.	MaxEnt	Maximum entropy classification	0
A max- iMaxEnt approach to combining word align- ments.	MaxEnt	mum entropy	1
A max- iMaxEnt approach to combining word align- ments.	MaxEnt	Maximum entropy	0
A max- iMaxEnt approach to combining word align- ments.	MaxEnt	Maximum entropy classification	0
A maxiMaxEnt word aligner for arabic-english machine translation.	MaxEnt	mum entropy	1
A maxiMaxEnt word aligner for arabic-english machine translation.	MaxEnt	Maximum entropy	0
A maxiMaxEnt word aligner for arabic-english machine translation.	MaxEnt	Maximum entropy classification	0
Note that this primal?dual relationship is very similar to the one between maximum likelihood and maxiMaxEnt.	MaxEnt	mum entropy	1
Note that this primal?dual relationship is very similar to the one between maximum likelihood and maxiMaxEnt.	MaxEnt	Maximum entropy	0
Note that this primal?dual relationship is very similar to the one between maximum likelihood and maxiMaxEnt.	MaxEnt	Maximum entropy classification	0
Supervised training methods already applied to PP attachment range from stochastic maximum likelihood (Collins and Brooks, 1995) or maxi- MaxEnt models (Ratnaparkhi et al, 1994) to the induction of transformation rules (Brill and Resnik, 1994), decision trees (Stetina and Nagao, 1997) and connectionist models (Sopena et al, 1998).	MaxEnt	mum entropy	1
Supervised training methods already applied to PP attachment range from stochastic maximum likelihood (Collins and Brooks, 1995) or maxi- MaxEnt models (Ratnaparkhi et al, 1994) to the induction of transformation rules (Brill and Resnik, 1994), decision trees (Stetina and Nagao, 1997) and connectionist models (Sopena et al, 1998).	MaxEnt	Maximum entropy	0
Supervised training methods already applied to PP attachment range from stochastic maximum likelihood (Collins and Brooks, 1995) or maxi- MaxEnt models (Ratnaparkhi et al, 1994) to the induction of transformation rules (Brill and Resnik, 1994), decision trees (Stetina and Nagao, 1997) and connectionist models (Sopena et al, 1998).	MaxEnt	Maximum entropy classification	0
Adaptation of maxiMaxEnt capitalizer: Little data can help a lot.	MaxEnt	mum entropy	1
Adaptation of maxiMaxEnt capitalizer: Little data can help a lot.	MaxEnt	Maximum entropy	0
Adaptation of maxiMaxEnt capitalizer: Little data can help a lot.	MaxEnt	Maximum entropy classification	0
Ratnaparkhi et al, 1994) 228 extended the problem instances to quadruples by also considering the kernel noun of the PP, and used maxiMaxEnt models to estimate the preferences.	MaxEnt	mum entropy	1
Ratnaparkhi et al, 1994) 228 extended the problem instances to quadruples by also considering the kernel noun of the PP, and used maxiMaxEnt models to estimate the preferences.	MaxEnt	Maximum entropy	0
Ratnaparkhi et al, 1994) 228 extended the problem instances to quadruples by also considering the kernel noun of the PP, and used maxiMaxEnt models to estimate the preferences.	MaxEnt	Maximum entropy classification	0
As with maxiMaxEnt, gradient computation involves computing an expectation under q(z | x), which can be performed efficiently if the features f(x, z) factor in the same way as the model p?(x, z), and t	MaxEnt	mum entropy	1
As with maxiMaxEnt, gradient computation involves computing an expectation under q(z | x), which can be performed efficiently if the features f(x, z) factor in the same way as the model p?(x, z), and t	MaxEnt	Maximum entropy	0
As with maxiMaxEnt, gradient computation involves computing an expectation under q(z | x), which can be performed efficiently if the features f(x, z) factor in the same way as the model p?(x, z), and t	MaxEnt	Maximum entropy classification	0
MaxEnt based restoration of arabic diacritics.	MaxEnt	mum entropy	0
MaxEnt based restoration of arabic diacritics.	MaxEnt	Maximum entropy	1
MaxEnt based restoration of arabic diacritics.	MaxEnt	Maximum entropy classification	0
Learned MaxEnt parameters x ct First-person pronouns features ?	MaxEnt	mum entropy	0
Learned MaxEnt parameters x ct First-person pronouns features ?	MaxEnt	Maximum entropy	1
Learned MaxEnt parameters x ct First-person pronouns features ?	MaxEnt	Maximum entropy classification	0
MaxEnt model for punctuation annotation from speech.	MaxEnt	mum entropy	0
MaxEnt model for punctuation annotation from speech.	MaxEnt	Maximum entropy	1
MaxEnt model for punctuation annotation from speech.	MaxEnt	Maximum entropy classification	0
the same experiment applied to TB2 dataset (TB2- 278 Description Accuracy Data Extra Supervision Always noun 55.0 RRR Most likely for each P 72.19 RRR Most likely for each P 72.30 TB2 Most likely for each P 81.73 FN Average human, headwords (Ratnaparkhi et al, 1994) 88.2 RRR Average human, whole sentence (Ratnaparkhi et al, 1994) 93.2 RRR Maximum Likelihood-based (Hindle and Rooth, 1993) 79.7 AP MaxEnt, words (Ratnaparkhi et al, 1994) 77.7 RRR MaxEnt, words & classes (Ratnaparkhi et al, 1994) 81.6 RRR Decision trees (Ratnaparkhi et al, 1994) 77.7 RRR Transformation-Based Learning (Brill and Resnik, 1994) 81.8 WordNet Maximum-Likelihood based (Collins and Brooks, 1995) 84.5 RRR Maximum-Likelihood based (Collins and Brooks, 1995) 86.1 TB2 Decision trees & WSD (Stetina and	MaxEnt	mum entropy	0
the same experiment applied to TB2 dataset (TB2- 278 Description Accuracy Data Extra Supervision Always noun 55.0 RRR Most likely for each P 72.19 RRR Most likely for each P 72.30 TB2 Most likely for each P 81.73 FN Average human, headwords (Ratnaparkhi et al, 1994) 88.2 RRR Average human, whole sentence (Ratnaparkhi et al, 1994) 93.2 RRR Maximum Likelihood-based (Hindle and Rooth, 1993) 79.7 AP MaxEnt, words (Ratnaparkhi et al, 1994) 77.7 RRR MaxEnt, words & classes (Ratnaparkhi et al, 1994) 81.6 RRR Decision trees (Ratnaparkhi et al, 1994) 77.7 RRR Transformation-Based Learning (Brill and Resnik, 1994) 81.8 WordNet Maximum-Likelihood based (Collins and Brooks, 1995) 84.5 RRR Maximum-Likelihood based (Collins and Brooks, 1995) 86.1 TB2 Decision trees & WSD (Stetina and	MaxEnt	Maximum entropy	1
the same experiment applied to TB2 dataset (TB2- 278 Description Accuracy Data Extra Supervision Always noun 55.0 RRR Most likely for each P 72.19 RRR Most likely for each P 72.30 TB2 Most likely for each P 81.73 FN Average human, headwords (Ratnaparkhi et al, 1994) 88.2 RRR Average human, whole sentence (Ratnaparkhi et al, 1994) 93.2 RRR Maximum Likelihood-based (Hindle and Rooth, 1993) 79.7 AP MaxEnt, words (Ratnaparkhi et al, 1994) 77.7 RRR MaxEnt, words & classes (Ratnaparkhi et al, 1994) 81.6 RRR Decision trees (Ratnaparkhi et al, 1994) 77.7 RRR Transformation-Based Learning (Brill and Resnik, 1994) 81.8 WordNet Maximum-Likelihood based (Collins and Brooks, 1995) 84.5 RRR Maximum-Likelihood based (Collins and Brooks, 1995) 86.1 TB2 Decision trees & WSD (Stetina and	MaxEnt	Maximum entropy classification	0
ription Accuracy Data Extra Supervision Always noun 55.0 RRR Most likely for each P 72.19 RRR Most likely for each P 72.30 TB2 Most likely for each P 81.73 FN Average human, headwords (Ratnaparkhi et al, 1994) 88.2 RRR Average human, whole sentence (Ratnaparkhi et al, 1994) 93.2 RRR Maximum Likelihood-based (Hindle and Rooth, 1993) 79.7 AP MaxEnt, words (Ratnaparkhi et al, 1994) 77.7 RRR MaxEnt, words & classes (Ratnaparkhi et al, 1994) 81.6 RRR Decision trees (Ratnaparkhi et al, 1994) 77.7 RRR Transformation-Based Learning (Brill and Resnik, 1994) 81.8 WordNet Maximum-Likelihood based (Collins and Brooks, 1995) 84.5 RRR Maximum-Likelihood based (Collins and Brooks, 1995) 86.1 TB2 Decision trees & WSD (Stetina and Nagao, 1997) 88.1 RRR WordNet Memory-based Learning (Zavre	MaxEnt	mum entropy	0
ription Accuracy Data Extra Supervision Always noun 55.0 RRR Most likely for each P 72.19 RRR Most likely for each P 72.30 TB2 Most likely for each P 81.73 FN Average human, headwords (Ratnaparkhi et al, 1994) 88.2 RRR Average human, whole sentence (Ratnaparkhi et al, 1994) 93.2 RRR Maximum Likelihood-based (Hindle and Rooth, 1993) 79.7 AP MaxEnt, words (Ratnaparkhi et al, 1994) 77.7 RRR MaxEnt, words & classes (Ratnaparkhi et al, 1994) 81.6 RRR Decision trees (Ratnaparkhi et al, 1994) 77.7 RRR Transformation-Based Learning (Brill and Resnik, 1994) 81.8 WordNet Maximum-Likelihood based (Collins and Brooks, 1995) 84.5 RRR Maximum-Likelihood based (Collins and Brooks, 1995) 86.1 TB2 Decision trees & WSD (Stetina and Nagao, 1997) 88.1 RRR WordNet Memory-based Learning (Zavre	MaxEnt	Maximum entropy	1
ription Accuracy Data Extra Supervision Always noun 55.0 RRR Most likely for each P 72.19 RRR Most likely for each P 72.30 TB2 Most likely for each P 81.73 FN Average human, headwords (Ratnaparkhi et al, 1994) 88.2 RRR Average human, whole sentence (Ratnaparkhi et al, 1994) 93.2 RRR Maximum Likelihood-based (Hindle and Rooth, 1993) 79.7 AP MaxEnt, words (Ratnaparkhi et al, 1994) 77.7 RRR MaxEnt, words & classes (Ratnaparkhi et al, 1994) 81.6 RRR Decision trees (Ratnaparkhi et al, 1994) 77.7 RRR Transformation-Based Learning (Brill and Resnik, 1994) 81.8 WordNet Maximum-Likelihood based (Collins and Brooks, 1995) 84.5 RRR Maximum-Likelihood based (Collins and Brooks, 1995) 86.1 TB2 Decision trees & WSD (Stetina and Nagao, 1997) 88.1 RRR WordNet Memory-based Learning (Zavre	MaxEnt	Maximum entropy classification	0
(Ratnaparkhi et al, 1994) 81.6 RRR Decision trees (Ratnaparkhi et al, 1994) 77.7 RRR Transformation-Based Learning (Brill and Resnik, 1994) 81.8 WordNet Maximum-Likelihood based (Collins and Brooks, 1995) 84.5 RRR Maximum-Likelihood based (Collins and Brooks, 1995) 86.1 TB2 Decision trees & WSD (Stetina and Nagao, 1997) 88.1 RRR WordNet Memory-based Learning (Zavrel et al, 1997) 84.4 RRR LexSpace MaxEnt, unsupervised (Ratnaparkhi, 1998) 81.9 MaxEnt, supervised (Ratnaparkhi, 1998) 83.7 RRR Neural Nets (Alegre et al, 1999) 86.0 RRR WordNet Boosting (Abney et al, 1999) 84.4 RRR Semi-probabilistic (Pantel and Lin, 2000) 84.31 RRR MaxEnt, ensemble (McLauchlan, 2001) 85.5 RRR LSA SVM (Vanschoenwinkel and Manderick, 2003) 84.8 RRR Nearest-neighbor (Zhao and Lin, 2004) 8	MaxEnt	mum entropy	0
(Ratnaparkhi et al, 1994) 81.6 RRR Decision trees (Ratnaparkhi et al, 1994) 77.7 RRR Transformation-Based Learning (Brill and Resnik, 1994) 81.8 WordNet Maximum-Likelihood based (Collins and Brooks, 1995) 84.5 RRR Maximum-Likelihood based (Collins and Brooks, 1995) 86.1 TB2 Decision trees & WSD (Stetina and Nagao, 1997) 88.1 RRR WordNet Memory-based Learning (Zavrel et al, 1997) 84.4 RRR LexSpace MaxEnt, unsupervised (Ratnaparkhi, 1998) 81.9 MaxEnt, supervised (Ratnaparkhi, 1998) 83.7 RRR Neural Nets (Alegre et al, 1999) 86.0 RRR WordNet Boosting (Abney et al, 1999) 84.4 RRR Semi-probabilistic (Pantel and Lin, 2000) 84.31 RRR MaxEnt, ensemble (McLauchlan, 2001) 85.5 RRR LSA SVM (Vanschoenwinkel and Manderick, 2003) 84.8 RRR Nearest-neighbor (Zhao and Lin, 2004) 8	MaxEnt	Maximum entropy	1
(Ratnaparkhi et al, 1994) 81.6 RRR Decision trees (Ratnaparkhi et al, 1994) 77.7 RRR Transformation-Based Learning (Brill and Resnik, 1994) 81.8 WordNet Maximum-Likelihood based (Collins and Brooks, 1995) 84.5 RRR Maximum-Likelihood based (Collins and Brooks, 1995) 86.1 TB2 Decision trees & WSD (Stetina and Nagao, 1997) 88.1 RRR WordNet Memory-based Learning (Zavrel et al, 1997) 84.4 RRR LexSpace MaxEnt, unsupervised (Ratnaparkhi, 1998) 81.9 MaxEnt, supervised (Ratnaparkhi, 1998) 83.7 RRR Neural Nets (Alegre et al, 1999) 86.0 RRR WordNet Boosting (Abney et al, 1999) 84.4 RRR Semi-probabilistic (Pantel and Lin, 2000) 84.31 RRR MaxEnt, ensemble (McLauchlan, 2001) 85.5 RRR LSA SVM (Vanschoenwinkel and Manderick, 2003) 84.8 RRR Nearest-neighbor (Zhao and Lin, 2004) 8	MaxEnt	Maximum entropy classification	0
2.2 MaxEnt The second step of our paraphrasing system consists of a supervised maximum entropy classification ap- proach.	MaxEnt	mum entropy	0
2.2 MaxEnt The second step of our paraphrasing system consists of a supervised maximum entropy classification ap- proach.	MaxEnt	Maximum entropy	0
2.2 MaxEnt The second step of our paraphrasing system consists of a supervised maximum entropy classification ap- proach.	MaxEnt	Maximum entropy classification	1
Girju et al (2005) apply both classic (SVM and decision trees) and novel supervised models (seman- tic scattering and ISS), using WordNet, word sense disambiguation, and a set of linguistic features.	ISS	Input source sentence	0
Girju et al (2005) apply both classic (SVM and decision trees) and novel supervised models (seman- tic scattering and ISS), using WordNet, word sense disambiguation, and a set of linguistic features.	ISS	iterative semantic specialization	1
Girju et al (2005) apply both classic (SVM and decision trees) and novel supervised models (seman- tic scattering and ISS), using WordNet, word sense disambiguation, and a set of linguistic features.	ISS	Semantic Specialization	0
In 2003, Girju, Badulescu, and Moldovan (Girju, Badulescu, and Moldovan, 2003) detected the PART- WHOLE relations for some of the most frequent patterns (including the genitives) using the Itera- tive ISS, a learning model that searches for constraints in the WordNet noun hierar- chies.	ISS	Input source sentence	0
In 2003, Girju, Badulescu, and Moldovan (Girju, Badulescu, and Moldovan, 2003) detected the PART- WHOLE relations for some of the most frequent patterns (including the genitives) using the Itera- tive ISS, a learning model that searches for constraints in the WordNet noun hierar- chies.	ISS	iterative semantic specialization	0
In 2003, Girju, Badulescu, and Moldovan (Girju, Badulescu, and Moldovan, 2003) detected the PART- WHOLE relations for some of the most frequent patterns (including the genitives) using the Itera- tive ISS, a learning model that searches for constraints in the WordNet noun hierar- chies.	ISS	Semantic Specialization	1
c?2009 ACL and AFNLP Graphemic Approximation of Phonological Context  for English-Chinese Transliteration      Oi Yee Kwong  Department of Chinese, Translation and Linguistics  City University of Hong Kong  Tat Chee Avenue, Kowloon, Hong Kong  Olivia.Kwong@cityu.edu.hk         Abstract  Although DOM has  been shown to outperform phoneme-based  methods in English-to-Chinese (E2C) translit- eration, it is observed that phonological con- text plays an important role in resolving gra- phemic ambiguity.	DOM	direct orthographic mapping	1
c?2009 ACL and AFNLP Graphemic Approximation of Phonological Context  for English-Chinese Transliteration      Oi Yee Kwong  Department of Chinese, Translation and Linguistics  City University of Hong Kong  Tat Chee Avenue, Kowloon, Hong Kong  Olivia.Kwong@cityu.edu.hk         Abstract  Although DOM has  been shown to outperform phoneme-based  methods in English-to-Chinese (E2C) translit- eration, it is observed that phonological con- text plays an important role in resolving gra- phemic ambiguity.	DOM	Document Object Model	0
c?2009 ACL and AFNLP Graphemic Approximation of Phonological Context  for English-Chinese Transliteration      Oi Yee Kwong  Department of Chinese, Translation and Linguistics  City University of Hong Kong  Tat Chee Avenue, Kowloon, Hong Kong  Olivia.Kwong@cityu.edu.hk         Abstract  Although DOM has  been shown to outperform phoneme-based  methods in English-to-Chinese (E2C) translit- eration, it is observed that phonological con- text plays an important role in resolving gra- phemic ambiguity.	DOM	Direct orthographical mapping	0
c?2009 ACL and AFNLP Graphemic Approximation of Phonological Context  for English-Chinese Transliteration      Oi Yee Kwong  Department of Chinese, Translation and Linguistics  City University of Hong Kong  Tat Chee Avenue, Kowloon, Hong Kong  Olivia.Kwong@cityu.edu.hk         Abstract  Although DOM has  been shown to outperform phoneme-based  methods in English-to-Chinese (E2C) translit- eration, it is observed that phonological con- text plays an important role in resolving gra- phemic ambiguity.	DOM	domain-aware method	0
Although DOM has  been shown to be an effective method, it is nev- ertheless observed that phonological context sig- nificantly contributes to the resolution of some  graphemic ambiguity.	DOM	direct orthographic mapping	1
Although DOM has  been shown to be an effective method, it is nev- ertheless observed that phonological context sig- nificantly contributes to the resolution of some  graphemic ambiguity.	DOM	Document Object Model	0
Although DOM has  been shown to be an effective method, it is nev- ertheless observed that phonological context sig- nificantly contributes to the resolution of some  graphemic ambiguity.	DOM	Direct orthographical mapping	0
Although DOM has  been shown to be an effective method, it is nev- ertheless observed that phonological context sig- nificantly contributes to the resolution of some  graphemic ambiguity.	DOM	domain-aware method	0
Although the DOM approach advo- cates a direct transfer of grapheme at run-time, we still need to establish the grapheme correspon- dence at the model training stage, when phoneme level alignment can help.	DOM	direct orthographic mapping	1
Although the DOM approach advo- cates a direct transfer of grapheme at run-time, we still need to establish the grapheme correspon- dence at the model training stage, when phoneme level alignment can help.	DOM	Document Object Model	0
Although the DOM approach advo- cates a direct transfer of grapheme at run-time, we still need to establish the grapheme correspon- dence at the model training stage, when phoneme level alignment can help.	DOM	Direct orthographical mapping	0
Although the DOM approach advo- cates a direct transfer of grapheme at run-time, we still need to establish the grapheme correspon- dence at the model training stage, when phoneme level alignment can help.	DOM	domain-aware method	0
The core of our systems is based on Li et al?s  (2004) Joint Source-Channel Model under the  DOM framework, which  skips the middle phonemic representation in  conventional phoneme-based methods and mod- els the segmentation and alignment preferences  by means of contextual n-grams of the translit- eration segment pairs (or token pairs in their ter- minology).	DOM	direct orthographic mapping	1
The core of our systems is based on Li et al?s  (2004) Joint Source-Channel Model under the  DOM framework, which  skips the middle phonemic representation in  conventional phoneme-based methods and mod- els the segmentation and alignment preferences  by means of contextual n-grams of the translit- eration segment pairs (or token pairs in their ter- minology).	DOM	Document Object Model	0
The core of our systems is based on Li et al?s  (2004) Joint Source-Channel Model under the  DOM framework, which  skips the middle phonemic representation in  conventional phoneme-based methods and mod- els the segmentation and alignment preferences  by means of contextual n-grams of the translit- eration segment pairs (or token pairs in their ter- minology).	DOM	Direct orthographical mapping	0
The core of our systems is based on Li et al?s  (2004) Joint Source-Channel Model under the  DOM framework, which  skips the middle phonemic representation in  conventional phoneme-based methods and mod- els the segmentation and alignment preferences  by means of contextual n-grams of the translit- eration segment pairs (or token pairs in their ter- minology).	DOM	domain-aware method	0
Grapheme- based method (Li et al, 2004) treats translitera- tion as a DOM and only uses orthography-related features while phoneme- based method (Knight and Graehl, 1998) makes use of phonetic correspondence to generate the transliteration.	DOM	direct orthographic mapping	1
Grapheme- based method (Li et al, 2004) treats translitera- tion as a DOM and only uses orthography-related features while phoneme- based method (Knight and Graehl, 1998) makes use of phonetic correspondence to generate the transliteration.	DOM	Document Object Model	0
Grapheme- based method (Li et al, 2004) treats translitera- tion as a DOM and only uses orthography-related features while phoneme- based method (Knight and Graehl, 1998) makes use of phonetic correspondence to generate the transliteration.	DOM	Direct orthographical mapping	0
Grapheme- based method (Li et al, 2004) treats translitera- tion as a DOM and only uses orthography-related features while phoneme- based method (Knight and Graehl, 1998) makes use of phonetic correspondence to generate the transliteration.	DOM	domain-aware method	0
5 DOM.	DOM	direct orthographic mapping	0
5 DOM.	DOM	Document Object Model	1
5 DOM.	DOM	Direct orthographical mapping	0
5 DOM.	DOM	domain-aware method	0
Gupta et al (2003) parsed HTML documents  to a DOM tree and to extract the  main content of a web page by removing the link  lists and empty tables.	DOM	direct orthographic mapping	0
Gupta et al (2003) parsed HTML documents  to a DOM tree and to extract the  main content of a web page by removing the link  lists and empty tables.	DOM	Document Object Model	1
Gupta et al (2003) parsed HTML documents  to a DOM tree and to extract the  main content of a web page by removing the link  lists and empty tables.	DOM	Direct orthographical mapping	0
Gupta et al (2003) parsed HTML documents  to a DOM tree and to extract the  main content of a web page by removing the link  lists and empty tables.	DOM	domain-aware method	0
If you intend to automatically process only a few selected fora, you will probably use XPath queries on the HTML DOM.	DOM	direct orthographic mapping	0
If you intend to automatically process only a few selected fora, you will probably use XPath queries on the HTML DOM.	DOM	Document Object Model	1
If you intend to automatically process only a few selected fora, you will probably use XPath queries on the HTML DOM.	DOM	Direct orthographical mapping	0
If you intend to automatically process only a few selected fora, you will probably use XPath queries on the HTML DOM.	DOM	domain-aware method	0
DOM.	DOM	direct orthographic mapping	0
DOM.	DOM	Document Object Model	1
DOM.	DOM	Direct orthographical mapping	0
DOM.	DOM	domain-aware method	0
The reason for working with this collection is the fact that we also have a manual translation of the SC docu- ments from English into one of the target languages used in the experiments (Romanian), which enables comparative evaluations of different scenarios (see Section 4).	SC	SemCor	1
The reason for working with this collection is the fact that we also have a manual translation of the SC docu- ments from English into one of the target languages used in the experiments (Romanian), which enables comparative evaluations of different scenarios (see Section 4).	SC	Section	0
The reason for working with this collection is the fact that we also have a manual translation of the SC docu- ments from English into one of the target languages used in the experiments (Romanian), which enables comparative evaluations of different scenarios (see Section 4).	SC	second	0
The reason for working with this collection is the fact that we also have a manual translation of the SC docu- ments from English into one of the target languages used in the experiments (Romanian), which enables comparative evaluations of different scenarios (see Section 4).	SC	Simple Classification	0
The reason for working with this collection is the fact that we also have a manual translation of the SC docu- ments from English into one of the target languages used in the experiments (Romanian), which enables comparative evaluations of different scenarios (see Section 4).	SC	sur face coerc ion	0
The reason for working with this collection is the fact that we also have a manual translation of the SC docu- ments from English into one of the target languages used in the experiments (Romanian), which enables comparative evaluations of different scenarios (see Section 4).	SC	score	0
The reason for working with this collection is the fact that we also have a manual translation of the SC docu- ments from English into one of the target languages used in the experiments (Romanian), which enables comparative evaluations of different scenarios (see Section 4).	SC	System Combination	0
The reason for working with this collection is the fact that we also have a manual translation of the SC docu- ments from English into one of the target languages used in the experiments (Romanian), which enables comparative evaluations of different scenarios (see Section 4).	SC	String Consistency	0
The reason for working with this collection is the fact that we also have a manual translation of the SC docu- ments from English into one of the target languages used in the experiments (Romanian), which enables comparative evaluations of different scenarios (see Section 4).	SC	simplified Chinese	0
The reason for working with this collection is the fact that we also have a manual translation of the SC docu- ments from English into one of the target languages used in the experiments (Romanian), which enables comparative evaluations of different scenarios (see Section 4).	SC	Score	0
The automatic translation of the MPQA and of the SC corpus was performed using Language Weaver,1 a commercial statistical machine transla- tion software.	SC	SemCor	1
The automatic translation of the MPQA and of the SC corpus was performed using Language Weaver,1 a commercial statistical machine transla- tion software.	SC	Section	0
The automatic translation of the MPQA and of the SC corpus was performed using Language Weaver,1 a commercial statistical machine transla- tion software.	SC	second	0
The automatic translation of the MPQA and of the SC corpus was performed using Language Weaver,1 a commercial statistical machine transla- tion software.	SC	Simple Classification	0
The automatic translation of the MPQA and of the SC corpus was performed using Language Weaver,1 a commercial statistical machine transla- tion software.	SC	sur face coerc ion	0
The automatic translation of the MPQA and of the SC corpus was performed using Language Weaver,1 a commercial statistical machine transla- tion software.	SC	score	0
The automatic translation of the MPQA and of the SC corpus was performed using Language Weaver,1 a commercial statistical machine transla- tion software.	SC	System Combination	0
The automatic translation of the MPQA and of the SC corpus was performed using Language Weaver,1 a commercial statistical machine transla- tion software.	SC	String Consistency	0
The automatic translation of the MPQA and of the SC corpus was performed using Language Weaver,1 a commercial statistical machine transla- tion software.	SC	simplified Chinese	0
The automatic translation of the MPQA and of the SC corpus was performed using Language Weaver,1 a commercial statistical machine transla- tion software.	SC	Score	0
P R F high-precision 86.7 32.6 47.4 high-coverage 79.4 70.6 74.7 Table 1: Precision (P), Recall (R) and F-measure (F) for the two OpinionFinder classifiers, as measured on the MPQA corpus As a raw corpus, we use a subset of the SC corpus (Miller et al, 1993), consisting of 107 docu- ments with roughly 11,000 sentences.	SC	SemCor	1
P R F high-precision 86.7 32.6 47.4 high-coverage 79.4 70.6 74.7 Table 1: Precision (P), Recall (R) and F-measure (F) for the two OpinionFinder classifiers, as measured on the MPQA corpus As a raw corpus, we use a subset of the SC corpus (Miller et al, 1993), consisting of 107 docu- ments with roughly 11,000 sentences.	SC	Section	0
P R F high-precision 86.7 32.6 47.4 high-coverage 79.4 70.6 74.7 Table 1: Precision (P), Recall (R) and F-measure (F) for the two OpinionFinder classifiers, as measured on the MPQA corpus As a raw corpus, we use a subset of the SC corpus (Miller et al, 1993), consisting of 107 docu- ments with roughly 11,000 sentences.	SC	second	0
P R F high-precision 86.7 32.6 47.4 high-coverage 79.4 70.6 74.7 Table 1: Precision (P), Recall (R) and F-measure (F) for the two OpinionFinder classifiers, as measured on the MPQA corpus As a raw corpus, we use a subset of the SC corpus (Miller et al, 1993), consisting of 107 docu- ments with roughly 11,000 sentences.	SC	Simple Classification	0
P R F high-precision 86.7 32.6 47.4 high-coverage 79.4 70.6 74.7 Table 1: Precision (P), Recall (R) and F-measure (F) for the two OpinionFinder classifiers, as measured on the MPQA corpus As a raw corpus, we use a subset of the SC corpus (Miller et al, 1993), consisting of 107 docu- ments with roughly 11,000 sentences.	SC	sur face coerc ion	0
P R F high-precision 86.7 32.6 47.4 high-coverage 79.4 70.6 74.7 Table 1: Precision (P), Recall (R) and F-measure (F) for the two OpinionFinder classifiers, as measured on the MPQA corpus As a raw corpus, we use a subset of the SC corpus (Miller et al, 1993), consisting of 107 docu- ments with roughly 11,000 sentences.	SC	score	0
P R F high-precision 86.7 32.6 47.4 high-coverage 79.4 70.6 74.7 Table 1: Precision (P), Recall (R) and F-measure (F) for the two OpinionFinder classifiers, as measured on the MPQA corpus As a raw corpus, we use a subset of the SC corpus (Miller et al, 1993), consisting of 107 docu- ments with roughly 11,000 sentences.	SC	System Combination	0
P R F high-precision 86.7 32.6 47.4 high-coverage 79.4 70.6 74.7 Table 1: Precision (P), Recall (R) and F-measure (F) for the two OpinionFinder classifiers, as measured on the MPQA corpus As a raw corpus, we use a subset of the SC corpus (Miller et al, 1993), consisting of 107 docu- ments with roughly 11,000 sentences.	SC	String Consistency	0
P R F high-precision 86.7 32.6 47.4 high-coverage 79.4 70.6 74.7 Table 1: Precision (P), Recall (R) and F-measure (F) for the two OpinionFinder classifiers, as measured on the MPQA corpus As a raw corpus, we use a subset of the SC corpus (Miller et al, 1993), consisting of 107 docu- ments with roughly 11,000 sentences.	SC	simplified Chinese	0
P R F high-precision 86.7 32.6 47.4 high-coverage 79.4 70.6 74.7 Table 1: Precision (P), Recall (R) and F-measure (F) for the two OpinionFinder classifiers, as measured on the MPQA corpus As a raw corpus, we use a subset of the SC corpus (Miller et al, 1993), consisting of 107 docu- ments with roughly 11,000 sentences.	SC	Score	0
In (Mihalcea et al, 2007), we used the manual translation of the SC corpus into Romanian to form an English-Romanian par- allel data set.	SC	SemCor	1
In (Mihalcea et al, 2007), we used the manual translation of the SC corpus into Romanian to form an English-Romanian par- allel data set.	SC	Section	0
In (Mihalcea et al, 2007), we used the manual translation of the SC corpus into Romanian to form an English-Romanian par- allel data set.	SC	second	0
In (Mihalcea et al, 2007), we used the manual translation of the SC corpus into Romanian to form an English-Romanian par- allel data set.	SC	Simple Classification	0
In (Mihalcea et al, 2007), we used the manual translation of the SC corpus into Romanian to form an English-Romanian par- allel data set.	SC	sur face coerc ion	0
In (Mihalcea et al, 2007), we used the manual translation of the SC corpus into Romanian to form an English-Romanian par- allel data set.	SC	score	0
In (Mihalcea et al, 2007), we used the manual translation of the SC corpus into Romanian to form an English-Romanian par- allel data set.	SC	System Combination	0
In (Mihalcea et al, 2007), we used the manual translation of the SC corpus into Romanian to form an English-Romanian par- allel data set.	SC	String Consistency	0
In (Mihalcea et al, 2007), we used the manual translation of the SC corpus into Romanian to form an English-Romanian par- allel data set.	SC	simplified Chinese	0
In (Mihalcea et al, 2007), we used the manual translation of the SC corpus into Romanian to form an English-Romanian par- allel data set.	SC	Score	0
These sentences represent the manual translation into Romanian of a small subset of the SC corpus, which was removed from the training corpora used in experi- ments two and three.	SC	SemCor	1
These sentences represent the manual translation into Romanian of a small subset of the SC corpus, which was removed from the training corpora used in experi- ments two and three.	SC	Section	0
These sentences represent the manual translation into Romanian of a small subset of the SC corpus, which was removed from the training corpora used in experi- ments two and three.	SC	second	0
These sentences represent the manual translation into Romanian of a small subset of the SC corpus, which was removed from the training corpora used in experi- ments two and three.	SC	Simple Classification	0
These sentences represent the manual translation into Romanian of a small subset of the SC corpus, which was removed from the training corpora used in experi- ments two and three.	SC	sur face coerc ion	0
These sentences represent the manual translation into Romanian of a small subset of the SC corpus, which was removed from the training corpora used in experi- ments two and three.	SC	score	0
These sentences represent the manual translation into Romanian of a small subset of the SC corpus, which was removed from the training corpora used in experi- ments two and three.	SC	System Combination	0
These sentences represent the manual translation into Romanian of a small subset of the SC corpus, which was removed from the training corpora used in experi- ments two and three.	SC	String Consistency	0
These sentences represent the manual translation into Romanian of a small subset of the SC corpus, which was removed from the training corpora used in experi- ments two and three.	SC	simplified Chinese	0
These sentences represent the manual translation into Romanian of a small subset of the SC corpus, which was removed from the training corpora used in experi- ments two and three.	SC	Score	0
Figure 3: Experiment three: machine translation of raw training data from target language into source language As before, we use the high-coverage classifier available in OpinionFinder, and the SC corpus.	SC	SemCor	1
Figure 3: Experiment three: machine translation of raw training data from target language into source language As before, we use the high-coverage classifier available in OpinionFinder, and the SC corpus.	SC	Section	0
Figure 3: Experiment three: machine translation of raw training data from target language into source language As before, we use the high-coverage classifier available in OpinionFinder, and the SC corpus.	SC	second	0
Figure 3: Experiment three: machine translation of raw training data from target language into source language As before, we use the high-coverage classifier available in OpinionFinder, and the SC corpus.	SC	Simple Classification	0
Figure 3: Experiment three: machine translation of raw training data from target language into source language As before, we use the high-coverage classifier available in OpinionFinder, and the SC corpus.	SC	sur face coerc ion	0
Figure 3: Experiment three: machine translation of raw training data from target language into source language As before, we use the high-coverage classifier available in OpinionFinder, and the SC corpus.	SC	score	0
Figure 3: Experiment three: machine translation of raw training data from target language into source language As before, we use the high-coverage classifier available in OpinionFinder, and the SC corpus.	SC	System Combination	0
Figure 3: Experiment three: machine translation of raw training data from target language into source language As before, we use the high-coverage classifier available in OpinionFinder, and the SC corpus.	SC	String Consistency	0
Figure 3: Experiment three: machine translation of raw training data from target language into source language As before, we use the high-coverage classifier available in OpinionFinder, and the SC corpus.	SC	simplified Chinese	0
Figure 3: Experiment three: machine translation of raw training data from target language into source language As before, we use the high-coverage classifier available in OpinionFinder, and the SC corpus.	SC	Score	0
nFinder classifiers, as measured on the MPQA corpus As a raw corpus, we use a subset of the SC corpus (Miller et al, 1993), consisting of 107 docu- ments with roughly 11,000 sentences.	SC	SemCor	1
nFinder classifiers, as measured on the MPQA corpus As a raw corpus, we use a subset of the SC corpus (Miller et al, 1993), consisting of 107 docu- ments with roughly 11,000 sentences.	SC	Section	0
nFinder classifiers, as measured on the MPQA corpus As a raw corpus, we use a subset of the SC corpus (Miller et al, 1993), consisting of 107 docu- ments with roughly 11,000 sentences.	SC	second	0
nFinder classifiers, as measured on the MPQA corpus As a raw corpus, we use a subset of the SC corpus (Miller et al, 1993), consisting of 107 docu- ments with roughly 11,000 sentences.	SC	Simple Classification	0
nFinder classifiers, as measured on the MPQA corpus As a raw corpus, we use a subset of the SC corpus (Miller et al, 1993), consisting of 107 docu- ments with roughly 11,000 sentences.	SC	sur face coerc ion	0
nFinder classifiers, as measured on the MPQA corpus As a raw corpus, we use a subset of the SC corpus (Miller et al, 1993), consisting of 107 docu- ments with roughly 11,000 sentences.	SC	score	0
nFinder classifiers, as measured on the MPQA corpus As a raw corpus, we use a subset of the SC corpus (Miller et al, 1993), consisting of 107 docu- ments with roughly 11,000 sentences.	SC	System Combination	0
nFinder classifiers, as measured on the MPQA corpus As a raw corpus, we use a subset of the SC corpus (Miller et al, 1993), consisting of 107 docu- ments with roughly 11,000 sentences.	SC	String Consistency	0
nFinder classifiers, as measured on the MPQA corpus As a raw corpus, we use a subset of the SC corpus (Miller et al, 1993), consisting of 107 docu- ments with roughly 11,000 sentences.	SC	simplified Chinese	0
nFinder classifiers, as measured on the MPQA corpus As a raw corpus, we use a subset of the SC corpus (Miller et al, 1993), consisting of 107 docu- ments with roughly 11,000 sentences.	SC	Score	0
We compare our results with those obtained by a previously proposed method that was based on the manual translation of the SC subjectivity- annotated corpus.	SC	SemCor	1
We compare our results with those obtained by a previously proposed method that was based on the manual translation of the SC subjectivity- annotated corpus.	SC	Section	0
We compare our results with those obtained by a previously proposed method that was based on the manual translation of the SC subjectivity- annotated corpus.	SC	second	0
We compare our results with those obtained by a previously proposed method that was based on the manual translation of the SC subjectivity- annotated corpus.	SC	Simple Classification	0
We compare our results with those obtained by a previously proposed method that was based on the manual translation of the SC subjectivity- annotated corpus.	SC	sur face coerc ion	0
We compare our results with those obtained by a previously proposed method that was based on the manual translation of the SC subjectivity- annotated corpus.	SC	score	0
We compare our results with those obtained by a previously proposed method that was based on the manual translation of the SC subjectivity- annotated corpus.	SC	System Combination	0
We compare our results with those obtained by a previously proposed method that was based on the manual translation of the SC subjectivity- annotated corpus.	SC	String Consistency	0
We compare our results with those obtained by a previously proposed method that was based on the manual translation of the SC subjectivity- annotated corpus.	SC	simplified Chinese	0
We compare our results with those obtained by a previously proposed method that was based on the manual translation of the SC subjectivity- annotated corpus.	SC	Score	0
The reason for working with this collection is the fact that we also have a manual translation of the SC docu- ments from English into one of the target languages used in the experiments (Ro	SC	SemCor	1
The reason for working with this collection is the fact that we also have a manual translation of the SC docu- ments from English into one of the target languages used in the experiments (Ro	SC	Section	0
The reason for working with this collection is the fact that we also have a manual translation of the SC docu- ments from English into one of the target languages used in the experiments (Ro	SC	second	0
The reason for working with this collection is the fact that we also have a manual translation of the SC docu- ments from English into one of the target languages used in the experiments (Ro	SC	Simple Classification	0
The reason for working with this collection is the fact that we also have a manual translation of the SC docu- ments from English into one of the target languages used in the experiments (Ro	SC	sur face coerc ion	0
The reason for working with this collection is the fact that we also have a manual translation of the SC docu- ments from English into one of the target languages used in the experiments (Ro	SC	score	0
The reason for working with this collection is the fact that we also have a manual translation of the SC docu- ments from English into one of the target languages used in the experiments (Ro	SC	System Combination	0
The reason for working with this collection is the fact that we also have a manual translation of the SC docu- ments from English into one of the target languages used in the experiments (Ro	SC	String Consistency	0
The reason for working with this collection is the fact that we also have a manual translation of the SC docu- ments from English into one of the target languages used in the experiments (Ro	SC	simplified Chinese	0
The reason for working with this collection is the fact that we also have a manual translation of the SC docu- ments from English into one of the target languages used in the experiments (Ro	SC	Score	0
The previous n para- graphs1 were searched and the pronoun under in- vestigation was mapped to the closest (in terms of textual proximity) story character that had the same gender as the pronoun (see SC 3.4.1 re- garding gender estimation).	SC	SemCor	0
The previous n para- graphs1 were searched and the pronoun under in- vestigation was mapped to the closest (in terms of textual proximity) story character that had the same gender as the pronoun (see SC 3.4.1 re- garding gender estimation).	SC	Section	1
The previous n para- graphs1 were searched and the pronoun under in- vestigation was mapped to the closest (in terms of textual proximity) story character that had the same gender as the pronoun (see SC 3.4.1 re- garding gender estimation).	SC	second	0
The previous n para- graphs1 were searched and the pronoun under in- vestigation was mapped to the closest (in terms of textual proximity) story character that had the same gender as the pronoun (see SC 3.4.1 re- garding gender estimation).	SC	Simple Classification	0
The previous n para- graphs1 were searched and the pronoun under in- vestigation was mapped to the closest (in terms of textual proximity) story character that had the same gender as the pronoun (see SC 3.4.1 re- garding gender estimation).	SC	sur face coerc ion	0
The previous n para- graphs1 were searched and the pronoun under in- vestigation was mapped to the closest (in terms of textual proximity) story character that had the same gender as the pronoun (see SC 3.4.1 re- garding gender estimation).	SC	score	0
The previous n para- graphs1 were searched and the pronoun under in- vestigation was mapped to the closest (in terms of textual proximity) story character that had the same gender as the pronoun (see SC 3.4.1 re- garding gender estimation).	SC	System Combination	0
The previous n para- graphs1 were searched and the pronoun under in- vestigation was mapped to the closest (in terms of textual proximity) story character that had the same gender as the pronoun (see SC 3.4.1 re- garding gender estimation).	SC	String Consistency	0
The previous n para- graphs1 were searched and the pronoun under in- vestigation was mapped to the closest (in terms of textual proximity) story character that had the same gender as the pronoun (see SC 3.4.1 re- garding gender estimation).	SC	simplified Chinese	0
The previous n para- graphs1 were searched and the pronoun under in- vestigation was mapped to the closest (in terms of textual proximity) story character that had the same gender as the pronoun (see SC 3.4.1 re- garding gender estimation).	SC	Score	0
The paper is structured as follows: SC 2 in- troduces the classification we are aiming at and the hypotheses that led to the experiments; SC 3 fo- cuses on the methodology used to produce the clas- sification; in SC 4 we discuss the results ob- tained so far; finally, SC 5 contains some con- clusions and proposals for further work.	SC	SemCor	0
The paper is structured as follows: SC 2 in- troduces the classification we are aiming at and the hypotheses that led to the experiments; SC 3 fo- cuses on the methodology used to produce the clas- sification; in SC 4 we discuss the results ob- tained so far; finally, SC 5 contains some con- clusions and proposals for further work.	SC	Section	1
The paper is structured as follows: SC 2 in- troduces the classification we are aiming at and the hypotheses that led to the experiments; SC 3 fo- cuses on the methodology used to produce the clas- sification; in SC 4 we discuss the results ob- tained so far; finally, SC 5 contains some con- clusions and proposals for further work.	SC	second	0
The paper is structured as follows: SC 2 in- troduces the classification we are aiming at and the hypotheses that led to the experiments; SC 3 fo- cuses on the methodology used to produce the clas- sification; in SC 4 we discuss the results ob- tained so far; finally, SC 5 contains some con- clusions and proposals for further work.	SC	Simple Classification	0
The paper is structured as follows: SC 2 in- troduces the classification we are aiming at and the hypotheses that led to the experiments; SC 3 fo- cuses on the methodology used to produce the clas- sification; in SC 4 we discuss the results ob- tained so far; finally, SC 5 contains some con- clusions and proposals for further work.	SC	sur face coerc ion	0
The paper is structured as follows: SC 2 in- troduces the classification we are aiming at and the hypotheses that led to the experiments; SC 3 fo- cuses on the methodology used to produce the clas- sification; in SC 4 we discuss the results ob- tained so far; finally, SC 5 contains some con- clusions and proposals for further work.	SC	score	0
The paper is structured as follows: SC 2 in- troduces the classification we are aiming at and the hypotheses that led to the experiments; SC 3 fo- cuses on the methodology used to produce the clas- sification; in SC 4 we discuss the results ob- tained so far; finally, SC 5 contains some con- clusions and proposals for further work.	SC	System Combination	0
The paper is structured as follows: SC 2 in- troduces the classification we are aiming at and the hypotheses that led to the experiments; SC 3 fo- cuses on the methodology used to produce the clas- sification; in SC 4 we discuss the results ob- tained so far; finally, SC 5 contains some con- clusions and proposals for further work.	SC	String Consistency	0
The paper is structured as follows: SC 2 in- troduces the classification we are aiming at and the hypotheses that led to the experiments; SC 3 fo- cuses on the methodology used to produce the clas- sification; in SC 4 we discuss the results ob- tained so far; finally, SC 5 contains some con- clusions and proposals for further work.	SC	simplified Chinese	0
The paper is structured as follows: SC 2 in- troduces the classification we are aiming at and the hypotheses that led to the experiments; SC 3 fo- cuses on the methodology used to produce the clas- sification; in SC 4 we discuss the results ob- tained so far; finally, SC 5 contains some con- clusions and proposals for further work.	SC	Score	0
This is applied to char- acters (rather than only speakers) because the gen- der information is exploited during the attribution of quotes (see SC 3.3).	SC	SemCor	0
This is applied to char- acters (rather than only speakers) because the gen- der information is exploited during the attribution of quotes (see SC 3.3).	SC	Section	1
This is applied to char- acters (rather than only speakers) because the gen- der information is exploited during the attribution of quotes (see SC 3.3).	SC	second	0
This is applied to char- acters (rather than only speakers) because the gen- der information is exploited during the attribution of quotes (see SC 3.3).	SC	Simple Classification	0
This is applied to char- acters (rather than only speakers) because the gen- der information is exploited during the attribution of quotes (see SC 3.3).	SC	sur face coerc ion	0
This is applied to char- acters (rather than only speakers) because the gen- der information is exploited during the attribution of quotes (see SC 3.3).	SC	score	0
This is applied to char- acters (rather than only speakers) because the gen- der information is exploited during the attribution of quotes (see SC 3.3).	SC	System Combination	0
This is applied to char- acters (rather than only speakers) because the gen- der information is exploited during the attribution of quotes (see SC 3.3).	SC	String Consistency	0
This is applied to char- acters (rather than only speakers) because the gen- der information is exploited during the attribution of quotes (see SC 3.3).	SC	simplified Chinese	0
This is applied to char- acters (rather than only speakers) because the gen- der information is exploited during the attribution of quotes (see SC 3.3).	SC	Score	0
Thus our SC method of finding the  pronoun/noun co-occurrences is simply to parse  the text and then assume that the noun-phrase  at Hobbs distance one is the antecedent.	SC	SemCor	0
Thus our SC method of finding the  pronoun/noun co-occurrences is simply to parse  the text and then assume that the noun-phrase  at Hobbs distance one is the antecedent.	SC	Section	0
Thus our SC method of finding the  pronoun/noun co-occurrences is simply to parse  the text and then assume that the noun-phrase  at Hobbs distance one is the antecedent.	SC	second	1
Thus our SC method of finding the  pronoun/noun co-occurrences is simply to parse  the text and then assume that the noun-phrase  at Hobbs distance one is the antecedent.	SC	Simple Classification	0
Thus our SC method of finding the  pronoun/noun co-occurrences is simply to parse  the text and then assume that the noun-phrase  at Hobbs distance one is the antecedent.	SC	sur face coerc ion	0
Thus our SC method of finding the  pronoun/noun co-occurrences is simply to parse  the text and then assume that the noun-phrase  at Hobbs distance one is the antecedent.	SC	score	0
Thus our SC method of finding the  pronoun/noun co-occurrences is simply to parse  the text and then assume that the noun-phrase  at Hobbs distance one is the antecedent.	SC	System Combination	0
Thus our SC method of finding the  pronoun/noun co-occurrences is simply to parse  the text and then assume that the noun-phrase  at Hobbs distance one is the antecedent.	SC	String Consistency	0
Thus our SC method of finding the  pronoun/noun co-occurrences is simply to parse  the text and then assume that the noun-phrase  at Hobbs distance one is the antecedent.	SC	simplified Chinese	0
Thus our SC method of finding the  pronoun/noun co-occurrences is simply to parse  the text and then assume that the noun-phrase  at Hobbs distance one is the antecedent.	SC	Score	0
The SC problem is resolving utterance chains with implicit speakers.	SC	SemCor	0
The SC problem is resolving utterance chains with implicit speakers.	SC	Section	0
The SC problem is resolving utterance chains with implicit speakers.	SC	second	1
The SC problem is resolving utterance chains with implicit speakers.	SC	Simple Classification	0
The SC problem is resolving utterance chains with implicit speakers.	SC	sur face coerc ion	0
The SC problem is resolving utterance chains with implicit speakers.	SC	score	0
The SC problem is resolving utterance chains with implicit speakers.	SC	System Combination	0
The SC problem is resolving utterance chains with implicit speakers.	SC	String Consistency	0
The SC problem is resolving utterance chains with implicit speakers.	SC	simplified Chinese	0
The SC problem is resolving utterance chains with implicit speakers.	SC	Score	0
The SC half of the paper describes a  method for using (portions of) t~e aforemen-  tioned program to learn automatically the typi-  cal gender of English words, information that is  itself used in the pronoun resolution program.	SC	SemCor	0
The SC half of the paper describes a  method for using (portions of) t~e aforemen-  tioned program to learn automatically the typi-  cal gender of English words, information that is  itself used in the pronoun resolution program.	SC	Section	0
The SC half of the paper describes a  method for using (portions of) t~e aforemen-  tioned program to learn automatically the typi-  cal gender of English words, information that is  itself used in the pronoun resolution program.	SC	second	1
The SC half of the paper describes a  method for using (portions of) t~e aforemen-  tioned program to learn automatically the typi-  cal gender of English words, information that is  itself used in the pronoun resolution program.	SC	Simple Classification	0
The SC half of the paper describes a  method for using (portions of) t~e aforemen-  tioned program to learn automatically the typi-  cal gender of English words, information that is  itself used in the pronoun resolution program.	SC	sur face coerc ion	0
The SC half of the paper describes a  method for using (portions of) t~e aforemen-  tioned program to learn automatically the typi-  cal gender of English words, information that is  itself used in the pronoun resolution program.	SC	score	0
The SC half of the paper describes a  method for using (portions of) t~e aforemen-  tioned program to learn automatically the typi-  cal gender of English words, information that is  itself used in the pronoun resolution program.	SC	System Combination	0
The SC half of the paper describes a  method for using (portions of) t~e aforemen-  tioned program to learn automatically the typi-  cal gender of English words, information that is  itself used in the pronoun resolution program.	SC	String Consistency	0
The SC half of the paper describes a  method for using (portions of) t~e aforemen-  tioned program to learn automatically the typi-  cal gender of English words, information that is  itself used in the pronoun resolution program.	SC	simplified Chinese	0
The SC half of the paper describes a  method for using (portions of) t~e aforemen-  tioned program to learn automatically the typi-  cal gender of English words, information that is  itself used in the pronoun resolution program.	SC	Score	0
3.2 Identification of Story Characters The SC step is identifying candidate charac- ters (i.e., entities) that appear in the stories under analysis.	SC	SemCor	0
3.2 Identification of Story Characters The SC step is identifying candidate charac- ters (i.e., entities) that appear in the stories under analysis.	SC	Section	0
3.2 Identification of Story Characters The SC step is identifying candidate charac- ters (i.e., entities) that appear in the stories under analysis.	SC	second	1
3.2 Identification of Story Characters The SC step is identifying candidate charac- ters (i.e., entities) that appear in the stories under analysis.	SC	Simple Classification	0
3.2 Identification of Story Characters The SC step is identifying candidate charac- ters (i.e., entities) that appear in the stories under analysis.	SC	sur face coerc ion	0
3.2 Identification of Story Characters The SC step is identifying candidate charac- ters (i.e., entities) that appear in the stories under analysis.	SC	score	0
3.2 Identification of Story Characters The SC step is identifying candidate charac- ters (i.e., entities) that appear in the stories under analysis.	SC	System Combination	0
3.2 Identification of Story Characters The SC step is identifying candidate charac- ters (i.e., entities) that appear in the stories under analysis.	SC	String Consistency	0
3.2 Identification of Story Characters The SC step is identifying candidate charac- ters (i.e., entities) that appear in the stories under analysis.	SC	simplified Chinese	0
3.2 Identification of Story Characters The SC step is identifying candidate charac- ters (i.e., entities) that appear in the stories under analysis.	SC	Score	0
Even if Kim's gender  was unknown before seeing the first sentence,  after the SC sentence, it is known.	SC	SemCor	0
Even if Kim's gender  was unknown before seeing the first sentence,  after the SC sentence, it is known.	SC	Section	0
Even if Kim's gender  was unknown before seeing the first sentence,  after the SC sentence, it is known.	SC	second	1
Even if Kim's gender  was unknown before seeing the first sentence,  after the SC sentence, it is known.	SC	Simple Classification	0
Even if Kim's gender  was unknown before seeing the first sentence,  after the SC sentence, it is known.	SC	sur face coerc ion	0
Even if Kim's gender  was unknown before seeing the first sentence,  after the SC sentence, it is known.	SC	score	0
Even if Kim's gender  was unknown before seeing the first sentence,  after the SC sentence, it is known.	SC	System Combination	0
Even if Kim's gender  was unknown before seeing the first sentence,  after the SC sentence, it is known.	SC	String Consistency	0
Even if Kim's gender  was unknown before seeing the first sentence,  after the SC sentence, it is known.	SC	simplified Chinese	0
Even if Kim's gender  was unknown before seeing the first sentence,  after the SC sentence, it is known.	SC	Score	0
The SC  experiment investigates a method for unsuper-  vised learning of gender/number/animaticity  information.	SC	SemCor	0
The SC  experiment investigates a method for unsuper-  vised learning of gender/number/animaticity  information.	SC	Section	0
The SC  experiment investigates a method for unsuper-  vised learning of gender/number/animaticity  information.	SC	second	1
The SC  experiment investigates a method for unsuper-  vised learning of gender/number/animaticity  information.	SC	Simple Classification	0
The SC  experiment investigates a method for unsuper-  vised learning of gender/number/animaticity  information.	SC	sur face coerc ion	0
The SC  experiment investigates a method for unsuper-  vised learning of gender/number/animaticity  information.	SC	score	0
The SC  experiment investigates a method for unsuper-  vised learning of gender/number/animaticity  information.	SC	System Combination	0
The SC  experiment investigates a method for unsuper-  vised learning of gender/number/animaticity  information.	SC	String Consistency	0
The SC  experiment investigates a method for unsuper-  vised learning of gender/number/animaticity  information.	SC	simplified Chinese	0
The SC  experiment investigates a method for unsuper-  vised learning of gender/number/animaticity  information.	SC	Score	0
5.1 Comparison with SC We compared the RAkEL algorithm with single- label (SL) classification.	SC	SemCor	0
5.1 Comparison with SC We compared the RAkEL algorithm with single- label (SL) classification.	SC	Section	0
5.1 Comparison with SC We compared the RAkEL algorithm with single- label (SL) classification.	SC	second	0
5.1 Comparison with SC We compared the RAkEL algorithm with single- label (SL) classification.	SC	Simple Classification	1
5.1 Comparison with SC We compared the RAkEL algorithm with single- label (SL) classification.	SC	sur face coerc ion	0
5.1 Comparison with SC We compared the RAkEL algorithm with single- label (SL) classification.	SC	score	0
5.1 Comparison with SC We compared the RAkEL algorithm with single- label (SL) classification.	SC	System Combination	0
5.1 Comparison with SC We compared the RAkEL algorithm with single- label (SL) classification.	SC	String Consistency	0
5.1 Comparison with SC We compared the RAkEL algorithm with single- label (SL) classification.	SC	simplified Chinese	0
5.1 Comparison with SC We compared the RAkEL algorithm with single- label (SL) classification.	SC	Score	0
Last but not least, we have to work on the definition of polysemy within our task, so that we can achieve significant agreement SCs among judges and in- tegrate this parameter in the experiment.	SC	SemCor	0
Last but not least, we have to work on the definition of polysemy within our task, so that we can achieve significant agreement SCs among judges and in- tegrate this parameter in the experiment.	SC	Section	0
Last but not least, we have to work on the definition of polysemy within our task, so that we can achieve significant agreement SCs among judges and in- tegrate this parameter in the experiment.	SC	second	0
Last but not least, we have to work on the definition of polysemy within our task, so that we can achieve significant agreement SCs among judges and in- tegrate this parameter in the experiment.	SC	Simple Classification	0
Last but not least, we have to work on the definition of polysemy within our task, so that we can achieve significant agreement SCs among judges and in- tegrate this parameter in the experiment.	SC	sur face coerc ion	0
Last but not least, we have to work on the definition of polysemy within our task, so that we can achieve significant agreement SCs among judges and in- tegrate this parameter in the experiment.	SC	score	1
Last but not least, we have to work on the definition of polysemy within our task, so that we can achieve significant agreement SCs among judges and in- tegrate this parameter in the experiment.	SC	System Combination	0
Last but not least, we have to work on the definition of polysemy within our task, so that we can achieve significant agreement SCs among judges and in- tegrate this parameter in the experiment.	SC	String Consistency	0
Last but not least, we have to work on the definition of polysemy within our task, so that we can achieve significant agreement SCs among judges and in- tegrate this parameter in the experiment.	SC	simplified Chinese	0
Last but not least, we have to work on the definition of polysemy within our task, so that we can achieve significant agreement SCs among judges and in- tegrate this parameter in the experiment.	SC	Score	0
We cannot perform any analysis on the clustering results with respect to polysemy until reliable SCs are obtained.	SC	SemCor	0
We cannot perform any analysis on the clustering results with respect to polysemy until reliable SCs are obtained.	SC	Section	0
We cannot perform any analysis on the clustering results with respect to polysemy until reliable SCs are obtained.	SC	second	0
We cannot perform any analysis on the clustering results with respect to polysemy until reliable SCs are obtained.	SC	Simple Classification	0
We cannot perform any analysis on the clustering results with respect to polysemy until reliable SCs are obtained.	SC	sur face coerc ion	0
We cannot perform any analysis on the clustering results with respect to polysemy until reliable SCs are obtained.	SC	score	1
We cannot perform any analysis on the clustering results with respect to polysemy until reliable SCs are obtained.	SC	System Combination	0
We cannot perform any analysis on the clustering results with respect to polysemy until reliable SCs are obtained.	SC	String Consistency	0
We cannot perform any analysis on the clustering results with respect to polysemy until reliable SCs are obtained.	SC	simplified Chinese	0
We cannot perform any analysis on the clustering results with respect to polysemy until reliable SCs are obtained.	SC	Score	0
However, the agreement SCs for polysemy judgments were not significant at all.	SC	SemCor	0
However, the agreement SCs for polysemy judgments were not significant at all.	SC	Section	0
However, the agreement SCs for polysemy judgments were not significant at all.	SC	second	0
However, the agreement SCs for polysemy judgments were not significant at all.	SC	Simple Classification	0
However, the agreement SCs for polysemy judgments were not significant at all.	SC	sur face coerc ion	0
However, the agreement SCs for polysemy judgments were not significant at all.	SC	score	1
However, the agreement SCs for polysemy judgments were not significant at all.	SC	System Combination	0
However, the agreement SCs for polysemy judgments were not significant at all.	SC	String Consistency	0
However, the agreement SCs for polysemy judgments were not significant at all.	SC	simplified Chinese	0
However, the agreement SCs for polysemy judgments were not significant at all.	SC	Score	0
The precision SC computed over all phrases  containing any of the target honorifics are 66.0%  l In effect, this is the same as admi t t ing  that  a ref-  erent  can be in different gender  classes across different  observations.	SC	SemCor	0
The precision SC computed over all phrases  containing any of the target honorifics are 66.0%  l In effect, this is the same as admi t t ing  that  a ref-  erent  can be in different gender  classes across different  observations.	SC	Section	0
The precision SC computed over all phrases  containing any of the target honorifics are 66.0%  l In effect, this is the same as admi t t ing  that  a ref-  erent  can be in different gender  classes across different  observations.	SC	second	0
The precision SC computed over all phrases  containing any of the target honorifics are 66.0%  l In effect, this is the same as admi t t ing  that  a ref-  erent  can be in different gender  classes across different  observations.	SC	Simple Classification	0
The precision SC computed over all phrases  containing any of the target honorifics are 66.0%  l In effect, this is the same as admi t t ing  that  a ref-  erent  can be in different gender  classes across different  observations.	SC	sur face coerc ion	0
The precision SC computed over all phrases  containing any of the target honorifics are 66.0%  l In effect, this is the same as admi t t ing  that  a ref-  erent  can be in different gender  classes across different  observations.	SC	score	1
The precision SC computed over all phrases  containing any of the target honorifics are 66.0%  l In effect, this is the same as admi t t ing  that  a ref-  erent  can be in different gender  classes across different  observations.	SC	System Combination	0
The precision SC computed over all phrases  containing any of the target honorifics are 66.0%  l In effect, this is the same as admi t t ing  that  a ref-  erent  can be in different gender  classes across different  observations.	SC	String Consistency	0
The precision SC computed over all phrases  containing any of the target honorifics are 66.0%  l In effect, this is the same as admi t t ing  that  a ref-  erent  can be in different gender  classes across different  observations.	SC	simplified Chinese	0
The precision SC computed over all phrases  containing any of the target honorifics are 66.0%  l In effect, this is the same as admi t t ing  that  a ref-  erent  can be in different gender  classes across different  observations.	SC	Score	0
The low- est agreement SCs are those of J2, the only judge who had not done research on adjectives.	SC	SemCor	0
The low- est agreement SCs are those of J2, the only judge who had not done research on adjectives.	SC	Section	0
The low- est agreement SCs are those of J2, the only judge who had not done research on adjectives.	SC	second	0
The low- est agreement SCs are those of J2, the only judge who had not done research on adjectives.	SC	Simple Classification	0
The low- est agreement SCs are those of J2, the only judge who had not done research on adjectives.	SC	sur face coerc ion	0
The low- est agreement SCs are those of J2, the only judge who had not done research on adjectives.	SC	score	1
The low- est agreement SCs are those of J2, the only judge who had not done research on adjectives.	SC	System Combination	0
The low- est agreement SCs are those of J2, the only judge who had not done research on adjectives.	SC	String Consistency	0
The low- est agreement SCs are those of J2, the only judge who had not done research on adjectives.	SC	simplified Chinese	0
The low- est agreement SCs are those of J2, the only judge who had not done research on adjectives.	SC	Score	0
The clustering procedure achieves a comparable agreement SC for one of the pa- rameters, and a little lower for the other.	SC	SemCor	0
The clustering procedure achieves a comparable agreement SC for one of the pa- rameters, and a little lower for the other.	SC	Section	0
The clustering procedure achieves a comparable agreement SC for one of the pa- rameters, and a little lower for the other.	SC	second	0
The clustering procedure achieves a comparable agreement SC for one of the pa- rameters, and a little lower for the other.	SC	Simple Classification	0
The clustering procedure achieves a comparable agreement SC for one of the pa- rameters, and a little lower for the other.	SC	sur face coerc ion	0
The clustering procedure achieves a comparable agreement SC for one of the pa- rameters, and a little lower for the other.	SC	score	1
The clustering procedure achieves a comparable agreement SC for one of the pa- rameters, and a little lower for the other.	SC	System Combination	0
The clustering procedure achieves a comparable agreement SC for one of the pa- rameters, and a little lower for the other.	SC	String Consistency	0
The clustering procedure achieves a comparable agreement SC for one of the pa- rameters, and a little lower for the other.	SC	simplified Chinese	0
The clustering procedure achieves a comparable agreement SC for one of the pa- rameters, and a little lower for the other.	SC	Score	0
2 SC Algorithm The median string of a set is defined as the string that minimises the sum of distances to the strings in the set.	SC	SemCor	0
2 SC Algorithm The median string of a set is defined as the string that minimises the sum of distances to the strings in the set.	SC	Section	0
2 SC Algorithm The median string of a set is defined as the string that minimises the sum of distances to the strings in the set.	SC	second	0
2 SC Algorithm The median string of a set is defined as the string that minimises the sum of distances to the strings in the set.	SC	Simple Classification	0
2 SC Algorithm The median string of a set is defined as the string that minimises the sum of distances to the strings in the set.	SC	sur face coerc ion	0
2 SC Algorithm The median string of a set is defined as the string that minimises the sum of distances to the strings in the set.	SC	score	0
2 SC Algorithm The median string of a set is defined as the string that minimises the sum of distances to the strings in the set.	SC	System Combination	1
2 SC Algorithm The median string of a set is defined as the string that minimises the sum of distances to the strings in the set.	SC	String Consistency	0
2 SC Algorithm The median string of a set is defined as the string that minimises the sum of distances to the strings in the set.	SC	simplified Chinese	0
2 SC Algorithm The median string of a set is defined as the string that minimises the sum of distances to the strings in the set.	SC	Score	0
1.3 SC Combination of (manual) rule-writing and statis- tical learning has been studied before.	SC	SemCor	0
1.3 SC Combination of (manual) rule-writing and statis- tical learning has been studied before.	SC	Section	0
1.3 SC Combination of (manual) rule-writing and statis- tical learning has been studied before.	SC	second	0
1.3 SC Combination of (manual) rule-writing and statis- tical learning has been studied before.	SC	Simple Classification	0
1.3 SC Combination of (manual) rule-writing and statis- tical learning has been studied before.	SC	sur face coerc ion	0
1.3 SC Combination of (manual) rule-writing and statis- tical learning has been studied before.	SC	score	0
1.3 SC Combination of (manual) rule-writing and statis- tical learning has been studied before.	SC	System Combination	1
1.3 SC Combination of (manual) rule-writing and statis- tical learning has been studied before.	SC	String Consistency	0
1.3 SC Combination of (manual) rule-writing and statis- tical learning has been studied before.	SC	simplified Chinese	0
1.3 SC Combination of (manual) rule-writing and statis- tical learning has been studied before.	SC	Score	0
3.3 SC Results Our framework to compute consensus translations allows multiple combinations varying the median string algorithm or the set of weight values used in the weighted sum of distances.	SC	SemCor	0
3.3 SC Results Our framework to compute consensus translations allows multiple combinations varying the median string algorithm or the set of weight values used in the weighted sum of distances.	SC	Section	0
3.3 SC Results Our framework to compute consensus translations allows multiple combinations varying the median string algorithm or the set of weight values used in the weighted sum of distances.	SC	second	0
3.3 SC Results Our framework to compute consensus translations allows multiple combinations varying the median string algorithm or the set of weight values used in the weighted sum of distances.	SC	Simple Classification	0
3.3 SC Results Our framework to compute consensus translations allows multiple combinations varying the median string algorithm or the set of weight values used in the weighted sum of distances.	SC	sur face coerc ion	0
3.3 SC Results Our framework to compute consensus translations allows multiple combinations varying the median string algorithm or the set of weight values used in the weighted sum of distances.	SC	score	0
3.3 SC Results Our framework to compute consensus translations allows multiple combinations varying the median string algorithm or the set of weight values used in the weighted sum of distances.	SC	System Combination	1
3.3 SC Results Our framework to compute consensus translations allows multiple combinations varying the median string algorithm or the set of weight values used in the weighted sum of distances.	SC	String Consistency	0
3.3 SC Results Our framework to compute consensus translations allows multiple combinations varying the median string algorithm or the set of weight values used in the weighted sum of distances.	SC	simplified Chinese	0
3.3 SC Results Our framework to compute consensus translations allows multiple combinations varying the median string algorithm or the set of weight values used in the weighted sum of distances.	SC	Score	0
2.3 SC System combination is used to produce consensus translations from multiple hypotheses generated with different translation engines.	SC	SemCor	0
2.3 SC System combination is used to produce consensus translations from multiple hypotheses generated with different translation engines.	SC	Section	0
2.3 SC System combination is used to produce consensus translations from multiple hypotheses generated with different translation engines.	SC	second	0
2.3 SC System combination is used to produce consensus translations from multiple hypotheses generated with different translation engines.	SC	Simple Classification	0
2.3 SC System combination is used to produce consensus translations from multiple hypotheses generated with different translation engines.	SC	sur face coerc ion	0
2.3 SC System combination is used to produce consensus translations from multiple hypotheses generated with different translation engines.	SC	score	0
2.3 SC System combination is used to produce consensus translations from multiple hypotheses generated with different translation engines.	SC	System Combination	1
2.3 SC System combination is used to produce consensus translations from multiple hypotheses generated with different translation engines.	SC	String Consistency	0
2.3 SC System combination is used to produce consensus translations from multiple hypotheses generated with different translation engines.	SC	simplified Chinese	0
2.3 SC System combination is used to produce consensus translations from multiple hypotheses generated with different translation engines.	SC	Score	0
5.3 Comparison with SC We re-implemented a state-of-the-art system com- bination method (Rosti et al, 2007).	SC	SemCor	0
5.3 Comparison with SC We re-implemented a state-of-the-art system com- bination method (Rosti et al, 2007).	SC	Section	0
5.3 Comparison with SC We re-implemented a state-of-the-art system com- bination method (Rosti et al, 2007).	SC	second	0
5.3 Comparison with SC We re-implemented a state-of-the-art system com- bination method (Rosti et al, 2007).	SC	Simple Classification	0
5.3 Comparison with SC We re-implemented a state-of-the-art system com- bination method (Rosti et al, 2007).	SC	sur face coerc ion	0
5.3 Comparison with SC We re-implemented a state-of-the-art system com- bination method (Rosti et al, 2007).	SC	score	0
5.3 Comparison with SC We re-implemented a state-of-the-art system com- bination method (Rosti et al, 2007).	SC	System Combination	1
5.3 Comparison with SC We re-implemented a state-of-the-art system com- bination method (Rosti et al, 2007).	SC	String Consistency	0
5.3 Comparison with SC We re-implemented a state-of-the-art system com- bination method (Rosti et al, 2007).	SC	simplified Chinese	0
5.3 Comparison with SC We re-implemented a state-of-the-art system com- bination method (Rosti et al, 2007).	SC	Score	0
c?2010 Association for Computational Linguistics An Augmented Three-Pass SC Framework: DCU Combination System for WMT 2010 Jinhua Du, Pavel Pecina, Andy Way CNGL, School of Computing Dublin City University Dublin 9, Ireland {jdu,ppecina,away}@computing.dcu.ie Abstract This paper describes the augmented three- pass system combination framework of the Dublin City University (DCU) MT group for the WMT 2010 system combi- nation task.	SC	SemCor	0
c?2010 Association for Computational Linguistics An Augmented Three-Pass SC Framework: DCU Combination System for WMT 2010 Jinhua Du, Pavel Pecina, Andy Way CNGL, School of Computing Dublin City University Dublin 9, Ireland {jdu,ppecina,away}@computing.dcu.ie Abstract This paper describes the augmented three- pass system combination framework of the Dublin City University (DCU) MT group for the WMT 2010 system combi- nation task.	SC	Section	0
c?2010 Association for Computational Linguistics An Augmented Three-Pass SC Framework: DCU Combination System for WMT 2010 Jinhua Du, Pavel Pecina, Andy Way CNGL, School of Computing Dublin City University Dublin 9, Ireland {jdu,ppecina,away}@computing.dcu.ie Abstract This paper describes the augmented three- pass system combination framework of the Dublin City University (DCU) MT group for the WMT 2010 system combi- nation task.	SC	second	0
c?2010 Association for Computational Linguistics An Augmented Three-Pass SC Framework: DCU Combination System for WMT 2010 Jinhua Du, Pavel Pecina, Andy Way CNGL, School of Computing Dublin City University Dublin 9, Ireland {jdu,ppecina,away}@computing.dcu.ie Abstract This paper describes the augmented three- pass system combination framework of the Dublin City University (DCU) MT group for the WMT 2010 system combi- nation task.	SC	Simple Classification	0
c?2010 Association for Computational Linguistics An Augmented Three-Pass SC Framework: DCU Combination System for WMT 2010 Jinhua Du, Pavel Pecina, Andy Way CNGL, School of Computing Dublin City University Dublin 9, Ireland {jdu,ppecina,away}@computing.dcu.ie Abstract This paper describes the augmented three- pass system combination framework of the Dublin City University (DCU) MT group for the WMT 2010 system combi- nation task.	SC	sur face coerc ion	0
c?2010 Association for Computational Linguistics An Augmented Three-Pass SC Framework: DCU Combination System for WMT 2010 Jinhua Du, Pavel Pecina, Andy Way CNGL, School of Computing Dublin City University Dublin 9, Ireland {jdu,ppecina,away}@computing.dcu.ie Abstract This paper describes the augmented three- pass system combination framework of the Dublin City University (DCU) MT group for the WMT 2010 system combi- nation task.	SC	score	0
c?2010 Association for Computational Linguistics An Augmented Three-Pass SC Framework: DCU Combination System for WMT 2010 Jinhua Du, Pavel Pecina, Andy Way CNGL, School of Computing Dublin City University Dublin 9, Ireland {jdu,ppecina,away}@computing.dcu.ie Abstract This paper describes the augmented three- pass system combination framework of the Dublin City University (DCU) MT group for the WMT 2010 system combi- nation task.	SC	System Combination	1
c?2010 Association for Computational Linguistics An Augmented Three-Pass SC Framework: DCU Combination System for WMT 2010 Jinhua Du, Pavel Pecina, Andy Way CNGL, School of Computing Dublin City University Dublin 9, Ireland {jdu,ppecina,away}@computing.dcu.ie Abstract This paper describes the augmented three- pass system combination framework of the Dublin City University (DCU) MT group for the WMT 2010 system combi- nation task.	SC	String Consistency	0
c?2010 Association for Computational Linguistics An Augmented Three-Pass SC Framework: DCU Combination System for WMT 2010 Jinhua Du, Pavel Pecina, Andy Way CNGL, School of Computing Dublin City University Dublin 9, Ireland {jdu,ppecina,away}@computing.dcu.ie Abstract This paper describes the augmented three- pass system combination framework of the Dublin City University (DCU) MT group for the WMT 2010 system combi- nation task.	SC	simplified Chinese	0
c?2010 Association for Computational Linguistics An Augmented Three-Pass SC Framework: DCU Combination System for WMT 2010 Jinhua Du, Pavel Pecina, Andy Way CNGL, School of Computing Dublin City University Dublin 9, Ireland {jdu,ppecina,away}@computing.dcu.ie Abstract This paper describes the augmented three- pass system combination framework of the Dublin City University (DCU) MT group for the WMT 2010 system combi- nation task.	SC	Score	0
To test this hypothesis, we devise the following classification problem: can we discriminate between 78 Baseline Dialog length (turns) Mean, standard deviation, min and max acts per turn Presence of special machine acts (flight offer and confirm) Presence of user acts (provide a dest city and arrival city) Proportion of acts which were provides SC Did the user provide inconsistent information about dest city?	SC	SemCor	0
To test this hypothesis, we devise the following classification problem: can we discriminate between 78 Baseline Dialog length (turns) Mean, standard deviation, min and max acts per turn Presence of special machine acts (flight offer and confirm) Presence of user acts (provide a dest city and arrival city) Proportion of acts which were provides SC Did the user provide inconsistent information about dest city?	SC	Section	0
To test this hypothesis, we devise the following classification problem: can we discriminate between 78 Baseline Dialog length (turns) Mean, standard deviation, min and max acts per turn Presence of special machine acts (flight offer and confirm) Presence of user acts (provide a dest city and arrival city) Proportion of acts which were provides SC Did the user provide inconsistent information about dest city?	SC	second	0
To test this hypothesis, we devise the following classification problem: can we discriminate between 78 Baseline Dialog length (turns) Mean, standard deviation, min and max acts per turn Presence of special machine acts (flight offer and confirm) Presence of user acts (provide a dest city and arrival city) Proportion of acts which were provides SC Did the user provide inconsistent information about dest city?	SC	Simple Classification	0
To test this hypothesis, we devise the following classification problem: can we discriminate between 78 Baseline Dialog length (turns) Mean, standard deviation, min and max acts per turn Presence of special machine acts (flight offer and confirm) Presence of user acts (provide a dest city and arrival city) Proportion of acts which were provides SC Did the user provide inconsistent information about dest city?	SC	sur face coerc ion	0
To test this hypothesis, we devise the following classification problem: can we discriminate between 78 Baseline Dialog length (turns) Mean, standard deviation, min and max acts per turn Presence of special machine acts (flight offer and confirm) Presence of user acts (provide a dest city and arrival city) Proportion of acts which were provides SC Did the user provide inconsistent information about dest city?	SC	score	0
To test this hypothesis, we devise the following classification problem: can we discriminate between 78 Baseline Dialog length (turns) Mean, standard deviation, min and max acts per turn Presence of special machine acts (flight offer and confirm) Presence of user acts (provide a dest city and arrival city) Proportion of acts which were provides SC Did the user provide inconsistent information about dest city?	SC	System Combination	0
To test this hypothesis, we devise the following classification problem: can we discriminate between 78 Baseline Dialog length (turns) Mean, standard deviation, min and max acts per turn Presence of special machine acts (flight offer and confirm) Presence of user acts (provide a dest city and arrival city) Proportion of acts which were provides SC Did the user provide inconsistent information about dest city?	SC	String Consistency	1
To test this hypothesis, we devise the following classification problem: can we discriminate between 78 Baseline Dialog length (turns) Mean, standard deviation, min and max acts per turn Presence of special machine acts (flight offer and confirm) Presence of user acts (provide a dest city and arrival city) Proportion of acts which were provides SC Did the user provide inconsistent information about dest city?	SC	simplified Chinese	0
To test this hypothesis, we devise the following classification problem: can we discriminate between 78 Baseline Dialog length (turns) Mean, standard deviation, min and max acts per turn Presence of special machine acts (flight offer and confirm) Presence of user acts (provide a dest city and arrival city) Proportion of acts which were provides SC Did the user provide inconsistent information about dest city?	SC	Score	0
he documents adopted in MET-2 are selected from newspapers in China, thus we have to transform SC characters in GB coding set to traditional Chinese characters in Big-5 coding set before testing.	SC	SemCor	0
he documents adopted in MET-2 are selected from newspapers in China, thus we have to transform SC characters in GB coding set to traditional Chinese characters in Big-5 coding set before testing.	SC	Section	0
he documents adopted in MET-2 are selected from newspapers in China, thus we have to transform SC characters in GB coding set to traditional Chinese characters in Big-5 coding set before testing.	SC	second	0
he documents adopted in MET-2 are selected from newspapers in China, thus we have to transform SC characters in GB coding set to traditional Chinese characters in Big-5 coding set before testing.	SC	Simple Classification	0
he documents adopted in MET-2 are selected from newspapers in China, thus we have to transform SC characters in GB coding set to traditional Chinese characters in Big-5 coding set before testing.	SC	sur face coerc ion	0
he documents adopted in MET-2 are selected from newspapers in China, thus we have to transform SC characters in GB coding set to traditional Chinese characters in Big-5 coding set before testing.	SC	score	0
he documents adopted in MET-2 are selected from newspapers in China, thus we have to transform SC characters in GB coding set to traditional Chinese characters in Big-5 coding set before testing.	SC	System Combination	0
he documents adopted in MET-2 are selected from newspapers in China, thus we have to transform SC characters in GB coding set to traditional Chinese characters in Big-5 coding set before testing.	SC	String Consistency	0
he documents adopted in MET-2 are selected from newspapers in China, thus we have to transform SC characters in GB coding set to traditional Chinese characters in Big-5 coding set before testing.	SC	simplified Chinese	1
he documents adopted in MET-2 are selected from newspapers in China, thus we have to transform SC characters in GB coding set to traditional Chinese characters in Big-5 coding set before testing.	SC	Score	0
The traditional Chinese sentences are transferred  into SC.	SC	SemCor	0
The traditional Chinese sentences are transferred  into SC.	SC	Section	0
The traditional Chinese sentences are transferred  into SC.	SC	second	0
The traditional Chinese sentences are transferred  into SC.	SC	Simple Classification	0
The traditional Chinese sentences are transferred  into SC.	SC	sur face coerc ion	0
The traditional Chinese sentences are transferred  into SC.	SC	score	0
The traditional Chinese sentences are transferred  into SC.	SC	System Combination	0
The traditional Chinese sentences are transferred  into SC.	SC	String Consistency	0
The traditional Chinese sentences are transferred  into SC.	SC	simplified Chinese	1
The traditional Chinese sentences are transferred  into SC.	SC	Score	0
The documents adopted in MET-2 are selected from newspapers in China, thus we have to transform SC characters in GB coding set to traditional Chinese characters in Big-5 coding set before testing.	SC	SemCor	0
The documents adopted in MET-2 are selected from newspapers in China, thus we have to transform SC characters in GB coding set to traditional Chinese characters in Big-5 coding set before testing.	SC	Section	0
The documents adopted in MET-2 are selected from newspapers in China, thus we have to transform SC characters in GB coding set to traditional Chinese characters in Big-5 coding set before testing.	SC	second	0
The documents adopted in MET-2 are selected from newspapers in China, thus we have to transform SC characters in GB coding set to traditional Chinese characters in Big-5 coding set before testing.	SC	Simple Classification	0
The documents adopted in MET-2 are selected from newspapers in China, thus we have to transform SC characters in GB coding set to traditional Chinese characters in Big-5 coding set before testing.	SC	sur face coerc ion	0
The documents adopted in MET-2 are selected from newspapers in China, thus we have to transform SC characters in GB coding set to traditional Chinese characters in Big-5 coding set before testing.	SC	score	0
The documents adopted in MET-2 are selected from newspapers in China, thus we have to transform SC characters in GB coding set to traditional Chinese characters in Big-5 coding set before testing.	SC	System Combination	0
The documents adopted in MET-2 are selected from newspapers in China, thus we have to transform SC characters in GB coding set to traditional Chinese characters in Big-5 coding set before testing.	SC	String Consistency	0
The documents adopted in MET-2 are selected from newspapers in China, thus we have to transform SC characters in GB coding set to traditional Chinese characters in Big-5 coding set before testing.	SC	simplified Chinese	1
The documents adopted in MET-2 are selected from newspapers in China, thus we have to transform SC characters in GB coding set to traditional Chinese characters in Big-5 coding set before testing.	SC	Score	0
The experiment results demonstrate that adopting our two strategies generally benefi- cial to NWI and CWS on both traditional and SC datasets.	SC	SemCor	0
The experiment results demonstrate that adopting our two strategies generally benefi- cial to NWI and CWS on both traditional and SC datasets.	SC	Section	0
The experiment results demonstrate that adopting our two strategies generally benefi- cial to NWI and CWS on both traditional and SC datasets.	SC	second	0
The experiment results demonstrate that adopting our two strategies generally benefi- cial to NWI and CWS on both traditional and SC datasets.	SC	Simple Classification	0
The experiment results demonstrate that adopting our two strategies generally benefi- cial to NWI and CWS on both traditional and SC datasets.	SC	sur face coerc ion	0
The experiment results demonstrate that adopting our two strategies generally benefi- cial to NWI and CWS on both traditional and SC datasets.	SC	score	0
The experiment results demonstrate that adopting our two strategies generally benefi- cial to NWI and CWS on both traditional and SC datasets.	SC	System Combination	0
The experiment results demonstrate that adopting our two strategies generally benefi- cial to NWI and CWS on both traditional and SC datasets.	SC	String Consistency	0
The experiment results demonstrate that adopting our two strategies generally benefi- cial to NWI and CWS on both traditional and SC datasets.	SC	simplified Chinese	1
The experiment results demonstrate that adopting our two strategies generally benefi- cial to NWI and CWS on both traditional and SC datasets.	SC	Score	0
It was converted to SC by using Wikipedia?s traditional-simplified conversion table http://svn.	SC	SemCor	0
It was converted to SC by using Wikipedia?s traditional-simplified conversion table http://svn.	SC	Section	0
It was converted to SC by using Wikipedia?s traditional-simplified conversion table http://svn.	SC	second	0
It was converted to SC by using Wikipedia?s traditional-simplified conversion table http://svn.	SC	Simple Classification	0
It was converted to SC by using Wikipedia?s traditional-simplified conversion table http://svn.	SC	sur face coerc ion	0
It was converted to SC by using Wikipedia?s traditional-simplified conversion table http://svn.	SC	score	0
It was converted to SC by using Wikipedia?s traditional-simplified conversion table http://svn.	SC	System Combination	0
It was converted to SC by using Wikipedia?s traditional-simplified conversion table http://svn.	SC	String Consistency	0
It was converted to SC by using Wikipedia?s traditional-simplified conversion table http://svn.	SC	simplified Chinese	1
It was converted to SC by using Wikipedia?s traditional-simplified conversion table http://svn.	SC	Score	0
Traditional  Chinese edition of a SC  edition published in 1984.)	SC	SemCor	0
Traditional  Chinese edition of a SC  edition published in 1984.)	SC	Section	0
Traditional  Chinese edition of a SC  edition published in 1984.)	SC	second	0
Traditional  Chinese edition of a SC  edition published in 1984.)	SC	Simple Classification	0
Traditional  Chinese edition of a SC  edition published in 1984.)	SC	sur face coerc ion	0
Traditional  Chinese edition of a SC  edition published in 1984.)	SC	score	0
Traditional  Chinese edition of a SC  edition published in 1984.)	SC	System Combination	0
Traditional  Chinese edition of a SC  edition published in 1984.)	SC	String Consistency	0
Traditional  Chinese edition of a SC  edition published in 1984.)	SC	simplified Chinese	1
Traditional  Chinese edition of a SC  edition published in 1984.)	SC	Score	0
However, "D" is used in SC characters and it is also a legal traditional Chinese character that denotes	SC	SemCor	0
However, "D" is used in SC characters and it is also a legal traditional Chinese character that denotes	SC	Section	0
However, "D" is used in SC characters and it is also a legal traditional Chinese character that denotes	SC	second	0
However, "D" is used in SC characters and it is also a legal traditional Chinese character that denotes	SC	Simple Classification	0
However, "D" is used in SC characters and it is also a legal traditional Chinese character that denotes	SC	sur face coerc ion	0
However, "D" is used in SC characters and it is also a legal traditional Chinese character that denotes	SC	score	0
However, "D" is used in SC characters and it is also a legal traditional Chinese character that denotes	SC	System Combination	0
However, "D" is used in SC characters and it is also a legal traditional Chinese character that denotes	SC	String Consistency	0
However, "D" is used in SC characters and it is also a legal traditional Chinese character that denotes	SC	simplified Chinese	1
However, "D" is used in SC characters and it is also a legal traditional Chinese character that denotes	SC	Score	0
However, "D" is used in SC characters and it is also a legal traditional Chinese character that denotes another meaning.	SC	SemCor	0
However, "D" is used in SC characters and it is also a legal traditional Chinese character that denotes another meaning.	SC	Section	0
However, "D" is used in SC characters and it is also a legal traditional Chinese character that denotes another meaning.	SC	second	0
However, "D" is used in SC characters and it is also a legal traditional Chinese character that denotes another meaning.	SC	Simple Classification	0
However, "D" is used in SC characters and it is also a legal traditional Chinese character that denotes another meaning.	SC	sur face coerc ion	0
However, "D" is used in SC characters and it is also a legal traditional Chinese character that denotes another meaning.	SC	score	0
However, "D" is used in SC characters and it is also a legal traditional Chinese character that denotes another meaning.	SC	System Combination	0
However, "D" is used in SC characters and it is also a legal traditional Chinese character that denotes another meaning.	SC	String Consistency	0
However, "D" is used in SC characters and it is also a legal traditional Chinese character that denotes another meaning.	SC	simplified Chinese	1
However, "D" is used in SC characters and it is also a legal traditional Chinese character that denotes another meaning.	SC	Score	0
and F-SC (LF ?	SC	SemCor	0
and F-SC (LF ?	SC	Section	0
and F-SC (LF ?	SC	second	0
and F-SC (LF ?	SC	Simple Classification	0
and F-SC (LF ?	SC	sur face coerc ion	0
and F-SC (LF ?	SC	score	0
and F-SC (LF ?	SC	System Combination	0
and F-SC (LF ?	SC	String Consistency	0
and F-SC (LF ?	SC	simplified Chinese	0
and F-SC (LF ?	SC	Score	1
All classifiers obtain a relatively high accuracy but vary in the precision, recall and F-SC values.	SC	SemCor	0
All classifiers obtain a relatively high accuracy but vary in the precision, recall and F-SC values.	SC	Section	0
All classifiers obtain a relatively high accuracy but vary in the precision, recall and F-SC values.	SC	second	0
All classifiers obtain a relatively high accuracy but vary in the precision, recall and F-SC values.	SC	Simple Classification	0
All classifiers obtain a relatively high accuracy but vary in the precision, recall and F-SC values.	SC	sur face coerc ion	0
All classifiers obtain a relatively high accuracy but vary in the precision, recall and F-SC values.	SC	score	0
All classifiers obtain a relatively high accuracy but vary in the precision, recall and F-SC values.	SC	System Combination	0
All classifiers obtain a relatively high accuracy but vary in the precision, recall and F-SC values.	SC	String Consistency	0
All classifiers obtain a relatively high accuracy but vary in the precision, recall and F-SC values.	SC	simplified Chinese	0
All classifiers obtain a relatively high accuracy but vary in the precision, recall and F-SC values.	SC	Score	1
Furthermore, by using rules to identify the metaphors in riddles, we get an improvement of 10.1% at Acc@1, which proves the validity of the 852 SC Criterion 5 Elegant metaphors, totally coherent 4 Correct metaphors, mostly coherent 3 Acceptable metaphors, more of less coherent 2 Tolerable metaphors, little coherent 1 Wrong metaphors, incoherent Table 7: The criterion of riddle evaluation rule we define.	SC	SemCor	0
Furthermore, by using rules to identify the metaphors in riddles, we get an improvement of 10.1% at Acc@1, which proves the validity of the 852 SC Criterion 5 Elegant metaphors, totally coherent 4 Correct metaphors, mostly coherent 3 Acceptable metaphors, more of less coherent 2 Tolerable metaphors, little coherent 1 Wrong metaphors, incoherent Table 7: The criterion of riddle evaluation rule we define.	SC	Section	0
Furthermore, by using rules to identify the metaphors in riddles, we get an improvement of 10.1% at Acc@1, which proves the validity of the 852 SC Criterion 5 Elegant metaphors, totally coherent 4 Correct metaphors, mostly coherent 3 Acceptable metaphors, more of less coherent 2 Tolerable metaphors, little coherent 1 Wrong metaphors, incoherent Table 7: The criterion of riddle evaluation rule we define.	SC	second	0
Furthermore, by using rules to identify the metaphors in riddles, we get an improvement of 10.1% at Acc@1, which proves the validity of the 852 SC Criterion 5 Elegant metaphors, totally coherent 4 Correct metaphors, mostly coherent 3 Acceptable metaphors, more of less coherent 2 Tolerable metaphors, little coherent 1 Wrong metaphors, incoherent Table 7: The criterion of riddle evaluation rule we define.	SC	Simple Classification	0
Furthermore, by using rules to identify the metaphors in riddles, we get an improvement of 10.1% at Acc@1, which proves the validity of the 852 SC Criterion 5 Elegant metaphors, totally coherent 4 Correct metaphors, mostly coherent 3 Acceptable metaphors, more of less coherent 2 Tolerable metaphors, little coherent 1 Wrong metaphors, incoherent Table 7: The criterion of riddle evaluation rule we define.	SC	sur face coerc ion	0
Furthermore, by using rules to identify the metaphors in riddles, we get an improvement of 10.1% at Acc@1, which proves the validity of the 852 SC Criterion 5 Elegant metaphors, totally coherent 4 Correct metaphors, mostly coherent 3 Acceptable metaphors, more of less coherent 2 Tolerable metaphors, little coherent 1 Wrong metaphors, incoherent Table 7: The criterion of riddle evaluation rule we define.	SC	score	0
Furthermore, by using rules to identify the metaphors in riddles, we get an improvement of 10.1% at Acc@1, which proves the validity of the 852 SC Criterion 5 Elegant metaphors, totally coherent 4 Correct metaphors, mostly coherent 3 Acceptable metaphors, more of less coherent 2 Tolerable metaphors, little coherent 1 Wrong metaphors, incoherent Table 7: The criterion of riddle evaluation rule we define.	SC	System Combination	0
Furthermore, by using rules to identify the metaphors in riddles, we get an improvement of 10.1% at Acc@1, which proves the validity of the 852 SC Criterion 5 Elegant metaphors, totally coherent 4 Correct metaphors, mostly coherent 3 Acceptable metaphors, more of less coherent 2 Tolerable metaphors, little coherent 1 Wrong metaphors, incoherent Table 7: The criterion of riddle evaluation rule we define.	SC	String Consistency	0
Furthermore, by using rules to identify the metaphors in riddles, we get an improvement of 10.1% at Acc@1, which proves the validity of the 852 SC Criterion 5 Elegant metaphors, totally coherent 4 Correct metaphors, mostly coherent 3 Acceptable metaphors, more of less coherent 2 Tolerable metaphors, little coherent 1 Wrong metaphors, incoherent Table 7: The criterion of riddle evaluation rule we define.	SC	simplified Chinese	0
Furthermore, by using rules to identify the metaphors in riddles, we get an improvement of 10.1% at Acc@1, which proves the validity of the 852 SC Criterion 5 Elegant metaphors, totally coherent 4 Correct metaphors, mostly coherent 3 Acceptable metaphors, more of less coherent 2 Tolerable metaphors, little coherent 1 Wrong metaphors, incoherent Table 7: The criterion of riddle evaluation rule we define.	SC	Score	1
The ranking score is calculated as SC(c) = m?	SC	SemCor	0
The ranking score is calculated as SC(c) = m?	SC	Section	0
The ranking score is calculated as SC(c) = m?	SC	second	0
The ranking score is calculated as SC(c) = m?	SC	Simple Classification	0
The ranking score is calculated as SC(c) = m?	SC	sur face coerc ion	0
The ranking score is calculated as SC(c) = m?	SC	score	0
The ranking score is calculated as SC(c) = m?	SC	System Combination	0
The ranking score is calculated as SC(c) = m?	SC	String Consistency	0
The ranking score is calculated as SC(c) = m?	SC	simplified Chinese	0
The ranking score is calculated as SC(c) = m?	SC	Score	1
haracters in riddle Max Freq Radical maximized number of frequencies of characters in riddle Number Alignment number of alignments used for generating the candidate Length Alignment length of words from alignments Number Rule number of rules used for generating the candidate Length Rule length of words from rules LM SC R score of language model trained by Chinese riddles, poems and couplets LM SC G score of language model trained by web documents Table 4: Features for riddle generation and count the unigram and bigram word frequency of the rest words.	SC	SemCor	0
haracters in riddle Max Freq Radical maximized number of frequencies of characters in riddle Number Alignment number of alignments used for generating the candidate Length Alignment length of words from alignments Number Rule number of rules used for generating the candidate Length Rule length of words from rules LM SC R score of language model trained by Chinese riddles, poems and couplets LM SC G score of language model trained by web documents Table 4: Features for riddle generation and count the unigram and bigram word frequency of the rest words.	SC	Section	0
haracters in riddle Max Freq Radical maximized number of frequencies of characters in riddle Number Alignment number of alignments used for generating the candidate Length Alignment length of words from alignments Number Rule number of rules used for generating the candidate Length Rule length of words from rules LM SC R score of language model trained by Chinese riddles, poems and couplets LM SC G score of language model trained by web documents Table 4: Features for riddle generation and count the unigram and bigram word frequency of the rest words.	SC	second	0
haracters in riddle Max Freq Radical maximized number of frequencies of characters in riddle Number Alignment number of alignments used for generating the candidate Length Alignment length of words from alignments Number Rule number of rules used for generating the candidate Length Rule length of words from rules LM SC R score of language model trained by Chinese riddles, poems and couplets LM SC G score of language model trained by web documents Table 4: Features for riddle generation and count the unigram and bigram word frequency of the rest words.	SC	Simple Classification	0
haracters in riddle Max Freq Radical maximized number of frequencies of characters in riddle Number Alignment number of alignments used for generating the candidate Length Alignment length of words from alignments Number Rule number of rules used for generating the candidate Length Rule length of words from rules LM SC R score of language model trained by Chinese riddles, poems and couplets LM SC G score of language model trained by web documents Table 4: Features for riddle generation and count the unigram and bigram word frequency of the rest words.	SC	sur face coerc ion	0
haracters in riddle Max Freq Radical maximized number of frequencies of characters in riddle Number Alignment number of alignments used for generating the candidate Length Alignment length of words from alignments Number Rule number of rules used for generating the candidate Length Rule length of words from rules LM SC R score of language model trained by Chinese riddles, poems and couplets LM SC G score of language model trained by web documents Table 4: Features for riddle generation and count the unigram and bigram word frequency of the rest words.	SC	score	0
haracters in riddle Max Freq Radical maximized number of frequencies of characters in riddle Number Alignment number of alignments used for generating the candidate Length Alignment length of words from alignments Number Rule number of rules used for generating the candidate Length Rule length of words from rules LM SC R score of language model trained by Chinese riddles, poems and couplets LM SC G score of language model trained by web documents Table 4: Features for riddle generation and count the unigram and bigram word frequency of the rest words.	SC	System Combination	0
haracters in riddle Max Freq Radical maximized number of frequencies of characters in riddle Number Alignment number of alignments used for generating the candidate Length Alignment length of words from alignments Number Rule number of rules used for generating the candidate Length Rule length of words from rules LM SC R score of language model trained by Chinese riddles, poems and couplets LM SC G score of language model trained by web documents Table 4: Features for riddle generation and count the unigram and bigram word frequency of the rest words.	SC	String Consistency	0
haracters in riddle Max Freq Radical maximized number of frequencies of characters in riddle Number Alignment number of alignments used for generating the candidate Length Alignment length of words from alignments Number Rule number of rules used for generating the candidate Length Rule length of words from rules LM SC R score of language model trained by Chinese riddles, poems and couplets LM SC G score of language model trained by web documents Table 4: Features for riddle generation and count the unigram and bigram word frequency of the rest words.	SC	simplified Chinese	0
haracters in riddle Max Freq Radical maximized number of frequencies of characters in riddle Number Alignment number of alignments used for generating the candidate Length Alignment length of words from alignments Number Rule number of rules used for generating the candidate Length Rule length of words from rules LM SC R score of language model trained by Chinese riddles, poems and couplets LM SC G score of language model trained by web documents Table 4: Features for riddle generation and count the unigram and bigram word frequency of the rest words.	SC	Score	1
that the character decompose Avg Freq Character average number of frequencies of characters in riddle Max Freq Radical maximized number of frequencies of characters in riddle Number Alignment number of alignments used for generating the candidate Length Alignment length of words from alignments Number Rule number of rules used for generating the candidate Length Rule length of words from rules LM SC R score of language model trained by Chinese riddles, poems and couplets LM SC G score of language model trained by web documents Table 4: Features for riddle generation and count the unigram and bigram word frequency of the rest words.	SC	SemCor	0
that the character decompose Avg Freq Character average number of frequencies of characters in riddle Max Freq Radical maximized number of frequencies of characters in riddle Number Alignment number of alignments used for generating the candidate Length Alignment length of words from alignments Number Rule number of rules used for generating the candidate Length Rule length of words from rules LM SC R score of language model trained by Chinese riddles, poems and couplets LM SC G score of language model trained by web documents Table 4: Features for riddle generation and count the unigram and bigram word frequency of the rest words.	SC	Section	0
that the character decompose Avg Freq Character average number of frequencies of characters in riddle Max Freq Radical maximized number of frequencies of characters in riddle Number Alignment number of alignments used for generating the candidate Length Alignment length of words from alignments Number Rule number of rules used for generating the candidate Length Rule length of words from rules LM SC R score of language model trained by Chinese riddles, poems and couplets LM SC G score of language model trained by web documents Table 4: Features for riddle generation and count the unigram and bigram word frequency of the rest words.	SC	second	0
that the character decompose Avg Freq Character average number of frequencies of characters in riddle Max Freq Radical maximized number of frequencies of characters in riddle Number Alignment number of alignments used for generating the candidate Length Alignment length of words from alignments Number Rule number of rules used for generating the candidate Length Rule length of words from rules LM SC R score of language model trained by Chinese riddles, poems and couplets LM SC G score of language model trained by web documents Table 4: Features for riddle generation and count the unigram and bigram word frequency of the rest words.	SC	Simple Classification	0
that the character decompose Avg Freq Character average number of frequencies of characters in riddle Max Freq Radical maximized number of frequencies of characters in riddle Number Alignment number of alignments used for generating the candidate Length Alignment length of words from alignments Number Rule number of rules used for generating the candidate Length Rule length of words from rules LM SC R score of language model trained by Chinese riddles, poems and couplets LM SC G score of language model trained by web documents Table 4: Features for riddle generation and count the unigram and bigram word frequency of the rest words.	SC	sur face coerc ion	0
that the character decompose Avg Freq Character average number of frequencies of characters in riddle Max Freq Radical maximized number of frequencies of characters in riddle Number Alignment number of alignments used for generating the candidate Length Alignment length of words from alignments Number Rule number of rules used for generating the candidate Length Rule length of words from rules LM SC R score of language model trained by Chinese riddles, poems and couplets LM SC G score of language model trained by web documents Table 4: Features for riddle generation and count the unigram and bigram word frequency of the rest words.	SC	score	0
that the character decompose Avg Freq Character average number of frequencies of characters in riddle Max Freq Radical maximized number of frequencies of characters in riddle Number Alignment number of alignments used for generating the candidate Length Alignment length of words from alignments Number Rule number of rules used for generating the candidate Length Rule length of words from rules LM SC R score of language model trained by Chinese riddles, poems and couplets LM SC G score of language model trained by web documents Table 4: Features for riddle generation and count the unigram and bigram word frequency of the rest words.	SC	System Combination	0
that the character decompose Avg Freq Character average number of frequencies of characters in riddle Max Freq Radical maximized number of frequencies of characters in riddle Number Alignment number of alignments used for generating the candidate Length Alignment length of words from alignments Number Rule number of rules used for generating the candidate Length Rule length of words from rules LM SC R score of language model trained by Chinese riddles, poems and couplets LM SC G score of language model trained by web documents Table 4: Features for riddle generation and count the unigram and bigram word frequency of the rest words.	SC	String Consistency	0
that the character decompose Avg Freq Character average number of frequencies of characters in riddle Max Freq Radical maximized number of frequencies of characters in riddle Number Alignment number of alignments used for generating the candidate Length Alignment length of words from alignments Number Rule number of rules used for generating the candidate Length Rule length of words from rules LM SC R score of language model trained by Chinese riddles, poems and couplets LM SC G score of language model trained by web documents Table 4: Features for riddle generation and count the unigram and bigram word frequency of the rest words.	SC	simplified Chinese	0
that the character decompose Avg Freq Character average number of frequencies of characters in riddle Max Freq Radical maximized number of frequencies of characters in riddle Number Alignment number of alignments used for generating the candidate Length Alignment length of words from alignments Number Rule number of rules used for generating the candidate Length Rule length of words from rules LM SC R score of language model trained by Chinese riddles, poems and couplets LM SC G score of language model trained by web documents Table 4: Features for riddle generation and count the unigram and bigram word frequency of the rest words.	SC	Score	1
with 0 referring to case-insensitivity, 1 being replacement of hyphens with spaces, 2 being removal of punc- tuation, 3 being removal of PMs, and 6 being removal of spaces; grouped digits indi- cate simultaneous invocation of each specified rule in the group.	PM	parenthesized material	1
with 0 referring to case-insensitivity, 1 being replacement of hyphens with spaces, 2 being removal of punc- tuation, 3 being removal of PMs, and 6 being removal of spaces; grouped digits indi- cate simultaneous invocation of each specified rule in the group.	PM	prosody model	0
with 0 referring to case-insensitivity, 1 being replacement of hyphens with spaces, 2 being removal of punc- tuation, 3 being removal of PMs, and 6 being removal of spaces; grouped digits indi- cate simultaneous invocation of each specified rule in the group.	PM	partial  match	0
with 0 referring to case-insensitivity, 1 being replacement of hyphens with spaces, 2 being removal of punc- tuation, 3 being removal of PMs, and 6 being removal of spaces; grouped digits indi- cate simultaneous invocation of each specified rule in the group.	PM	Priority Model	0
with 0 referring to case-insensitivity, 1 being replacement of hyphens with spaces, 2 being removal of punc- tuation, 3 being removal of PMs, and 6 being removal of spaces; grouped digits indi- cate simultaneous invocation of each specified rule in the group.	PM	phrasometer	0
Removal of PMs.	PM	parenthesized material	1
Removal of PMs.	PM	prosody model	0
Removal of PMs.	PM	partial  match	0
Removal of PMs.	PM	Priority Model	0
Removal of PMs.	PM	phrasometer	0
Even strict pattern matches and forms that  vary only with respect to inflectional  morphology (i.e., the plurals) yield a nontrivial  percentage of false positives?a percentage  which is actually higher than two of our  heuristics (optionality of hyphenation and  optionality of PM).	PM	parenthesized material	1
Even strict pattern matches and forms that  vary only with respect to inflectional  morphology (i.e., the plurals) yield a nontrivial  percentage of false positives?a percentage  which is actually higher than two of our  heuristics (optionality of hyphenation and  optionality of PM).	PM	prosody model	0
Even strict pattern matches and forms that  vary only with respect to inflectional  morphology (i.e., the plurals) yield a nontrivial  percentage of false positives?a percentage  which is actually higher than two of our  heuristics (optionality of hyphenation and  optionality of PM).	PM	partial  match	0
Even strict pattern matches and forms that  vary only with respect to inflectional  morphology (i.e., the plurals) yield a nontrivial  percentage of false positives?a percentage  which is actually higher than two of our  heuristics (optionality of hyphenation and  optionality of PM).	PM	Priority Model	0
Even strict pattern matches and forms that  vary only with respect to inflectional  morphology (i.e., the plurals) yield a nontrivial  percentage of false positives?a percentage  which is actually higher than two of our  heuristics (optionality of hyphenation and  optionality of PM).	PM	phrasometer	0
Optionality of PM: for any  regular expression representing a gene name,  substitute the regular expression formed by  making any paired parentheses and the material  they enclose (and surrounding whitespace, as  appropriate) optional.	PM	parenthesized material	1
Optionality of PM: for any  regular expression representing a gene name,  substitute the regular expression formed by  making any paired parentheses and the material  they enclose (and surrounding whitespace, as  appropriate) optional.	PM	prosody model	0
Optionality of PM: for any  regular expression representing a gene name,  substitute the regular expression formed by  making any paired parentheses and the material  they enclose (and surrounding whitespace, as  appropriate) optional.	PM	partial  match	0
Optionality of PM: for any  regular expression representing a gene name,  substitute the regular expression formed by  making any paired parentheses and the material  they enclose (and surrounding whitespace, as  appropriate) optional.	PM	Priority Model	0
Optionality of PM: for any  regular expression representing a gene name,  substitute the regular expression formed by  making any paired parentheses and the material  they enclose (and surrounding whitespace, as  appropriate) optional.	PM	phrasometer	0
For example in pr tence 36 patients with inflammatory bowel disease  (11 with ulcerative colitis and 25 with Crohn?s disease),  the PM caused SemRep to incor- rectly  returned ?	PM	parenthesized material	1
For example in pr tence 36 patients with inflammatory bowel disease  (11 with ulcerative colitis and 25 with Crohn?s disease),  the PM caused SemRep to incor- rectly  returned ?	PM	prosody model	0
For example in pr tence 36 patients with inflammatory bowel disease  (11 with ulcerative colitis and 25 with Crohn?s disease),  the PM caused SemRep to incor- rectly  returned ?	PM	partial  match	0
For example in pr tence 36 patients with inflammatory bowel disease  (11 with ulcerative colitis and 25 with Crohn?s disease),  the PM caused SemRep to incor- rectly  returned ?	PM	Priority Model	0
For example in pr tence 36 patients with inflammatory bowel disease  (11 with ulcerative colitis and 25 with Crohn?s disease),  the PM caused SemRep to incor- rectly  returned ?	PM	phrasometer	0
These consisted of mapping vowel  sequences to a constant string (the purpose of  this being to look at American vs. British  dialectal differences in gene names);  replacement of hyphens with spaces; removal of  PM; and normalization of  case.	PM	parenthesized material	1
These consisted of mapping vowel  sequences to a constant string (the purpose of  this being to look at American vs. British  dialectal differences in gene names);  replacement of hyphens with spaces; removal of  PM; and normalization of  case.	PM	prosody model	0
These consisted of mapping vowel  sequences to a constant string (the purpose of  this being to look at American vs. British  dialectal differences in gene names);  replacement of hyphens with spaces; removal of  PM; and normalization of  case.	PM	partial  match	0
These consisted of mapping vowel  sequences to a constant string (the purpose of  this being to look at American vs. British  dialectal differences in gene names);  replacement of hyphens with spaces; removal of  PM; and normalization of  case.	PM	Priority Model	0
These consisted of mapping vowel  sequences to a constant string (the purpose of  this being to look at American vs. British  dialectal differences in gene names);  replacement of hyphens with spaces; removal of  PM; and normalization of  case.	PM	phrasometer	0
Punctuation annotation using statisti- cal PMs.	PM	parenthesized material	0
Punctuation annotation using statisti- cal PMs.	PM	prosody model	1
Punctuation annotation using statisti- cal PMs.	PM	partial  match	0
Punctuation annotation using statisti- cal PMs.	PM	Priority Model	0
Punctuation annotation using statisti- cal PMs.	PM	phrasometer	0
Recent PM im- provements include the use of bagging techniques in deci- sion tree training to reduce the variability due to a single tree (Liu et al, 2003).	PM	parenthesized material	0
Recent PM im- provements include the use of bagging techniques in deci- sion tree training to reduce the variability due to a single tree (Liu et al, 2003).	PM	prosody model	1
Recent PM im- provements include the use of bagging techniques in deci- sion tree training to reduce the variability due to a single tree (Liu et al, 2003).	PM	partial  match	0
Recent PM im- provements include the use of bagging techniques in deci- sion tree training to reduce the variability due to a single tree (Liu et al, 2003).	PM	Priority Model	0
Recent PM im- provements include the use of bagging techniques in deci- sion tree training to reduce the variability due to a single tree (Liu et al, 2003).	PM	phrasometer	0
Recent PM im- provements include the use of bagging techniques in deci- sion tree training to reduce the	PM	parenthesized material	0
Recent PM im- provements include the use of bagging techniques in deci- sion tree training to reduce the	PM	prosody model	1
Recent PM im- provements include the use of bagging techniques in deci- sion tree training to reduce the	PM	partial  match	0
Recent PM im- provements include the use of bagging techniques in deci- sion tree training to reduce the	PM	Priority Model	0
Recent PM im- provements include the use of bagging techniques in deci- sion tree training to reduce the	PM	phrasometer	0
Punctua- tion annotation using statistical PMs.	PM	parenthesized material	0
Punctua- tion annotation using statistical PMs.	PM	prosody model	1
Punctua- tion annotation using statistical PMs.	PM	partial  match	0
Punctua- tion annotation using statistical PMs.	PM	Priority Model	0
Punctua- tion annotation using statistical PMs.	PM	phrasometer	0
Identifying the function words is  important for applications such as information  retrieval, PMing in speech synthesis,  semantic role labeling, and dependency parsing.	PM	parenthesized material	0
Identifying the function words is  important for applications such as information  retrieval, PMing in speech synthesis,  semantic role labeling, and dependency parsing.	PM	prosody model	1
Identifying the function words is  important for applications such as information  retrieval, PMing in speech synthesis,  semantic role labeling, and dependency parsing.	PM	partial  match	0
Identifying the function words is  important for applications such as information  retrieval, PMing in speech synthesis,  semantic role labeling, and dependency parsing.	PM	Priority Model	0
Identifying the function words is  important for applications such as information  retrieval, PMing in speech synthesis,  semantic role labeling, and dependency parsing.	PM	phrasometer	0
Punctu- ation annotation using statistical PMs.	PM	parenthesized material	0
Punctu- ation annotation using statistical PMs.	PM	prosody model	1
Punctu- ation annotation using statistical PMs.	PM	partial  match	0
Punctu- ation annotation using statistical PMs.	PM	Priority Model	0
Punctu- ation annotation using statistical PMs.	PM	phrasometer	0
The PM is a de- cision tree classifier that generates the posterior probabil- ity of an SU boundary at each interword boundary given the prosodic features.	PM	parenthesized material	0
The PM is a de- cision tree classifier that generates the posterior probabil- ity of an SU boundary at each interword boundary given the prosodic features.	PM	prosody model	1
The PM is a de- cision tree classifier that generates the posterior probabil- ity of an SU boundary at each interword boundary given the prosodic features.	PM	partial  match	0
The PM is a de- cision tree classifier that generates the posterior probabil- ity of an SU boundary at each interword boundary given the prosodic features.	PM	Priority Model	0
The PM is a de- cision tree classifier that generates the posterior probabil- ity of an SU boundary at each interword boundary given the prosodic features.	PM	phrasometer	0
To  overcome this problem, we combine BANNER  with two other predictors, a Sematic Model and a  PM.	PM	parenthesized material	0
To  overcome this problem, we combine BANNER  with two other predictors, a Sematic Model and a  PM.	PM	prosody model	0
To  overcome this problem, we combine BANNER  with two other predictors, a Sematic Model and a  PM.	PM	partial  match	0
To  overcome this problem, we combine BANNER  with two other predictors, a Sematic Model and a  PM.	PM	Priority Model	1
To  overcome this problem, we combine BANNER  with two other predictors, a Sematic Model and a  PM.	PM	phrasometer	0
Second, these two models learn  name patterns in different ways, i.e., semantic rela- tionships for the Semantic Model and positional  and lexical information for the PM.	PM	parenthesized material	0
Second, these two models learn  name patterns in different ways, i.e., semantic rela- tionships for the Semantic Model and positional  and lexical information for the PM.	PM	prosody model	0
Second, these two models learn  name patterns in different ways, i.e., semantic rela- tionships for the Semantic Model and positional  and lexical information for the PM.	PM	partial  match	0
Second, these two models learn  name patterns in different ways, i.e., semantic rela- tionships for the Semantic Model and positional  and lexical information for the PM.	PM	Priority Model	1
Second, these two models learn  name patterns in different ways, i.e., semantic rela- tionships for the Semantic Model and positional  and lexical information for the PM.	PM	phrasometer	0
First, the Semantic Model and the  PM do not use previous gold-standard  sets for training.	PM	parenthesized material	0
First, the Semantic Model and the  PM do not use previous gold-standard  sets for training.	PM	prosody model	0
First, the Semantic Model and the  PM do not use previous gold-standard  sets for training.	PM	partial  match	0
First, the Semantic Model and the  PM do not use previous gold-standard  sets for training.	PM	Priority Model	1
First, the Semantic Model and the  PM do not use previous gold-standard  sets for training.	PM	phrasometer	0
These phrases are then evaluated using the  PM.	PM	parenthesized material	0
These phrases are then evaluated using the  PM.	PM	prosody model	0
These phrases are then evaluated using the  PM.	PM	partial  match	0
These phrases are then evaluated using the  PM.	PM	Priority Model	1
These phrases are then evaluated using the  PM.	PM	phrasometer	0
2.4 PM  The Semantic Model detects four different catego- ries for a single word.	PM	parenthesized material	0
2.4 PM  The Semantic Model detects four different catego- ries for a single word.	PM	prosody model	0
2.4 PM  The Semantic Model detects four different catego- ries for a single word.	PM	partial  match	0
2.4 PM  The Semantic Model detects four different catego- ries for a single word.	PM	Priority Model	1
2.4 PM  The Semantic Model detects four different catego- ries for a single word.	PM	phrasometer	0
The PM is a statistical language  model for named entity recognition (Tanabe and  Wilbur, 2006).	PM	parenthesized material	0
The PM is a statistical language  model for named entity recognition (Tanabe and  Wilbur, 2006).	PM	prosody model	0
The PM is a statistical language  model for named entity recognition (Tanabe and  Wilbur, 2006).	PM	partial  match	0
The PM is a statistical language  model for named entity recognition (Tanabe and  Wilbur, 2006).	PM	Priority Model	1
The PM is a statistical language  model for named entity recognition (Tanabe and  Wilbur, 2006).	PM	phrasometer	0
However, the PM  captures gene name patterns by analyzing the order  of words and the character strings making up  words.	PM	parenthesized material	0
However, the PM  captures gene name patterns by analyzing the order  of words and the character strings making up  words.	PM	prosody model	0
However, the PM  captures gene name patterns by analyzing the order  of words and the character strings making up  words.	PM	partial  match	0
However, the PM  captures gene name patterns by analyzing the order  of words and the character strings making up  words.	PM	Priority Model	1
However, the PM  captures gene name patterns by analyzing the order  of words and the character strings making up  words.	PM	phrasometer	0
In our experiments we trained DSSMs using  mini-batch SGD.	SGD	Stochastic Gradient Descent	1
In our experiments we trained DSSMs using  mini-batch SGD.	SGD	stochastic gradient descent	0
k?3?i?k {f 1 , f 2 , f 3 } Bernoulli-Bernoulli RBM was applied to pre- train DNNs and SGD with cross-entropy criterion to fine-tune DNNs.	SGD	Stochastic Gradient Descent	1
k?3?i?k {f 1 , f 2 , f 3 } Bernoulli-Bernoulli RBM was applied to pre- train DNNs and SGD with cross-entropy criterion to fine-tune DNNs.	SGD	stochastic gradient descent	0
SGD Training for L1-regularized Log-linear Models with Cumulative Penalty.	SGD	Stochastic Gradient Descent	1
SGD Training for L1-regularized Log-linear Models with Cumulative Penalty.	SGD	stochastic gradient descent	0
GU-MLT-LT:  Sentiment Analysis of Short Messages using Lin- guistic Features and SGD.	SGD	Stochastic Gradient Descent	1
GU-MLT-LT:  Sentiment Analysis of Short Messages using Lin- guistic Features and SGD.	SGD	stochastic gradient descent	0
3 SGD For M3N optimization, Taskar et al (2003) has proposed a reparametrization of the dual variables to take advantage of the network structure of the labeling sequence problem.	SGD	Stochastic Gradient Descent	1
3 SGD For M3N optimization, Taskar et al (2003) has proposed a reparametrization of the dual variables to take advantage of the network structure of the labeling sequence problem.	SGD	stochastic gradient descent	0
The opti- mum has been found using the online method (SGD).	SGD	Stochastic Gradient Descent	1
The opti- mum has been found using the online method (SGD).	SGD	stochastic gradient descent	0
In particular, we developed an efficient parallelized implementation of our SGD algorithm using the message-passing interface (MPI).	SGD	Stochastic Gradient Descent	0
In particular, we developed an efficient parallelized implementation of our SGD algorithm using the message-passing interface (MPI).	SGD	stochastic gradient descent	1
The SGD is adopted to optimize the parameters.	SGD	Stochastic Gradient Descent	0
The SGD is adopted to optimize the parameters.	SGD	stochastic gradient descent	1
Large-scale machine learning with SGD.	SGD	Stochastic Gradient Descent	0
Large-scale machine learning with SGD.	SGD	stochastic gradient descent	1
Training is performed with SGD by per- forming a gradient step against the violating tag.	SGD	Stochastic Gradient Descent	0
Training is performed with SGD by per- forming a gradient step against the violating tag.	SGD	stochastic gradient descent	1
2010) and SGD code from deeplearning.net/tutorial (Bengio, 2009).	SGD	Stochastic Gradient Descent	0
2010) and SGD code from deeplearning.net/tutorial (Bengio, 2009).	SGD	stochastic gradient descent	1
i is updated using SGD.	SGD	Stochastic Gradient Descent	0
i is updated using SGD.	SGD	stochastic gradient descent	1
Parsing goes basically bottom-up with top-down  confirmation, improving the so called LC  technique.	LC	Left Corner	1
Parsing goes basically bottom-up with top-down  confirmation, improving the so called LC  technique.	LC	Lexical Cohesion	0
Parsing goes basically bottom-up with top-down  confirmation, improving the so called LC  technique.	LC	less computerized	0
Parsing goes basically bottom-up with top-down  confirmation, improving the so called LC  technique.	LC	lexical cohesion	0
Parsing goes basically bottom-up with top-down  confirmation, improving the so called LC  technique.	LC	low constant	0
Parsing goes basically bottom-up with top-down  confirmation, improving the so called LC  technique.	LC	Lexical Classifier	0
Parsing goes basically bottom-up with top-down  confirmation, improving the so called LC  technique.	LC	Left Context	0
LC Transforms and Fi-  nite State Approximations.	LC	Left Corner	1
LC Transforms and Fi-  nite State Approximations.	LC	Lexical Cohesion	0
LC Transforms and Fi-  nite State Approximations.	LC	less computerized	0
LC Transforms and Fi-  nite State Approximations.	LC	lexical cohesion	0
LC Transforms and Fi-  nite State Approximations.	LC	low constant	0
LC Transforms and Fi-  nite State Approximations.	LC	Lexical Classifier	0
LC Transforms and Fi-  nite State Approximations.	LC	Left Context	0
One dimension in this space is represented by the parsing algorithm used: For example, within the framework of Generalized LC Pars- ing (Demers, 1977), algorithms can be char- acterized in terms of the point at which a context-free rule is recognized, in relation to the recognition-point of the symbols on its right- hand side.	LC	Left Corner	1
One dimension in this space is represented by the parsing algorithm used: For example, within the framework of Generalized LC Pars- ing (Demers, 1977), algorithms can be char- acterized in terms of the point at which a context-free rule is recognized, in relation to the recognition-point of the symbols on its right- hand side.	LC	Lexical Cohesion	0
One dimension in this space is represented by the parsing algorithm used: For example, within the framework of Generalized LC Pars- ing (Demers, 1977), algorithms can be char- acterized in terms of the point at which a context-free rule is recognized, in relation to the recognition-point of the symbols on its right- hand side.	LC	less computerized	0
One dimension in this space is represented by the parsing algorithm used: For example, within the framework of Generalized LC Pars- ing (Demers, 1977), algorithms can be char- acterized in terms of the point at which a context-free rule is recognized, in relation to the recognition-point of the symbols on its right- hand side.	LC	lexical cohesion	0
One dimension in this space is represented by the parsing algorithm used: For example, within the framework of Generalized LC Pars- ing (Demers, 1977), algorithms can be char- acterized in terms of the point at which a context-free rule is recognized, in relation to the recognition-point of the symbols on its right- hand side.	LC	low constant	0
One dimension in this space is represented by the parsing algorithm used: For example, within the framework of Generalized LC Pars- ing (Demers, 1977), algorithms can be char- acterized in terms of the point at which a context-free rule is recognized, in relation to the recognition-point of the symbols on its right- hand side.	LC	Lexical Classifier	0
One dimension in this space is represented by the parsing algorithm used: For example, within the framework of Generalized LC Pars- ing (Demers, 1977), algorithms can be char- acterized in terms of the point at which a context-free rule is recognized, in relation to the recognition-point of the symbols on its right- hand side.	LC	Left Context	0
Rosenkrantz, D. J / P. M. Lewis (1970) l)e-  terministic LC Parser, IEEE Con\[er-  ence Record of the l l th Annual Symposium on  Switching and Automata Theory, 139-152.	LC	Left Corner	1
Rosenkrantz, D. J / P. M. Lewis (1970) l)e-  terministic LC Parser, IEEE Con\[er-  ence Record of the l l th Annual Symposium on  Switching and Automata Theory, 139-152.	LC	Lexical Cohesion	0
Rosenkrantz, D. J / P. M. Lewis (1970) l)e-  terministic LC Parser, IEEE Con\[er-  ence Record of the l l th Annual Symposium on  Switching and Automata Theory, 139-152.	LC	less computerized	0
Rosenkrantz, D. J / P. M. Lewis (1970) l)e-  terministic LC Parser, IEEE Con\[er-  ence Record of the l l th Annual Symposium on  Switching and Automata Theory, 139-152.	LC	lexical cohesion	0
Rosenkrantz, D. J / P. M. Lewis (1970) l)e-  terministic LC Parser, IEEE Con\[er-  ence Record of the l l th Annual Symposium on  Switching and Automata Theory, 139-152.	LC	low constant	0
Rosenkrantz, D. J / P. M. Lewis (1970) l)e-  terministic LC Parser, IEEE Con\[er-  ence Record of the l l th Annual Symposium on  Switching and Automata Theory, 139-152.	LC	Lexical Classifier	0
Rosenkrantz, D. J / P. M. Lewis (1970) l)e-  terministic LC Parser, IEEE Con\[er-  ence Record of the l l th Annual Symposium on  Switching and Automata Theory, 139-152.	LC	Left Context	0
IS_A_LEFT_CORNER(  \[Real_LC Cat,Structure\],  \[Real GoalCat,Structurel,   Input Str ing,RestStr ing  /* reflexive closure of the relation "being a left  corne r" * /  is_a_leftcorner(   \ [Rea iLe f tCornerCategory ,   Rea lLe f tCornerCategory_St ructure \ ] ,   \[Real GoalCategory,RealGoal_Category_Structure\] ,   String,String )  uni fy(RealLeftCorner_Category,   Real_Goal_Category ) .	LC	Left Corner	1
IS_A_LEFT_CORNER(  \[Real_LC Cat,Structure\],  \[Real GoalCat,Structurel,   Input Str ing,RestStr ing  /* reflexive closure of the relation "being a left  corne r" * /  is_a_leftcorner(   \ [Rea iLe f tCornerCategory ,   Rea lLe f tCornerCategory_St ructure \ ] ,   \[Real GoalCategory,RealGoal_Category_Structure\] ,   String,String )  uni fy(RealLeftCorner_Category,   Real_Goal_Category ) .	LC	Lexical Cohesion	0
IS_A_LEFT_CORNER(  \[Real_LC Cat,Structure\],  \[Real GoalCat,Structurel,   Input Str ing,RestStr ing  /* reflexive closure of the relation "being a left  corne r" * /  is_a_leftcorner(   \ [Rea iLe f tCornerCategory ,   Rea lLe f tCornerCategory_St ructure \ ] ,   \[Real GoalCategory,RealGoal_Category_Structure\] ,   String,String )  uni fy(RealLeftCorner_Category,   Real_Goal_Category ) .	LC	less computerized	0
IS_A_LEFT_CORNER(  \[Real_LC Cat,Structure\],  \[Real GoalCat,Structurel,   Input Str ing,RestStr ing  /* reflexive closure of the relation "being a left  corne r" * /  is_a_leftcorner(   \ [Rea iLe f tCornerCategory ,   Rea lLe f tCornerCategory_St ructure \ ] ,   \[Real GoalCategory,RealGoal_Category_Structure\] ,   String,String )  uni fy(RealLeftCorner_Category,   Real_Goal_Category ) .	LC	lexical cohesion	0
IS_A_LEFT_CORNER(  \[Real_LC Cat,Structure\],  \[Real GoalCat,Structurel,   Input Str ing,RestStr ing  /* reflexive closure of the relation "being a left  corne r" * /  is_a_leftcorner(   \ [Rea iLe f tCornerCategory ,   Rea lLe f tCornerCategory_St ructure \ ] ,   \[Real GoalCategory,RealGoal_Category_Structure\] ,   String,String )  uni fy(RealLeftCorner_Category,   Real_Goal_Category ) .	LC	low constant	0
IS_A_LEFT_CORNER(  \[Real_LC Cat,Structure\],  \[Real GoalCat,Structurel,   Input Str ing,RestStr ing  /* reflexive closure of the relation "being a left  corne r" * /  is_a_leftcorner(   \ [Rea iLe f tCornerCategory ,   Rea lLe f tCornerCategory_St ructure \ ] ,   \[Real GoalCategory,RealGoal_Category_Structure\] ,   String,String )  uni fy(RealLeftCorner_Category,   Real_Goal_Category ) .	LC	Lexical Classifier	0
IS_A_LEFT_CORNER(  \[Real_LC Cat,Structure\],  \[Real GoalCat,Structurel,   Input Str ing,RestStr ing  /* reflexive closure of the relation "being a left  corne r" * /  is_a_leftcorner(   \ [Rea iLe f tCornerCategory ,   Rea lLe f tCornerCategory_St ructure \ ] ,   \[Real GoalCategory,RealGoal_Category_Structure\] ,   String,String )  uni fy(RealLeftCorner_Category,   Real_Goal_Category ) .	LC	Left Context	0
These include well stud- ied grammars such as Hierarchical Phrase Structure Grammars and Combinatory Categorial Grammars, and transforms that rearrange the tree such as the LC Transform used in Roark and Johnson (1999).	LC	Left Corner	1
These include well stud- ied grammars such as Hierarchical Phrase Structure Grammars and Combinatory Categorial Grammars, and transforms that rearrange the tree such as the LC Transform used in Roark and Johnson (1999).	LC	Lexical Cohesion	0
These include well stud- ied grammars such as Hierarchical Phrase Structure Grammars and Combinatory Categorial Grammars, and transforms that rearrange the tree such as the LC Transform used in Roark and Johnson (1999).	LC	less computerized	0
These include well stud- ied grammars such as Hierarchical Phrase Structure Grammars and Combinatory Categorial Grammars, and transforms that rearrange the tree such as the LC Transform used in Roark and Johnson (1999).	LC	lexical cohesion	0
These include well stud- ied grammars such as Hierarchical Phrase Structure Grammars and Combinatory Categorial Grammars, and transforms that rearrange the tree such as the LC Transform used in Roark and Johnson (1999).	LC	low constant	0
These include well stud- ied grammars such as Hierarchical Phrase Structure Grammars and Combinatory Categorial Grammars, and transforms that rearrange the tree such as the LC Transform used in Roark and Johnson (1999).	LC	Lexical Classifier	0
These include well stud- ied grammars such as Hierarchical Phrase Structure Grammars and Combinatory Categorial Grammars, and transforms that rearrange the tree such as the LC Transform used in Roark and Johnson (1999).	LC	Left Context	0
c?2007 Association for Computational Linguistics UofL: Word Sense Disambiguation Using LC  Yllias Chali           Department of Computer Science  University of Lethbridge   Lethbridge, Alberta, Canada, T1K 3M4  chali@cs.uleth.ca  Shafiq R. Joty  Department of Computer Science  University of Lethbridge   Lethbridge, Alberta, Canada, T1K 3M4  jotys@cs.uleth.ca      Abstract  One of the main challenges in the applica- tions (i.e.: text summarization, question an- swering,	LC	Left Corner	0
c?2007 Association for Computational Linguistics UofL: Word Sense Disambiguation Using LC  Yllias Chali           Department of Computer Science  University of Lethbridge   Lethbridge, Alberta, Canada, T1K 3M4  chali@cs.uleth.ca  Shafiq R. Joty  Department of Computer Science  University of Lethbridge   Lethbridge, Alberta, Canada, T1K 3M4  jotys@cs.uleth.ca      Abstract  One of the main challenges in the applica- tions (i.e.: text summarization, question an- swering,	LC	Lexical Cohesion	1
c?2007 Association for Computational Linguistics UofL: Word Sense Disambiguation Using LC  Yllias Chali           Department of Computer Science  University of Lethbridge   Lethbridge, Alberta, Canada, T1K 3M4  chali@cs.uleth.ca  Shafiq R. Joty  Department of Computer Science  University of Lethbridge   Lethbridge, Alberta, Canada, T1K 3M4  jotys@cs.uleth.ca      Abstract  One of the main challenges in the applica- tions (i.e.: text summarization, question an- swering,	LC	less computerized	0
c?2007 Association for Computational Linguistics UofL: Word Sense Disambiguation Using LC  Yllias Chali           Department of Computer Science  University of Lethbridge   Lethbridge, Alberta, Canada, T1K 3M4  chali@cs.uleth.ca  Shafiq R. Joty  Department of Computer Science  University of Lethbridge   Lethbridge, Alberta, Canada, T1K 3M4  jotys@cs.uleth.ca      Abstract  One of the main challenges in the applica- tions (i.e.: text summarization, question an- swering,	LC	lexical cohesion	0
c?2007 Association for Computational Linguistics UofL: Word Sense Disambiguation Using LC  Yllias Chali           Department of Computer Science  University of Lethbridge   Lethbridge, Alberta, Canada, T1K 3M4  chali@cs.uleth.ca  Shafiq R. Joty  Department of Computer Science  University of Lethbridge   Lethbridge, Alberta, Canada, T1K 3M4  jotys@cs.uleth.ca      Abstract  One of the main challenges in the applica- tions (i.e.: text summarization, question an- swering,	LC	low constant	0
c?2007 Association for Computational Linguistics UofL: Word Sense Disambiguation Using LC  Yllias Chali           Department of Computer Science  University of Lethbridge   Lethbridge, Alberta, Canada, T1K 3M4  chali@cs.uleth.ca  Shafiq R. Joty  Department of Computer Science  University of Lethbridge   Lethbridge, Alberta, Canada, T1K 3M4  jotys@cs.uleth.ca      Abstract  One of the main challenges in the applica- tions (i.e.: text summarization, question an- swering,	LC	Lexical Classifier	0
c?2007 Association for Computational Linguistics UofL: Word Sense Disambiguation Using LC  Yllias Chali           Department of Computer Science  University of Lethbridge   Lethbridge, Alberta, Canada, T1K 3M4  chali@cs.uleth.ca  Shafiq R. Joty  Department of Computer Science  University of Lethbridge   Lethbridge, Alberta, Canada, T1K 3M4  jotys@cs.uleth.ca      Abstract  One of the main challenges in the applica- tions (i.e.: text summarization, question an- swering,	LC	Left Context	0
Morris J. and Hirst G. (1991) "LC  Computed by Thesaural Relations as an  Indicator of the Structure of Text."	LC	Left Corner	0
Morris J. and Hirst G. (1991) "LC  Computed by Thesaural Relations as an  Indicator of the Structure of Text."	LC	Lexical Cohesion	1
Morris J. and Hirst G. (1991) "LC  Computed by Thesaural Relations as an  Indicator of the Structure of Text."	LC	less computerized	0
Morris J. and Hirst G. (1991) "LC  Computed by Thesaural Relations as an  Indicator of the Structure of Text."	LC	lexical cohesion	0
Morris J. and Hirst G. (1991) "LC  Computed by Thesaural Relations as an  Indicator of the Structure of Text."	LC	low constant	0
Morris J. and Hirst G. (1991) "LC  Computed by Thesaural Relations as an  Indicator of the Structure of Text."	LC	Lexical Classifier	0
Morris J. and Hirst G. (1991) "LC  Computed by Thesaural Relations as an  Indicator of the Structure of Text."	LC	Left Context	0
LC Com-  puted by Thesaural Relations as an Indicator of the  Structure of Text.	LC	Left Corner	0
LC Com-  puted by Thesaural Relations as an Indicator of the  Structure of Text.	LC	Lexical Cohesion	1
LC Com-  puted by Thesaural Relations as an Indicator of the  Structure of Text.	LC	less computerized	0
LC Com-  puted by Thesaural Relations as an Indicator of the  Structure of Text.	LC	lexical cohesion	0
LC Com-  puted by Thesaural Relations as an Indicator of the  Structure of Text.	LC	low constant	0
LC Com-  puted by Thesaural Relations as an Indicator of the  Structure of Text.	LC	Lexical Classifier	0
LC Com-  puted by Thesaural Relations as an Indicator of the  Structure of Text.	LC	Left Context	0
G. 1991, LC  Computed by Thesaural Relations as an Indica- tor of the Structure of Text .Computational Lin- guistics, 17(1):21-48.	LC	Left Corner	0
G. 1991, LC  Computed by Thesaural Relations as an Indica- tor of the Structure of Text .Computational Lin- guistics, 17(1):21-48.	LC	Lexical Cohesion	1
G. 1991, LC  Computed by Thesaural Relations as an Indica- tor of the Structure of Text .Computational Lin- guistics, 17(1):21-48.	LC	less computerized	0
G. 1991, LC  Computed by Thesaural Relations as an Indica- tor of the Structure of Text .Computational Lin- guistics, 17(1):21-48.	LC	lexical cohesion	0
G. 1991, LC  Computed by Thesaural Relations as an Indica- tor of the Structure of Text .Computational Lin- guistics, 17(1):21-48.	LC	low constant	0
G. 1991, LC  Computed by Thesaural Relations as an Indica- tor of the Structure of Text .Computational Lin- guistics, 17(1):21-48.	LC	Lexical Classifier	0
G. 1991, LC  Computed by Thesaural Relations as an Indica- tor of the Structure of Text .Computational Lin- guistics, 17(1):21-48.	LC	Left Context	0
Complexity .983* 1.404*  LC -.266* -.440*  Interactive/Conv.	LC	Left Corner	0
Complexity .983* 1.404*  LC -.266* -.440*  Interactive/Conv.	LC	Lexical Cohesion	1
Complexity .983* 1.404*  LC -.266* -.440*  Interactive/Conv.	LC	less computerized	0
Complexity .983* 1.404*  LC -.266* -.440*  Interactive/Conv.	LC	lexical cohesion	0
Complexity .983* 1.404*  LC -.266* -.440*  Interactive/Conv.	LC	low constant	0
Complexity .983* 1.404*  LC -.266* -.440*  Interactive/Conv.	LC	Lexical Classifier	0
Complexity .983* 1.404*  LC -.266* -.440*  Interactive/Conv.	LC	Left Context	0
c?2013 Association for Computational Linguistics Bilingual LC Trigger Model for Document-Level Machine Translation Guosheng Ben?	LC	Left Corner	0
c?2013 Association for Computational Linguistics Bilingual LC Trigger Model for Document-Level Machine Translation Guosheng Ben?	LC	Lexical Cohesion	1
c?2013 Association for Computational Linguistics Bilingual LC Trigger Model for Document-Level Machine Translation Guosheng Ben?	LC	less computerized	0
c?2013 Association for Computational Linguistics Bilingual LC Trigger Model for Document-Level Machine Translation Guosheng Ben?	LC	lexical cohesion	0
c?2013 Association for Computational Linguistics Bilingual LC Trigger Model for Document-Level Machine Translation Guosheng Ben?	LC	low constant	0
c?2013 Association for Computational Linguistics Bilingual LC Trigger Model for Document-Level Machine Translation Guosheng Ben?	LC	Lexical Classifier	0
c?2013 Association for Computational Linguistics Bilingual LC Trigger Model for Document-Level Machine Translation Guosheng Ben?	LC	Left Context	0
To  relate the language resources among the LC languages has brought us to the following  open questions:    1.	LC	Left Corner	0
To  relate the language resources among the LC languages has brought us to the following  open questions:    1.	LC	Lexical Cohesion	0
To  relate the language resources among the LC languages has brought us to the following  open questions:    1.	LC	less computerized	1
To  relate the language resources among the LC languages has brought us to the following  open questions:    1.	LC	lexical cohesion	0
To  relate the language resources among the LC languages has brought us to the following  open questions:    1.	LC	low constant	0
To  relate the language resources among the LC languages has brought us to the following  open questions:    1.	LC	Lexical Classifier	0
To  relate the language resources among the LC languages has brought us to the following  open questions:    1.	LC	Left Context	0
t for encodings, as many of these LC languages do not have one stan- dard encoding that is used by all.	LC	Left Corner	0
t for encodings, as many of these LC languages do not have one stan- dard encoding that is used by all.	LC	Lexical Cohesion	0
t for encodings, as many of these LC languages do not have one stan- dard encoding that is used by all.	LC	less computerized	1
t for encodings, as many of these LC languages do not have one stan- dard encoding that is used by all.	LC	lexical cohesion	0
t for encodings, as many of these LC languages do not have one stan- dard encoding that is used by all.	LC	low constant	0
t for encodings, as many of these LC languages do not have one stan- dard encoding that is used by all.	LC	Lexical Classifier	0
t for encodings, as many of these LC languages do not have one stan- dard encoding that is used by all.	LC	Left Context	0
The problem is magnified when we need to deal with the LC lan- guages.	LC	Left Corner	0
The problem is magnified when we need to deal with the LC lan- guages.	LC	Lexical Cohesion	0
The problem is magnified when we need to deal with the LC lan- guages.	LC	less computerized	1
The problem is magnified when we need to deal with the LC lan- guages.	LC	lexical cohesion	0
The problem is magnified when we need to deal with the LC lan- guages.	LC	low constant	0
The problem is magnified when we need to deal with the LC lan- guages.	LC	Lexical Classifier	0
The problem is magnified when we need to deal with the LC lan- guages.	LC	Left Context	0
In this paper, we will present a method of providing language and encod- ing support for LC languages.	LC	Left Corner	0
In this paper, we will present a method of providing language and encod- ing support for LC languages.	LC	Lexical Cohesion	0
In this paper, we will present a method of providing language and encod- ing support for LC languages.	LC	less computerized	1
In this paper, we will present a method of providing language and encod- ing support for LC languages.	LC	lexical cohesion	0
In this paper, we will present a method of providing language and encod- ing support for LC languages.	LC	low constant	0
In this paper, we will present a method of providing language and encod- ing support for LC languages.	LC	Lexical Classifier	0
In this paper, we will present a method of providing language and encod- ing support for LC languages.	LC	Left Context	0
The major concern in the  LC languages is how to leverage the technology for those languages which will result in  scaling up the number of online language populations.	LC	Left Corner	0
The major concern in the  LC languages is how to leverage the technology for those languages which will result in  scaling up the number of online language populations.	LC	Lexical Cohesion	0
The major concern in the  LC languages is how to leverage the technology for those languages which will result in  scaling up the number of online language populations.	LC	less computerized	1
The major concern in the  LC languages is how to leverage the technology for those languages which will result in  scaling up the number of online language populations.	LC	lexical cohesion	0
The major concern in the  LC languages is how to leverage the technology for those languages which will result in  scaling up the number of online language populations.	LC	low constant	0
The major concern in the  LC languages is how to leverage the technology for those languages which will result in  scaling up the number of online language populations.	LC	Lexical Classifier	0
The major concern in the  LC languages is how to leverage the technology for those languages which will result in  scaling up the number of online language populations.	LC	Left Context	0
A related problem is that of support for encodings, as many of these LC languages do not have one stan- dard encoding that is used by all.	LC	Left Corner	0
A related problem is that of support for encodings, as many of these LC languages do not have one stan- dard encoding that is used by all.	LC	Lexical Cohesion	0
A related problem is that of support for encodings, as many of these LC languages do not have one stan- dard encoding that is used by all.	LC	less computerized	1
A related problem is that of support for encodings, as many of these LC languages do not have one stan- dard encoding that is used by all.	LC	lexical cohesion	0
A related problem is that of support for encodings, as many of these LC languages do not have one stan- dard encoding that is used by all.	LC	low constant	0
A related problem is that of support for encodings, as many of these LC languages do not have one stan- dard encoding that is used by all.	LC	Lexical Classifier	0
A related problem is that of support for encodings, as many of these LC languages do not have one stan- dard encoding that is used by all.	LC	Left Context	0
We then describe an ed- itor called Sanchay Editor, which uses this method and also has many other facilities useful for those using LC lan- guages for simple text editing or for Nat- ural Language Processing purposes, espe- cially for annotation.	LC	Left Corner	0
We then describe an ed- itor called Sanchay Editor, which uses this method and also has many other facilities useful for those using LC lan- guages for simple text editing or for Nat- ural Language Processing purposes, espe- cially for annotation.	LC	Lexical Cohesion	0
We then describe an ed- itor called Sanchay Editor, which uses this method and also has many other facilities useful for those using LC lan- guages for simple text editing or for Nat- ural Language Processing purposes, espe- cially for annotation.	LC	less computerized	1
We then describe an ed- itor called Sanchay Editor, which uses this method and also has many other facilities useful for those using LC lan- guages for simple text editing or for Nat- ural Language Processing purposes, espe- cially for annotation.	LC	lexical cohesion	0
We then describe an ed- itor called Sanchay Editor, which uses this method and also has many other facilities useful for those using LC lan- guages for simple text editing or for Nat- ural Language Processing purposes, espe- cially for annotation.	LC	low constant	0
We then describe an ed- itor called Sanchay Editor, which uses this method and also has many other facilities useful for those using LC lan- guages for simple text editing or for Nat- ural Language Processing purposes, espe- cially for annotation.	LC	Lexical Classifier	0
We then describe an ed- itor called Sanchay Editor, which uses this method and also has many other facilities useful for those using LC lan- guages for simple text editing or for Nat- ural Language Processing purposes, espe- cially for annotation.	LC	Left Context	0
r between words works as an indi-  cator of the LC.	LC	Left Corner	0
r between words works as an indi-  cator of the LC.	LC	Lexical Cohesion	0
r between words works as an indi-  cator of the LC.	LC	less computerized	0
r between words works as an indi-  cator of the LC.	LC	lexical cohesion	1
r between words works as an indi-  cator of the LC.	LC	low constant	0
r between words works as an indi-  cator of the LC.	LC	Lexical Classifier	0
r between words works as an indi-  cator of the LC.	LC	Left Context	0
This is because c(X) is based only on the  LC of the words in X.  6 Discussion  The structure of Paradigme represents the knowl-  edge system of English, and an activated state pro-  duced on it represents word meaning.	LC	Left Corner	0
This is because c(X) is based only on the  LC of the words in X.  6 Discussion  The structure of Paradigme represents the knowl-  edge system of English, and an activated state pro-  duced on it represents word meaning.	LC	Lexical Cohesion	0
This is because c(X) is based only on the  LC of the words in X.  6 Discussion  The structure of Paradigme represents the knowl-  edge system of English, and an activated state pro-  duced on it represents word meaning.	LC	less computerized	0
This is because c(X) is based only on the  LC of the words in X.  6 Discussion  The structure of Paradigme represents the knowl-  edge system of English, and an activated state pro-  duced on it represents word meaning.	LC	lexical cohesion	1
This is because c(X) is based only on the  LC of the words in X.  6 Discussion  The structure of Paradigme represents the knowl-  edge system of English, and an activated state pro-  duced on it represents word meaning.	LC	low constant	0
This is because c(X) is based only on the  LC of the words in X.  6 Discussion  The structure of Paradigme represents the knowl-  edge system of English, and an activated state pro-  duced on it represents word meaning.	LC	Lexical Classifier	0
This is because c(X) is based only on the  LC of the words in X.  6 Discussion  The structure of Paradigme represents the knowl-  edge system of English, and an activated state pro-  duced on it represents word meaning.	LC	Left Context	0
The similarity rep-  resents the strength of LC or  semantic relation, and also provides valu-  able information about similarity and co-  herence of texts.	LC	Left Corner	0
The similarity rep-  resents the strength of LC or  semantic relation, and also provides valu-  able information about similarity and co-  herence of texts.	LC	Lexical Cohesion	0
The similarity rep-  resents the strength of LC or  semantic relation, and also provides valu-  able information about similarity and co-  herence of texts.	LC	less computerized	0
The similarity rep-  resents the strength of LC or  semantic relation, and also provides valu-  able information about similarity and co-  herence of texts.	LC	lexical cohesion	1
The similarity rep-  resents the strength of LC or  semantic relation, and also provides valu-  able information about similarity and co-  herence of texts.	LC	low constant	0
The similarity rep-  resents the strength of LC or  semantic relation, and also provides valu-  able information about similarity and co-  herence of texts.	LC	Lexical Classifier	0
The similarity rep-  resents the strength of LC or  semantic relation, and also provides valu-  able information about similarity and co-  herence of texts.	LC	Left Context	0
Recogniz-  ing the structure of text is an essential task in text  understanding.\[Grosz andSidner, 1986\]  One of the valuable indicators of the structure of  text is LC.\[Halliday nd Hasan, 1976\]  Lexical cohesion is the relationship between words,  classified as follows:  1.	LC	Left Corner	0
Recogniz-  ing the structure of text is an essential task in text  understanding.\[Grosz andSidner, 1986\]  One of the valuable indicators of the structure of  text is LC.\[Halliday nd Hasan, 1976\]  Lexical cohesion is the relationship between words,  classified as follows:  1.	LC	Lexical Cohesion	0
Recogniz-  ing the structure of text is an essential task in text  understanding.\[Grosz andSidner, 1986\]  One of the valuable indicators of the structure of  text is LC.\[Halliday nd Hasan, 1976\]  Lexical cohesion is the relationship between words,  classified as follows:  1.	LC	less computerized	0
Recogniz-  ing the structure of text is an essential task in text  understanding.\[Grosz andSidner, 1986\]  One of the valuable indicators of the structure of  text is LC.\[Halliday nd Hasan, 1976\]  Lexical cohesion is the relationship between words,  classified as follows:  1.	LC	lexical cohesion	1
Recogniz-  ing the structure of text is an essential task in text  understanding.\[Grosz andSidner, 1986\]  One of the valuable indicators of the structure of  text is LC.\[Halliday nd Hasan, 1976\]  Lexical cohesion is the relationship between words,  classified as follows:  1.	LC	low constant	0
Recogniz-  ing the structure of text is an essential task in text  understanding.\[Grosz andSidner, 1986\]  One of the valuable indicators of the structure of  text is LC.\[Halliday nd Hasan, 1976\]  Lexical cohesion is the relationship between words,  classified as follows:  1.	LC	Lexical Classifier	0
Recogniz-  ing the structure of text is an essential task in text  understanding.\[Grosz andSidner, 1986\]  One of the valuable indicators of the structure of  text is LC.\[Halliday nd Hasan, 1976\]  Lexical cohesion is the relationship between words,  classified as follows:  1.	LC	Left Context	0
We consider LC as semantic similarity  between words.	LC	Left Corner	0
We consider LC as semantic similarity  between words.	LC	Lexical Cohesion	0
We consider LC as semantic similarity  between words.	LC	less computerized	0
We consider LC as semantic similarity  between words.	LC	lexical cohesion	1
We consider LC as semantic similarity  between words.	LC	low constant	0
We consider LC as semantic similarity  between words.	LC	Lexical Classifier	0
We consider LC as semantic similarity  between words.	LC	Left Context	0
2009 ACL and AFNLP A Cohesion Graph Based Approach for Unsupervised Recognition of Literal and Non-literal Use of Multiword Expressions Linlin Li and Caroline Sporleder Saarland University Postfach 15 11 50 66041 Saarbr?ucken Germany {linlin,csporled}@coli.uni-saarland.de Abstract We present a graph-based model for rep- resenting the LC of a dis- course.	LC	Left Corner	0
2009 ACL and AFNLP A Cohesion Graph Based Approach for Unsupervised Recognition of Literal and Non-literal Use of Multiword Expressions Linlin Li and Caroline Sporleder Saarland University Postfach 15 11 50 66041 Saarbr?ucken Germany {linlin,csporled}@coli.uni-saarland.de Abstract We present a graph-based model for rep- resenting the LC of a dis- course.	LC	Lexical Cohesion	0
2009 ACL and AFNLP A Cohesion Graph Based Approach for Unsupervised Recognition of Literal and Non-literal Use of Multiword Expressions Linlin Li and Caroline Sporleder Saarland University Postfach 15 11 50 66041 Saarbr?ucken Germany {linlin,csporled}@coli.uni-saarland.de Abstract We present a graph-based model for rep- resenting the LC of a dis- course.	LC	less computerized	0
2009 ACL and AFNLP A Cohesion Graph Based Approach for Unsupervised Recognition of Literal and Non-literal Use of Multiword Expressions Linlin Li and Caroline Sporleder Saarland University Postfach 15 11 50 66041 Saarbr?ucken Germany {linlin,csporled}@coli.uni-saarland.de Abstract We present a graph-based model for rep- resenting the LC of a dis- course.	LC	lexical cohesion	1
2009 ACL and AFNLP A Cohesion Graph Based Approach for Unsupervised Recognition of Literal and Non-literal Use of Multiword Expressions Linlin Li and Caroline Sporleder Saarland University Postfach 15 11 50 66041 Saarbr?ucken Germany {linlin,csporled}@coli.uni-saarland.de Abstract We present a graph-based model for rep- resenting the LC of a dis- course.	LC	low constant	0
2009 ACL and AFNLP A Cohesion Graph Based Approach for Unsupervised Recognition of Literal and Non-literal Use of Multiword Expressions Linlin Li and Caroline Sporleder Saarland University Postfach 15 11 50 66041 Saarbr?ucken Germany {linlin,csporled}@coli.uni-saarland.de Abstract We present a graph-based model for rep- resenting the LC of a dis- course.	LC	Lexical Classifier	0
2009 ACL and AFNLP A Cohesion Graph Based Approach for Unsupervised Recognition of Literal and Non-literal Use of Multiword Expressions Linlin Li and Caroline Sporleder Saarland University Postfach 15 11 50 66041 Saarbr?ucken Germany {linlin,csporled}@coli.uni-saarland.de Abstract We present a graph-based model for rep- resenting the LC of a dis- course.	LC	Left Context	0
Since r tends to be quite small and can be bounded by a LC, this already gives polynomial time complexity.	LC	Left Corner	0
Since r tends to be quite small and can be bounded by a LC, this already gives polynomial time complexity.	LC	Lexical Cohesion	0
Since r tends to be quite small and can be bounded by a LC, this already gives polynomial time complexity.	LC	less computerized	0
Since r tends to be quite small and can be bounded by a LC, this already gives polynomial time complexity.	LC	lexical cohesion	0
Since r tends to be quite small and can be bounded by a LC, this already gives polynomial time complexity.	LC	low constant	1
Since r tends to be quite small and can be bounded by a LC, this already gives polynomial time complexity.	LC	Lexical Classifier	0
Since r tends to be quite small and can be bounded by a LC, this already gives polynomial time complexity.	LC	Left Context	0
Since that the size of the right-hand-side and the number of outgoing d-edges per node are practically bounded by LCs, applying k rules on a tree T yields a linear increase in the size of the forest.	LC	Left Corner	0
Since that the size of the right-hand-side and the number of outgoing d-edges per node are practically bounded by LCs, applying k rules on a tree T yields a linear increase in the size of the forest.	LC	Lexical Cohesion	0
Since that the size of the right-hand-side and the number of outgoing d-edges per node are practically bounded by LCs, applying k rules on a tree T yields a linear increase in the size of the forest.	LC	less computerized	0
Since that the size of the right-hand-side and the number of outgoing d-edges per node are practically bounded by LCs, applying k rules on a tree T yields a linear increase in the size of the forest.	LC	lexical cohesion	0
Since that the size of the right-hand-side and the number of outgoing d-edges per node are practically bounded by LCs, applying k rules on a tree T yields a linear increase in the size of the forest.	LC	low constant	1
Since that the size of the right-hand-side and the number of outgoing d-edges per node are practically bounded by LCs, applying k rules on a tree T yields a linear increase in the size of the forest.	LC	Lexical Classifier	0
Since that the size of the right-hand-side and the number of outgoing d-edges per node are practically bounded by LCs, applying k rules on a tree T yields a linear increase in the size of the forest.	LC	Left Context	0
We  use hash tables to alLC time access to the  language model data.	LC	Left Corner	0
We  use hash tables to alLC time access to the  language model data.	LC	Lexical Cohesion	0
We  use hash tables to alLC time access to the  language model data.	LC	less computerized	0
We  use hash tables to alLC time access to the  language model data.	LC	lexical cohesion	0
We  use hash tables to alLC time access to the  language model data.	LC	low constant	1
We  use hash tables to alLC time access to the  language model data.	LC	Lexical Classifier	0
We  use hash tables to alLC time access to the  language model data.	LC	Left Context	0
Furthermore, they are very different 1Logarithmic values are used in the actual implementation which are floored to a LC in case of zero ?	LC	Left Corner	0
Furthermore, they are very different 1Logarithmic values are used in the actual implementation which are floored to a LC in case of zero ?	LC	Lexical Cohesion	0
Furthermore, they are very different 1Logarithmic values are used in the actual implementation which are floored to a LC in case of zero ?	LC	less computerized	0
Furthermore, they are very different 1Logarithmic values are used in the actual implementation which are floored to a LC in case of zero ?	LC	lexical cohesion	0
Furthermore, they are very different 1Logarithmic values are used in the actual implementation which are floored to a LC in case of zero ?	LC	low constant	1
Furthermore, they are very different 1Logarithmic values are used in the actual implementation which are floored to a LC in case of zero ?	LC	Lexical Classifier	0
Furthermore, they are very different 1Logarithmic values are used in the actual implementation which are floored to a LC in case of zero ?	LC	Left Context	0
LC Focus Right Context Combined Class - - - - - a a n b i d ?	LC	Left Corner	0
LC Focus Right Context Combined Class - - - - - a a n b i d ?	LC	Lexical Cohesion	0
LC Focus Right Context Combined Class - - - - - a a n b i d ?	LC	less computerized	0
LC Focus Right Context Combined Class - - - - - a a n b i d ?	LC	lexical cohesion	0
LC Focus Right Context Combined Class - - - - - a a n b i d ?	LC	low constant	0
LC Focus Right Context Combined Class - - - - - a a n b i d ?	LC	Lexical Classifier	0
LC Focus Right Context Combined Class - - - - - a a n b i d ?	LC	Left Context	1
Context Accuracy (%) LC Only 91.31 Right Context Only 88.26 Both Contexts 92.54 Table 3: The effect of using both left and right context.	LC	Left Corner	0
Context Accuracy (%) LC Only 91.31 Right Context Only 88.26 Both Contexts 92.54 Table 3: The effect of using both left and right context.	LC	Lexical Cohesion	0
Context Accuracy (%) LC Only 91.31 Right Context Only 88.26 Both Contexts 92.54 Table 3: The effect of using both left and right context.	LC	less computerized	0
Context Accuracy (%) LC Only 91.31 Right Context Only 88.26 Both Contexts 92.54 Table 3: The effect of using both left and right context.	LC	lexical cohesion	0
Context Accuracy (%) LC Only 91.31 Right Context Only 88.26 Both Contexts 92.54 Table 3: The effect of using both left and right context.	LC	low constant	0
Context Accuracy (%) LC Only 91.31 Right Context Only 88.26 Both Contexts 92.54 Table 3: The effect of using both left and right context.	LC	Lexical Classifier	0
Context Accuracy (%) LC Only 91.31 Right Context Only 88.26 Both Contexts 92.54 Table 3: The effect of using both left and right context.	LC	Left Context	1
	   shi ,1Lo  zhang  ma  ying  jiu  biao  shi ,2Lo ,1Co ,2Co ,3Co ,1Ro ,2Ro 	  	  LC Right Context Candidate to be parsed), it will be rejected immediately.	LC	Left Corner	0
	   shi ,1Lo  zhang  ma  ying  jiu  biao  shi ,2Lo ,1Co ,2Co ,3Co ,1Ro ,2Ro 	  	  LC Right Context Candidate to be parsed), it will be rejected immediately.	LC	Lexical Cohesion	0
	   shi ,1Lo  zhang  ma  ying  jiu  biao  shi ,2Lo ,1Co ,2Co ,3Co ,1Ro ,2Ro 	  	  LC Right Context Candidate to be parsed), it will be rejected immediately.	LC	less computerized	0
	   shi ,1Lo  zhang  ma  ying  jiu  biao  shi ,2Lo ,1Co ,2Co ,3Co ,1Ro ,2Ro 	  	  LC Right Context Candidate to be parsed), it will be rejected immediately.	LC	lexical cohesion	0
	   shi ,1Lo  zhang  ma  ying  jiu  biao  shi ,2Lo ,1Co ,2Co ,3Co ,1Ro ,2Ro 	  	  LC Right Context Candidate to be parsed), it will be rejected immediately.	LC	low constant	0
	   shi ,1Lo  zhang  ma  ying  jiu  biao  shi ,2Lo ,1Co ,2Co ,3Co ,1Ro ,2Ro 	  	  LC Right Context Candidate to be parsed), it will be rejected immediately.	LC	Lexical Classifier	0
	   shi ,1Lo  zhang  ma  ying  jiu  biao  shi ,2Lo ,1Co ,2Co ,3Co ,1Ro ,2Ro 	  	  LC Right Context Candidate to be parsed), it will be rejected immediately.	LC	Left Context	1
LC (-lc): nodes in this partition  do not cover any phrase word and they are  all in the left of the left phrase.	LC	Left Corner	0
LC (-lc): nodes in this partition  do not cover any phrase word and they are  all in the left of the left phrase.	LC	Lexical Cohesion	0
LC (-lc): nodes in this partition  do not cover any phrase word and they are  all in the left of the left phrase.	LC	less computerized	0
LC (-lc): nodes in this partition  do not cover any phrase word and they are  all in the left of the left phrase.	LC	lexical cohesion	0
LC (-lc): nodes in this partition  do not cover any phrase word and they are  all in the left of the left phrase.	LC	low constant	0
LC (-lc): nodes in this partition  do not cover any phrase word and they are  all in the left of the left phrase.	LC	Lexical Classifier	0
LC (-lc): nodes in this partition  do not cover any phrase word and they are  all in the left of the left phrase.	LC	Left Context	1
Description Feature Trigram + Context x1x2x3x4x5 Trigram x2x3x4 LC x1x2 Right Context x4x5 Center Word x3 Trigram - Center Word x2x4 Left Word + Right Context x2x4x5 Right Word + LC x1x2x3 Type of Trigram: number, punctuation, alphabetic letter and other t(x2)t(x3)t(x4) Table 2: Features employed to measure the sim- ilarity between two vertices, in a given tex- t ?	LC	Left Corner	0
Description Feature Trigram + Context x1x2x3x4x5 Trigram x2x3x4 LC x1x2 Right Context x4x5 Center Word x3 Trigram - Center Word x2x4 Left Word + Right Context x2x4x5 Right Word + LC x1x2x3 Type of Trigram: number, punctuation, alphabetic letter and other t(x2)t(x3)t(x4) Table 2: Features employed to measure the sim- ilarity between two vertices, in a given tex- t ?	LC	Lexical Cohesion	0
Description Feature Trigram + Context x1x2x3x4x5 Trigram x2x3x4 LC x1x2 Right Context x4x5 Center Word x3 Trigram - Center Word x2x4 Left Word + Right Context x2x4x5 Right Word + LC x1x2x3 Type of Trigram: number, punctuation, alphabetic letter and other t(x2)t(x3)t(x4) Table 2: Features employed to measure the sim- ilarity between two vertices, in a given tex- t ?	LC	less computerized	0
Description Feature Trigram + Context x1x2x3x4x5 Trigram x2x3x4 LC x1x2 Right Context x4x5 Center Word x3 Trigram - Center Word x2x4 Left Word + Right Context x2x4x5 Right Word + LC x1x2x3 Type of Trigram: number, punctuation, alphabetic letter and other t(x2)t(x3)t(x4) Table 2: Features employed to measure the sim- ilarity between two vertices, in a given tex- t ?	LC	lexical cohesion	0
Description Feature Trigram + Context x1x2x3x4x5 Trigram x2x3x4 LC x1x2 Right Context x4x5 Center Word x3 Trigram - Center Word x2x4 Left Word + Right Context x2x4x5 Right Word + LC x1x2x3 Type of Trigram: number, punctuation, alphabetic letter and other t(x2)t(x3)t(x4) Table 2: Features employed to measure the sim- ilarity between two vertices, in a given tex- t ?	LC	low constant	0
Description Feature Trigram + Context x1x2x3x4x5 Trigram x2x3x4 LC x1x2 Right Context x4x5 Center Word x3 Trigram - Center Word x2x4 Left Word + Right Context x2x4x5 Right Word + LC x1x2x3 Type of Trigram: number, punctuation, alphabetic letter and other t(x2)t(x3)t(x4) Table 2: Features employed to measure the sim- ilarity between two vertices, in a given tex- t ?	LC	Lexical Classifier	0
Description Feature Trigram + Context x1x2x3x4x5 Trigram x2x3x4 LC x1x2 Right Context x4x5 Center Word x3 Trigram - Center Word x2x4 Left Word + Right Context x2x4x5 Right Word + LC x1x2x3 Type of Trigram: number, punctuation, alphabetic letter and other t(x2)t(x3)t(x4) Table 2: Features employed to measure the sim- ilarity between two vertices, in a given tex- t ?	LC	Left Context	1
|  201/206 Pre-processing Configurations & MeasuresSpearman?s Rank Correlation 00,1 0,20,3 0,40,5 0,6 N, V, A Nouns Keywords EBEB+SYNEB+HypoLINESA-WordESA-Text 17.08.08 |  Computer Science Department | Ubiquitous Knowledge Processing Lab  |  202/206 ESA-Text tf.idf with Different Lexical-Semantic Resources Nouns,Verbs,Adjectives Nouns Keywords0 0.050.1 0.150.2 0.250.3 0.350.4 0.450.5 0.550.6 0.65 MAP WikipediaGermaNet HyperGermaNet RadialWiktionary 17.08.08 |  Computer Science Department | Ubiquitous Knowledge Processing Lab  |  203/206 ?	MAP	Mean Average Precision	1
|  201/206 Pre-processing Configurations & MeasuresSpearman?s Rank Correlation 00,1 0,20,3 0,40,5 0,6 N, V, A Nouns Keywords EBEB+SYNEB+HypoLINESA-WordESA-Text 17.08.08 |  Computer Science Department | Ubiquitous Knowledge Processing Lab  |  202/206 ESA-Text tf.idf with Different Lexical-Semantic Resources Nouns,Verbs,Adjectives Nouns Keywords0 0.050.1 0.150.2 0.250.3 0.350.4 0.450.5 0.550.6 0.65 MAP WikipediaGermaNet HyperGermaNet RadialWiktionary 17.08.08 |  Computer Science Department | Ubiquitous Knowledge Processing Lab  |  203/206 ?	MAP	Maximum A Posteriori	0
|  201/206 Pre-processing Configurations & MeasuresSpearman?s Rank Correlation 00,1 0,20,3 0,40,5 0,6 N, V, A Nouns Keywords EBEB+SYNEB+HypoLINESA-WordESA-Text 17.08.08 |  Computer Science Department | Ubiquitous Knowledge Processing Lab  |  202/206 ESA-Text tf.idf with Different Lexical-Semantic Resources Nouns,Verbs,Adjectives Nouns Keywords0 0.050.1 0.150.2 0.250.3 0.350.4 0.450.5 0.550.6 0.65 MAP WikipediaGermaNet HyperGermaNet RadialWiktionary 17.08.08 |  Computer Science Department | Ubiquitous Knowledge Processing Lab  |  203/206 ?	MAP	maximum a posteriori	0
|  201/206 Pre-processing Configurations & MeasuresSpearman?s Rank Correlation 00,1 0,20,3 0,40,5 0,6 N, V, A Nouns Keywords EBEB+SYNEB+HypoLINESA-WordESA-Text 17.08.08 |  Computer Science Department | Ubiquitous Knowledge Processing Lab  |  202/206 ESA-Text tf.idf with Different Lexical-Semantic Resources Nouns,Verbs,Adjectives Nouns Keywords0 0.050.1 0.150.2 0.250.3 0.350.4 0.450.5 0.550.6 0.65 MAP WikipediaGermaNet HyperGermaNet RadialWiktionary 17.08.08 |  Computer Science Department | Ubiquitous Knowledge Processing Lab  |  203/206 ?	MAP	Maximum a posteriori	0
|  201/206 Pre-processing Configurations & MeasuresSpearman?s Rank Correlation 00,1 0,20,3 0,40,5 0,6 N, V, A Nouns Keywords EBEB+SYNEB+HypoLINESA-WordESA-Text 17.08.08 |  Computer Science Department | Ubiquitous Knowledge Processing Lab  |  202/206 ESA-Text tf.idf with Different Lexical-Semantic Resources Nouns,Verbs,Adjectives Nouns Keywords0 0.050.1 0.150.2 0.250.3 0.350.4 0.450.5 0.550.6 0.65 MAP WikipediaGermaNet HyperGermaNet RadialWiktionary 17.08.08 |  Computer Science Department | Ubiquitous Knowledge Processing Lab  |  203/206 ?	MAP	mean average precision	0
Collection PL2 TFIDF FV LSI CLEF?03 35.7 16.4 23.7 9.2 TREC-1&2 22.6 12.4 10.8 6.5 ROBUST 24.8 12.6 10.5 4.5 Table 4: MAP(%) for the PL2 and TFIDF model on the three IR Collections compared to Fisher Vector and LSI These results are not surprising as it has been shown experimentally in many studies that latent- based approaches such as LSI are generally out- performed by state-of-the-art IR models in Ad- Hoc tasks.	MAP	Mean Average Precision	1
Collection PL2 TFIDF FV LSI CLEF?03 35.7 16.4 23.7 9.2 TREC-1&2 22.6 12.4 10.8 6.5 ROBUST 24.8 12.6 10.5 4.5 Table 4: MAP(%) for the PL2 and TFIDF model on the three IR Collections compared to Fisher Vector and LSI These results are not surprising as it has been shown experimentally in many studies that latent- based approaches such as LSI are generally out- performed by state-of-the-art IR models in Ad- Hoc tasks.	MAP	Maximum A Posteriori	0
Collection PL2 TFIDF FV LSI CLEF?03 35.7 16.4 23.7 9.2 TREC-1&2 22.6 12.4 10.8 6.5 ROBUST 24.8 12.6 10.5 4.5 Table 4: MAP(%) for the PL2 and TFIDF model on the three IR Collections compared to Fisher Vector and LSI These results are not surprising as it has been shown experimentally in many studies that latent- based approaches such as LSI are generally out- performed by state-of-the-art IR models in Ad- Hoc tasks.	MAP	maximum a posteriori	0
Collection PL2 TFIDF FV LSI CLEF?03 35.7 16.4 23.7 9.2 TREC-1&2 22.6 12.4 10.8 6.5 ROBUST 24.8 12.6 10.5 4.5 Table 4: MAP(%) for the PL2 and TFIDF model on the three IR Collections compared to Fisher Vector and LSI These results are not surprising as it has been shown experimentally in many studies that latent- based approaches such as LSI are generally out- performed by state-of-the-art IR models in Ad- Hoc tasks.	MAP	Maximum a posteriori	0
Collection PL2 TFIDF FV LSI CLEF?03 35.7 16.4 23.7 9.2 TREC-1&2 22.6 12.4 10.8 6.5 ROBUST 24.8 12.6 10.5 4.5 Table 4: MAP(%) for the PL2 and TFIDF model on the three IR Collections compared to Fisher Vector and LSI These results are not surprising as it has been shown experimentally in many studies that latent- based approaches such as LSI are generally out- performed by state-of-the-art IR models in Ad- Hoc tasks.	MAP	mean average precision	0
The NIST-B2 system with a higher WER (46.6%) has an improvement in MAP of 6.5%.	MAP	Mean Average Precision	1
The NIST-B2 system with a higher WER (46.6%) has an improvement in MAP of 6.5%.	MAP	Maximum A Posteriori	0
The NIST-B2 system with a higher WER (46.6%) has an improvement in MAP of 6.5%.	MAP	maximum a posteriori	0
The NIST-B2 system with a higher WER (46.6%) has an improvement in MAP of 6.5%.	MAP	Maximum a posteriori	0
The NIST-B2 system with a higher WER (46.6%) has an improvement in MAP of 6.5%.	MAP	mean average precision	0
Below, we summarize the three measures we used: MAP, TOP1, and Aver- age Rank of Last Synonym.	MAP	Mean Average Precision	1
Below, we summarize the three measures we used: MAP, TOP1, and Aver- age Rank of Last Synonym.	MAP	Maximum A Posteriori	0
Below, we summarize the three measures we used: MAP, TOP1, and Aver- age Rank of Last Synonym.	MAP	maximum a posteriori	0
Below, we summarize the three measures we used: MAP, TOP1, and Aver- age Rank of Last Synonym.	MAP	Maximum a posteriori	0
Below, we summarize the three measures we used: MAP, TOP1, and Aver- age Rank of Last Synonym.	MAP	mean average precision	0
MAP (APR) Perf implements a definition of average preci- sion sometimes called ?	MAP	Mean Average Precision	1
MAP (APR) Perf implements a definition of average preci- sion sometimes called ?	MAP	Maximum A Posteriori	0
MAP (APR) Perf implements a definition of average preci- sion sometimes called ?	MAP	maximum a posteriori	0
MAP (APR) Perf implements a definition of average preci- sion sometimes called ?	MAP	Maximum a posteriori	0
MAP (APR) Perf implements a definition of average preci- sion sometimes called ?	MAP	mean average precision	0
5.1 Data The data set we use is from the TAC-KBP2013 Slot Filling Validation (SFV) task, which consists of the merged responses returned by 52 runs (regarded as systems in MTM) from 18 teams submitted to the Slot 1573 Methods Precision Recall F-measure Accuracy MAP 1.Random 28.64% 50.48% 36.54% 50.54% 34% 2.Voting 42.16% 70.18% 52.68% 62.54% 62% 3.Linguistic Indicators 50.24% 70.69% 58.73% 72.29% 60% 4.SVM (3 + System + Source) 56.59% 48.72% 52.36% 75.86% 56% 5.MTM (3 + System + Source) 53.94% 72.11% 61.72% 81.57% 70% Table 2: Overall Performance Comparison.	MAP	Mean Average Precision	1
5.1 Data The data set we use is from the TAC-KBP2013 Slot Filling Validation (SFV) task, which consists of the merged responses returned by 52 runs (regarded as systems in MTM) from 18 teams submitted to the Slot 1573 Methods Precision Recall F-measure Accuracy MAP 1.Random 28.64% 50.48% 36.54% 50.54% 34% 2.Voting 42.16% 70.18% 52.68% 62.54% 62% 3.Linguistic Indicators 50.24% 70.69% 58.73% 72.29% 60% 4.SVM (3 + System + Source) 56.59% 48.72% 52.36% 75.86% 56% 5.MTM (3 + System + Source) 53.94% 72.11% 61.72% 81.57% 70% Table 2: Overall Performance Comparison.	MAP	Maximum A Posteriori	0
5.1 Data The data set we use is from the TAC-KBP2013 Slot Filling Validation (SFV) task, which consists of the merged responses returned by 52 runs (regarded as systems in MTM) from 18 teams submitted to the Slot 1573 Methods Precision Recall F-measure Accuracy MAP 1.Random 28.64% 50.48% 36.54% 50.54% 34% 2.Voting 42.16% 70.18% 52.68% 62.54% 62% 3.Linguistic Indicators 50.24% 70.69% 58.73% 72.29% 60% 4.SVM (3 + System + Source) 56.59% 48.72% 52.36% 75.86% 56% 5.MTM (3 + System + Source) 53.94% 72.11% 61.72% 81.57% 70% Table 2: Overall Performance Comparison.	MAP	maximum a posteriori	0
5.1 Data The data set we use is from the TAC-KBP2013 Slot Filling Validation (SFV) task, which consists of the merged responses returned by 52 runs (regarded as systems in MTM) from 18 teams submitted to the Slot 1573 Methods Precision Recall F-measure Accuracy MAP 1.Random 28.64% 50.48% 36.54% 50.54% 34% 2.Voting 42.16% 70.18% 52.68% 62.54% 62% 3.Linguistic Indicators 50.24% 70.69% 58.73% 72.29% 60% 4.SVM (3 + System + Source) 56.59% 48.72% 52.36% 75.86% 56% 5.MTM (3 + System + Source) 53.94% 72.11% 61.72% 81.57% 70% Table 2: Overall Performance Comparison.	MAP	Maximum a posteriori	0
5.1 Data The data set we use is from the TAC-KBP2013 Slot Filling Validation (SFV) task, which consists of the merged responses returned by 52 runs (regarded as systems in MTM) from 18 teams submitted to the Slot 1573 Methods Precision Recall F-measure Accuracy MAP 1.Random 28.64% 50.48% 36.54% 50.54% 34% 2.Voting 42.16% 70.18% 52.68% 62.54% 62% 3.Linguistic Indicators 50.24% 70.69% 58.73% 72.29% 60% 4.SVM (3 + System + Source) 56.59% 48.72% 52.36% 75.86% 56% 5.MTM (3 + System + Source) 53.94% 72.11% 61.72% 81.57% 70% Table 2: Overall Performance Comparison.	MAP	mean average precision	0
7.2 Comparison to Dirichlet MAP Solutions The transformation T(?,?)	MAP	Mean Average Precision	0
7.2 Comparison to Dirichlet MAP Solutions The transformation T(?,?)	MAP	Maximum A Posteriori	1
7.2 Comparison to Dirichlet MAP Solutions The transformation T(?,?)	MAP	maximum a posteriori	0
7.2 Comparison to Dirichlet MAP Solutions The transformation T(?,?)	MAP	Maximum a posteriori	0
7.2 Comparison to Dirichlet MAP Solutions The transformation T(?,?)	MAP	mean average precision	0
Specifically, we will interpolate the translation models as in Foster and Kuhn (2007), including a MAP combination (Bacchiani et al 2006).	MAP	Mean Average Precision	0
Specifically, we will interpolate the translation models as in Foster and Kuhn (2007), including a MAP combination (Bacchiani et al 2006).	MAP	Maximum A Posteriori	0
Specifically, we will interpolate the translation models as in Foster and Kuhn (2007), including a MAP combination (Bacchiani et al 2006).	MAP	maximum a posteriori	1
Specifically, we will interpolate the translation models as in Foster and Kuhn (2007), including a MAP combination (Bacchiani et al 2006).	MAP	Maximum a posteriori	0
Specifically, we will interpolate the translation models as in Foster and Kuhn (2007), including a MAP combination (Bacchiani et al 2006).	MAP	mean average precision	0
Future work might explore sensitivity to these choices, or empirical Bayesian or MAP inference for their values (Johnson and Goldwater, 2009).	MAP	Mean Average Precision	0
Future work might explore sensitivity to these choices, or empirical Bayesian or MAP inference for their values (Johnson and Goldwater, 2009).	MAP	Maximum A Posteriori	0
Future work might explore sensitivity to these choices, or empirical Bayesian or MAP inference for their values (Johnson and Goldwater, 2009).	MAP	maximum a posteriori	1
Future work might explore sensitivity to these choices, or empirical Bayesian or MAP inference for their values (Johnson and Goldwater, 2009).	MAP	Maximum a posteriori	0
Future work might explore sensitivity to these choices, or empirical Bayesian or MAP inference for their values (Johnson and Goldwater, 2009).	MAP	mean average precision	0
In this study, we use the MAP es- timation with Gaussian priors for parameter estima- tion.	MAP	Mean Average Precision	0
In this study, we use the MAP es- timation with Gaussian priors for parameter estima- tion.	MAP	Maximum A Posteriori	0
In this study, we use the MAP es- timation with Gaussian priors for parameter estima- tion.	MAP	maximum a posteriori	1
In this study, we use the MAP es- timation with Gaussian priors for parameter estima- tion.	MAP	Maximum a posteriori	0
In this study, we use the MAP es- timation with Gaussian priors for parameter estima- tion.	MAP	mean average precision	0
2.2.3 Regularization Schemes One can convert a maximum likelihood problem into MAP using Bayes?	MAP	Mean Average Precision	0
2.2.3 Regularization Schemes One can convert a maximum likelihood problem into MAP using Bayes?	MAP	Maximum A Posteriori	0
2.2.3 Regularization Schemes One can convert a maximum likelihood problem into MAP using Bayes?	MAP	maximum a posteriori	1
2.2.3 Regularization Schemes One can convert a maximum likelihood problem into MAP using Bayes?	MAP	Maximum a posteriori	0
2.2.3 Regularization Schemes One can convert a maximum likelihood problem into MAP using Bayes?	MAP	mean average precision	0
4 Evaluation We compared the Gibbs sampling compressor (GS) against a version of MAP EM (with Dirichlet parameter greater than 1) and a discriminative STSG based on SVM training (Cohn and Lapata, 2008) (SVM).	MAP	Mean Average Precision	0
4 Evaluation We compared the Gibbs sampling compressor (GS) against a version of MAP EM (with Dirichlet parameter greater than 1) and a discriminative STSG based on SVM training (Cohn and Lapata, 2008) (SVM).	MAP	Maximum A Posteriori	0
4 Evaluation We compared the Gibbs sampling compressor (GS) against a version of MAP EM (with Dirichlet parameter greater than 1) and a discriminative STSG based on SVM training (Cohn and Lapata, 2008) (SVM).	MAP	maximum a posteriori	1
4 Evaluation We compared the Gibbs sampling compressor (GS) against a version of MAP EM (with Dirichlet parameter greater than 1) and a discriminative STSG based on SVM training (Cohn and Lapata, 2008) (SVM).	MAP	Maximum a posteriori	0
4 Evaluation We compared the Gibbs sampling compressor (GS) against a version of MAP EM (with Dirichlet parameter greater than 1) and a discriminative STSG based on SVM training (Cohn and Lapata, 2008) (SVM).	MAP	mean average precision	0
While they train the parameters using a MAP estima- tor, we extend the MERT algorithm (Och, 2003) to take the evaluation metric into account.	MAP	Mean Average Precision	0
While they train the parameters using a MAP estima- tor, we extend the MERT algorithm (Och, 2003) to take the evaluation metric into account.	MAP	Maximum A Posteriori	0
While they train the parameters using a MAP estima- tor, we extend the MERT algorithm (Och, 2003) to take the evaluation metric into account.	MAP	maximum a posteriori	1
While they train the parameters using a MAP estima- tor, we extend the MERT algorithm (Och, 2003) to take the evaluation metric into account.	MAP	Maximum a posteriori	0
While they train the parameters using a MAP estima- tor, we extend the MERT algorithm (Och, 2003) to take the evaluation metric into account.	MAP	mean average precision	0
4 Bayesian Learning for word-dependent  transition models   4.1 MAP training   Using ML training, we can obtain the estimation  formula for word dependent transition probabilities  { }( | , , )p i i e I?	MAP	Mean Average Precision	0
4 Bayesian Learning for word-dependent  transition models   4.1 MAP training   Using ML training, we can obtain the estimation  formula for word dependent transition probabilities  { }( | , , )p i i e I?	MAP	Maximum A Posteriori	0
4 Bayesian Learning for word-dependent  transition models   4.1 MAP training   Using ML training, we can obtain the estimation  formula for word dependent transition probabilities  { }( | , , )p i i e I?	MAP	maximum a posteriori	0
4 Bayesian Learning for word-dependent  transition models   4.1 MAP training   Using ML training, we can obtain the estimation  formula for word dependent transition probabilities  { }( | , , )p i i e I?	MAP	Maximum a posteriori	1
4 Bayesian Learning for word-dependent  transition models   4.1 MAP training   Using ML training, we can obtain the estimation  formula for word dependent transition probabilities  { }( | , , )p i i e I?	MAP	mean average precision	0
The MAP for a run consisting of  multiple topics is the mean of the average precision  scores of each of the individual topics in the run.	MAP	Mean Average Precision	0
The MAP for a run consisting of  multiple topics is the mean of the average precision  scores of each of the individual topics in the run.	MAP	Maximum A Posteriori	0
The MAP for a run consisting of  multiple topics is the mean of the average precision  scores of each of the individual topics in the run.	MAP	maximum a posteriori	0
The MAP for a run consisting of  multiple topics is the mean of the average precision  scores of each of the individual topics in the run.	MAP	Maximum a posteriori	0
The MAP for a run consisting of  multiple topics is the mean of the average precision  scores of each of the individual topics in the run.	MAP	mean average precision	1
Each line in the graph con-  nects the MAP scores produced by  each version of the system for a single test.	MAP	Mean Average Precision	0
Each line in the graph con-  nects the MAP scores produced by  each version of the system for a single test.	MAP	Maximum A Posteriori	0
Each line in the graph con-  nects the MAP scores produced by  each version of the system for a single test.	MAP	maximum a posteriori	0
Each line in the graph con-  nects the MAP scores produced by  each version of the system for a single test.	MAP	Maximum a posteriori	0
Each line in the graph con-  nects the MAP scores produced by  each version of the system for a single test.	MAP	mean average precision	1
The last  row shows the MAPs from a 10-fold  cross validation to learn how to distinguish each class  from the union of the other three.	MAP	Mean Average Precision	0
The last  row shows the MAPs from a 10-fold  cross validation to learn how to distinguish each class  from the union of the other three.	MAP	Maximum A Posteriori	0
The last  row shows the MAPs from a 10-fold  cross validation to learn how to distinguish each class  from the union of the other three.	MAP	maximum a posteriori	0
The last  row shows the MAPs from a 10-fold  cross validation to learn how to distinguish each class  from the union of the other three.	MAP	Maximum a posteriori	0
The last  row shows the MAPs from a 10-fold  cross validation to learn how to distinguish each class  from the union of the other three.	MAP	mean average precision	1
For instance, the weighted MAP of the previous best ap- proach in (Riedel et al.,	MAP	Mean Average Precision	0
For instance, the weighted MAP of the previous best ap- proach in (Riedel et al.,	MAP	Maximum A Posteriori	0
For instance, the weighted MAP of the previous best ap- proach in (Riedel et al.,	MAP	maximum a posteriori	0
For instance, the weighted MAP of the previous best ap- proach in (Riedel et al.,	MAP	Maximum a posteriori	0
For instance, the weighted MAP of the previous best ap- proach in (Riedel et al.,	MAP	mean average precision	1
For instance, this strategy im- proves the weighted MAP of the best approach in (Riedel et al.,	MAP	Mean Average Precision	0
For instance, this strategy im- proves the weighted MAP of the best approach in (Riedel et al.,	MAP	Maximum A Posteriori	0
For instance, this strategy im- proves the weighted MAP of the best approach in (Riedel et al.,	MAP	maximum a posteriori	0
For instance, this strategy im- proves the weighted MAP of the best approach in (Riedel et al.,	MAP	Maximum a posteriori	0
For instance, this strategy im- proves the weighted MAP of the best approach in (Riedel et al.,	MAP	mean average precision	1
In addition, when ap- plied to a relation extraction task, our ap- proach alone is comparable to several ex- isting systems, and improves the weighted MAP of a state-of-the- art method by 10 points when used as a subcomponent.	MAP	Mean Average Precision	0
In addition, when ap- plied to a relation extraction task, our ap- proach alone is comparable to several ex- isting systems, and improves the weighted MAP of a state-of-the- art method by 10 points when used as a subcomponent.	MAP	Maximum A Posteriori	0
In addition, when ap- plied to a relation extraction task, our ap- proach alone is comparable to several ex- isting systems, and improves the weighted MAP of a state-of-the- art method by 10 points when used as a subcomponent.	MAP	maximum a posteriori	0
In addition, when ap- plied to a relation extraction task, our ap- proach alone is comparable to several ex- isting systems, and improves the weighted MAP of a state-of-the- art method by 10 points when used as a subcomponent.	MAP	Maximum a posteriori	0
In addition, when ap- plied to a relation extraction task, our ap- proach alone is comparable to several ex- isting systems, and improves the weighted MAP of a state-of-the- art method by 10 points when used as a subcomponent.	MAP	mean average precision	1
We incorpo-  rate multiple anaphora resolution factors into  a statistical framework - -  specifically the dis-  tance between the pronoun and the proposed  antecedent, gender/number/animaticity of the  proposed antecedent, governing HD informa-  tion and noun phrase repetition.	HD	head	1
We incorpo-  rate multiple anaphora resolution factors into  a statistical framework - -  specifically the dis-  tance between the pronoun and the proposed  antecedent, gender/number/animaticity of the  proposed antecedent, governing HD informa-  tion and noun phrase repetition.	HD	hierarchical discriminative	0
We incorpo-  rate multiple anaphora resolution factors into  a statistical framework - -  specifically the dis-  tance between the pronoun and the proposed  antecedent, gender/number/animaticity of the  proposed antecedent, governing HD informa-  tion and noun phrase repetition.	HD	Head-Driven	0
We incorpo-  rate multiple anaphora resolution factors into  a statistical framework - -  specifically the dis-  tance between the pronoun and the proposed  antecedent, gender/number/animaticity of the  proposed antecedent, governing HD informa-  tion and noun phrase repetition.	HD	Head	0
In the conditioning events, h is the  HD constituent above p, l~ r is the list of candi-  date antecedents o be considered, t is the type  of phrase of the proposed antecedent (always  a noun-phrase in this study), I is the type of  the HD constituent, sp describes the syntactic  structure in which p appears, dspecifies the dis-  tance of each antecedent from p and M" is the  number of times the referent is mentioned.	HD	head	1
In the conditioning events, h is the  HD constituent above p, l~ r is the list of candi-  date antecedents o be considered, t is the type  of phrase of the proposed antecedent (always  a noun-phrase in this study), I is the type of  the HD constituent, sp describes the syntactic  structure in which p appears, dspecifies the dis-  tance of each antecedent from p and M" is the  number of times the referent is mentioned.	HD	hierarchical discriminative	0
In the conditioning events, h is the  HD constituent above p, l~ r is the list of candi-  date antecedents o be considered, t is the type  of phrase of the proposed antecedent (always  a noun-phrase in this study), I is the type of  the HD constituent, sp describes the syntactic  structure in which p appears, dspecifies the dis-  tance of each antecedent from p and M" is the  number of times the referent is mentioned.	HD	Head-Driven	0
In the conditioning events, h is the  HD constituent above p, l~ r is the list of candi-  date antecedents o be considered, t is the type  of phrase of the proposed antecedent (always  a noun-phrase in this study), I is the type of  the HD constituent, sp describes the syntactic  structure in which p appears, dspecifies the dis-  tance of each antecedent from p and M" is the  number of times the referent is mentioned.	HD	Head	0
It is also useful to note the interaction be-  tween the HD constituent of the pronoun p  and the antecedent.	HD	head	1
It is also useful to note the interaction be-  tween the HD constituent of the pronoun p  and the antecedent.	HD	hierarchical discriminative	0
It is also useful to note the interaction be-  tween the HD constituent of the pronoun p  and the antecedent.	HD	Head-Driven	0
It is also useful to note the interaction be-  tween the HD constituent of the pronoun p  and the antecedent.	HD	Head	0
Also, some HD  verbs are too general to restrict he selection of  any NP.	HD	head	1
Also, some HD  verbs are too general to restrict he selection of  any NP.	HD	hierarchical discriminative	0
Also, some HD  verbs are too general to restrict he selection of  any NP.	HD	Head-Driven	0
Also, some HD  verbs are too general to restrict he selection of  any NP.	HD	Head	0
To avoid the sparse-data prob-  lem, the HDs h are clustered according to how  they behave in P(w~lh, t, l).	HD	head	1
To avoid the sparse-data prob-  lem, the HDs h are clustered according to how  they behave in P(w~lh, t, l).	HD	hierarchical discriminative	0
To avoid the sparse-data prob-  lem, the HDs h are clustered according to how  they behave in P(w~lh, t, l).	HD	Head-Driven	0
To avoid the sparse-data prob-  lem, the HDs h are clustered according to how  they behave in P(w~lh, t, l).	HD	Head	0
We introduce a HD clas- sification method for text-based geotagging.	HD	head	0
We introduce a HD clas- sification method for text-based geotagging.	HD	hierarchical discriminative	1
We introduce a HD clas- sification method for text-based geotagging.	HD	Head-Driven	0
We introduce a HD clas- sification method for text-based geotagging.	HD	Head	0
For each test segment, the judges were presented with its text, and 3 alternative titles consisting of the reference and the titles produced by the HD model, and the best performing baseline.	HD	head	0
For each test segment, the judges were presented with its text, and 3 alternative titles consisting of the reference and the titles produced by the HD model, and the best performing baseline.	HD	hierarchical discriminative	1
For each test segment, the judges were presented with its text, and 3 alternative titles consisting of the reference and the titles produced by the HD model, and the best performing baseline.	HD	Head-Driven	0
For each test segment, the judges were presented with its text, and 3 alternative titles consisting of the reference and the titles produced by the HD model, and the best performing baseline.	HD	Head	0
2.2 Factor Graph Model To construct the entity factoid hierarchy, we make use of a HD factor graph model.	HD	head	0
2.2 Factor Graph Model To construct the entity factoid hierarchy, we make use of a HD factor graph model.	HD	hierarchical discriminative	1
2.2 Factor Graph Model To construct the entity factoid hierarchy, we make use of a HD factor graph model.	HD	Head-Driven	0
2.2 Factor Graph Model To construct the entity factoid hierarchy, we make use of a HD factor graph model.	HD	Head	0
This paper presents a HD approach for table-of-contents generation.	HD	head	0
This paper presents a HD approach for table-of-contents generation.	HD	hierarchical discriminative	1
This paper presents a HD approach for table-of-contents generation.	HD	Head-Driven	0
This paper presents a HD approach for table-of-contents generation.	HD	Head	0
The HD method consis- tently outperforms the four baselines according to all ROUGE metrics.	HD	head	0
The HD method consis- tently outperforms the four baselines according to all ROUGE metrics.	HD	hierarchical discriminative	1
The HD method consis- tently outperforms the four baselines according to all ROUGE metrics.	HD	Head-Driven	0
The HD method consis- tently outperforms the four baselines according to all ROUGE metrics.	HD	Head	0
HD Phrase  Structure Grammar.	HD	head	0
HD Phrase  Structure Grammar.	HD	hierarchical discriminative	0
HD Phrase  Structure Grammar.	HD	Head-Driven	1
HD Phrase  Structure Grammar.	HD	Head	0
In Ger-  man in HD Phrase Structure Grammar.	HD	head	0
In Ger-  man in HD Phrase Structure Grammar.	HD	hierarchical discriminative	0
In Ger-  man in HD Phrase Structure Grammar.	HD	Head-Driven	1
In Ger-  man in HD Phrase Structure Grammar.	HD	Head	0
2 Background 2.1 The LinGO Grammar Matrix The LinGO Grammar Matrix is situated theoreti- cally within HD Phrase Structure Gram- mar (HPSG; Pollard and Sag, 1994), a lexicalist, constraint-based framework.	HD	head	0
2 Background 2.1 The LinGO Grammar Matrix The LinGO Grammar Matrix is situated theoreti- cally within HD Phrase Structure Gram- mar (HPSG; Pollard and Sag, 1994), a lexicalist, constraint-based framework.	HD	hierarchical discriminative	0
2 Background 2.1 The LinGO Grammar Matrix The LinGO Grammar Matrix is situated theoreti- cally within HD Phrase Structure Gram- mar (HPSG; Pollard and Sag, 1994), a lexicalist, constraint-based framework.	HD	Head-Driven	1
2 Background 2.1 The LinGO Grammar Matrix The LinGO Grammar Matrix is situated theoreti- cally within HD Phrase Structure Gram- mar (HPSG; Pollard and Sag, 1994), a lexicalist, constraint-based framework.	HD	Head	0
HD Statistical Models for Natural Language Parsing.	HD	head	0
HD Statistical Models for Natural Language Parsing.	HD	hierarchical discriminative	0
HD Statistical Models for Natural Language Parsing.	HD	Head-Driven	1
HD Statistical Models for Natural Language Parsing.	HD	Head	0
HD Phrase Structure Grammar.	HD	head	0
HD Phrase Structure Grammar.	HD	hierarchical discriminative	0
HD Phrase Structure Grammar.	HD	Head-Driven	1
HD Phrase Structure Grammar.	HD	Head	0
ency annotations we can also determine how many instances need addi- 175 Location Main Fact Subfact Synonym Extra Title 3.3 ( 0.2) 1.9 ( 0.7) 0.0 ( 0.0) 0.8 ( 0.8) Abstract 19.1 (10.1) 9.3 ( 5.1) 36.2 (21.7) 25.8 (14.8) Introduction 11.3 ( 5.2) 8.3 ( 3.4) 30.4 (17.4) 17.2 ( 7.8) Results 31.0 (13.8) 37.6 (16.1) 20.3 (15.9) 32.0 (12.5) Discussion 21.8 ( 7.3) 19.5 ( 6.6) 2.9 ( 1.4) 9.4 ( 3.1) Figure HDing 5.0 ( 0.6) 10.7 ( 3.8) 1.4 ( 1.4) 2.3 ( 0.0) Figure Legend 3.1 ( 1.3) 4.8 ( 2.0) 0.0 ( 0.0) 7.0 ( 4.7) Table Data 0.0 ( 0.0) 0.2 ( 0.0) 0.0 ( 0.0) 0.0 ( 0.0) Methods 0.2 ( 0.0) 0.1 ( 0.1) 0.0 ( 0.0) 4.7 ( 0.8) Conclusion 0.6 ( 0.4) 0.1 ( 0.0) 0.0 ( 0.0) 0.0 ( 0.0) Footnotes 0.0 ( 0.0) 0.0 ( 0.0) 5.8 ( 2.9) 0.0 ( 0.0) HDings 4.8 ( 0.6) 7.5 ( 2.7) 2.9 ( 1.4) 0.8 ( 0.8) Full-text 100.0 (39.	HD	head	0
ency annotations we can also determine how many instances need addi- 175 Location Main Fact Subfact Synonym Extra Title 3.3 ( 0.2) 1.9 ( 0.7) 0.0 ( 0.0) 0.8 ( 0.8) Abstract 19.1 (10.1) 9.3 ( 5.1) 36.2 (21.7) 25.8 (14.8) Introduction 11.3 ( 5.2) 8.3 ( 3.4) 30.4 (17.4) 17.2 ( 7.8) Results 31.0 (13.8) 37.6 (16.1) 20.3 (15.9) 32.0 (12.5) Discussion 21.8 ( 7.3) 19.5 ( 6.6) 2.9 ( 1.4) 9.4 ( 3.1) Figure HDing 5.0 ( 0.6) 10.7 ( 3.8) 1.4 ( 1.4) 2.3 ( 0.0) Figure Legend 3.1 ( 1.3) 4.8 ( 2.0) 0.0 ( 0.0) 7.0 ( 4.7) Table Data 0.0 ( 0.0) 0.2 ( 0.0) 0.0 ( 0.0) 0.0 ( 0.0) Methods 0.2 ( 0.0) 0.1 ( 0.1) 0.0 ( 0.0) 4.7 ( 0.8) Conclusion 0.6 ( 0.4) 0.1 ( 0.0) 0.0 ( 0.0) 0.0 ( 0.0) Footnotes 0.0 ( 0.0) 0.0 ( 0.0) 5.8 ( 2.9) 0.0 ( 0.0) HDings 4.8 ( 0.6) 7.5 ( 2.7) 2.9 ( 1.4) 0.8 ( 0.8) Full-text 100.0 (39.	HD	hierarchical discriminative	0
ency annotations we can also determine how many instances need addi- 175 Location Main Fact Subfact Synonym Extra Title 3.3 ( 0.2) 1.9 ( 0.7) 0.0 ( 0.0) 0.8 ( 0.8) Abstract 19.1 (10.1) 9.3 ( 5.1) 36.2 (21.7) 25.8 (14.8) Introduction 11.3 ( 5.2) 8.3 ( 3.4) 30.4 (17.4) 17.2 ( 7.8) Results 31.0 (13.8) 37.6 (16.1) 20.3 (15.9) 32.0 (12.5) Discussion 21.8 ( 7.3) 19.5 ( 6.6) 2.9 ( 1.4) 9.4 ( 3.1) Figure HDing 5.0 ( 0.6) 10.7 ( 3.8) 1.4 ( 1.4) 2.3 ( 0.0) Figure Legend 3.1 ( 1.3) 4.8 ( 2.0) 0.0 ( 0.0) 7.0 ( 4.7) Table Data 0.0 ( 0.0) 0.2 ( 0.0) 0.0 ( 0.0) 0.0 ( 0.0) Methods 0.2 ( 0.0) 0.1 ( 0.1) 0.0 ( 0.0) 4.7 ( 0.8) Conclusion 0.6 ( 0.4) 0.1 ( 0.0) 0.0 ( 0.0) 0.0 ( 0.0) Footnotes 0.0 ( 0.0) 0.0 ( 0.0) 5.8 ( 2.9) 0.0 ( 0.0) HDings 4.8 ( 0.6) 7.5 ( 2.7) 2.9 ( 1.4) 0.8 ( 0.8) Full-text 100.0 (39.	HD	Head-Driven	0
ency annotations we can also determine how many instances need addi- 175 Location Main Fact Subfact Synonym Extra Title 3.3 ( 0.2) 1.9 ( 0.7) 0.0 ( 0.0) 0.8 ( 0.8) Abstract 19.1 (10.1) 9.3 ( 5.1) 36.2 (21.7) 25.8 (14.8) Introduction 11.3 ( 5.2) 8.3 ( 3.4) 30.4 (17.4) 17.2 ( 7.8) Results 31.0 (13.8) 37.6 (16.1) 20.3 (15.9) 32.0 (12.5) Discussion 21.8 ( 7.3) 19.5 ( 6.6) 2.9 ( 1.4) 9.4 ( 3.1) Figure HDing 5.0 ( 0.6) 10.7 ( 3.8) 1.4 ( 1.4) 2.3 ( 0.0) Figure Legend 3.1 ( 1.3) 4.8 ( 2.0) 0.0 ( 0.0) 7.0 ( 4.7) Table Data 0.0 ( 0.0) 0.2 ( 0.0) 0.0 ( 0.0) 0.0 ( 0.0) Methods 0.2 ( 0.0) 0.1 ( 0.1) 0.0 ( 0.0) 4.7 ( 0.8) Conclusion 0.6 ( 0.4) 0.1 ( 0.0) 0.0 ( 0.0) 0.0 ( 0.0) Footnotes 0.0 ( 0.0) 0.0 ( 0.0) 5.8 ( 2.9) 0.0 ( 0.0) HDings 4.8 ( 0.6) 7.5 ( 2.7) 2.9 ( 1.4) 0.8 ( 0.8) Full-text 100.0 (39.	HD	Head	1
0 (12.5) Discussion 21.8 ( 7.3) 19.5 ( 6.6) 2.9 ( 1.4) 9.4 ( 3.1) Figure HDing 5.0 ( 0.6) 10.7 ( 3.8) 1.4 ( 1.4) 2.3 ( 0.0) Figure Legend 3.1 ( 1.3) 4.8 ( 2.0) 0.0 ( 0.0) 7.0 ( 4.7) Table Data 0.0 ( 0.0) 0.2 ( 0.0) 0.0 ( 0.0) 0.0 ( 0.0) Methods 0.2 ( 0.0) 0.1 ( 0.1) 0.0 ( 0.0) 4.7 ( 0.8) Conclusion 0.6 ( 0.4) 0.1 ( 0.0) 0.0 ( 0.0) 0.0 ( 0.0) Footnotes 0.0 ( 0.0) 0.0 ( 0.0) 5.8 ( 2.9) 0.0 ( 0.0) HDings 4.8 ( 0.6) 7.5 ( 2.7) 2.9 ( 1.4) 0.8 ( 0.8) Full-text 100.0 (39.4) 100.0 (40.6) 100.0 (62.3) 100.0 (45.3) Table 6: Instances found excluding (including) all dependencies Fact Type # Created # Found # Instances Main Fact 170 156 523 Subfact 251 251 1196 Synonym 155 62 69 Extra 152 87 128 Total 728 556 1916 Table 7: Distribution of fact types in corpus tional knowledge outside of the curren	HD	head	0
0 (12.5) Discussion 21.8 ( 7.3) 19.5 ( 6.6) 2.9 ( 1.4) 9.4 ( 3.1) Figure HDing 5.0 ( 0.6) 10.7 ( 3.8) 1.4 ( 1.4) 2.3 ( 0.0) Figure Legend 3.1 ( 1.3) 4.8 ( 2.0) 0.0 ( 0.0) 7.0 ( 4.7) Table Data 0.0 ( 0.0) 0.2 ( 0.0) 0.0 ( 0.0) 0.0 ( 0.0) Methods 0.2 ( 0.0) 0.1 ( 0.1) 0.0 ( 0.0) 4.7 ( 0.8) Conclusion 0.6 ( 0.4) 0.1 ( 0.0) 0.0 ( 0.0) 0.0 ( 0.0) Footnotes 0.0 ( 0.0) 0.0 ( 0.0) 5.8 ( 2.9) 0.0 ( 0.0) HDings 4.8 ( 0.6) 7.5 ( 2.7) 2.9 ( 1.4) 0.8 ( 0.8) Full-text 100.0 (39.4) 100.0 (40.6) 100.0 (62.3) 100.0 (45.3) Table 6: Instances found excluding (including) all dependencies Fact Type # Created # Found # Instances Main Fact 170 156 523 Subfact 251 251 1196 Synonym 155 62 69 Extra 152 87 128 Total 728 556 1916 Table 7: Distribution of fact types in corpus tional knowledge outside of the curren	HD	hierarchical discriminative	0
0 (12.5) Discussion 21.8 ( 7.3) 19.5 ( 6.6) 2.9 ( 1.4) 9.4 ( 3.1) Figure HDing 5.0 ( 0.6) 10.7 ( 3.8) 1.4 ( 1.4) 2.3 ( 0.0) Figure Legend 3.1 ( 1.3) 4.8 ( 2.0) 0.0 ( 0.0) 7.0 ( 4.7) Table Data 0.0 ( 0.0) 0.2 ( 0.0) 0.0 ( 0.0) 0.0 ( 0.0) Methods 0.2 ( 0.0) 0.1 ( 0.1) 0.0 ( 0.0) 4.7 ( 0.8) Conclusion 0.6 ( 0.4) 0.1 ( 0.0) 0.0 ( 0.0) 0.0 ( 0.0) Footnotes 0.0 ( 0.0) 0.0 ( 0.0) 5.8 ( 2.9) 0.0 ( 0.0) HDings 4.8 ( 0.6) 7.5 ( 2.7) 2.9 ( 1.4) 0.8 ( 0.8) Full-text 100.0 (39.4) 100.0 (40.6) 100.0 (62.3) 100.0 (45.3) Table 6: Instances found excluding (including) all dependencies Fact Type # Created # Found # Instances Main Fact 170 156 523 Subfact 251 251 1196 Synonym 155 62 69 Extra 152 87 128 Total 728 556 1916 Table 7: Distribution of fact types in corpus tional knowledge outside of the curren	HD	Head-Driven	0
0 (12.5) Discussion 21.8 ( 7.3) 19.5 ( 6.6) 2.9 ( 1.4) 9.4 ( 3.1) Figure HDing 5.0 ( 0.6) 10.7 ( 3.8) 1.4 ( 1.4) 2.3 ( 0.0) Figure Legend 3.1 ( 1.3) 4.8 ( 2.0) 0.0 ( 0.0) 7.0 ( 4.7) Table Data 0.0 ( 0.0) 0.2 ( 0.0) 0.0 ( 0.0) 0.0 ( 0.0) Methods 0.2 ( 0.0) 0.1 ( 0.1) 0.0 ( 0.0) 4.7 ( 0.8) Conclusion 0.6 ( 0.4) 0.1 ( 0.0) 0.0 ( 0.0) 0.0 ( 0.0) Footnotes 0.0 ( 0.0) 0.0 ( 0.0) 5.8 ( 2.9) 0.0 ( 0.0) HDings 4.8 ( 0.6) 7.5 ( 2.7) 2.9 ( 1.4) 0.8 ( 0.8) Full-text 100.0 (39.4) 100.0 (40.6) 100.0 (62.3) 100.0 (45.3) Table 6: Instances found excluding (including) all dependencies Fact Type # Created # Found # Instances Main Fact 170 156 523 Subfact 251 251 1196 Synonym 155 62 69 Extra 152 87 128 Total 728 556 1916 Table 7: Distribution of fact types in corpus tional knowledge outside of the curren	HD	Head	1
But the model has also been applied to several other grammatical frameworks, e.g. Tree-Insertion Grammar (Hoogweg 2000), Tree-Adjoining Grammar (Neumann 1998), Lexical-Functional Grammar (Bod & Kaplan 1998; Cormons 1999), HD-driven Phrase Structure Grammar (Neumann & Flickinger 1999), and Montague Grammar (Bonnema et al 1997; Bod 1999).	HD	head	0
But the model has also been applied to several other grammatical frameworks, e.g. Tree-Insertion Grammar (Hoogweg 2000), Tree-Adjoining Grammar (Neumann 1998), Lexical-Functional Grammar (Bod & Kaplan 1998; Cormons 1999), HD-driven Phrase Structure Grammar (Neumann & Flickinger 1999), and Montague Grammar (Bonnema et al 1997; Bod 1999).	HD	hierarchical discriminative	0
But the model has also been applied to several other grammatical frameworks, e.g. Tree-Insertion Grammar (Hoogweg 2000), Tree-Adjoining Grammar (Neumann 1998), Lexical-Functional Grammar (Bod & Kaplan 1998; Cormons 1999), HD-driven Phrase Structure Grammar (Neumann & Flickinger 1999), and Montague Grammar (Bonnema et al 1997; Bod 1999).	HD	Head-Driven	0
But the model has also been applied to several other grammatical frameworks, e.g. Tree-Insertion Grammar (Hoogweg 2000), Tree-Adjoining Grammar (Neumann 1998), Lexical-Functional Grammar (Bod & Kaplan 1998; Cormons 1999), HD-driven Phrase Structure Grammar (Neumann & Flickinger 1999), and Montague Grammar (Bonnema et al 1997; Bod 1999).	HD	Head	1
2 Background 2.1 The LinGO Grammar Matrix The LinGO Grammar Matrix is situated theoreti- cally within HD-Driven Phrase Structure Gram- mar (HPSG; Pollard and Sag, 1994), a lexicalist, constraint-based framework.	HD	head	0
2 Background 2.1 The LinGO Grammar Matrix The LinGO Grammar Matrix is situated theoreti- cally within HD-Driven Phrase Structure Gram- mar (HPSG; Pollard and Sag, 1994), a lexicalist, constraint-based framework.	HD	hierarchical discriminative	0
2 Background 2.1 The LinGO Grammar Matrix The LinGO Grammar Matrix is situated theoreti- cally within HD-Driven Phrase Structure Gram- mar (HPSG; Pollard and Sag, 1994), a lexicalist, constraint-based framework.	HD	Head-Driven	0
2 Background 2.1 The LinGO Grammar Matrix The LinGO Grammar Matrix is situated theoreti- cally within HD-Driven Phrase Structure Gram- mar (HPSG; Pollard and Sag, 1994), a lexicalist, constraint-based framework.	HD	Head	1
HD-Driven Statistical Models for Natural Language Parsing.	HD	head	0
HD-Driven Statistical Models for Natural Language Parsing.	HD	hierarchical discriminative	0
HD-Driven Statistical Models for Natural Language Parsing.	HD	Head-Driven	0
HD-Driven Statistical Models for Natural Language Parsing.	HD	Head	1
HD-Driven Phrase Structure Grammar.	HD	head	0
HD-Driven Phrase Structure Grammar.	HD	hierarchical discriminative	0
HD-Driven Phrase Structure Grammar.	HD	Head-Driven	0
HD-Driven Phrase Structure Grammar.	HD	Head	1
1089 predicting grades as compared to a pure ML approach and to using CG only?	CG	Crowd Grades	1
1089 predicting grades as compared to a pure ML approach and to using CG only?	CG	Cancer Genetics	0
1089 predicting grades as compared to a pure ML approach and to using CG only?	CG	Categorial Grammar	0
0 Table 1: Regression Results Technique Model Code Feature Type Train r Validation r RR-1 RS/LR 0.51 0.47 Ridge Regression RR-2 Pure ML 0.54 0.47 RR-3 CG 0.63 0.57 RR-4 ML-CS 0.55 0.60 RR-5 All 0.76 0.76 SVM-1 RS/LR 0.50 0.46 SVM SVM-2 Pure ML 0.53 0.46 SVM-3 CG 0.62 0.57 SVM-4 ML-CS 0.60 0.61 SVM-5 All 0.75 0.74 NN-1 RS/LR 0.56 0.51 Neural Networks NN-2 Pure ML 0.60 0.44 NN-3 CG 0.63 0.57 NN-4 ML-CS 0.66 0.57 NN-5 All 0.80 0.76 between 1 and 1000, was selected based on the the least RMS error in cross-validation.	CG	Crowd Grades	1
0 Table 1: Regression Results Technique Model Code Feature Type Train r Validation r RR-1 RS/LR 0.51 0.47 Ridge Regression RR-2 Pure ML 0.54 0.47 RR-3 CG 0.63 0.57 RR-4 ML-CS 0.55 0.60 RR-5 All 0.76 0.76 SVM-1 RS/LR 0.50 0.46 SVM SVM-2 Pure ML 0.53 0.46 SVM-3 CG 0.62 0.57 SVM-4 ML-CS 0.60 0.61 SVM-5 All 0.75 0.74 NN-1 RS/LR 0.56 0.51 Neural Networks NN-2 Pure ML 0.60 0.44 NN-3 CG 0.63 0.57 NN-4 ML-CS 0.66 0.57 NN-5 All 0.80 0.76 between 1 and 1000, was selected based on the the least RMS error in cross-validation.	CG	Cancer Genetics	0
0 Table 1: Regression Results Technique Model Code Feature Type Train r Validation r RR-1 RS/LR 0.51 0.47 Ridge Regression RR-2 Pure ML 0.54 0.47 RR-3 CG 0.63 0.57 RR-4 ML-CS 0.55 0.60 RR-5 All 0.76 0.76 SVM-1 RS/LR 0.50 0.46 SVM SVM-2 Pure ML 0.53 0.46 SVM-3 CG 0.62 0.57 SVM-4 ML-CS 0.60 0.61 SVM-5 All 0.75 0.74 NN-1 RS/LR 0.56 0.51 Neural Networks NN-2 Pure ML 0.60 0.44 NN-3 CG 0.63 0.57 NN-4 ML-CS 0.66 0.57 NN-5 All 0.80 0.76 between 1 and 1000, was selected based on the the least RMS error in cross-validation.	CG	Categorial Grammar	0
Do CG add additional value in predicting grades over and above the features derived from the crowdsourced transcription?	CG	Crowd Grades	1
Do CG add additional value in predicting grades over and above the features derived from the crowdsourced transcription?	CG	Cancer Genetics	0
Do CG add additional value in predicting grades over and above the features derived from the crowdsourced transcription?	CG	Categorial Grammar	0
CG: The crowd transcribes the speech in addition to providing scores on each of the following?	CG	Crowd Grades	1
CG: The crowd transcribes the speech in addition to providing scores on each of the following?	CG	Cancer Genetics	0
CG: The crowd transcribes the speech in addition to providing scores on each of the following?	CG	Categorial Grammar	0
1090 Table 1: Regression Results Technique Model Code Feature Type Train r Validation r RR-1 RS/LR 0.51 0.47 Ridge Regression RR-2 Pure ML 0.54 0.47 RR-3 CG 0.63 0.57 RR-4 ML-CS 0.55 0.60 RR-5 All 0.76 0.76 SVM-1 RS/LR 0.50 0.46 SVM SVM-2 Pure ML 0.53 0.46 SVM-3 CG 0.62 0.57 SVM-4 ML-CS 0.60 0.61 SVM-5 All 0.75 0.74 NN-1 RS/LR 0.56 0.51 Neural Networks NN-2 Pure ML 0.60 0.44 NN-3 CG 0.63 0.57 NN-4 ML-CS 0.66 0.57 NN-5 All 0.80 0.76 between 1 and 1000, was selected based on the the least RMS error in cross-validation.	CG	Crowd Grades	1
1090 Table 1: Regression Results Technique Model Code Feature Type Train r Validation r RR-1 RS/LR 0.51 0.47 Ridge Regression RR-2 Pure ML 0.54 0.47 RR-3 CG 0.63 0.57 RR-4 ML-CS 0.55 0.60 RR-5 All 0.76 0.76 SVM-1 RS/LR 0.50 0.46 SVM SVM-2 Pure ML 0.53 0.46 SVM-3 CG 0.62 0.57 SVM-4 ML-CS 0.60 0.61 SVM-5 All 0.75 0.74 NN-1 RS/LR 0.56 0.51 Neural Networks NN-2 Pure ML 0.60 0.44 NN-3 CG 0.63 0.57 NN-4 ML-CS 0.66 0.57 NN-5 All 0.80 0.76 between 1 and 1000, was selected based on the the least RMS error in cross-validation.	CG	Cancer Genetics	0
1090 Table 1: Regression Results Technique Model Code Feature Type Train r Validation r RR-1 RS/LR 0.51 0.47 Ridge Regression RR-2 Pure ML 0.54 0.47 RR-3 CG 0.63 0.57 RR-4 ML-CS 0.55 0.60 RR-5 All 0.76 0.76 SVM-1 RS/LR 0.50 0.46 SVM SVM-2 Pure ML 0.53 0.46 SVM-3 CG 0.62 0.57 SVM-4 ML-CS 0.60 0.61 SVM-5 All 0.75 0.74 NN-1 RS/LR 0.56 0.51 Neural Networks NN-2 Pure ML 0.60 0.44 NN-3 CG 0.63 0.57 NN-4 ML-CS 0.66 0.57 NN-5 All 0.80 0.76 between 1 and 1000, was selected based on the the least RMS error in cross-validation.	CG	Categorial Grammar	0
1University of Engineering and Technology, VNU, Hanoi, Vietnam  2National Institute of Informatics, Tokyo, Japan  3European Bioinformatics Institute, Cambridge, UK  {vutm,lhquynh,thuypv,binhpt}@vnu.edu.vn, collier@ebi.ac.uk     Abstract  We describe a high precision system for ex- tracting events of biomedical significance that  was developed during the BioNLP shared task  2013 and tested on the CG data  set.	CG	Crowd Grades	0
1University of Engineering and Technology, VNU, Hanoi, Vietnam  2National Institute of Informatics, Tokyo, Japan  3European Bioinformatics Institute, Cambridge, UK  {vutm,lhquynh,thuypv,binhpt}@vnu.edu.vn, collier@ebi.ac.uk     Abstract  We describe a high precision system for ex- tracting events of biomedical significance that  was developed during the BioNLP shared task  2013 and tested on the CG data  set.	CG	Cancer Genetics	1
1University of Engineering and Technology, VNU, Hanoi, Vietnam  2National Institute of Informatics, Tokyo, Japan  3European Bioinformatics Institute, Cambridge, UK  {vutm,lhquynh,thuypv,binhpt}@vnu.edu.vn, collier@ebi.ac.uk     Abstract  We describe a high precision system for ex- tracting events of biomedical significance that  was developed during the BioNLP shared task  2013 and tested on the CG data  set.	CG	Categorial Grammar	0
S. V. Ramanan RelAgent Private Ltd. 56, Venkatratnam Nagar Adyar, Chennai 600020 ramanan@npjoint.com P. Senthil Nathan RelAgent Private Ltd. 56, Venkatratnam Nagar Adyar, Chennai 600020 senthil@npjoint.com Abstract We tested a linguistically motivated rule- based system in the CG task  of the BioNLP13 shared task challenge.	CG	Crowd Grades	0
S. V. Ramanan RelAgent Private Ltd. 56, Venkatratnam Nagar Adyar, Chennai 600020 ramanan@npjoint.com P. Senthil Nathan RelAgent Private Ltd. 56, Venkatratnam Nagar Adyar, Chennai 600020 senthil@npjoint.com Abstract We tested a linguistically motivated rule- based system in the CG task  of the BioNLP13 shared task challenge.	CG	Cancer Genetics	1
S. V. Ramanan RelAgent Private Ltd. 56, Venkatratnam Nagar Adyar, Chennai 600020 ramanan@npjoint.com P. Senthil Nathan RelAgent Private Ltd. 56, Venkatratnam Nagar Adyar, Chennai 600020 senthil@npjoint.com Abstract We tested a linguistically motivated rule- based system in the CG task  of the BioNLP13 shared task challenge.	CG	Categorial Grammar	0
I pre- pared a total of 17 such lexicons, which include 7 that were entered by hand (Greek letters, amino acids, chemical elements, known viruses, plus abbreviations of all these), and 4 corre- sponding to genes, chromosome locations, pro- teins, and cell lines, drawn from online public databases (CGWeb,2 BBID,3 Swis- sProt,4 and the Cell Line Database5).	CG	Crowd Grades	0
I pre- pared a total of 17 such lexicons, which include 7 that were entered by hand (Greek letters, amino acids, chemical elements, known viruses, plus abbreviations of all these), and 4 corre- sponding to genes, chromosome locations, pro- teins, and cell lines, drawn from online public databases (CGWeb,2 BBID,3 Swis- sProt,4 and the Cell Line Database5).	CG	Cancer Genetics	1
I pre- pared a total of 17 such lexicons, which include 7 that were entered by hand (Greek letters, amino acids, chemical elements, known viruses, plus abbreviations of all these), and 4 corre- sponding to genes, chromosome locations, pro- teins, and cell lines, drawn from online public databases (CGWeb,2 BBID,3 Swis- sProt,4 and the Cell Line Database5).	CG	Categorial Grammar	0
c?2013 Association for Computational Linguistics Generalizing an Approximate Subgraph Matching-based System to Extract Events in Molecular Biology and CG Haibin Liu haibin.liu@nih.gov NCBI, Bethesda, MD, USA Karin Verspoor karin.verspoor@nicta.com.au NICTA, Melbourne, VIC, Australia Donald C. Comeau comeau@ncbi.nlm.nih.gov NCBI, Bethesda, MD, USA Andrew MacKinlay andrew.mackinlay@nicta.com.au NICTA, Melbourne, VIC, Australia W. John Wilbur wilbur@ncbi.nlm.nih.gov NCBI, Bethesda, MD, USA Abstract We participated in the BioNLP 2013 sh	CG	Crowd Grades	0
c?2013 Association for Computational Linguistics Generalizing an Approximate Subgraph Matching-based System to Extract Events in Molecular Biology and CG Haibin Liu haibin.liu@nih.gov NCBI, Bethesda, MD, USA Karin Verspoor karin.verspoor@nicta.com.au NICTA, Melbourne, VIC, Australia Donald C. Comeau comeau@ncbi.nlm.nih.gov NCBI, Bethesda, MD, USA Andrew MacKinlay andrew.mackinlay@nicta.com.au NICTA, Melbourne, VIC, Australia W. John Wilbur wilbur@ncbi.nlm.nih.gov NCBI, Bethesda, MD, USA Abstract We participated in the BioNLP 2013 sh	CG	Cancer Genetics	1
c?2013 Association for Computational Linguistics Generalizing an Approximate Subgraph Matching-based System to Extract Events in Molecular Biology and CG Haibin Liu haibin.liu@nih.gov NCBI, Bethesda, MD, USA Karin Verspoor karin.verspoor@nicta.com.au NICTA, Melbourne, VIC, Australia Donald C. Comeau comeau@ncbi.nlm.nih.gov NCBI, Bethesda, MD, USA Andrew MacKinlay andrew.mackinlay@nicta.com.au NICTA, Melbourne, VIC, Australia W. John Wilbur wilbur@ncbi.nlm.nih.gov NCBI, Bethesda, MD, USA Abstract We participated in the BioNLP 2013 sh	CG	Categorial Grammar	0
1987  CG, Unification Grammar and Parsing.	CG	Crowd Grades	0
1987  CG, Unification Grammar and Parsing.	CG	Cancer Genetics	0
1987  CG, Unification Grammar and Parsing.	CG	Categorial Grammar	1
The framework is based on  Combinatory CGs and it  uses the morpheme as the basic building  block of the categorial lexicon.	CG	Crowd Grades	0
The framework is based on  Combinatory CGs and it  uses the morpheme as the basic building  block of the categorial lexicon.	CG	Cancer Genetics	0
The framework is based on  Combinatory CGs and it  uses the morpheme as the basic building  block of the categorial lexicon.	CG	Categorial Grammar	1
CG.	CG	Crowd Grades	0
CG.	CG	Cancer Genetics	0
CG.	CG	Categorial Grammar	1
1987 CGs and Natural Language Structures, D.  Reidel, Dordrecht, Holland.	CG	Crowd Grades	0
1987 CGs and Natural Language Structures, D.  Reidel, Dordrecht, Holland.	CG	Cancer Genetics	0
1987 CGs and Natural Language Structures, D.  Reidel, Dordrecht, Holland.	CG	Categorial Grammar	1
In order to perform morphological nd syntactic  compositions in a unified framework, the slash oper-  ators of CG must be enriched with  the knowledge about the type of process and the  type of morpheme.	CG	Crowd Grades	0
In order to perform morphological nd syntactic  compositions in a unified framework, the slash oper-  ators of CG must be enriched with  the knowledge about the type of process and the  type of morpheme.	CG	Cancer Genetics	0
In order to perform morphological nd syntactic  compositions in a unified framework, the slash oper-  ators of CG must be enriched with  the knowledge about the type of process and the  type of morpheme.	CG	Categorial Grammar	1
\[Calder, 89\] J. Calder, M. Reape, and H. Zeevat,  "An Algorithm for Generation i  Unification  CG", Proceedings of the 4th  Conference of the European Chapter of the  ACL, pp.	CG	Crowd Grades	0
\[Calder, 89\] J. Calder, M. Reape, and H. Zeevat,  "An Algorithm for Generation i  Unification  CG", Proceedings of the 4th  Conference of the European Chapter of the  ACL, pp.	CG	Cancer Genetics	0
\[Calder, 89\] J. Calder, M. Reape, and H. Zeevat,  "An Algorithm for Generation i  Unification  CG", Proceedings of the 4th  Conference of the European Chapter of the  ACL, pp.	CG	Categorial Grammar	1
Proceedings of the 3rd Workshop on Computational LNguistics for Literature (CLfL) @ EACL 2014, pages 40?49, Gothenburg, Sweden, April 27, 2014.	LN	Lin	1
Proceedings of the 3rd Workshop on Computational LNguistics for Literature (CLfL) @ EACL 2014, pages 40?49, Gothenburg, Sweden, April 27, 2014.	LN	lemma ngrams	0
Proceedings of the 3rd Workshop on Computational LNguistics for Literature (CLfL) @ EACL 2014, pages 40?49, Gothenburg, Sweden, April 27, 2014.	LN	location name	0
Proceedings of the 3rd Workshop on Computational LNguistics for Literature (CLfL) @ EACL 2014, pages 40?49, Gothenburg, Sweden, April 27, 2014.	LN	Logical Necessity	0
Proceedings of the 3rd Workshop on Computational LNguistics for Literature (CLfL) @ EACL 2014, pages 40?49, Gothenburg, Sweden, April 27, 2014.	LN	Location Name	0
Associa-  tion of Computational LNguistics.	LN	Lin	1
Associa-  tion of Computational LNguistics.	LN	lemma ngrams	0
Associa-  tion of Computational LNguistics.	LN	location name	0
Associa-  tion of Computational LNguistics.	LN	Logical Necessity	0
Associa-  tion of Computational LNguistics.	LN	Location Name	0
c?2014 Association for Computational LNguistics From Speaker Identification to Affective Analysis: A Multi-Step System for Analyzing Children?s Stories Elias Iosif?	LN	Lin	1
c?2014 Association for Computational LNguistics From Speaker Identification to Affective Analysis: A Multi-Step System for Analyzing Children?s Stories Elias Iosif?	LN	lemma ngrams	0
c?2014 Association for Computational LNguistics From Speaker Identification to Affective Analysis: A Multi-Step System for Analyzing Children?s Stories Elias Iosif?	LN	location name	0
c?2014 Association for Computational LNguistics From Speaker Identification to Affective Analysis: A Multi-Step System for Analyzing Children?s Stories Elias Iosif?	LN	Logical Necessity	0
c?2014 Association for Computational LNguistics From Speaker Identification to Affective Analysis: A Multi-Step System for Analyzing Children?s Stories Elias Iosif?	LN	Location Name	0
Com-  putational LNguistics, 19(1), March.	LN	Lin	1
Com-  putational LNguistics, 19(1), March.	LN	lemma ngrams	0
Com-  putational LNguistics, 19(1), March.	LN	location name	0
Com-  putational LNguistics, 19(1), March.	LN	Logical Necessity	0
Com-  putational LNguistics, 19(1), March.	LN	Location Name	0
Computational LNguistics,  19:313-330.	LN	Lin	1
Computational LNguistics,  19:313-330.	LN	lemma ngrams	0
Computational LNguistics,  19:313-330.	LN	location name	0
Computational LNguistics,  19:313-330.	LN	Logical Necessity	0
Computational LNguistics,  19:313-330.	LN	Location Name	0
Computational LNguistics, pages 535-  "561.	LN	Lin	1
Computational LNguistics, pages 535-  "561.	LN	lemma ngrams	0
Computational LNguistics, pages 535-  "561.	LN	location name	0
Computational LNguistics, pages 535-  "561.	LN	Logical Necessity	0
Computational LNguistics, pages 535-  "561.	LN	Location Name	0
We filter most such cases by allowing only capital- ized LNs.	LN	Lin	0
We filter most such cases by allowing only capital- ized LNs.	LN	lemma ngrams	0
We filter most such cases by allowing only capital- ized LNs.	LN	location name	1
We filter most such cases by allowing only capital- ized LNs.	LN	Logical Necessity	0
We filter most such cases by allowing only capital- ized LNs.	LN	Location Name	0
which allows LNs and connections to be captured starting from the given seed location set.	LN	Lin	0
which allows LNs and connections to be captured starting from the given seed location set.	LN	lemma ngrams	0
which allows LNs and connections to be captured starting from the given seed location set.	LN	location name	1
which allows LNs and connections to be captured starting from the given seed location set.	LN	Logical Necessity	0
which allows LNs and connections to be captured starting from the given seed location set.	LN	Location Name	0
We acquire search engine snip- pets and extract contexts where LNs co- appear.	LN	Lin	0
We acquire search engine snip- pets and extract contexts where LNs co- appear.	LN	lemma ngrams	0
We acquire search engine snip- pets and extract contexts where LNs co- appear.	LN	location name	1
We acquire search engine snip- pets and extract contexts where LNs co- appear.	LN	Logical Necessity	0
We acquire search engine snip- pets and extract contexts where LNs co- appear.	LN	Location Name	0
refers to at least 5 LNs including farms in Africa and Australia and a locality in Ireland.	LN	Lin	0
refers to at least 5 LNs including farms in Africa and Australia and a locality in Ireland.	LN	lemma ngrams	0
refers to at least 5 LNs including farms in Africa and Australia and a locality in Ireland.	LN	location name	1
refers to at least 5 LNs including farms in Africa and Australia and a locality in Ireland.	LN	Logical Necessity	0
refers to at least 5 LNs including farms in Africa and Australia and a locality in Ireland.	LN	Location Name	0
In this paper we present a framework that given a few seed locations as a specification of a region, discovers additional locations (including alternate LNs) and map-like travel paths through this region labeled by transport type labels.	LN	Lin	0
In this paper we present a framework that given a few seed locations as a specification of a region, discovers additional locations (including alternate LNs) and map-like travel paths through this region labeled by transport type labels.	LN	lemma ngrams	0
In this paper we present a framework that given a few seed locations as a specification of a region, discovers additional locations (including alternate LNs) and map-like travel paths through this region labeled by transport type labels.	LN	location name	1
In this paper we present a framework that given a few seed locations as a specification of a region, discovers additional locations (including alternate LNs) and map-like travel paths through this region labeled by transport type labels.	LN	Logical Necessity	0
In this paper we present a framework that given a few seed locations as a specification of a region, discovers additional locations (including alternate LNs) and map-like travel paths through this region labeled by transport type labels.	LN	Location Name	0
Given a LN L we start the search with patterns ?	LN	Lin	0
Given a LN L we start the search with patterns ?	LN	lemma ngrams	0
Given a LN L we start the search with patterns ?	LN	location name	1
Given a LN L we start the search with patterns ?	LN	Logical Necessity	0
Given a LN L we start the search with patterns ?	LN	Location Name	0
3 The Algorithm As input to our algorithm we are given a seed of a few LNs specifiying some geograph- ical region.	LN	Lin	0
3 The Algorithm As input to our algorithm we are given a seed of a few LNs specifiying some geograph- ical region.	LN	lemma ngrams	0
3 The Algorithm As input to our algorithm we are given a seed of a few LNs specifiying some geograph- ical region.	LN	location name	1
3 The Algorithm As input to our algorithm we are given a seed of a few LNs specifiying some geograph- ical region.	LN	Logical Necessity	0
3 The Algorithm As input to our algorithm we are given a seed of a few LNs specifiying some geograph- ical region.	LN	Location Name	0
LN.	LN	Lin	0
LN.	LN	lemma ngrams	0
LN.	LN	location name	0
LN.	LN	Logical Necessity	1
LN.	LN	Location Name	0
= 2.3 Recognition of Chinese LNs  Based on SVM  The identification process of location names is  the same as that of person names except for the  features extraction.	LN	Lin	0
= 2.3 Recognition of Chinese LNs  Based on SVM  The identification process of location names is  the same as that of person names except for the  features extraction.	LN	lemma ngrams	0
= 2.3 Recognition of Chinese LNs  Based on SVM  The identification process of location names is  the same as that of person names except for the  features extraction.	LN	location name	0
= 2.3 Recognition of Chinese LNs  Based on SVM  The identification process of location names is  the same as that of person names except for the  features extraction.	LN	Logical Necessity	0
= 2.3 Recognition of Chinese LNs  Based on SVM  The identification process of location names is  the same as that of person names except for the  features extraction.	LN	Location Name	1
LC: uni-gram characters in LN or Geopo- litical entity.	LN	Lin	0
LC: uni-gram characters in LN or Geopo- litical entity.	LN	lemma ngrams	0
LC: uni-gram characters in LN or Geopo- litical entity.	LN	location name	0
LC: uni-gram characters in LN or Geopo- litical entity.	LN	Logical Necessity	0
LC: uni-gram characters in LN or Geopo- litical entity.	LN	Location Name	1
= n i if if f FP FP FP                                  (12)  where ,  is the  number of F )2)((log)( 2 += iif FCFP )( iFC i as the i-th middle character of loca- tion names in the Chinese LNs Re- cord.	LN	Lin	0
= n i if if f FP FP FP                                  (12)  where ,  is the  number of F )2)((log)( 2 += iif FCFP )( iFC i as the i-th middle character of loca- tion names in the Chinese LNs Re- cord.	LN	lemma ngrams	0
= n i if if f FP FP FP                                  (12)  where ,  is the  number of F )2)((log)( 2 += iif FCFP )( iFC i as the i-th middle character of loca- tion names in the Chinese LNs Re- cord.	LN	location name	0
= n i if if f FP FP FP                                  (12)  where ,  is the  number of F )2)((log)( 2 += iif FCFP )( iFC i as the i-th middle character of loca- tion names in the Chinese LNs Re- cord.	LN	Logical Necessity	0
= n i if if f FP FP FP                                  (12)  where ,  is the  number of F )2)((log)( 2 += iif FCFP )( iFC i as the i-th middle character of loca- tion names in the Chinese LNs Re- cord.	LN	Location Name	1
i in the Chinese LNs  Record.	LN	Lin	0
i in the Chinese LNs  Record.	LN	lemma ngrams	0
i in the Chinese LNs  Record.	LN	location name	0
i in the Chinese LNs  Record.	LN	Logical Necessity	0
i in the Chinese LNs  Record.	LN	Location Name	1
=                                          (13)  where ,  is the  number of  S as the last character of location  names in the Chinese LNs Record.	LN	Lin	0
=                                          (13)  where ,  is the  number of  S as the last character of location  names in the Chinese LNs Record.	LN	lemma ngrams	0
=                                          (13)  where ,  is the  number of  S as the last character of location  names in the Chinese LNs Record.	LN	location name	0
=                                          (13)  where ,  is the  number of  S as the last character of location  names in the Chinese LNs Record.	LN	Logical Necessity	0
=                                          (13)  where ,  is the  number of  S as the last character of location  names in the Chinese LNs Record.	LN	Location Name	1
=                                          (13)  where ,  is the  number of  S as the last character of location  names in the Chinese LNs	LN	Lin	0
=                                          (13)  where ,  is the  number of  S as the last character of location  names in the Chinese LNs	LN	lemma ngrams	0
=                                          (13)  where ,  is the  number of  S as the last character of location  names in the Chinese LNs	LN	location name	0
=                                          (13)  where ,  is the  number of  S as the last character of location  names in the Chinese LNs	LN	Logical Necessity	0
=                                          (13)  where ,  is the  number of  S as the last character of location  names in the Chinese LNs	LN	Location Name	1
0 in the Chinese LNs  Record.	LN	Lin	0
0 in the Chinese LNs  Record.	LN	lemma ngrams	0
0 in the Chinese LNs  Record.	LN	location name	0
0 in the Chinese LNs  Record.	LN	Logical Necessity	0
0 in the Chinese LNs  Record.	LN	Location Name	1
=                                      (11)  where ,  is the  number of F )2)((log)( 0200 += FCFPh )( 0FC 0 as the first character of location  names in the Chinese LNs Record.	LN	Lin	0
=                                      (11)  where ,  is the  number of F )2)((log)( 0200 += FCFPh )( 0FC 0 as the first character of location  names in the Chinese LNs Record.	LN	lemma ngrams	0
=                                      (11)  where ,  is the  number of F )2)((log)( 0200 += FCFPh )( 0FC 0 as the first character of location  names in the Chinese LNs Record.	LN	location name	0
=                                      (11)  where ,  is the  number of F )2)((log)( 0200 += FCFPh )( 0FC 0 as the first character of location  names in the Chinese LNs Record.	LN	Logical Necessity	0
=                                      (11)  where ,  is the  number of F )2)((log)( 0200 += FCFPh )( 0FC 0 as the first character of location  names in the Chinese LNs Record.	LN	Location Name	1
5.2 Extracting Chinese LNs  We use 1.5M characters corpus of year 1998  from the People?s Daily as the training corpus  and extract sentences of year 2000 from the Peo- ple?s Daily (containing 2919 Chinese location  names) as testing corpus to conduct an open test  experiment.	LN	Lin	0
5.2 Extracting Chinese LNs  We use 1.5M characters corpus of year 1998  from the People?s Daily as the training corpus  and extract sentences of year 2000 from the Peo- ple?s Daily (containing 2919 Chinese location  names) as testing corpus to conduct an open test  experiment.	LN	lemma ngrams	0
5.2 Extracting Chinese LNs  We use 1.5M characters corpus of year 1998  from the People?s Daily as the training corpus  and extract sentences of year 2000 from the Peo- ple?s Daily (containing 2919 Chinese location  names) as testing corpus to conduct an open test  experiment.	LN	location name	0
5.2 Extracting Chinese LNs  We use 1.5M characters corpus of year 1998  from the People?s Daily as the training corpus  and extract sentences of year 2000 from the Peo- ple?s Daily (containing 2919 Chinese location  names) as testing corpus to conduct an open test  experiment.	LN	Logical Necessity	0
5.2 Extracting Chinese LNs  We use 1.5M characters corpus of year 1998  from the People?s Daily as the training corpus  and extract sentences of year 2000 from the Peo- ple?s Daily (containing 2919 Chinese location  names) as testing corpus to conduct an open test  experiment.	LN	Location Name	1
Then the GE score is  computed as:    ????	GE	global entropy	1
Then the GE score is  computed as:    ????	GE	Generalized Expectation	0
Then the GE score is  computed as:    ????	GE	General Event	0
Then the GE score is  computed as:    ????	GE	GE	0
Then the GE score is  computed as:    ????	GE	Genia	0
Then the GE score is  computed as:    ????	GE	Genia Event	0
Then the GE score is  computed as:    ????	GE	generalized expectation	0
Then the GE score is  computed as:    ????	GE	General  Event	0
The GE combines the local entropies for all antecedent candidates of a given anaphor.	GE	global entropy	1
The GE combines the local entropies for all antecedent candidates of a given anaphor.	GE	Generalized Expectation	0
The GE combines the local entropies for all antecedent candidates of a given anaphor.	GE	General Event	0
The GE combines the local entropies for all antecedent candidates of a given anaphor.	GE	GE	0
The GE combines the local entropies for all antecedent candidates of a given anaphor.	GE	Genia	0
The GE combines the local entropies for all antecedent candidates of a given anaphor.	GE	Genia Event	0
The GE combines the local entropies for all antecedent candidates of a given anaphor.	GE	generalized expectation	0
The GE combines the local entropies for all antecedent candidates of a given anaphor.	GE	General  Event	0
GE?	GE	global entropy	1
GE?	GE	Generalized Expectation	0
GE?	GE	General Event	0
GE?	GE	GE	0
GE?	GE	Genia	0
GE?	GE	Genia Event	0
GE?	GE	generalized expectation	0
GE?	GE	General  Event	0
The higher the GE, the higher the uncertainty of the model about the antecedent for an anaphor.	GE	global entropy	1
The higher the GE, the higher the uncertainty of the model about the antecedent for an anaphor.	GE	Generalized Expectation	0
The higher the GE, the higher the uncertainty of the model about the antecedent for an anaphor.	GE	General Event	0
The higher the GE, the higher the uncertainty of the model about the antecedent for an anaphor.	GE	GE	0
The higher the GE, the higher the uncertainty of the model about the antecedent for an anaphor.	GE	Genia	0
The higher the GE, the higher the uncertainty of the model about the antecedent for an anaphor.	GE	Genia Event	0
The higher the GE, the higher the uncertainty of the model about the antecedent for an anaphor.	GE	generalized expectation	0
The higher the GE, the higher the uncertainty of the model about the antecedent for an anaphor.	GE	General  Event	0
The GE aims to combine the uncertainty information from all antecedent candi- dates for a given anaphor (instead of considering only a single candidate-anaphor pair as for LE).	GE	global entropy	1
The GE aims to combine the uncertainty information from all antecedent candi- dates for a given anaphor (instead of considering only a single candidate-anaphor pair as for LE).	GE	Generalized Expectation	0
The GE aims to combine the uncertainty information from all antecedent candi- dates for a given anaphor (instead of considering only a single candidate-anaphor pair as for LE).	GE	General Event	0
The GE aims to combine the uncertainty information from all antecedent candi- dates for a given anaphor (instead of considering only a single candidate-anaphor pair as for LE).	GE	GE	0
The GE aims to combine the uncertainty information from all antecedent candi- dates for a given anaphor (instead of considering only a single candidate-anaphor pair as for LE).	GE	Genia	0
The GE aims to combine the uncertainty information from all antecedent candi- dates for a given anaphor (instead of considering only a single candidate-anaphor pair as for LE).	GE	Genia Event	0
The GE aims to combine the uncertainty information from all antecedent candi- dates for a given anaphor (instead of considering only a single candidate-anaphor pair as for LE).	GE	generalized expectation	0
The GE aims to combine the uncertainty information from all antecedent candi- dates for a given anaphor (instead of considering only a single candidate-anaphor pair as for LE).	GE	General  Event	0
The GE c	GE	global entropy	1
The GE c	GE	Generalized Expectation	0
The GE c	GE	General Event	0
The GE c	GE	GE	0
The GE c	GE	Genia	0
The GE c	GE	Genia Event	0
The GE c	GE	generalized expectation	0
The GE c	GE	General  Event	0
We scaled the rows of the matrix using GE weights and used L = 0.9A and U = 1.1A. 4.4 Term Weighting and Dimension Choice for Multi-Document Summarization A natural term weighting can be obtained by com- puting the row sums of the dimension-reduced approximation to the term-sentence matrix.	GE	global entropy	1
We scaled the rows of the matrix using GE weights and used L = 0.9A and U = 1.1A. 4.4 Term Weighting and Dimension Choice for Multi-Document Summarization A natural term weighting can be obtained by com- puting the row sums of the dimension-reduced approximation to the term-sentence matrix.	GE	Generalized Expectation	0
We scaled the rows of the matrix using GE weights and used L = 0.9A and U = 1.1A. 4.4 Term Weighting and Dimension Choice for Multi-Document Summarization A natural term weighting can be obtained by com- puting the row sums of the dimension-reduced approximation to the term-sentence matrix.	GE	General Event	0
We scaled the rows of the matrix using GE weights and used L = 0.9A and U = 1.1A. 4.4 Term Weighting and Dimension Choice for Multi-Document Summarization A natural term weighting can be obtained by com- puting the row sums of the dimension-reduced approximation to the term-sentence matrix.	GE	GE	0
We scaled the rows of the matrix using GE weights and used L = 0.9A and U = 1.1A. 4.4 Term Weighting and Dimension Choice for Multi-Document Summarization A natural term weighting can be obtained by com- puting the row sums of the dimension-reduced approximation to the term-sentence matrix.	GE	Genia	0
We scaled the rows of the matrix using GE weights and used L = 0.9A and U = 1.1A. 4.4 Term Weighting and Dimension Choice for Multi-Document Summarization A natural term weighting can be obtained by com- puting the row sums of the dimension-reduced approximation to the term-sentence matrix.	GE	Genia Event	0
We scaled the rows of the matrix using GE weights and used L = 0.9A and U = 1.1A. 4.4 Term Weighting and Dimension Choice for Multi-Document Summarization A natural term weighting can be obtained by com- puting the row sums of the dimension-reduced approximation to the term-sentence matrix.	GE	generalized expectation	0
We scaled the rows of the matrix using GE weights and used L = 0.9A and U = 1.1A. 4.4 Term Weighting and Dimension Choice for Multi-Document Summarization A natural term weighting can be obtained by com- puting the row sums of the dimension-reduced approximation to the term-sentence matrix.	GE	General  Event	0
c?2008 Association for Computational Linguistics GE Criteria for Semi-Supervised Learning of Conditional Random Fields Gideon S. Mann Google Inc. 76 Ninth Avenue New York, NY 10011 Andrew McCallum Department of Computer Science University of Massachusetts 140 Governors Drive Amherst, MA 01003 Abstract This paper presents a semi-supervised train- ing method for linear-chain conditional ran- dom fields that makes use of labele	GE	global entropy	0
c?2008 Association for Computational Linguistics GE Criteria for Semi-Supervised Learning of Conditional Random Fields Gideon S. Mann Google Inc. 76 Ninth Avenue New York, NY 10011 Andrew McCallum Department of Computer Science University of Massachusetts 140 Governors Drive Amherst, MA 01003 Abstract This paper presents a semi-supervised train- ing method for linear-chain conditional ran- dom fields that makes use of labele	GE	Generalized Expectation	1
c?2008 Association for Computational Linguistics GE Criteria for Semi-Supervised Learning of Conditional Random Fields Gideon S. Mann Google Inc. 76 Ninth Avenue New York, NY 10011 Andrew McCallum Department of Computer Science University of Massachusetts 140 Governors Drive Amherst, MA 01003 Abstract This paper presents a semi-supervised train- ing method for linear-chain conditional ran- dom fields that makes use of labele	GE	General Event	0
c?2008 Association for Computational Linguistics GE Criteria for Semi-Supervised Learning of Conditional Random Fields Gideon S. Mann Google Inc. 76 Ninth Avenue New York, NY 10011 Andrew McCallum Department of Computer Science University of Massachusetts 140 Governors Drive Amherst, MA 01003 Abstract This paper presents a semi-supervised train- ing method for linear-chain conditional ran- dom fields that makes use of labele	GE	GE	0
c?2008 Association for Computational Linguistics GE Criteria for Semi-Supervised Learning of Conditional Random Fields Gideon S. Mann Google Inc. 76 Ninth Avenue New York, NY 10011 Andrew McCallum Department of Computer Science University of Massachusetts 140 Governors Drive Amherst, MA 01003 Abstract This paper presents a semi-supervised train- ing method for linear-chain conditional ran- dom fields that makes use of labele	GE	Genia	0
c?2008 Association for Computational Linguistics GE Criteria for Semi-Supervised Learning of Conditional Random Fields Gideon S. Mann Google Inc. 76 Ninth Avenue New York, NY 10011 Andrew McCallum Department of Computer Science University of Massachusetts 140 Governors Drive Amherst, MA 01003 Abstract This paper presents a semi-supervised train- ing method for linear-chain conditional ran- dom fields that makes use of labele	GE	Genia Event	0
c?2008 Association for Computational Linguistics GE Criteria for Semi-Supervised Learning of Conditional Random Fields Gideon S. Mann Google Inc. 76 Ninth Avenue New York, NY 10011 Andrew McCallum Department of Computer Science University of Massachusetts 140 Governors Drive Amherst, MA 01003 Abstract This paper presents a semi-supervised train- ing method for linear-chain conditional ran- dom fields that makes use of labele	GE	generalized expectation	0
c?2008 Association for Computational Linguistics GE Criteria for Semi-Supervised Learning of Conditional Random Fields Gideon S. Mann Google Inc. 76 Ninth Avenue New York, NY 10011 Andrew McCallum Department of Computer Science University of Massachusetts 140 Governors Drive Amherst, MA 01003 Abstract This paper presents a semi-supervised train- ing method for linear-chain conditional ran- dom fields that makes use of labele	GE	General  Event	0
GE Criteria for Semi-Supervised Learning of Conditional Random Fields.	GE	global entropy	0
GE Criteria for Semi-Supervised Learning of Conditional Random Fields.	GE	Generalized Expectation	1
GE Criteria for Semi-Supervised Learning of Conditional Random Fields.	GE	General Event	0
GE Criteria for Semi-Supervised Learning of Conditional Random Fields.	GE	GE	0
GE Criteria for Semi-Supervised Learning of Conditional Random Fields.	GE	Genia	0
GE Criteria for Semi-Supervised Learning of Conditional Random Fields.	GE	Genia Event	0
GE Criteria for Semi-Supervised Learning of Conditional Random Fields.	GE	generalized expectation	0
GE Criteria for Semi-Supervised Learning of Conditional Random Fields.	GE	General  Event	0
Additionally, the alternat- ing projection method is computationally more effi- cient than GE (Mann and Mc- Callum, 2008) and can be applied in an online fash- ion using stochastic gradient.	GE	global entropy	0
Additionally, the alternat- ing projection method is computationally more effi- cient than GE (Mann and Mc- Callum, 2008) and can be applied in an online fash- ion using stochastic gradient.	GE	Generalized Expectation	1
Additionally, the alternat- ing projection method is computationally more effi- cient than GE (Mann and Mc- Callum, 2008) and can be applied in an online fash- ion using stochastic gradient.	GE	General Event	0
Additionally, the alternat- ing projection method is computationally more effi- cient than GE (Mann and Mc- Callum, 2008) and can be applied in an online fash- ion using stochastic gradient.	GE	GE	0
Additionally, the alternat- ing projection method is computationally more effi- cient than GE (Mann and Mc- Callum, 2008) and can be applied in an online fash- ion using stochastic gradient.	GE	Genia	0
Additionally, the alternat- ing projection method is computationally more effi- cient than GE (Mann and Mc- Callum, 2008) and can be applied in an online fash- ion using stochastic gradient.	GE	Genia Event	0
Additionally, the alternat- ing projection method is computationally more effi- cient than GE (Mann and Mc- Callum, 2008) and can be applied in an online fash- ion using stochastic gradient.	GE	generalized expectation	0
Additionally, the alternat- ing projection method is computationally more effi- cient than GE (Mann and Mc- Callum, 2008) and can be applied in an online fash- ion using stochastic gradient.	GE	General  Event	0
Druck et al, 2008) constrain the predictions of a multinomial logistic regression model on unla- beled instances in a GE for- mulation for learning from labeled features.	GE	global entropy	0
Druck et al, 2008) constrain the predictions of a multinomial logistic regression model on unla- beled instances in a GE for- mulation for learning from labeled features.	GE	Generalized Expectation	1
Druck et al, 2008) constrain the predictions of a multinomial logistic regression model on unla- beled instances in a GE for- mulation for learning from labeled features.	GE	General Event	0
Druck et al, 2008) constrain the predictions of a multinomial logistic regression model on unla- beled instances in a GE for- mulation for learning from labeled features.	GE	GE	0
Druck et al, 2008) constrain the predictions of a multinomial logistic regression model on unla- beled instances in a GE for- mulation for learning from labeled features.	GE	Genia	0
Druck et al, 2008) constrain the predictions of a multinomial logistic regression model on unla- beled instances in a GE for- mulation for learning from labeled features.	GE	Genia Event	0
Druck et al, 2008) constrain the predictions of a multinomial logistic regression model on unla- beled instances in a GE for- mulation for learning from labeled features.	GE	generalized expectation	0
Druck et al, 2008) constrain the predictions of a multinomial logistic regression model on unla- beled instances in a GE for- mulation for learning from labeled features.	GE	General  Event	0
ME with GE The objec- tive function and the constraints on expectations in (1) can be generalized to: max ?	GE	global entropy	0
ME with GE The objec- tive function and the constraints on expectations in (1) can be generalized to: max ?	GE	Generalized Expectation	1
ME with GE The objec- tive function and the constraints on expectations in (1) can be generalized to: max ?	GE	General Event	0
ME with GE The objec- tive function and the constraints on expectations in (1) can be generalized to: max ?	GE	GE	0
ME with GE The objec- tive function and the constraints on expectations in (1) can be generalized to: max ?	GE	Genia	0
ME with GE The objec- tive function and the constraints on expectations in (1) can be generalized to: max ?	GE	Genia Event	0
ME with GE The objec- tive function and the constraints on expectations in (1) can be generalized to: max ?	GE	generalized expectation	0
ME with GE The objec- tive function and the constraints on expectations in (1) can be generalized to: max ?	GE	General  Event	0
2009 ACL and AFNLP GE Criteria for Bootstrapping Extractors using Record-Text Alignment Kedar Bellare Dept.	GE	global entropy	0
2009 ACL and AFNLP GE Criteria for Bootstrapping Extractors using Record-Text Alignment Kedar Bellare Dept.	GE	Generalized Expectation	1
2009 ACL and AFNLP GE Criteria for Bootstrapping Extractors using Record-Text Alignment Kedar Bellare Dept.	GE	General Event	0
2009 ACL and AFNLP GE Criteria for Bootstrapping Extractors using Record-Text Alignment Kedar Bellare Dept.	GE	GE	0
2009 ACL and AFNLP GE Criteria for Bootstrapping Extractors using Record-Text Alignment Kedar Bellare Dept.	GE	Genia	0
2009 ACL and AFNLP GE Criteria for Bootstrapping Extractors using Record-Text Alignment Kedar Bellare Dept.	GE	Genia Event	0
2009 ACL and AFNLP GE Criteria for Bootstrapping Extractors using Record-Text Alignment Kedar Bellare Dept.	GE	generalized expectation	0
2009 ACL and AFNLP GE Criteria for Bootstrapping Extractors using Record-Text Alignment Kedar Bellare Dept.	GE	General  Event	0
The GE Event and Protein Coreference tasks of the BioNLP Shared Task 2011.	GE	global entropy	0
The GE Event and Protein Coreference tasks of the BioNLP Shared Task 2011.	GE	Generalized Expectation	0
The GE Event and Protein Coreference tasks of the BioNLP Shared Task 2011.	GE	General Event	0
The GE Event and Protein Coreference tasks of the BioNLP Shared Task 2011.	GE	GE	0
The GE Event and Protein Coreference tasks of the BioNLP Shared Task 2011.	GE	Genia	1
The GE Event and Protein Coreference tasks of the BioNLP Shared Task 2011.	GE	Genia Event	0
The GE Event and Protein Coreference tasks of the BioNLP Shared Task 2011.	GE	generalized expectation	0
The GE Event and Protein Coreference tasks of the BioNLP Shared Task 2011.	GE	General  Event	0
We further used some simple fea- tures from syntactic phrases (OpenNLP4 parser) and dependency parse trees (McDonald et al, 2005), ex- tracted using parsers trained on GE corpora.	GE	global entropy	0
We further used some simple fea- tures from syntactic phrases (OpenNLP4 parser) and dependency parse trees (McDonald et al, 2005), ex- tracted using parsers trained on GE corpora.	GE	Generalized Expectation	0
We further used some simple fea- tures from syntactic phrases (OpenNLP4 parser) and dependency parse trees (McDonald et al, 2005), ex- tracted using parsers trained on GE corpora.	GE	General Event	0
We further used some simple fea- tures from syntactic phrases (OpenNLP4 parser) and dependency parse trees (McDonald et al, 2005), ex- tracted using parsers trained on GE corpora.	GE	GE	0
We further used some simple fea- tures from syntactic phrases (OpenNLP4 parser) and dependency parse trees (McDonald et al, 2005), ex- tracted using parsers trained on GE corpora.	GE	Genia	1
We further used some simple fea- tures from syntactic phrases (OpenNLP4 parser) and dependency parse trees (McDonald et al, 2005), ex- tracted using parsers trained on GE corpora.	GE	Genia Event	0
We further used some simple fea- tures from syntactic phrases (OpenNLP4 parser) and dependency parse trees (McDonald et al, 2005), ex- tracted using parsers trained on GE corpora.	GE	generalized expectation	0
We further used some simple fea- tures from syntactic phrases (OpenNLP4 parser) and dependency parse trees (McDonald et al, 2005), ex- tracted using parsers trained on GE corpora.	GE	General  Event	0
n from the GE event cor- pus (Kim et al, 2008), a small handpicked set of event triggers and a list of English stop words.	GE	global entropy	0
n from the GE event cor- pus (Kim et al, 2008), a small handpicked set of event triggers and a list of English stop words.	GE	Generalized Expectation	0
n from the GE event cor- pus (Kim et al, 2008), a small handpicked set of event triggers and a list of English stop words.	GE	General Event	0
n from the GE event cor- pus (Kim et al, 2008), a small handpicked set of event triggers and a list of English stop words.	GE	GE	0
n from the GE event cor- pus (Kim et al, 2008), a small handpicked set of event triggers and a list of English stop words.	GE	Genia	1
n from the GE event cor- pus (Kim et al, 2008), a small handpicked set of event triggers and a list of English stop words.	GE	Genia Event	0
n from the GE event cor- pus (Kim et al, 2008), a small handpicked set of event triggers and a list of English stop words.	GE	generalized expectation	0
n from the GE event cor- pus (Kim et al, 2008), a small handpicked set of event triggers and a list of English stop words.	GE	General  Event	0
3 Trigger Word Tagging The training and the development abstracts were first tokenized and split into sentences using maxi- mum entropy models trained on the GE3 corpora.	GE	global entropy	0
3 Trigger Word Tagging The training and the development abstracts were first tokenized and split into sentences using maxi- mum entropy models trained on the GE3 corpora.	GE	Generalized Expectation	0
3 Trigger Word Tagging The training and the development abstracts were first tokenized and split into sentences using maxi- mum entropy models trained on the GE3 corpora.	GE	General Event	0
3 Trigger Word Tagging The training and the development abstracts were first tokenized and split into sentences using maxi- mum entropy models trained on the GE3 corpora.	GE	GE	0
3 Trigger Word Tagging The training and the development abstracts were first tokenized and split into sentences using maxi- mum entropy models trained on the GE3 corpora.	GE	Genia	1
3 Trigger Word Tagging The training and the development abstracts were first tokenized and split into sentences using maxi- mum entropy models trained on the GE3 corpora.	GE	Genia Event	0
3 Trigger Word Tagging The training and the development abstracts were first tokenized and split into sentences using maxi- mum entropy models trained on the GE3 corpora.	GE	generalized expectation	0
3 Trigger Word Tagging The training and the development abstracts were first tokenized and split into sentences using maxi- mum entropy models trained on the GE3 corpora.	GE	General  Event	0
dict(i,d) i appears in dictionary d. genia(i,p) i is event clue in the GE corpus with precision p. dep(i,j,d,parser) i is head of token j with dependency d according to parser parser.	GE	global entropy	0
dict(i,d) i appears in dictionary d. genia(i,p) i is event clue in the GE corpus with precision p. dep(i,j,d,parser) i is head of token j with dependency d according to parser parser.	GE	Generalized Expectation	0
dict(i,d) i appears in dictionary d. genia(i,p) i is event clue in the GE corpus with precision p. dep(i,j,d,parser) i is head of token j with dependency d according to parser parser.	GE	General Event	0
dict(i,d) i appears in dictionary d. genia(i,p) i is event clue in the GE corpus with precision p. dep(i,j,d,parser) i is head of token j with dependency d according to parser parser.	GE	GE	0
dict(i,d) i appears in dictionary d. genia(i,p) i is event clue in the GE corpus with precision p. dep(i,j,d,parser) i is head of token j with dependency d according to parser parser.	GE	Genia	1
dict(i,d) i appears in dictionary d. genia(i,p) i is event clue in the GE corpus with precision p. dep(i,j,d,parser) i is head of token j with dependency d according to parser parser.	GE	Genia Event	0
dict(i,d) i appears in dictionary d. genia(i,p) i is event clue in the GE corpus with precision p. dep(i,j,d,parser) i is head of token j with dependency d according to parser parser.	GE	generalized expectation	0
dict(i,d) i appears in dictionary d. genia(i,p) i is event clue in the GE corpus with precision p. dep(i,j,d,parser) i is head of token j with dependency d according to parser parser.	GE	General  Event	0
dict(i,d) i appears in dictionary d. genia(i,p) i is event clue in the GE corpu	GE	global entropy	0
dict(i,d) i appears in dictionary d. genia(i,p) i is event clue in the GE corpu	GE	Generalized Expectation	0
dict(i,d) i appears in dictionary d. genia(i,p) i is event clue in the GE corpu	GE	General Event	0
dict(i,d) i appears in dictionary d. genia(i,p) i is event clue in the GE corpu	GE	GE	0
dict(i,d) i appears in dictionary d. genia(i,p) i is event clue in the GE corpu	GE	Genia	1
dict(i,d) i appears in dictionary d. genia(i,p) i is event clue in the GE corpu	GE	Genia Event	0
dict(i,d) i appears in dictionary d. genia(i,p) i is event clue in the GE corpu	GE	generalized expectation	0
dict(i,d) i appears in dictionary d. genia(i,p) i is event clue in the GE corpu	GE	General  Event	0
As dictionaries we use a collection of cellular lo- cation terms taken from the GE event cor- pus (Kim et al, 2008), a small handpicked set of event triggers and a list of English stop words.	GE	global entropy	0
As dictionaries we use a collection of cellular lo- cation terms taken from the GE event cor- pus (Kim et al, 2008), a small handpicked set of event triggers and a list of English stop words.	GE	Generalized Expectation	0
As dictionaries we use a collection of cellular lo- cation terms taken from the GE event cor- pus (Kim et al, 2008), a small handpicked set of event triggers and a list of English stop words.	GE	General Event	0
As dictionaries we use a collection of cellular lo- cation terms taken from the GE event cor- pus (Kim et al, 2008), a small handpicked set of event triggers and a list of English stop words.	GE	GE	0
As dictionaries we use a collection of cellular lo- cation terms taken from the GE event cor- pus (Kim et al, 2008), a small handpicked set of event triggers and a list of English stop words.	GE	Genia	1
As dictionaries we use a collection of cellular lo- cation terms taken from the GE event cor- pus (Kim et al, 2008), a small handpicked set of event triggers and a list of English stop words.	GE	Genia Event	0
As dictionaries we use a collection of cellular lo- cation terms taken from the GE event cor- pus (Kim et al, 2008), a small handpicked set of event triggers and a list of English stop words.	GE	generalized expectation	0
As dictionaries we use a collection of cellular lo- cation terms taken from the GE event cor- pus (Kim et al, 2008), a small handpicked set of event triggers and a list of English stop words.	GE	General  Event	0
Overview of the GE Event task in BioNLP Shared Task 2011.	GE	global entropy	0
Overview of the GE Event task in BioNLP Shared Task 2011.	GE	Generalized Expectation	0
Overview of the GE Event task in BioNLP Shared Task 2011.	GE	General Event	0
Overview of the GE Event task in BioNLP Shared Task 2011.	GE	GE	0
Overview of the GE Event task in BioNLP Shared Task 2011.	GE	Genia	1
Overview of the GE Event task in BioNLP Shared Task 2011.	GE	Genia Event	0
Overview of the GE Event task in BioNLP Shared Task 2011.	GE	generalized expectation	0
Overview of the GE Event task in BioNLP Shared Task 2011.	GE	General  Event	0
Overview of the GE task in BioNLP Shared Task 2011.	GE	global entropy	0
Overview of the GE task in BioNLP Shared Task 2011.	GE	Generalized Expectation	0
Overview of the GE task in BioNLP Shared Task 2011.	GE	General Event	0
Overview of the GE task in BioNLP Shared Task 2011.	GE	GE	0
Overview of the GE task in BioNLP Shared Task 2011.	GE	Genia	0
Overview of the GE task in BioNLP Shared Task 2011.	GE	Genia Event	1
Overview of the GE task in BioNLP Shared Task 2011.	GE	generalized expectation	0
Overview of the GE task in BioNLP Shared Task 2011.	GE	General  Event	0
The GE corpus (Kim, Ohta, and Tsujii 2008) contains 9,372 sentences where biological events are annotated with negation and uncertainty.	GE	global entropy	0
The GE corpus (Kim, Ohta, and Tsujii 2008) contains 9,372 sentences where biological events are annotated with negation and uncertainty.	GE	Generalized Expectation	0
The GE corpus (Kim, Ohta, and Tsujii 2008) contains 9,372 sentences where biological events are annotated with negation and uncertainty.	GE	General Event	0
The GE corpus (Kim, Ohta, and Tsujii 2008) contains 9,372 sentences where biological events are annotated with negation and uncertainty.	GE	GE	0
The GE corpus (Kim, Ohta, and Tsujii 2008) contains 9,372 sentences where biological events are annotated with negation and uncertainty.	GE	Genia	0
The GE corpus (Kim, Ohta, and Tsujii 2008) contains 9,372 sentences where biological events are annotated with negation and uncertainty.	GE	Genia Event	1
The GE corpus (Kim, Ohta, and Tsujii 2008) contains 9,372 sentences where biological events are annotated with negation and uncertainty.	GE	generalized expectation	0
The GE corpus (Kim, Ohta, and Tsujii 2008) contains 9,372 sentences where biological events are annotated with negation and uncertainty.	GE	General  Event	0
Because the GE and BioScope corpus share 958 abstracts, it is possible to compare their annotations, as it is done by Vincze et al (2010).	GE	global entropy	0
Because the GE and BioScope corpus share 958 abstracts, it is possible to compare their annotations, as it is done by Vincze et al (2010).	GE	Generalized Expectation	0
Because the GE and BioScope corpus share 958 abstracts, it is possible to compare their annotations, as it is done by Vincze et al (2010).	GE	General Event	0
Because the GE and BioScope corpus share 958 abstracts, it is possible to compare their annotations, as it is done by Vincze et al (2010).	GE	GE	0
Because the GE and BioScope corpus share 958 abstracts, it is possible to compare their annotations, as it is done by Vincze et al (2010).	GE	Genia	0
Because the GE and BioScope corpus share 958 abstracts, it is possible to compare their annotations, as it is done by Vincze et al (2010).	GE	Genia Event	1
Because the GE and BioScope corpus share 958 abstracts, it is possible to compare their annotations, as it is done by Vincze et al (2010).	GE	generalized expectation	0
Because the GE and BioScope corpus share 958 abstracts, it is possible to compare their annotations, as it is done by Vincze et al (2010).	GE	General  Event	0
The GE and Protein Coreference tasks of the BioNLP Shared Task 2011.	GE	global entropy	0
The GE and Protein Coreference tasks of the BioNLP Shared Task 2011.	GE	Generalized Expectation	0
The GE and Protein Coreference tasks of the BioNLP Shared Task 2011.	GE	General Event	0
The GE and Protein Coreference tasks of the BioNLP Shared Task 2011.	GE	GE	0
The GE and Protein Coreference tasks of the BioNLP Shared Task 2011.	GE	Genia	0
The GE and Protein Coreference tasks of the BioNLP Shared Task 2011.	GE	Genia Event	1
The GE and Protein Coreference tasks of the BioNLP Shared Task 2011.	GE	generalized expectation	0
The GE and Protein Coreference tasks of the BioNLP Shared Task 2011.	GE	General  Event	0
Lever- aging existing resources using GE criteria.	GE	global entropy	0
Lever- aging existing resources using GE criteria.	GE	Generalized Expectation	0
Lever- aging existing resources using GE criteria.	GE	General Event	0
Lever- aging existing resources using GE criteria.	GE	GE	0
Lever- aging existing resources using GE criteria.	GE	Genia	0
Lever- aging existing resources using GE criteria.	GE	Genia Event	0
Lever- aging existing resources using GE criteria.	GE	generalized expectation	1
Lever- aging existing resources using GE criteria.	GE	General  Event	0
Equation (5) is an instance of GE criteria (Mann and Mc- Callum, 2008) that penalizes the divergence of a specific model expectation from a given target value.	GE	global entropy	0
Equation (5) is an instance of GE criteria (Mann and Mc- Callum, 2008) that penalizes the divergence of a specific model expectation from a given target value.	GE	Generalized Expectation	0
Equation (5) is an instance of GE criteria (Mann and Mc- Callum, 2008) that penalizes the divergence of a specific model expectation from a given target value.	GE	General Event	0
Equation (5) is an instance of GE criteria (Mann and Mc- Callum, 2008) that penalizes the divergence of a specific model expectation from a given target value.	GE	GE	0
Equation (5) is an instance of GE criteria (Mann and Mc- Callum, 2008) that penalizes the divergence of a specific model expectation from a given target value.	GE	Genia	0
Equation (5) is an instance of GE criteria (Mann and Mc- Callum, 2008) that penalizes the divergence of a specific model expectation from a given target value.	GE	Genia Event	0
Equation (5) is an instance of GE criteria (Mann and Mc- Callum, 2008) that penalizes the divergence of a specific model expectation from a given target value.	GE	generalized expectation	1
Equation (5) is an instance of GE criteria (Mann and Mc- Callum, 2008) that penalizes the divergence of a specific model expectation from a given target value.	GE	General  Event	0
The use of GE criteria allows for a dramatic reduction in annotation time by shifting from traditional instance-labeling to feature-labeling, and the methods presented outperform traditional CRF training and other semi-supervised methods when limited human effort is available.	GE	global entropy	0
The use of GE criteria allows for a dramatic reduction in annotation time by shifting from traditional instance-labeling to feature-labeling, and the methods presented outperform traditional CRF training and other semi-supervised methods when limited human effort is available.	GE	Generalized Expectation	0
The use of GE criteria allows for a dramatic reduction in annotation time by shifting from traditional instance-labeling to feature-labeling, and the methods presented outperform traditional CRF training and other semi-supervised methods when limited human effort is available.	GE	General Event	0
The use of GE criteria allows for a dramatic reduction in annotation time by shifting from traditional instance-labeling to feature-labeling, and the methods presented outperform traditional CRF training and other semi-supervised methods when limited human effort is available.	GE	GE	0
The use of GE criteria allows for a dramatic reduction in annotation time by shifting from traditional instance-labeling to feature-labeling, and the methods presented outperform traditional CRF training and other semi-supervised methods when limited human effort is available.	GE	Genia	0
The use of GE criteria allows for a dramatic reduction in annotation time by shifting from traditional instance-labeling to feature-labeling, and the methods presented outperform traditional CRF training and other semi-supervised methods when limited human effort is available.	GE	Genia Event	0
The use of GE criteria allows for a dramatic reduction in annotation time by shifting from traditional instance-labeling to feature-labeling, and the methods presented outperform traditional CRF training and other semi-supervised methods when limited human effort is available.	GE	generalized expectation	1
The use of GE criteria allows for a dramatic reduction in annotation time by shifting from traditional instance-labeling to feature-labeling, and the methods presented outperform traditional CRF training and other semi-supervised methods when limited human effort is available.	GE	General  Event	0
The use of GE criteria allow	GE	global entropy	0
The use of GE criteria allow	GE	Generalized Expectation	0
The use of GE criteria allow	GE	General Event	0
The use of GE criteria allow	GE	GE	0
The use of GE criteria allow	GE	Genia	0
The use of GE criteria allow	GE	Genia Event	0
The use of GE criteria allow	GE	generalized expectation	1
The use of GE criteria allow	GE	General  Event	0
This is accom- plished by using GE cri- teria to express a preference for parameter set- tings in which the model?s distribution on un- labeled data matches a target distribution.	GE	global entropy	0
This is accom- plished by using GE cri- teria to express a preference for parameter set- tings in which the model?s distribution on un- labeled data matches a target distribution.	GE	Generalized Expectation	0
This is accom- plished by using GE cri- teria to express a preference for parameter set- tings in which the model?s distribution on un- labeled data matches a target distribution.	GE	General Event	0
This is accom- plished by using GE cri- teria to express a preference for parameter set- tings in which the model?s distribution on un- labeled data matches a target distribution.	GE	GE	0
This is accom- plished by using GE cri- teria to express a preference for parameter set- tings in which the model?s distribution on un- labeled data matches a target distribution.	GE	Genia	0
This is accom- plished by using GE cri- teria to express a preference for parameter set- tings in which the model?s distribution on un- labeled data matches a target distribution.	GE	Genia Event	0
This is accom- plished by using GE cri- teria to express a preference for parameter set- tings in which the model?s distribution on un- labeled data matches a target distribution.	GE	generalized expectation	1
This is accom- plished by using GE cri- teria to express a preference for parameter set- tings in which the model?s distribution on un- labeled data matches a target distribution.	GE	General  Event	0
6 Conclusion We have presented GE criteria for linear-chain conditional random fields, a new semi-supervised training method that makes use of labeled features rather than labeled instances.	GE	global entropy	0
6 Conclusion We have presented GE criteria for linear-chain conditional random fields, a new semi-supervised training method that makes use of labeled features rather than labeled instances.	GE	Generalized Expectation	0
6 Conclusion We have presented GE criteria for linear-chain conditional random fields, a new semi-supervised training method that makes use of labeled features rather than labeled instances.	GE	General Event	0
6 Conclusion We have presented GE criteria for linear-chain conditional random fields, a new semi-supervised training method that makes use of labeled features rather than labeled instances.	GE	GE	0
6 Conclusion We have presented GE criteria for linear-chain conditional random fields, a new semi-supervised training method that makes use of labeled features rather than labeled instances.	GE	Genia	0
6 Conclusion We have presented GE criteria for linear-chain conditional random fields, a new semi-supervised training method that makes use of labeled features rather than labeled instances.	GE	Genia Event	0
6 Conclusion We have presented GE criteria for linear-chain conditional random fields, a new semi-supervised training method that makes use of labeled features rather than labeled instances.	GE	generalized expectation	1
6 Conclusion We have presented GE criteria for linear-chain conditional random fields, a new semi-supervised training method that makes use of labeled features rather than labeled instances.	GE	General  Event	0
Semi-supervised learning of dependency parsers using GE criteria.	GE	global entropy	0
Semi-supervised learning of dependency parsers using GE criteria.	GE	Generalized Expectation	0
Semi-supervised learning of dependency parsers using GE criteria.	GE	General Event	0
Semi-supervised learning of dependency parsers using GE criteria.	GE	GE	0
Semi-supervised learning of dependency parsers using GE criteria.	GE	Genia	0
Semi-supervised learning of dependency parsers using GE criteria.	GE	Genia Event	0
Semi-supervised learning of dependency parsers using GE criteria.	GE	generalized expectation	1
Semi-supervised learning of dependency parsers using GE criteria.	GE	General  Event	0
is a MOD of ngabulu milk.	MOD	modifier	1
is a MOD of ngabulu milk.	MOD	modals	0
It has a rich system of case marking, and ad- nominal MODs agree with the heads they modify in case, number, and four genders.	MOD	modifier	1
It has a rich system of case marking, and ad- nominal MODs agree with the heads they modify in case, number, and four genders.	MOD	modals	0
Furthermore, head nouns are gener- ally not required: argument positions can be instan- tiated by MODs only, or, if the referent is clear from the context, by no nominal constituent of any kind.	MOD	modifier	1
Furthermore, head nouns are gener- ally not required: argument positions can be instan- tiated by MODs only, or, if the referent is clear from the context, by no nominal constituent of any kind.	MOD	modals	0
marking, and ad- nominal MODs agree with the heads they modify in case, number, and four genders.	MOD	modifier	1
marking, and ad- nominal MODs agree with the heads they modify in case, number, and four genders.	MOD	modals	0
That is, aside from the constraint that verbal clauses require a clitic cluster (marking subject and object agreement and tense, aspect and mood) in second position, the word order is otherwise free, to the point that noun phrases can be non-contiguous, with head nouns and their MODs separated by un- related words.	MOD	modifier	1
That is, aside from the constraint that verbal clauses require a clitic cluster (marking subject and object agreement and tense, aspect and mood) in second position, the word order is otherwise free, to the point that noun phrases can be non-contiguous, with head nouns and their MODs separated by un- related words.	MOD	modals	0
rder is otherwise free, to the point that noun phrases can be non-contiguous, with head nouns and their MODs separated by un- related words.	MOD	modifier	1
rder is otherwise free, to the point that noun phrases can be non-contiguous, with head nouns and their MODs separated by un- related words.	MOD	modals	0
the constraint that verbal clauses require a clitic cluster (marking subject and object agreement and tense, aspect and mood) in second position, the word order is otherwise free, to the point that noun phrases can be non-contiguous, with head nouns and their MODs separated by un- related words.	MOD	modifier	1
the constraint that verbal clauses require a clitic cluster (marking subject and object agreement and tense, aspect and mood) in second position, the word order is otherwise free, to the point that noun phrases can be non-contiguous, with head nouns and their MODs separated by un- related words.	MOD	modals	0
are predicated of the same entity re- quires a departure from the ordinary way that heads are combined with arguments and MODs com- bined with heads in HPSG in general and in the Matrix in particular.3 In the Grammar Matrix, as in most work in HPSG, lexical heads record the de- pendents they require in valence lists (SUBJ, COMPS, SPR).	MOD	modifier	1
are predicated of the same entity re- quires a departure from the ordinary way that heads are combined with arguments and MODs com- bined with heads in HPSG in general and in the Matrix in particular.3 In the Grammar Matrix, as in most work in HPSG, lexical heads record the de- pendents they require in valence lists (SUBJ, COMPS, SPR).	MOD	modals	0
For the same reason, they will appear in postnominal posi- tion when acting as MODs.	MOD	modifier	1
For the same reason, they will appear in postnominal posi- tion when acting as MODs.	MOD	modals	0
However~ it has been argued convincingly by  Ross  (1967a) and others that MOD  should be analyzed as main  ver"bs  o?	MOD	modifier	0
However~ it has been argued convincingly by  Ross  (1967a) and others that MOD  should be analyzed as main  ver"bs  o?	MOD	modals	1
Verb moods are composed with the aid of MOD, while tenses and expressions are built with the aid of auxiliary verbs.	MOD	modifier	0
Verb moods are composed with the aid of MOD, while tenses and expressions are built with the aid of auxiliary verbs.	MOD	modals	1
We also assign the main clause to past, present, or future tense by using the fine-grained POS tag and a set of heuristics (for example, to check for MOD).4 We assign a tense agreement score to each verb v as follows.	MOD	modifier	0
We also assign the main clause to past, present, or future tense by using the fine-grained POS tag and a set of heuristics (for example, to check for MOD).4 We assign a tense agreement score to each verb v as follows.	MOD	modals	1
Many lexical and grammatical devices aid hedging (expressions such as epistemics verbs, MOD, adjectives, etc.	MOD	modifier	0
Many lexical and grammatical devices aid hedging (expressions such as epistemics verbs, MOD, adjectives, etc.	MOD	modals	1
The present pr`oblem~ is  in  fact~ another  point in favor` of  the v iew that  MOD  o r ig inate  in a  higher" sentence~ because i t  enables us to acknowledge the s imi la r i ty   of  the anomaly  in (10) and ( \ ]1 ) .	MOD	modifier	0
The present pr`oblem~ is  in  fact~ another  point in favor` of  the v iew that  MOD  o r ig inate  in a  higher" sentence~ because i t  enables us to acknowledge the s imi la r i ty   of  the anomaly  in (10) and ( \ ]1 ) .	MOD	modals	1
Secondly, since many stop words (e.g. determiners, MOD) typi- cally demonstrate little variation in the dependencies they engage in, we ignore type equivalences for stop words and implement only exact matching of depen- dencies. (	MOD	modifier	0
Secondly, since many stop words (e.g. determiners, MOD) typi- cally demonstrate little variation in the dependencies they engage in, we ignore type equivalences for stop words and implement only exact matching of depen- dencies. (	MOD	modals	1
CPs in Urdu.	CP	Complex Predicate	1
CPs in Urdu.	CP	cluster purity	0
CPs in Urdu.	CP	Correspondence Part	0
CPs in Urdu.	CP	Constraint Programming	0
CPs.	CP	Complex Predicate	1
CPs.	CP	cluster purity	0
CPs.	CP	Correspondence Part	0
CPs.	CP	Constraint Programming	0
CPs:Structure and  Theory.	CP	Complex Predicate	1
CPs:Structure and  Theory.	CP	cluster purity	0
CPs:Structure and  Theory.	CP	Correspondence Part	0
CPs:Structure and  Theory.	CP	Constraint Programming	0
Words by Default:  Inheritance and the Persian CP  Construction.?	CP	Complex Predicate	1
Words by Default:  Inheritance and the Persian CP  Construction.?	CP	cluster purity	0
Words by Default:  Inheritance and the Persian CP  Construction.?	CP	Correspondence Part	0
Words by Default:  Inheritance and the Persian CP  Construction.?	CP	Constraint Programming	0
CPs in In- dian Language Wordnets, Lexical Resources and  Evaluation Journal, 40 (3-4).	CP	Complex Predicate	1
CPs in In- dian Language Wordnets, Lexical Resources and  Evaluation Journal, 40 (3-4).	CP	cluster purity	0
CPs in In- dian Language Wordnets, Lexical Resources and  Evaluation Journal, 40 (3-4).	CP	Correspondence Part	0
CPs in In- dian Language Wordnets, Lexical Resources and  Evaluation Journal, 40 (3-4).	CP	Constraint Programming	0
Detecting CPs in Hindi using  POS Projection across Parallel Corpora, Proceed- ings of the Workshop on Multiword Expressions:  Identifying and Exploiting Underlying Properties,  Sydney, 11?18,  Alex Alsina.	CP	Complex Predicate	1
Detecting CPs in Hindi using  POS Projection across Parallel Corpora, Proceed- ings of the Workshop on Multiword Expressions:  Identifying and Exploiting Underlying Properties,  Sydney, 11?18,  Alex Alsina.	CP	cluster purity	0
Detecting CPs in Hindi using  POS Projection across Parallel Corpora, Proceed- ings of the Workshop on Multiword Expressions:  Identifying and Exploiting Underlying Properties,  Sydney, 11?18,  Alex Alsina.	CP	Correspondence Part	0
Detecting CPs in Hindi using  POS Projection across Parallel Corpora, Proceed- ings of the Workshop on Multiword Expressions:  Identifying and Exploiting Underlying Properties,  Sydney, 11?18,  Alex Alsina.	CP	Constraint Programming	0
Multidimensionality of  Representation: NV CPs in Hindi.	CP	Complex Predicate	1
Multidimensionality of  Representation: NV CPs in Hindi.	CP	cluster purity	0
Multidimensionality of  Representation: NV CPs in Hindi.	CP	Correspondence Part	0
Multidimensionality of  Representation: NV CPs in Hindi.	CP	Constraint Programming	0
On the task of identifying cognates from over 21,000 words in 218 different lan- guages from the Oceanic language family, our model achieves a CP score over 91%, while maintaining pairwise recall over 62%.	CP	Complex Predicate	0
On the task of identifying cognates from over 21,000 words in 218 different lan- guages from the Oceanic language family, our model achieves a CP score over 91%, while maintaining pairwise recall over 62%.	CP	cluster purity	1
On the task of identifying cognates from over 21,000 words in 218 different lan- guages from the Oceanic language family, our model achieves a CP score over 91%, while maintaining pairwise recall over 62%.	CP	Correspondence Part	0
On the task of identifying cognates from over 21,000 words in 218 different lan- guages from the Oceanic language family, our model achieves a CP score over 91%, while maintaining pairwise recall over 62%.	CP	Constraint Programming	0
On the larger Oceanic data, our model can achieve CP scores of 91.8%, while maintaining pairwise recall of 62.1%.	CP	Complex Predicate	0
On the larger Oceanic data, our model can achieve CP scores of 91.8%, while maintaining pairwise recall of 62.1%.	CP	cluster purity	1
On the larger Oceanic data, our model can achieve CP scores of 91.8%, while maintaining pairwise recall of 62.1%.	CP	Correspondence Part	0
On the larger Oceanic data, our model can achieve CP scores of 91.8%, while maintaining pairwise recall of 62.1%.	CP	Constraint Programming	0
The weighted average of  CP (i.e. the number of predominant tags  divided by cluster size) was measured at 88.8%,  which exceeds significantly the precision of 53%  on word type as reported by Sch?tze (1995) on a  related task.	CP	Complex Predicate	0
The weighted average of  CP (i.e. the number of predominant tags  divided by cluster size) was measured at 88.8%,  which exceeds significantly the precision of 53%  on word type as reported by Sch?tze (1995) on a  related task.	CP	cluster purity	1
The weighted average of  CP (i.e. the number of predominant tags  divided by cluster size) was measured at 88.8%,  which exceeds significantly the precision of 53%  on word type as reported by Sch?tze (1995) on a  related task.	CP	Correspondence Part	0
The weighted average of  CP (i.e. the number of predominant tags  divided by cluster size) was measured at 88.8%,  which exceeds significantly the precision of 53%  on word type as reported by Sch?tze (1995) on a  related task.	CP	Constraint Programming	0
We report CP, accuracy, precision, recall, and F1 for our latent variable logistic classifier (LogLV) and a baseline that assigns arguments to clusters accord- ing to their syntactic function (SyntFunc).	CP	Complex Predicate	0
We report CP, accuracy, precision, recall, and F1 for our latent variable logistic classifier (LogLV) and a baseline that assigns arguments to clusters accord- ing to their syntactic function (SyntFunc).	CP	cluster purity	1
We report CP, accuracy, precision, recall, and F1 for our latent variable logistic classifier (LogLV) and a baseline that assigns arguments to clusters accord- ing to their syntactic function (SyntFunc).	CP	Correspondence Part	0
We report CP, accuracy, precision, recall, and F1 for our latent variable logistic classifier (LogLV) and a baseline that assigns arguments to clusters accord- ing to their syntactic function (SyntFunc).	CP	Constraint Programming	0
Our model achieves a CP score of 90.3% on this dataset com- pared to 89.7% reported in Grenager and Manning.	CP	Complex Predicate	0
Our model achieves a CP score of 90.3% on this dataset com- pared to 89.7% reported in Grenager and Manning.	CP	cluster purity	1
Our model achieves a CP score of 90.3% on this dataset com- pared to 89.7% reported in Grenager and Manning.	CP	Correspondence Part	0
Our model achieves a CP score of 90.3% on this dataset com- pared to 89.7% reported in Grenager and Manning.	CP	Constraint Programming	0
First, we report CP, which is a kind of pre- cision measure for clusterings.	CP	Complex Predicate	0
First, we report CP, which is a kind of pre- cision measure for clusterings.	CP	cluster purity	1
First, we report CP, which is a kind of pre- cision measure for clusterings.	CP	Correspondence Part	0
First, we report CP, which is a kind of pre- cision measure for clusterings.	CP	Constraint Programming	0
215  Chart Parsing and CP  Frank Morawietz  Seminar flit Sprachwissenschaft  Universifiit Tfibingen  Wilhehnstr.	CP	Complex Predicate	0
215  Chart Parsing and CP  Frank Morawietz  Seminar flit Sprachwissenschaft  Universifiit Tfibingen  Wilhehnstr.	CP	cluster purity	0
215  Chart Parsing and CP  Frank Morawietz  Seminar flit Sprachwissenschaft  Universifiit Tfibingen  Wilhehnstr.	CP	Correspondence Part	0
215  Chart Parsing and CP  Frank Morawietz  Seminar flit Sprachwissenschaft  Universifiit Tfibingen  Wilhehnstr.	CP	Constraint Programming	1
This solver has been developed in the CP paradigm using the constraint satisfaction approach of (Duchier and Niehren, 2000).	CP	Complex Predicate	0
This solver has been developed in the CP paradigm using the constraint satisfaction approach of (Duchier and Niehren, 2000).	CP	cluster purity	0
This solver has been developed in the CP paradigm using the constraint satisfaction approach of (Duchier and Niehren, 2000).	CP	Correspondence Part	0
This solver has been developed in the CP paradigm using the constraint satisfaction approach of (Duchier and Niehren, 2000).	CP	Constraint Programming	1
Concurrent  CP Languages.	CP	Complex Predicate	0
Concurrent  CP Languages.	CP	cluster purity	0
Concurrent  CP Languages.	CP	Correspondence Part	0
Concurrent  CP Languages.	CP	Constraint Programming	1
Po-  sition paper for the First Workshop on Princi-  ples and Practice of CP  (Rhode Island, April 1993).	CP	Complex Predicate	0
Po-  sition paper for the First Workshop on Princi-  ples and Practice of CP  (Rhode Island, April 1993).	CP	cluster purity	0
Po-  sition paper for the First Workshop on Princi-  ples and Practice of CP  (Rhode Island, April 1993).	CP	Correspondence Part	0
Po-  sition paper for the First Workshop on Princi-  ples and Practice of CP  (Rhode Island, April 1993).	CP	Constraint Programming	1
Principles of CP.	CP	Complex Predicate	0
Principles of CP.	CP	cluster purity	0
Principles of CP.	CP	Correspondence Part	0
Principles of CP.	CP	Constraint Programming	1
In P. van Hentenryck and  V. Saraswat, editors, Principles and Practice of  CP, chapter 2, pages 27-  48.	CP	Complex Predicate	0
In P. van Hentenryck and  V. Saraswat, editors, Principles and Practice of  CP, chapter 2, pages 27-  48.	CP	cluster purity	0
In P. van Hentenryck and  V. Saraswat, editors, Principles and Practice of  CP, chapter 2, pages 27-  48.	CP	Correspondence Part	0
In P. van Hentenryck and  V. Saraswat, editors, Principles and Practice of  CP, chapter 2, pages 27-  48.	CP	Constraint Programming	1
Figure 2 shows the convergence plot of the composite pairwise coreference function based on RF8.	RF	Random Forest	1
Figure 2 shows the convergence plot of the composite pairwise coreference function based on RF8.	RF	REQUEST FOR FEEDBACK	0
Figure 2 shows the convergence plot of the composite pairwise coreference function based on RF8.	RF	Reduction Factor	0
Figure 2 shows the convergence plot of the composite pairwise coreference function based on RF8.	RF	Ratio of Frequencie	0
In a 10-fold cross-validation experiment, we tested a RF classifier (Breiman, 2001) and an SVM (Platt, 1998) with polynomial ker- nel.	RF	Random Forest	1
In a 10-fold cross-validation experiment, we tested a RF classifier (Breiman, 2001) and an SVM (Platt, 1998) with polynomial ker- nel.	RF	REQUEST FOR FEEDBACK	0
In a 10-fold cross-validation experiment, we tested a RF classifier (Breiman, 2001) and an SVM (Platt, 1998) with polynomial ker- nel.	RF	Reduction Factor	0
In a 10-fold cross-validation experiment, we tested a RF classifier (Breiman, 2001) and an SVM (Platt, 1998) with polynomial ker- nel.	RF	Ratio of Frequencie	0
190 To understand the mistakes of the classifier, we manually assessed error patterns within the model of the RF classifier.	RF	Random Forest	1
190 To understand the mistakes of the classifier, we manually assessed error patterns within the model of the RF classifier.	RF	REQUEST FOR FEEDBACK	0
190 To understand the mistakes of the classifier, we manually assessed error patterns within the model of the RF classifier.	RF	Reduction Factor	0
190 To understand the mistakes of the classifier, we manually assessed error patterns within the model of the RF classifier.	RF	Ratio of Frequencie	0
With an overall macro-averaged F1 of .79, RF yielded the best results, both with respect to precision as well as recall.	RF	Random Forest	1
With an overall macro-averaged F1 of .79, RF yielded the best results, both with respect to precision as well as recall.	RF	REQUEST FOR FEEDBACK	0
With an overall macro-averaged F1 of .79, RF yielded the best results, both with respect to precision as well as recall.	RF	Reduction Factor	0
With an overall macro-averaged F1 of .79, RF yielded the best results, both with respect to precision as well as recall.	RF	Ratio of Frequencie	0
RFs.	RF	Random Forest	1
RFs.	RF	REQUEST FOR FEEDBACK	0
RFs.	RF	Reduction Factor	0
RFs.	RF	Ratio of Frequencie	0
2 ranker improved the results for both RF as well as the SVM, so we limited our feature set to the 100 best features.	RF	Random Forest	1
2 ranker improved the results for both RF as well as the SVM, so we limited our feature set to the 100 best features.	RF	REQUEST FOR FEEDBACK	0
2 ranker improved the results for both RF as well as the SVM, so we limited our feature set to the 100 best features.	RF	Reduction Factor	0
2 ranker improved the results for both RF as well as the SVM, so we limited our feature set to the 100 best features.	RF	Ratio of Frequencie	0
The results suggest that knowledge of the student?s confusion-related facial expressions can signifi-cantly enhance dialogue act classification for two types of dialogue acts, GROUNDING and RF.	RF	Random Forest	0
The results suggest that knowledge of the student?s confusion-related facial expressions can signifi-cantly enhance dialogue act classification for two types of dialogue acts, GROUNDING and RF.	RF	REQUEST FOR FEEDBACK	1
The results suggest that knowledge of the student?s confusion-related facial expressions can signifi-cantly enhance dialogue act classification for two types of dialogue acts, GROUNDING and RF.	RF	Reduction Factor	0
The results suggest that knowledge of the student?s confusion-related facial expressions can signifi-cantly enhance dialogue act classification for two types of dialogue acts, GROUNDING and RF.	RF	Ratio of Frequencie	0
This paper has reported on a first step toward using knowledge of user facial expres-sions to improve a dialogue act classification mod-el for tutorial dialogue, and the results demonstrate that facial expressions hold great promise for dis-tinguishing the pedagogically relevant dialogue act RF, and the conversational moves of GROUNDING.	RF	Random Forest	0
This paper has reported on a first step toward using knowledge of user facial expres-sions to improve a dialogue act classification mod-el for tutorial dialogue, and the results demonstrate that facial expressions hold great promise for dis-tinguishing the pedagogically relevant dialogue act RF, and the conversational moves of GROUNDING.	RF	REQUEST FOR FEEDBACK	1
This paper has reported on a first step toward using knowledge of user facial expres-sions to improve a dialogue act classification mod-el for tutorial dialogue, and the results demonstrate that facial expressions hold great promise for dis-tinguishing the pedagogically relevant dialogue act RF, and the conversational moves of GROUNDING.	RF	Reduction Factor	0
This paper has reported on a first step toward using knowledge of user facial expres-sions to improve a dialogue act classification mod-el for tutorial dialogue, and the results demonstrate that facial expressions hold great promise for dis-tinguishing the pedagogically relevant dialogue act RF, and the conversational moves of GROUNDING.	RF	Ratio of Frequencie	0
For example, a student?s RF requires the tutor to assess the condition of the task, rather than to query the in-domain factual knowledge base.	RF	Random Forest	0
For example, a student?s RF requires the tutor to assess the condition of the task, rather than to query the in-domain factual knowledge base.	RF	REQUEST FOR FEEDBACK	1
For example, a student?s RF requires the tutor to assess the condition of the task, rather than to query the in-domain factual knowledge base.	RF	Reduction Factor	0
For example, a student?s RF requires the tutor to assess the condition of the task, rather than to query the in-domain factual knowledge base.	RF	Ratio of Frequencie	0
For RF, the predictive features were presence or absence of AU4 within ten seconds of the longest available history (three turns in the past), as well as the presence of AU4 within five seconds of the current utterance (the utterance whose dialogue act is being classified).	RF	Random Forest	0
For RF, the predictive features were presence or absence of AU4 within ten seconds of the longest available history (three turns in the past), as well as the presence of AU4 within five seconds of the current utterance (the utterance whose dialogue act is being classified).	RF	REQUEST FOR FEEDBACK	1
For RF, the predictive features were presence or absence of AU4 within ten seconds of the longest available history (three turns in the past), as well as the presence of AU4 within five seconds of the current utterance (the utterance whose dialogue act is being classified).	RF	Reduction Factor	0
For RF, the predictive features were presence or absence of AU4 within ten seconds of the longest available history (three turns in the past), as well as the presence of AU4 within five seconds of the current utterance (the utterance whose dialogue act is being classified).	RF	Ratio of Frequencie	0
NICTA Victoria mhlui@unimelb.edu.au, ned@nedletcher.net, oadams@student.unimelb.edu.au, lduong@student.unimelb.edu.au, paulcook@unimelb.edu.au, tb@ldwin.net Abstract The Discriminating between Similar Languages (DSL) shared task at VarDial challenged partici- pants to build an automatic LI system to discriminate between 13 languages in 6 groups of highly-similar languages (or national varieties of the same language).	LI	language identification	1
NICTA Victoria mhlui@unimelb.edu.au, ned@nedletcher.net, oadams@student.unimelb.edu.au, lduong@student.unimelb.edu.au, paulcook@unimelb.edu.au, tb@ldwin.net Abstract The Discriminating between Similar Languages (DSL) shared task at VarDial challenged partici- pants to build an automatic LI system to discriminate between 13 languages in 6 groups of highly-similar languages (or national varieties of the same language).	LI	linear interpolation	0
NICTA Victoria mhlui@unimelb.edu.au, ned@nedletcher.net, oadams@student.unimelb.edu.au, lduong@student.unimelb.edu.au, paulcook@unimelb.edu.au, tb@ldwin.net Abstract The Discriminating between Similar Languages (DSL) shared task at VarDial challenged partici- pants to build an automatic LI system to discriminate between 13 languages in 6 groups of highly-similar languages (or national varieties of the same language).	LI	Linguist	0
Cross-domain feature selection for LI.	LI	language identification	1
Cross-domain feature selection for LI.	LI	linear interpolation	0
Cross-domain feature selection for LI.	LI	Linguist	0
We examined a number of text representations for the per-group language identifiers, including a standard representation for LI based on language-indicative byte sequences, as well as with de-lexicalized text representations.	LI	language identification	1
We examined a number of text representations for the per-group language identifiers, including a standard representation for LI based on language-indicative byte sequences, as well as with de-lexicalized text representations.	LI	linear interpolation	0
We examined a number of text representations for the per-group language identifiers, including a standard representation for LI based on language-indicative byte sequences, as well as with de-lexicalized text representations.	LI	Linguist	0
4 Conclusion Discriminating between similar languages is an interesting sub-problem in LI, and the DSL shared task at VarDial has given us an opportunity to examine possible solutions in greater detail.	LI	language identification	1
4 Conclusion Discriminating between similar languages is an interesting sub-problem in LI, and the DSL shared task at VarDial has given us an opportunity to examine possible solutions in greater detail.	LI	linear interpolation	0
4 Conclusion Discriminating between similar languages is an interesting sub-problem in LI, and the DSL shared task at VarDial has given us an opportunity to examine possible solutions in greater detail.	LI	Linguist	0
langid.py: An off-the-shelf LI tool.	LI	language identification	1
langid.py: An off-the-shelf LI tool.	LI	linear interpolation	0
langid.py: An off-the-shelf LI tool.	LI	Linguist	0
Linguini: LI for multilingual documents.	LI	language identification	1
Linguini: LI for multilingual documents.	LI	linear interpolation	0
Linguini: LI for multilingual documents.	LI	Linguist	0
 a bucketed LI smoothing for both models.	LI	language identification	0
 a bucketed LI smoothing for both models.	LI	linear interpolation	1
 a bucketed LI smoothing for both models.	LI	Linguist	0
Compensation vectors for new envi-  ronments are obtained by LI of several of the  MFCDCN compensation vectors:  ?	LI	language identification	0
Compensation vectors for new envi-  ronments are obtained by LI of several of the  MFCDCN compensation vectors:  ?	LI	linear interpolation	1
Compensation vectors for new envi-  ronments are obtained by LI of several of the  MFCDCN compensation vectors:  ?	LI	Linguist	0
We tested the LI of the in- and general-domain translation models as follows: Given one model which assigns the probability P1(t|s) to the trans- lation of source string s into target string t, and a second model which assigns the probability P2(t|s) to the same event, then the interpolated translation probability is: P (t|s) = ?	LI	language identification	0
We tested the LI of the in- and general-domain translation models as follows: Given one model which assigns the probability P1(t|s) to the trans- lation of source string s into target string t, and a second model which assigns the probability P2(t|s) to the same event, then the interpolated translation probability is: P (t|s) = ?	LI	linear interpolation	1
We tested the LI of the in- and general-domain translation models as follows: Given one model which assigns the probability P1(t|s) to the trans- lation of source string s into target string t, and a second model which assigns the probability P2(t|s) to the same event, then the interpolated translation probability is: P (t|s) = ?	LI	Linguist	0
Table 2 shows the resulting interpolation coefficients for the tag language model using the usual LI smoothing formula ZL}-~IPA?x?)[ffi] ^_ ]A^fi`  1 ]A^ffi`bac ??	LI	language identification	0
Table 2 shows the resulting interpolation coefficients for the tag language model using the usual LI smoothing formula ZL}-~IPA?x?)[ffi] ^_ ]A^fi`  1 ]A^ffi`bac ??	LI	linear interpolation	1
Table 2 shows the resulting interpolation coefficients for the tag language model using the usual LI smoothing formula ZL}-~IPA?x?)[ffi] ^_ ]A^fi`  1 ]A^ffi`bac ??	LI	Linguist	0
Relative frequency can be used directly for es-  timating the word probabilities, and trigram  backoff and LI can be used for  estimating the POS probabilities.	LI	language identification	0
Relative frequency can be used directly for es-  timating the word probabilities, and trigram  backoff and LI can be used for  estimating the POS probabilities.	LI	linear interpolation	1
Relative frequency can be used directly for es-  timating the word probabilities, and trigram  backoff and LI can be used for  estimating the POS probabilities.	LI	Linguist	0
Given a derivation d, most existing phrase- based models approximate the derivation probabil- ity through a LI of a finite set of feature functions (?(	LI	language identification	0
Given a derivation d, most existing phrase- based models approximate the derivation probabil- ity through a LI of a finite set of feature functions (?(	LI	linear interpolation	1
Given a derivation d, most existing phrase- based models approximate the derivation probabil- ity through a LI of a finite set of feature functions (?(	LI	Linguist	0
Proceedings of the 3rd Workshop on Computational LIics for Literature (CLfL) @ EACL 2014, pages 40?49, Gothenburg, Sweden, April 27, 2014.	LI	language identification	0
Proceedings of the 3rd Workshop on Computational LIics for Literature (CLfL) @ EACL 2014, pages 40?49, Gothenburg, Sweden, April 27, 2014.	LI	linear interpolation	0
Proceedings of the 3rd Workshop on Computational LIics for Literature (CLfL) @ EACL 2014, pages 40?49, Gothenburg, Sweden, April 27, 2014.	LI	Linguist	1
Associa-  tion of Computational LIics.	LI	language identification	0
Associa-  tion of Computational LIics.	LI	linear interpolation	0
Associa-  tion of Computational LIics.	LI	Linguist	1
c?2014 Association for Computational LIics From Speaker Identification to Affective Analysis: A Multi-Step System for Analyzing Children?s Stories Elias Iosif?	LI	language identification	0
c?2014 Association for Computational LIics From Speaker Identification to Affective Analysis: A Multi-Step System for Analyzing Children?s Stories Elias Iosif?	LI	linear interpolation	0
c?2014 Association for Computational LIics From Speaker Identification to Affective Analysis: A Multi-Step System for Analyzing Children?s Stories Elias Iosif?	LI	Linguist	1
Com-  putational LIics, 19(1), March.	LI	language identification	0
Com-  putational LIics, 19(1), March.	LI	linear interpolation	0
Com-  putational LIics, 19(1), March.	LI	Linguist	1
Computational LIics,  19:313-330.	LI	language identification	0
Computational LIics,  19:313-330.	LI	linear interpolation	0
Computational LIics,  19:313-330.	LI	Linguist	1
Computational LIics, pages 535-  "561.	LI	language identification	0
Computational LIics, pages 535-  "561.	LI	linear interpolation	0
Computational LIics, pages 535-  "561.	LI	Linguist	1
Statistical analysis of  LNRE and related  problems.	LNRE	large number of rare events	1
Statistical analysis of  LNRE and related  problems.	LNRE	Large Number of Rare Events	0
The statistical analysis of LNRE?,	LNRE	large number of rare events	1
The statistical analysis of LNRE?,	LNRE	Large Number of Rare Events	0
2 Statistical considerations  2.1 Highly skewed distributions  As first observed e.g. by Zipf (1935, 1949) the  frequency of words and other linguistic units tend  to follow highly skewed distributions in which  there are a LNRE.	LNRE	large number of rare events	1
2 Statistical considerations  2.1 Highly skewed distributions  As first observed e.g. by Zipf (1935, 1949) the  frequency of words and other linguistic units tend  to follow highly skewed distributions in which  there are a LNRE.	LNRE	Large Number of Rare Events	0
Sta-  tistical Analysis of LNRE  and Related Problems.	LNRE	large number of rare events	0
Sta-  tistical Analysis of LNRE  and Related Problems.	LNRE	Large Number of Rare Events	1
x)] Since Alchemy outputs a probability of entailment and not a binary judgment, it is necessary to specify a PT indicating entailment.	PT	probability threshold	1
x)] Since Alchemy outputs a probability of entailment and not a binary judgment, it is necessary to specify a PT indicating entailment.	PT	parse tree	0
x)] Since Alchemy outputs a probability of entailment and not a binary judgment, it is necessary to specify a PT indicating entailment.	PT	phrase table	0
x)] Since Alchemy outputs a probability of entailment and not a binary judgment, it is necessary to specify a PT indicating entailment.	PT	Portuguese	0
x)] Since Alchemy outputs a probability of entailment and not a binary judgment, it is necessary to specify a PT indicating entailment.	PT	Parse Trees	0
x)] Since Alchemy outputs a probability of entailment and not a binary judgment, it is necessary to specify a PT indicating entailment.	PT	part	0
x)] Since Alchemy outputs a probability of entailment and not a binary judgment, it is necessary to specify a PT indicating entailment.	PT	Portugese	0
x)] Since Alchemy outputs a probability of entailment and not a binary judgment, it is necessary to specify a PT indicating entailment.	PT	Prompts	0
x)] Since Alchemy outputs a probability of entailment and not a binary judgment, it is necessary to specify a PT indicating entailment.	PT	Probable alignments	0
x)] Since Alchemy outputs a probability of entailment and not a binary judgment, it is necessary to specify a PT indicating entailment.	PT	Parse tree	0
x)] Since Alchemy outputs a probability of entailment and not a binary judgment, it is necessary to specify a PT indicating entailment.	PT	partial tree	0
x)] Since Alchemy outputs a probability of entailment and not a binary judgment, it is necessary to specify a PT indicating entailment.	PT	Phrase Table	0
A more  sophisticated one is the PT strategy,  in which all the categories above a user-defined thresh-  old are assigned to a document.	PT	probability threshold	1
A more  sophisticated one is the PT strategy,  in which all the categories above a user-defined thresh-  old are assigned to a document.	PT	parse tree	0
A more  sophisticated one is the PT strategy,  in which all the categories above a user-defined thresh-  old are assigned to a document.	PT	phrase table	0
A more  sophisticated one is the PT strategy,  in which all the categories above a user-defined thresh-  old are assigned to a document.	PT	Portuguese	0
A more  sophisticated one is the PT strategy,  in which all the categories above a user-defined thresh-  old are assigned to a document.	PT	Parse Trees	0
A more  sophisticated one is the PT strategy,  in which all the categories above a user-defined thresh-  old are assigned to a document.	PT	part	0
A more  sophisticated one is the PT strategy,  in which all the categories above a user-defined thresh-  old are assigned to a document.	PT	Portugese	0
A more  sophisticated one is the PT strategy,  in which all the categories above a user-defined thresh-  old are assigned to a document.	PT	Prompts	0
A more  sophisticated one is the PT strategy,  in which all the categories above a user-defined thresh-  old are assigned to a document.	PT	Probable alignments	0
A more  sophisticated one is the PT strategy,  in which all the categories above a user-defined thresh-  old are assigned to a document.	PT	Parse tree	0
A more  sophisticated one is the PT strategy,  in which all the categories above a user-defined thresh-  old are assigned to a document.	PT	partial tree	0
A more  sophisticated one is the PT strategy,  in which all the categories above a user-defined thresh-  old are assigned to a document.	PT	Phrase Table	0
log-PT - -  the difference between  the log-probability score of the top-most hypothesis  and the bottom-most hypothesis at any given state  of the stack cannot be larger than a given threshold.	PT	probability threshold	1
log-PT - -  the difference between  the log-probability score of the top-most hypothesis  and the bottom-most hypothesis at any given state  of the stack cannot be larger than a given threshold.	PT	parse tree	0
log-PT - -  the difference between  the log-probability score of the top-most hypothesis  and the bottom-most hypothesis at any given state  of the stack cannot be larger than a given threshold.	PT	phrase table	0
log-PT - -  the difference between  the log-probability score of the top-most hypothesis  and the bottom-most hypothesis at any given state  of the stack cannot be larger than a given threshold.	PT	Portuguese	0
log-PT - -  the difference between  the log-probability score of the top-most hypothesis  and the bottom-most hypothesis at any given state  of the stack cannot be larger than a given threshold.	PT	Parse Trees	0
log-PT - -  the difference between  the log-probability score of the top-most hypothesis  and the bottom-most hypothesis at any given state  of the stack cannot be larger than a given threshold.	PT	part	0
log-PT - -  the difference between  the log-probability score of the top-most hypothesis  and the bottom-most hypothesis at any given state  of the stack cannot be larger than a given threshold.	PT	Portugese	0
log-PT - -  the difference between  the log-probability score of the top-most hypothesis  and the bottom-most hypothesis at any given state  of the stack cannot be larger than a given threshold.	PT	Prompts	0
log-PT - -  the difference between  the log-probability score of the top-most hypothesis  and the bottom-most hypothesis at any given state  of the stack cannot be larger than a given threshold.	PT	Probable alignments	0
log-PT - -  the difference between  the log-probability score of the top-most hypothesis  and the bottom-most hypothesis at any given state  of the stack cannot be larger than a given threshold.	PT	Parse tree	0
log-PT - -  the difference between  the log-probability score of the top-most hypothesis  and the bottom-most hypothesis at any given state  of the stack cannot be larger than a given threshold.	PT	partial tree	0
log-PT - -  the difference between  the log-probability score of the top-most hypothesis  and the bottom-most hypothesis at any given state  of the stack cannot be larger than a given threshold.	PT	Phrase Table	0
A lower PT of 0.01 was set, so that hun- dreds of tags of equal likelihood were not produced in the case where the tagger was unable to make an informed prediction.	PT	probability threshold	1
A lower PT of 0.01 was set, so that hun- dreds of tags of equal likelihood were not produced in the case where the tagger was unable to make an informed prediction.	PT	parse tree	0
A lower PT of 0.01 was set, so that hun- dreds of tags of equal likelihood were not produced in the case where the tagger was unable to make an informed prediction.	PT	phrase table	0
A lower PT of 0.01 was set, so that hun- dreds of tags of equal likelihood were not produced in the case where the tagger was unable to make an informed prediction.	PT	Portuguese	0
A lower PT of 0.01 was set, so that hun- dreds of tags of equal likelihood were not produced in the case where the tagger was unable to make an informed prediction.	PT	Parse Trees	0
A lower PT of 0.01 was set, so that hun- dreds of tags of equal likelihood were not produced in the case where the tagger was unable to make an informed prediction.	PT	part	0
A lower PT of 0.01 was set, so that hun- dreds of tags of equal likelihood were not produced in the case where the tagger was unable to make an informed prediction.	PT	Portugese	0
A lower PT of 0.01 was set, so that hun- dreds of tags of equal likelihood were not produced in the case where the tagger was unable to make an informed prediction.	PT	Prompts	0
A lower PT of 0.01 was set, so that hun- dreds of tags of equal likelihood were not produced in the case where the tagger was unable to make an informed prediction.	PT	Probable alignments	0
A lower PT of 0.01 was set, so that hun- dreds of tags of equal likelihood were not produced in the case where the tagger was unable to make an informed prediction.	PT	Parse tree	0
A lower PT of 0.01 was set, so that hun- dreds of tags of equal likelihood were not produced in the case where the tagger was unable to make an informed prediction.	PT	partial tree	0
A lower PT of 0.01 was set, so that hun- dreds of tags of equal likelihood were not produced in the case where the tagger was unable to make an informed prediction.	PT	Phrase Table	0
Ju- rafsky (1996) assumes that processing difficulty is triggered if the correct analysis falls below a certain PT (i.e., is pruned by the parser).	PT	probability threshold	1
Ju- rafsky (1996) assumes that processing difficulty is triggered if the correct analysis falls below a certain PT (i.e., is pruned by the parser).	PT	parse tree	0
Ju- rafsky (1996) assumes that processing difficulty is triggered if the correct analysis falls below a certain PT (i.e., is pruned by the parser).	PT	phrase table	0
Ju- rafsky (1996) assumes that processing difficulty is triggered if the correct analysis falls below a certain PT (i.e., is pruned by the parser).	PT	Portuguese	0
Ju- rafsky (1996) assumes that processing difficulty is triggered if the correct analysis falls below a certain PT (i.e., is pruned by the parser).	PT	Parse Trees	0
Ju- rafsky (1996) assumes that processing difficulty is triggered if the correct analysis falls below a certain PT (i.e., is pruned by the parser).	PT	part	0
Ju- rafsky (1996) assumes that processing difficulty is triggered if the correct analysis falls below a certain PT (i.e., is pruned by the parser).	PT	Portugese	0
Ju- rafsky (1996) assumes that processing difficulty is triggered if the correct analysis falls below a certain PT (i.e., is pruned by the parser).	PT	Prompts	0
Ju- rafsky (1996) assumes that processing difficulty is triggered if the correct analysis falls below a certain PT (i.e., is pruned by the parser).	PT	Probable alignments	0
Ju- rafsky (1996) assumes that processing difficulty is triggered if the correct analysis falls below a certain PT (i.e., is pruned by the parser).	PT	Parse tree	0
Ju- rafsky (1996) assumes that processing difficulty is triggered if the correct analysis falls below a certain PT (i.e., is pruned by the parser).	PT	partial tree	0
Ju- rafsky (1996) assumes that processing difficulty is triggered if the correct analysis falls below a certain PT (i.e., is pruned by the parser).	PT	Phrase Table	0
Given a PT  (0 <  < 1) and a span threshold s, the document is said to support the hypothesis ??	PT	probability threshold	1
Given a PT  (0 <  < 1) and a span threshold s, the document is said to support the hypothesis ??	PT	parse tree	0
Given a PT  (0 <  < 1) and a span threshold s, the document is said to support the hypothesis ??	PT	phrase table	0
Given a PT  (0 <  < 1) and a span threshold s, the document is said to support the hypothesis ??	PT	Portuguese	0
Given a PT  (0 <  < 1) and a span threshold s, the document is said to support the hypothesis ??	PT	Parse Trees	0
Given a PT  (0 <  < 1) and a span threshold s, the document is said to support the hypothesis ??	PT	part	0
Given a PT  (0 <  < 1) and a span threshold s, the document is said to support the hypothesis ??	PT	Portugese	0
Given a PT  (0 <  < 1) and a span threshold s, the document is said to support the hypothesis ??	PT	Prompts	0
Given a PT  (0 <  < 1) and a span threshold s, the document is said to support the hypothesis ??	PT	Probable alignments	0
Given a PT  (0 <  < 1) and a span threshold s, the document is said to support the hypothesis ??	PT	Parse tree	0
Given a PT  (0 <  < 1) and a span threshold s, the document is said to support the hypothesis ??	PT	partial tree	0
Given a PT  (0 <  < 1) and a span threshold s, the document is said to support the hypothesis ??	PT	Phrase Table	0
This algorithm searches the PT in a left-  to-right, breadth-first fashion that obeys the  major reflexive pronoun constraints while giv-  ing a preference to antecedents hat are closer  to the pronoun.	PT	probability threshold	0
This algorithm searches the PT in a left-  to-right, breadth-first fashion that obeys the  major reflexive pronoun constraints while giv-  ing a preference to antecedents hat are closer  to the pronoun.	PT	parse tree	1
This algorithm searches the PT in a left-  to-right, breadth-first fashion that obeys the  major reflexive pronoun constraints while giv-  ing a preference to antecedents hat are closer  to the pronoun.	PT	phrase table	0
This algorithm searches the PT in a left-  to-right, breadth-first fashion that obeys the  major reflexive pronoun constraints while giv-  ing a preference to antecedents hat are closer  to the pronoun.	PT	Portuguese	0
This algorithm searches the PT in a left-  to-right, breadth-first fashion that obeys the  major reflexive pronoun constraints while giv-  ing a preference to antecedents hat are closer  to the pronoun.	PT	Parse Trees	0
This algorithm searches the PT in a left-  to-right, breadth-first fashion that obeys the  major reflexive pronoun constraints while giv-  ing a preference to antecedents hat are closer  to the pronoun.	PT	part	0
This algorithm searches the PT in a left-  to-right, breadth-first fashion that obeys the  major reflexive pronoun constraints while giv-  ing a preference to antecedents hat are closer  to the pronoun.	PT	Portugese	0
This algorithm searches the PT in a left-  to-right, breadth-first fashion that obeys the  major reflexive pronoun constraints while giv-  ing a preference to antecedents hat are closer  to the pronoun.	PT	Prompts	0
This algorithm searches the PT in a left-  to-right, breadth-first fashion that obeys the  major reflexive pronoun constraints while giv-  ing a preference to antecedents hat are closer  to the pronoun.	PT	Probable alignments	0
This algorithm searches the PT in a left-  to-right, breadth-first fashion that obeys the  major reflexive pronoun constraints while giv-  ing a preference to antecedents hat are closer  to the pronoun.	PT	Parse tree	0
This algorithm searches the PT in a left-  to-right, breadth-first fashion that obeys the  major reflexive pronoun constraints while giv-  ing a preference to antecedents hat are closer  to the pronoun.	PT	partial tree	0
This algorithm searches the PT in a left-  to-right, breadth-first fashion that obeys the  major reflexive pronoun constraints while giv-  ing a preference to antecedents hat are closer  to the pronoun.	PT	Phrase Table	0
We have im-  plemented a slightly modified version of Hobbs  algorithm for the Tree-bank PTs.	PT	probability threshold	0
We have im-  plemented a slightly modified version of Hobbs  algorithm for the Tree-bank PTs.	PT	parse tree	1
We have im-  plemented a slightly modified version of Hobbs  algorithm for the Tree-bank PTs.	PT	phrase table	0
We have im-  plemented a slightly modified version of Hobbs  algorithm for the Tree-bank PTs.	PT	Portuguese	0
We have im-  plemented a slightly modified version of Hobbs  algorithm for the Tree-bank PTs.	PT	Parse Trees	0
We have im-  plemented a slightly modified version of Hobbs  algorithm for the Tree-bank PTs.	PT	part	0
We have im-  plemented a slightly modified version of Hobbs  algorithm for the Tree-bank PTs.	PT	Portugese	0
We have im-  plemented a slightly modified version of Hobbs  algorithm for the Tree-bank PTs.	PT	Prompts	0
We have im-  plemented a slightly modified version of Hobbs  algorithm for the Tree-bank PTs.	PT	Probable alignments	0
We have im-  plemented a slightly modified version of Hobbs  algorithm for the Tree-bank PTs.	PT	Parse tree	0
We have im-  plemented a slightly modified version of Hobbs  algorithm for the Tree-bank PTs.	PT	partial tree	0
We have im-  plemented a slightly modified version of Hobbs  algorithm for the Tree-bank PTs.	PT	Phrase Table	0
Feature Sets entity type of e1 and e2 words in e1 and e2 word bigrams in e1 and e2 POS of e1 and e2 words between e1 and e2 word bigrams between e1 and e2 POS between e1 and e2 distance between e1 and e2 distance between e1 and e2 in the dependency graph steps in PT to get e1 and e2 in the same phrase various combinations of the above features Table 2: Our feature set for the MIRA classifier that pre- dicts binary relations.	PT	probability threshold	0
Feature Sets entity type of e1 and e2 words in e1 and e2 word bigrams in e1 and e2 POS of e1 and e2 words between e1 and e2 word bigrams between e1 and e2 POS between e1 and e2 distance between e1 and e2 distance between e1 and e2 in the dependency graph steps in PT to get e1 and e2 in the same phrase various combinations of the above features Table 2: Our feature set for the MIRA classifier that pre- dicts binary relations.	PT	parse tree	1
Feature Sets entity type of e1 and e2 words in e1 and e2 word bigrams in e1 and e2 POS of e1 and e2 words between e1 and e2 word bigrams between e1 and e2 POS between e1 and e2 distance between e1 and e2 distance between e1 and e2 in the dependency graph steps in PT to get e1 and e2 in the same phrase various combinations of the above features Table 2: Our feature set for the MIRA classifier that pre- dicts binary relations.	PT	phrase table	0
Feature Sets entity type of e1 and e2 words in e1 and e2 word bigrams in e1 and e2 POS of e1 and e2 words between e1 and e2 word bigrams between e1 and e2 POS between e1 and e2 distance between e1 and e2 distance between e1 and e2 in the dependency graph steps in PT to get e1 and e2 in the same phrase various combinations of the above features Table 2: Our feature set for the MIRA classifier that pre- dicts binary relations.	PT	Portuguese	0
Feature Sets entity type of e1 and e2 words in e1 and e2 word bigrams in e1 and e2 POS of e1 and e2 words between e1 and e2 word bigrams between e1 and e2 POS between e1 and e2 distance between e1 and e2 distance between e1 and e2 in the dependency graph steps in PT to get e1 and e2 in the same phrase various combinations of the above features Table 2: Our feature set for the MIRA classifier that pre- dicts binary relations.	PT	Parse Trees	0
Feature Sets entity type of e1 and e2 words in e1 and e2 word bigrams in e1 and e2 POS of e1 and e2 words between e1 and e2 word bigrams between e1 and e2 POS between e1 and e2 distance between e1 and e2 distance between e1 and e2 in the dependency graph steps in PT to get e1 and e2 in the same phrase various combinations of the above features Table 2: Our feature set for the MIRA classifier that pre- dicts binary relations.	PT	part	0
Feature Sets entity type of e1 and e2 words in e1 and e2 word bigrams in e1 and e2 POS of e1 and e2 words between e1 and e2 word bigrams between e1 and e2 POS between e1 and e2 distance between e1 and e2 distance between e1 and e2 in the dependency graph steps in PT to get e1 and e2 in the same phrase various combinations of the above features Table 2: Our feature set for the MIRA classifier that pre- dicts binary relations.	PT	Portugese	0
Feature Sets entity type of e1 and e2 words in e1 and e2 word bigrams in e1 and e2 POS of e1 and e2 words between e1 and e2 word bigrams between e1 and e2 POS between e1 and e2 distance between e1 and e2 distance between e1 and e2 in the dependency graph steps in PT to get e1 and e2 in the same phrase various combinations of the above features Table 2: Our feature set for the MIRA classifier that pre- dicts binary relations.	PT	Prompts	0
Feature Sets entity type of e1 and e2 words in e1 and e2 word bigrams in e1 and e2 POS of e1 and e2 words between e1 and e2 word bigrams between e1 and e2 POS between e1 and e2 distance between e1 and e2 distance between e1 and e2 in the dependency graph steps in PT to get e1 and e2 in the same phrase various combinations of the above features Table 2: Our feature set for the MIRA classifier that pre- dicts binary relations.	PT	Probable alignments	0
Feature Sets entity type of e1 and e2 words in e1 and e2 word bigrams in e1 and e2 POS of e1 and e2 words between e1 and e2 word bigrams between e1 and e2 POS between e1 and e2 distance between e1 and e2 distance between e1 and e2 in the dependency graph steps in PT to get e1 and e2 in the same phrase various combinations of the above features Table 2: Our feature set for the MIRA classifier that pre- dicts binary relations.	PT	Parse tree	0
Feature Sets entity type of e1 and e2 words in e1 and e2 word bigrams in e1 and e2 POS of e1 and e2 words between e1 and e2 word bigrams between e1 and e2 POS between e1 and e2 distance between e1 and e2 distance between e1 and e2 in the dependency graph steps in PT to get e1 and e2 in the same phrase various combinations of the above features Table 2: Our feature set for the MIRA classifier that pre- dicts binary relations.	PT	partial tree	0
Feature Sets entity type of e1 and e2 words in e1 and e2 word bigrams in e1 and e2 POS of e1 and e2 words between e1 and e2 word bigrams between e1 and e2 POS between e1 and e2 distance between e1 and e2 distance between e1 and e2 in the dependency graph steps in PT to get e1 and e2 in the same phrase various combinations of the above features Table 2: Our feature set for the MIRA classifier that pre- dicts binary relations.	PT	Phrase Table	0
Due to relatively large differences between  Tree:bank PTs and Hobbs' trees, our  Hobbs' implementation does not yield as high  an accuracy as it would have if we had had  perfect Hobbs' tree representations.	PT	probability threshold	0
Due to relatively large differences between  Tree:bank PTs and Hobbs' trees, our  Hobbs' implementation does not yield as high  an accuracy as it would have if we had had  perfect Hobbs' tree representations.	PT	parse tree	1
Due to relatively large differences between  Tree:bank PTs and Hobbs' trees, our  Hobbs' implementation does not yield as high  an accuracy as it would have if we had had  perfect Hobbs' tree representations.	PT	phrase table	0
Due to relatively large differences between  Tree:bank PTs and Hobbs' trees, our  Hobbs' implementation does not yield as high  an accuracy as it would have if we had had  perfect Hobbs' tree representations.	PT	Portuguese	0
Due to relatively large differences between  Tree:bank PTs and Hobbs' trees, our  Hobbs' implementation does not yield as high  an accuracy as it would have if we had had  perfect Hobbs' tree representations.	PT	Parse Trees	0
Due to relatively large differences between  Tree:bank PTs and Hobbs' trees, our  Hobbs' implementation does not yield as high  an accuracy as it would have if we had had  perfect Hobbs' tree representations.	PT	part	0
Due to relatively large differences between  Tree:bank PTs and Hobbs' trees, our  Hobbs' implementation does not yield as high  an accuracy as it would have if we had had  perfect Hobbs' tree representations.	PT	Portugese	0
Due to relatively large differences between  Tree:bank PTs and Hobbs' trees, our  Hobbs' implementation does not yield as high  an accuracy as it would have if we had had  perfect Hobbs' tree representations.	PT	Prompts	0
Due to relatively large differences between  Tree:bank PTs and Hobbs' trees, our  Hobbs' implementation does not yield as high  an accuracy as it would have if we had had  perfect Hobbs' tree representations.	PT	Probable alignments	0
Due to relatively large differences between  Tree:bank PTs and Hobbs' trees, our  Hobbs' implementation does not yield as high  an accuracy as it would have if we had had  perfect Hobbs' tree representations.	PT	Parse tree	0
Due to relatively large differences between  Tree:bank PTs and Hobbs' trees, our  Hobbs' implementation does not yield as high  an accuracy as it would have if we had had  perfect Hobbs' tree representations.	PT	partial tree	0
Due to relatively large differences between  Tree:bank PTs and Hobbs' trees, our  Hobbs' implementation does not yield as high  an accuracy as it would have if we had had  perfect Hobbs' tree representations.	PT	Phrase Table	0
In order to evaluate the alignments we computed the fraction of correctly transferred edges as a function of the average number of edges transferred by using supervised PTs on the target side.	PT	probability threshold	0
In order to evaluate the alignments we computed the fraction of correctly transferred edges as a function of the average number of edges transferred by using supervised PTs on the target side.	PT	parse tree	1
In order to evaluate the alignments we computed the fraction of correctly transferred edges as a function of the average number of edges transferred by using supervised PTs on the target side.	PT	phrase table	0
In order to evaluate the alignments we computed the fraction of correctly transferred edges as a function of the average number of edges transferred by using supervised PTs on the target side.	PT	Portuguese	0
In order to evaluate the alignments we computed the fraction of correctly transferred edges as a function of the average number of edges transferred by using supervised PTs on the target side.	PT	Parse Trees	0
In order to evaluate the alignments we computed the fraction of correctly transferred edges as a function of the average number of edges transferred by using supervised PTs on the target side.	PT	part	0
In order to evaluate the alignments we computed the fraction of correctly transferred edges as a function of the average number of edges transferred by using supervised PTs on the target side.	PT	Portugese	0
In order to evaluate the alignments we computed the fraction of correctly transferred edges as a function of the average number of edges transferred by using supervised PTs on the target side.	PT	Prompts	0
In order to evaluate the alignments we computed the fraction of correctly transferred edges as a function of the average number of edges transferred by using supervised PTs on the target side.	PT	Probable alignments	0
In order to evaluate the alignments we computed the fraction of correctly transferred edges as a function of the average number of edges transferred by using supervised PTs on the target side.	PT	Parse tree	0
In order to evaluate the alignments we computed the fraction of correctly transferred edges as a function of the average number of edges transferred by using supervised PTs on the target side.	PT	partial tree	0
In order to evaluate the alignments we computed the fraction of correctly transferred edges as a function of the average number of edges transferred by using supervised PTs on the target side.	PT	Phrase Table	0
We consider a PT on the source language as a set of dependency edges to be transferred.	PT	probability threshold	0
We consider a PT on the source language as a set of dependency edges to be transferred.	PT	parse tree	1
We consider a PT on the source language as a set of dependency edges to be transferred.	PT	phrase table	0
We consider a PT on the source language as a set of dependency edges to be transferred.	PT	Portuguese	0
We consider a PT on the source language as a set of dependency edges to be transferred.	PT	Parse Trees	0
We consider a PT on the source language as a set of dependency edges to be transferred.	PT	part	0
We consider a PT on the source language as a set of dependency edges to be transferred.	PT	Portugese	0
We consider a PT on the source language as a set of dependency edges to be transferred.	PT	Prompts	0
We consider a PT on the source language as a set of dependency edges to be transferred.	PT	Probable alignments	0
We consider a PT on the source language as a set of dependency edges to be transferred.	PT	Parse tree	0
We consider a PT on the source language as a set of dependency edges to be transferred.	PT	partial tree	0
We consider a PT on the source language as a set of dependency edges to be transferred.	PT	Phrase Table	0
The full pipeline consists of the following steps: (1) prepare the data (lowercase, tokenize, and filter long sentences); (2) build language models; (3) create word alignments in each direction; (4) symmetrize directional word alignments; (5) build PT; (6) tune weights for the PT.	PT	probability threshold	0
The full pipeline consists of the following steps: (1) prepare the data (lowercase, tokenize, and filter long sentences); (2) build language models; (3) create word alignments in each direction; (4) symmetrize directional word alignments; (5) build PT; (6) tune weights for the PT.	PT	parse tree	0
The full pipeline consists of the following steps: (1) prepare the data (lowercase, tokenize, and filter long sentences); (2) build language models; (3) create word alignments in each direction; (4) symmetrize directional word alignments; (5) build PT; (6) tune weights for the PT.	PT	phrase table	1
The full pipeline consists of the following steps: (1) prepare the data (lowercase, tokenize, and filter long sentences); (2) build language models; (3) create word alignments in each direction; (4) symmetrize directional word alignments; (5) build PT; (6) tune weights for the PT.	PT	Portuguese	0
The full pipeline consists of the following steps: (1) prepare the data (lowercase, tokenize, and filter long sentences); (2) build language models; (3) create word alignments in each direction; (4) symmetrize directional word alignments; (5) build PT; (6) tune weights for the PT.	PT	Parse Trees	0
The full pipeline consists of the following steps: (1) prepare the data (lowercase, tokenize, and filter long sentences); (2) build language models; (3) create word alignments in each direction; (4) symmetrize directional word alignments; (5) build PT; (6) tune weights for the PT.	PT	part	0
The full pipeline consists of the following steps: (1) prepare the data (lowercase, tokenize, and filter long sentences); (2) build language models; (3) create word alignments in each direction; (4) symmetrize directional word alignments; (5) build PT; (6) tune weights for the PT.	PT	Portugese	0
The full pipeline consists of the following steps: (1) prepare the data (lowercase, tokenize, and filter long sentences); (2) build language models; (3) create word alignments in each direction; (4) symmetrize directional word alignments; (5) build PT; (6) tune weights for the PT.	PT	Prompts	0
The full pipeline consists of the following steps: (1) prepare the data (lowercase, tokenize, and filter long sentences); (2) build language models; (3) create word alignments in each direction; (4) symmetrize directional word alignments; (5) build PT; (6) tune weights for the PT.	PT	Probable alignments	0
The full pipeline consists of the following steps: (1) prepare the data (lowercase, tokenize, and filter long sentences); (2) build language models; (3) create word alignments in each direction; (4) symmetrize directional word alignments; (5) build PT; (6) tune weights for the PT.	PT	Parse tree	0
The full pipeline consists of the following steps: (1) prepare the data (lowercase, tokenize, and filter long sentences); (2) build language models; (3) create word alignments in each direction; (4) symmetrize directional word alignments; (5) build PT; (6) tune weights for the PT.	PT	partial tree	0
The full pipeline consists of the following steps: (1) prepare the data (lowercase, tokenize, and filter long sentences); (2) build language models; (3) create word alignments in each direction; (4) symmetrize directional word alignments; (5) build PT; (6) tune weights for the PT.	PT	Phrase Table	0
Similarly, the PT size is significantly re- duced by 35%, while the gains on the tail docu- ments range from 0.6 to 1.4.	PT	probability threshold	0
Similarly, the PT size is significantly re- duced by 35%, while the gains on the tail docu- ments range from 0.6 to 1.4.	PT	parse tree	0
Similarly, the PT size is significantly re- duced by 35%, while the gains on the tail docu- ments range from 0.6 to 1.4.	PT	phrase table	1
Similarly, the PT size is significantly re- duced by 35%, while the gains on the tail docu- ments range from 0.6 to 1.4.	PT	Portuguese	0
Similarly, the PT size is significantly re- duced by 35%, while the gains on the tail docu- ments range from 0.6 to 1.4.	PT	Parse Trees	0
Similarly, the PT size is significantly re- duced by 35%, while the gains on the tail docu- ments range from 0.6 to 1.4.	PT	part	0
Similarly, the PT size is significantly re- duced by 35%, while the gains on the tail docu- ments range from 0.6 to 1.4.	PT	Portugese	0
Similarly, the PT size is significantly re- duced by 35%, while the gains on the tail docu- ments range from 0.6 to 1.4.	PT	Prompts	0
Similarly, the PT size is significantly re- duced by 35%, while the gains on the tail docu- ments range from 0.6 to 1.4.	PT	Probable alignments	0
Similarly, the PT size is significantly re- duced by 35%, while the gains on the tail docu- ments range from 0.6 to 1.4.	PT	Parse tree	0
Similarly, the PT size is significantly re- duced by 35%, while the gains on the tail docu- ments range from 0.6 to 1.4.	PT	partial tree	0
Similarly, the PT size is significantly re- duced by 35%, while the gains on the tail docu- ments range from 0.6 to 1.4.	PT	Phrase Table	0
Contrary to conventional wisdom in the MT community, bigger PTs did not always perform better.	PT	probability threshold	0
Contrary to conventional wisdom in the MT community, bigger PTs did not always perform better.	PT	parse tree	0
Contrary to conventional wisdom in the MT community, bigger PTs did not always perform better.	PT	phrase table	1
Contrary to conventional wisdom in the MT community, bigger PTs did not always perform better.	PT	Portuguese	0
Contrary to conventional wisdom in the MT community, bigger PTs did not always perform better.	PT	Parse Trees	0
Contrary to conventional wisdom in the MT community, bigger PTs did not always perform better.	PT	part	0
Contrary to conventional wisdom in the MT community, bigger PTs did not always perform better.	PT	Portugese	0
Contrary to conventional wisdom in the MT community, bigger PTs did not always perform better.	PT	Prompts	0
Contrary to conventional wisdom in the MT community, bigger PTs did not always perform better.	PT	Probable alignments	0
Contrary to conventional wisdom in the MT community, bigger PTs did not always perform better.	PT	Parse tree	0
Contrary to conventional wisdom in the MT community, bigger PTs did not always perform better.	PT	partial tree	0
Contrary to conventional wisdom in the MT community, bigger PTs did not always perform better.	PT	Phrase Table	0
In 14 out of 18 cases, the threshold picked was 0.4 (medium size PTs) and the other four times 0.2 was picked (smaller PTs).	PT	probability threshold	0
In 14 out of 18 cases, the threshold picked was 0.4 (medium size PTs) and the other four times 0.2 was picked (smaller PTs).	PT	parse tree	0
In 14 out of 18 cases, the threshold picked was 0.4 (medium size PTs) and the other four times 0.2 was picked (smaller PTs).	PT	phrase table	1
In 14 out of 18 cases, the threshold picked was 0.4 (medium size PTs) and the other four times 0.2 was picked (smaller PTs).	PT	Portuguese	0
In 14 out of 18 cases, the threshold picked was 0.4 (medium size PTs) and the other four times 0.2 was picked (smaller PTs).	PT	Parse Trees	0
In 14 out of 18 cases, the threshold picked was 0.4 (medium size PTs) and the other four times 0.2 was picked (smaller PTs).	PT	part	0
In 14 out of 18 cases, the threshold picked was 0.4 (medium size PTs) and the other four times 0.2 was picked (smaller PTs).	PT	Portugese	0
In 14 out of 18 cases, the threshold picked was 0.4 (medium size PTs) and the other four times 0.2 was picked (smaller PTs).	PT	Prompts	0
In 14 out of 18 cases, the threshold picked was 0.4 (medium size PTs) and the other four times 0.2 was picked (smaller PTs).	PT	Probable alignments	0
In 14 out of 18 cases, the threshold picked was 0.4 (medium size PTs) and the other four times 0.2 was picked (smaller PTs).	PT	Parse tree	0
In 14 out of 18 cases, the threshold picked was 0.4 (medium size PTs) and the other four times 0.2 was picked (smaller PTs).	PT	partial tree	0
In 14 out of 18 cases, the threshold picked was 0.4 (medium size PTs) and the other four times 0.2 was picked (smaller PTs).	PT	Phrase Table	0
We also used the English/ PT (En-Pt), PT/Spanish (Pt-Es), PT/French (Pt-Fr), and Spanish/French (Es-Fr) portions of the Europarl corpus using annotations described by Grac?a et al (2008), where we split the gold alignments into a dev/test set in a ratio of 40%/60%.	PT	probability threshold	0
We also used the English/ PT (En-Pt), PT/Spanish (Pt-Es), PT/French (Pt-Fr), and Spanish/French (Es-Fr) portions of the Europarl corpus using annotations described by Grac?a et al (2008), where we split the gold alignments into a dev/test set in a ratio of 40%/60%.	PT	parse tree	0
We also used the English/ PT (En-Pt), PT/Spanish (Pt-Es), PT/French (Pt-Fr), and Spanish/French (Es-Fr) portions of the Europarl corpus using annotations described by Grac?a et al (2008), where we split the gold alignments into a dev/test set in a ratio of 40%/60%.	PT	phrase table	0
We also used the English/ PT (En-Pt), PT/Spanish (Pt-Es), PT/French (Pt-Fr), and Spanish/French (Es-Fr) portions of the Europarl corpus using annotations described by Grac?a et al (2008), where we split the gold alignments into a dev/test set in a ratio of 40%/60%.	PT	Portuguese	1
We also used the English/ PT (En-Pt), PT/Spanish (Pt-Es), PT/French (Pt-Fr), and Spanish/French (Es-Fr) portions of the Europarl corpus using annotations described by Grac?a et al (2008), where we split the gold alignments into a dev/test set in a ratio of 40%/60%.	PT	Parse Trees	0
We also used the English/ PT (En-Pt), PT/Spanish (Pt-Es), PT/French (Pt-Fr), and Spanish/French (Es-Fr) portions of the Europarl corpus using annotations described by Grac?a et al (2008), where we split the gold alignments into a dev/test set in a ratio of 40%/60%.	PT	part	0
We also used the English/ PT (En-Pt), PT/Spanish (Pt-Es), PT/French (Pt-Fr), and Spanish/French (Es-Fr) portions of the Europarl corpus using annotations described by Grac?a et al (2008), where we split the gold alignments into a dev/test set in a ratio of 40%/60%.	PT	Portugese	0
We also used the English/ PT (En-Pt), PT/Spanish (Pt-Es), PT/French (Pt-Fr), and Spanish/French (Es-Fr) portions of the Europarl corpus using annotations described by Grac?a et al (2008), where we split the gold alignments into a dev/test set in a ratio of 40%/60%.	PT	Prompts	0
We also used the English/ PT (En-Pt), PT/Spanish (Pt-Es), PT/French (Pt-Fr), and Spanish/French (Es-Fr) portions of the Europarl corpus using annotations described by Grac?a et al (2008), where we split the gold alignments into a dev/test set in a ratio of 40%/60%.	PT	Probable alignments	0
We also used the English/ PT (En-Pt), PT/Spanish (Pt-Es), PT/French (Pt-Fr), and Spanish/French (Es-Fr) portions of the Europarl corpus using annotations described by Grac?a et al (2008), where we split the gold alignments into a dev/test set in a ratio of 40%/60%.	PT	Parse tree	0
We also used the English/ PT (En-Pt), PT/Spanish (Pt-Es), PT/French (Pt-Fr), and Spanish/French (Es-Fr) portions of the Europarl corpus using annotations described by Grac?a et al (2008), where we split the gold alignments into a dev/test set in a ratio of 40%/60%.	PT	partial tree	0
We also used the English/ PT (En-Pt), PT/Spanish (Pt-Es), PT/French (Pt-Fr), and Spanish/French (Es-Fr) portions of the Europarl corpus using annotations described by Grac?a et al (2008), where we split the gold alignments into a dev/test set in a ratio of 40%/60%.	PT	Phrase Table	0
articles required in one language but optional in the other (e.g., English Cars use gas and PT Os carros usam gasolina), cases where the content is expressed using multiple words in one language and a single word in the other language (e.g., agglutination such as English weapons of mass destruction and German Massenvernichtungswaffen), and expres- sions translated indirectly.	PT	probability threshold	0
articles required in one language but optional in the other (e.g., English Cars use gas and PT Os carros usam gasolina), cases where the content is expressed using multiple words in one language and a single word in the other language (e.g., agglutination such as English weapons of mass destruction and German Massenvernichtungswaffen), and expres- sions translated indirectly.	PT	parse tree	0
articles required in one language but optional in the other (e.g., English Cars use gas and PT Os carros usam gasolina), cases where the content is expressed using multiple words in one language and a single word in the other language (e.g., agglutination such as English weapons of mass destruction and German Massenvernichtungswaffen), and expres- sions translated indirectly.	PT	phrase table	0
articles required in one language but optional in the other (e.g., English Cars use gas and PT Os carros usam gasolina), cases where the content is expressed using multiple words in one language and a single word in the other language (e.g., agglutination such as English weapons of mass destruction and German Massenvernichtungswaffen), and expres- sions translated indirectly.	PT	Portuguese	1
articles required in one language but optional in the other (e.g., English Cars use gas and PT Os carros usam gasolina), cases where the content is expressed using multiple words in one language and a single word in the other language (e.g., agglutination such as English weapons of mass destruction and German Massenvernichtungswaffen), and expres- sions translated indirectly.	PT	Parse Trees	0
articles required in one language but optional in the other (e.g., English Cars use gas and PT Os carros usam gasolina), cases where the content is expressed using multiple words in one language and a single word in the other language (e.g., agglutination such as English weapons of mass destruction and German Massenvernichtungswaffen), and expres- sions translated indirectly.	PT	part	0
articles required in one language but optional in the other (e.g., English Cars use gas and PT Os carros usam gasolina), cases where the content is expressed using multiple words in one language and a single word in the other language (e.g., agglutination such as English weapons of mass destruction and German Massenvernichtungswaffen), and expres- sions translated indirectly.	PT	Portugese	0
articles required in one language but optional in the other (e.g., English Cars use gas and PT Os carros usam gasolina), cases where the content is expressed using multiple words in one language and a single word in the other language (e.g., agglutination such as English weapons of mass destruction and German Massenvernichtungswaffen), and expres- sions translated indirectly.	PT	Prompts	0
articles required in one language but optional in the other (e.g., English Cars use gas and PT Os carros usam gasolina), cases where the content is expressed using multiple words in one language and a single word in the other language (e.g., agglutination such as English weapons of mass destruction and German Massenvernichtungswaffen), and expres- sions translated indirectly.	PT	Probable alignments	0
articles required in one language but optional in the other (e.g., English Cars use gas and PT Os carros usam gasolina), cases where the content is expressed using multiple words in one language and a single word in the other language (e.g., agglutination such as English weapons of mass destruction and German Massenvernichtungswaffen), and expres- sions translated indirectly.	PT	Parse tree	0
articles required in one language but optional in the other (e.g., English Cars use gas and PT Os carros usam gasolina), cases where the content is expressed using multiple words in one language and a single word in the other language (e.g., agglutination such as English weapons of mass destruction and German Massenvernichtungswaffen), and expres- sions translated indirectly.	PT	partial tree	0
articles required in one language but optional in the other (e.g., English Cars use gas and PT Os carros usam gasolina), cases where the content is expressed using multiple words in one language and a single word in the other language (e.g., agglutination such as English weapons of mass destruction and German Massenvernichtungswaffen), and expres- sions translated indirectly.	PT	Phrase Table	0
483 Computational Linguistics Volume 36, Number 3 Table 1 Test corpora statistics: English?French, English?Spanish, English?PT, PT?Spanish, PT?French, and Spanish?French.	PT	probability threshold	0
483 Computational Linguistics Volume 36, Number 3 Table 1 Test corpora statistics: English?French, English?Spanish, English?PT, PT?Spanish, PT?French, and Spanish?French.	PT	parse tree	0
483 Computational Linguistics Volume 36, Number 3 Table 1 Test corpora statistics: English?French, English?Spanish, English?PT, PT?Spanish, PT?French, and Spanish?French.	PT	phrase table	0
483 Computational Linguistics Volume 36, Number 3 Table 1 Test corpora statistics: English?French, English?Spanish, English?PT, PT?Spanish, PT?French, and Spanish?French.	PT	Portuguese	1
483 Computational Linguistics Volume 36, Number 3 Table 1 Test corpora statistics: English?French, English?Spanish, English?PT, PT?Spanish, PT?French, and Spanish?French.	PT	Parse Trees	0
483 Computational Linguistics Volume 36, Number 3 Table 1 Test corpora statistics: English?French, English?Spanish, English?PT, PT?Spanish, PT?French, and Spanish?French.	PT	part	0
483 Computational Linguistics Volume 36, Number 3 Table 1 Test corpora statistics: English?French, English?Spanish, English?PT, PT?Spanish, PT?French, and Spanish?French.	PT	Portugese	0
483 Computational Linguistics Volume 36, Number 3 Table 1 Test corpora statistics: English?French, English?Spanish, English?PT, PT?Spanish, PT?French, and Spanish?French.	PT	Prompts	0
483 Computational Linguistics Volume 36, Number 3 Table 1 Test corpora statistics: English?French, English?Spanish, English?PT, PT?Spanish, PT?French, and Spanish?French.	PT	Probable alignments	0
483 Computational Linguistics Volume 36, Number 3 Table 1 Test corpora statistics: English?French, English?Spanish, English?PT, PT?Spanish, PT?French, and Spanish?French.	PT	Parse tree	0
483 Computational Linguistics Volume 36, Number 3 Table 1 Test corpora statistics: English?French, English?Spanish, English?PT, PT?Spanish, PT?French, and Spanish?French.	PT	partial tree	0
483 Computational Linguistics Volume 36, Number 3 Table 1 Test corpora statistics: English?French, English?Spanish, English?PT, PT?Spanish, PT?French, and Spanish?French.	PT	Phrase Table	0
G. & C. Merriam Co., Springfield, Mass.  158  Appendix A. PT  DECL PRON*  VERB* "ate"  DET  NOUN*  ?	PT	probability threshold	0
G. & C. Merriam Co., Springfield, Mass.  158  Appendix A. PT  DECL PRON*  VERB* "ate"  DET  NOUN*  ?	PT	parse tree	0
G. & C. Merriam Co., Springfield, Mass.  158  Appendix A. PT  DECL PRON*  VERB* "ate"  DET  NOUN*  ?	PT	phrase table	0
G. & C. Merriam Co., Springfield, Mass.  158  Appendix A. PT  DECL PRON*  VERB* "ate"  DET  NOUN*  ?	PT	Portuguese	0
G. & C. Merriam Co., Springfield, Mass.  158  Appendix A. PT  DECL PRON*  VERB* "ate"  DET  NOUN*  ?	PT	Parse Trees	1
G. & C. Merriam Co., Springfield, Mass.  158  Appendix A. PT  DECL PRON*  VERB* "ate"  DET  NOUN*  ?	PT	part	0
G. & C. Merriam Co., Springfield, Mass.  158  Appendix A. PT  DECL PRON*  VERB* "ate"  DET  NOUN*  ?	PT	Portugese	0
G. & C. Merriam Co., Springfield, Mass.  158  Appendix A. PT  DECL PRON*  VERB* "ate"  DET  NOUN*  ?	PT	Prompts	0
G. & C. Merriam Co., Springfield, Mass.  158  Appendix A. PT  DECL PRON*  VERB* "ate"  DET  NOUN*  ?	PT	Probable alignments	0
G. & C. Merriam Co., Springfield, Mass.  158  Appendix A. PT  DECL PRON*  VERB* "ate"  DET  NOUN*  ?	PT	Parse tree	0
G. & C. Merriam Co., Springfield, Mass.  158  Appendix A. PT  DECL PRON*  VERB* "ate"  DET  NOUN*  ?	PT	partial tree	0
G. & C. Merriam Co., Springfield, Mass.  158  Appendix A. PT  DECL PRON*  VERB* "ate"  DET  NOUN*  ?	PT	Phrase Table	0
However, these attachments become  unsatisfactory after the system understands the next  20 Computational Linguistics, Volume 15, Number 1, March 1989  Jungyun Seo and Robert F. Simmons Syntactic Graphs: A Representation for the Union of All Ambiguous PT  sentence.	PT	probability threshold	0
However, these attachments become  unsatisfactory after the system understands the next  20 Computational Linguistics, Volume 15, Number 1, March 1989  Jungyun Seo and Robert F. Simmons Syntactic Graphs: A Representation for the Union of All Ambiguous PT  sentence.	PT	parse tree	0
However, these attachments become  unsatisfactory after the system understands the next  20 Computational Linguistics, Volume 15, Number 1, March 1989  Jungyun Seo and Robert F. Simmons Syntactic Graphs: A Representation for the Union of All Ambiguous PT  sentence.	PT	phrase table	0
However, these attachments become  unsatisfactory after the system understands the next  20 Computational Linguistics, Volume 15, Number 1, March 1989  Jungyun Seo and Robert F. Simmons Syntactic Graphs: A Representation for the Union of All Ambiguous PT  sentence.	PT	Portuguese	0
However, these attachments become  unsatisfactory after the system understands the next  20 Computational Linguistics, Volume 15, Number 1, March 1989  Jungyun Seo and Robert F. Simmons Syntactic Graphs: A Representation for the Union of All Ambiguous PT  sentence.	PT	Parse Trees	1
However, these attachments become  unsatisfactory after the system understands the next  20 Computational Linguistics, Volume 15, Number 1, March 1989  Jungyun Seo and Robert F. Simmons Syntactic Graphs: A Representation for the Union of All Ambiguous PT  sentence.	PT	part	0
However, these attachments become  unsatisfactory after the system understands the next  20 Computational Linguistics, Volume 15, Number 1, March 1989  Jungyun Seo and Robert F. Simmons Syntactic Graphs: A Representation for the Union of All Ambiguous PT  sentence.	PT	Portugese	0
However, these attachments become  unsatisfactory after the system understands the next  20 Computational Linguistics, Volume 15, Number 1, March 1989  Jungyun Seo and Robert F. Simmons Syntactic Graphs: A Representation for the Union of All Ambiguous PT  sentence.	PT	Prompts	0
However, these attachments become  unsatisfactory after the system understands the next  20 Computational Linguistics, Volume 15, Number 1, March 1989  Jungyun Seo and Robert F. Simmons Syntactic Graphs: A Representation for the Union of All Ambiguous PT  sentence.	PT	Probable alignments	0
However, these attachments become  unsatisfactory after the system understands the next  20 Computational Linguistics, Volume 15, Number 1, March 1989  Jungyun Seo and Robert F. Simmons Syntactic Graphs: A Representation for the Union of All Ambiguous PT  sentence.	PT	Parse tree	0
However, these attachments become  unsatisfactory after the system understands the next  20 Computational Linguistics, Volume 15, Number 1, March 1989  Jungyun Seo and Robert F. Simmons Syntactic Graphs: A Representation for the Union of All Ambiguous PT  sentence.	PT	partial tree	0
However, these attachments become  unsatisfactory after the system understands the next  20 Computational Linguistics, Volume 15, Number 1, March 1989  Jungyun Seo and Robert F. Simmons Syntactic Graphs: A Representation for the Union of All Ambiguous PT  sentence.	PT	Phrase Table	0
3 Reranking PT Using SRL Information Here we give the general framework for the rerank- ing methods that we present in the next section.	PT	probability threshold	0
3 Reranking PT Using SRL Information Here we give the general framework for the rerank- ing methods that we present in the next section.	PT	parse tree	0
3 Reranking PT Using SRL Information Here we give the general framework for the rerank- ing methods that we present in the next section.	PT	phrase table	0
3 Reranking PT Using SRL Information Here we give the general framework for the rerank- ing methods that we present in the next section.	PT	Portuguese	0
3 Reranking PT Using SRL Information Here we give the general framework for the rerank- ing methods that we present in the next section.	PT	Parse Trees	1
3 Reranking PT Using SRL Information Here we give the general framework for the rerank- ing methods that we present in the next section.	PT	part	0
3 Reranking PT Using SRL Information Here we give the general framework for the rerank- ing methods that we present in the next section.	PT	Portugese	0
3 Reranking PT Using SRL Information Here we give the general framework for the rerank- ing methods that we present in the next section.	PT	Prompts	0
3 Reranking PT Using SRL Information Here we give the general framework for the rerank- ing methods that we present in the next section.	PT	Probable alignments	0
3 Reranking PT Using SRL Information Here we give the general framework for the rerank- ing methods that we present in the next section.	PT	Parse tree	0
3 Reranking PT Using SRL Information Here we give the general framework for the rerank- ing methods that we present in the next section.	PT	partial tree	0
3 Reranking PT Using SRL Information Here we give the general framework for the rerank- ing methods that we present in the next section.	PT	Phrase Table	0
0362-613X/89/010019-32503.00  Computational Linguistics, Volume 15, Number 1, March 1989 19  Jungyun Seo and Robert F. Simmons Syntactic Graphs: A Representation for the Union of All Ambiguous PT  . ,	PT	probability threshold	0
0362-613X/89/010019-32503.00  Computational Linguistics, Volume 15, Number 1, March 1989 19  Jungyun Seo and Robert F. Simmons Syntactic Graphs: A Representation for the Union of All Ambiguous PT  . ,	PT	parse tree	0
0362-613X/89/010019-32503.00  Computational Linguistics, Volume 15, Number 1, March 1989 19  Jungyun Seo and Robert F. Simmons Syntactic Graphs: A Representation for the Union of All Ambiguous PT  . ,	PT	phrase table	0
0362-613X/89/010019-32503.00  Computational Linguistics, Volume 15, Number 1, March 1989 19  Jungyun Seo and Robert F. Simmons Syntactic Graphs: A Representation for the Union of All Ambiguous PT  . ,	PT	Portuguese	0
0362-613X/89/010019-32503.00  Computational Linguistics, Volume 15, Number 1, March 1989 19  Jungyun Seo and Robert F. Simmons Syntactic Graphs: A Representation for the Union of All Ambiguous PT  . ,	PT	Parse Trees	1
0362-613X/89/010019-32503.00  Computational Linguistics, Volume 15, Number 1, March 1989 19  Jungyun Seo and Robert F. Simmons Syntactic Graphs: A Representation for the Union of All Ambiguous PT  . ,	PT	part	0
0362-613X/89/010019-32503.00  Computational Linguistics, Volume 15, Number 1, March 1989 19  Jungyun Seo and Robert F. Simmons Syntactic Graphs: A Representation for the Union of All Ambiguous PT  . ,	PT	Portugese	0
0362-613X/89/010019-32503.00  Computational Linguistics, Volume 15, Number 1, March 1989 19  Jungyun Seo and Robert F. Simmons Syntactic Graphs: A Representation for the Union of All Ambiguous PT  . ,	PT	Prompts	0
0362-613X/89/010019-32503.00  Computational Linguistics, Volume 15, Number 1, March 1989 19  Jungyun Seo and Robert F. Simmons Syntactic Graphs: A Representation for the Union of All Ambiguous PT  . ,	PT	Probable alignments	0
0362-613X/89/010019-32503.00  Computational Linguistics, Volume 15, Number 1, March 1989 19  Jungyun Seo and Robert F. Simmons Syntactic Graphs: A Representation for the Union of All Ambiguous PT  . ,	PT	Parse tree	0
0362-613X/89/010019-32503.00  Computational Linguistics, Volume 15, Number 1, March 1989 19  Jungyun Seo and Robert F. Simmons Syntactic Graphs: A Representation for the Union of All Ambiguous PT  . ,	PT	partial tree	0
0362-613X/89/010019-32503.00  Computational Linguistics, Volume 15, Number 1, March 1989 19  Jungyun Seo and Robert F. Simmons Syntactic Graphs: A Representation for the Union of All Ambiguous PT  . ,	PT	Phrase Table	0
The  node \[1,8oo,v\] represents a VP with head word  21  Jungyun See and Robert F. Simmons Syntactic Graphs: ,% Representation for the Union of All Ambiguous PT  GRAMMAR RULES AND CORRESPONDING TRIPLES:  Grammar rules arc-name head modifier  i. SNT--~NP VP snp header  VP head of NP  2.	PT	probability threshold	0
The  node \[1,8oo,v\] represents a VP with head word  21  Jungyun See and Robert F. Simmons Syntactic Graphs: ,% Representation for the Union of All Ambiguous PT  GRAMMAR RULES AND CORRESPONDING TRIPLES:  Grammar rules arc-name head modifier  i. SNT--~NP VP snp header  VP head of NP  2.	PT	parse tree	0
The  node \[1,8oo,v\] represents a VP with head word  21  Jungyun See and Robert F. Simmons Syntactic Graphs: ,% Representation for the Union of All Ambiguous PT  GRAMMAR RULES AND CORRESPONDING TRIPLES:  Grammar rules arc-name head modifier  i. SNT--~NP VP snp header  VP head of NP  2.	PT	phrase table	0
The  node \[1,8oo,v\] represents a VP with head word  21  Jungyun See and Robert F. Simmons Syntactic Graphs: ,% Representation for the Union of All Ambiguous PT  GRAMMAR RULES AND CORRESPONDING TRIPLES:  Grammar rules arc-name head modifier  i. SNT--~NP VP snp header  VP head of NP  2.	PT	Portuguese	0
The  node \[1,8oo,v\] represents a VP with head word  21  Jungyun See and Robert F. Simmons Syntactic Graphs: ,% Representation for the Union of All Ambiguous PT  GRAMMAR RULES AND CORRESPONDING TRIPLES:  Grammar rules arc-name head modifier  i. SNT--~NP VP snp header  VP head of NP  2.	PT	Parse Trees	1
The  node \[1,8oo,v\] represents a VP with head word  21  Jungyun See and Robert F. Simmons Syntactic Graphs: ,% Representation for the Union of All Ambiguous PT  GRAMMAR RULES AND CORRESPONDING TRIPLES:  Grammar rules arc-name head modifier  i. SNT--~NP VP snp header  VP head of NP  2.	PT	part	0
The  node \[1,8oo,v\] represents a VP with head word  21  Jungyun See and Robert F. Simmons Syntactic Graphs: ,% Representation for the Union of All Ambiguous PT  GRAMMAR RULES AND CORRESPONDING TRIPLES:  Grammar rules arc-name head modifier  i. SNT--~NP VP snp header  VP head of NP  2.	PT	Portugese	0
The  node \[1,8oo,v\] represents a VP with head word  21  Jungyun See and Robert F. Simmons Syntactic Graphs: ,% Representation for the Union of All Ambiguous PT  GRAMMAR RULES AND CORRESPONDING TRIPLES:  Grammar rules arc-name head modifier  i. SNT--~NP VP snp header  VP head of NP  2.	PT	Prompts	0
The  node \[1,8oo,v\] represents a VP with head word  21  Jungyun See and Robert F. Simmons Syntactic Graphs: ,% Representation for the Union of All Ambiguous PT  GRAMMAR RULES AND CORRESPONDING TRIPLES:  Grammar rules arc-name head modifier  i. SNT--~NP VP snp header  VP head of NP  2.	PT	Probable alignments	0
The  node \[1,8oo,v\] represents a VP with head word  21  Jungyun See and Robert F. Simmons Syntactic Graphs: ,% Representation for the Union of All Ambiguous PT  GRAMMAR RULES AND CORRESPONDING TRIPLES:  Grammar rules arc-name head modifier  i. SNT--~NP VP snp header  VP head of NP  2.	PT	Parse tree	0
The  node \[1,8oo,v\] represents a VP with head word  21  Jungyun See and Robert F. Simmons Syntactic Graphs: ,% Representation for the Union of All Ambiguous PT  GRAMMAR RULES AND CORRESPONDING TRIPLES:  Grammar rules arc-name head modifier  i. SNT--~NP VP snp header  VP head of NP  2.	PT	partial tree	0
The  node \[1,8oo,v\] represents a VP with head word  21  Jungyun See and Robert F. Simmons Syntactic Graphs: ,% Representation for the Union of All Ambiguous PT  GRAMMAR RULES AND CORRESPONDING TRIPLES:  Grammar rules arc-name head modifier  i. SNT--~NP VP snp header  VP head of NP  2.	PT	Phrase Table	0
PT Used SRL F1 Gold 77.1 1-best 63.9 Reranked by gold parse F1 68.1 Reranked by gold frame F1 74.2 Simple SRL combination (?	PT	probability threshold	0
PT Used SRL F1 Gold 77.1 1-best 63.9 Reranked by gold parse F1 68.1 Reranked by gold frame F1 74.2 Simple SRL combination (?	PT	parse tree	0
PT Used SRL F1 Gold 77.1 1-best 63.9 Reranked by gold parse F1 68.1 Reranked by gold frame F1 74.2 Simple SRL combination (?	PT	phrase table	0
PT Used SRL F1 Gold 77.1 1-best 63.9 Reranked by gold parse F1 68.1 Reranked by gold frame F1 74.2 Simple SRL combination (?	PT	Portuguese	0
PT Used SRL F1 Gold 77.1 1-best 63.9 Reranked by gold parse F1 68.1 Reranked by gold frame F1 74.2 Simple SRL combination (?	PT	Parse Trees	1
PT Used SRL F1 Gold 77.1 1-best 63.9 Reranked by gold parse F1 68.1 Reranked by gold frame F1 74.2 Simple SRL combination (?	PT	part	0
PT Used SRL F1 Gold 77.1 1-best 63.9 Reranked by gold parse F1 68.1 Reranked by gold frame F1 74.2 Simple SRL combination (?	PT	Portugese	0
PT Used SRL F1 Gold 77.1 1-best 63.9 Reranked by gold parse F1 68.1 Reranked by gold frame F1 74.2 Simple SRL combination (?	PT	Prompts	0
PT Used SRL F1 Gold 77.1 1-best 63.9 Reranked by gold parse F1 68.1 Reranked by gold frame F1 74.2 Simple SRL combination (?	PT	Probable alignments	0
PT Used SRL F1 Gold 77.1 1-best 63.9 Reranked by gold parse F1 68.1 Reranked by gold frame F1 74.2 Simple SRL combination (?	PT	Parse tree	0
PT Used SRL F1 Gold 77.1 1-best 63.9 Reranked by gold parse F1 68.1 Reranked by gold frame F1 74.2 Simple SRL combination (?	PT	partial tree	0
PT Used SRL F1 Gold 77.1 1-best 63.9 Reranked by gold parse F1 68.1 Reranked by gold frame F1 74.2 Simple SRL combination (?	PT	Phrase Table	0
This is PTly because selection restrictions are  not clearcut in many cases.	PT	probability threshold	0
This is PTly because selection restrictions are  not clearcut in many cases.	PT	parse tree	0
This is PTly because selection restrictions are  not clearcut in many cases.	PT	phrase table	0
This is PTly because selection restrictions are  not clearcut in many cases.	PT	Portuguese	0
This is PTly because selection restrictions are  not clearcut in many cases.	PT	Parse Trees	0
This is PTly because selection restrictions are  not clearcut in many cases.	PT	part	1
This is PTly because selection restrictions are  not clearcut in many cases.	PT	Portugese	0
This is PTly because selection restrictions are  not clearcut in many cases.	PT	Prompts	0
This is PTly because selection restrictions are  not clearcut in many cases.	PT	Probable alignments	0
This is PTly because selection restrictions are  not clearcut in many cases.	PT	Parse tree	0
This is PTly because selection restrictions are  not clearcut in many cases.	PT	partial tree	0
This is PTly because selection restrictions are  not clearcut in many cases.	PT	Phrase Table	0
The probability that a referent is in a PTic-  166  ular gender class is just the relative frequency  with which that referent is referred to by a pro-  noun p that is PT of that gender class.	PT	probability threshold	0
The probability that a referent is in a PTic-  166  ular gender class is just the relative frequency  with which that referent is referred to by a pro-  noun p that is PT of that gender class.	PT	parse tree	0
The probability that a referent is in a PTic-  166  ular gender class is just the relative frequency  with which that referent is referred to by a pro-  noun p that is PT of that gender class.	PT	phrase table	0
The probability that a referent is in a PTic-  166  ular gender class is just the relative frequency  with which that referent is referred to by a pro-  noun p that is PT of that gender class.	PT	Portuguese	0
The probability that a referent is in a PTic-  166  ular gender class is just the relative frequency  with which that referent is referred to by a pro-  noun p that is PT of that gender class.	PT	Parse Trees	0
The probability that a referent is in a PTic-  166  ular gender class is just the relative frequency  with which that referent is referred to by a pro-  noun p that is PT of that gender class.	PT	part	1
The probability that a referent is in a PTic-  166  ular gender class is just the relative frequency  with which that referent is referred to by a pro-  noun p that is PT of that gender class.	PT	Portugese	0
The probability that a referent is in a PTic-  166  ular gender class is just the relative frequency  with which that referent is referred to by a pro-  noun p that is PT of that gender class.	PT	Prompts	0
The probability that a referent is in a PTic-  166  ular gender class is just the relative frequency  with which that referent is referred to by a pro-  noun p that is PT of that gender class.	PT	Probable alignments	0
The probability that a referent is in a PTic-  166  ular gender class is just the relative frequency  with which that referent is referred to by a pro-  noun p that is PT of that gender class.	PT	Parse tree	0
The probability that a referent is in a PTic-  166  ular gender class is just the relative frequency  with which that referent is referred to by a pro-  noun p that is PT of that gender class.	PT	partial tree	0
The probability that a referent is in a PTic-  166  ular gender class is just the relative frequency  with which that referent is referred to by a pro-  noun p that is PT of that gender class.	PT	Phrase Table	0
Given a PTicular choice of the antecedent  candidates, the distance is independent of  distances of candidates other than the an-  tecedent (and the distance to non-referents  can be ignored):  P(so, d~a, 2~) o?	PT	probability threshold	0
Given a PTicular choice of the antecedent  candidates, the distance is independent of  distances of candidates other than the an-  tecedent (and the distance to non-referents  can be ignored):  P(so, d~a, 2~) o?	PT	parse tree	0
Given a PTicular choice of the antecedent  candidates, the distance is independent of  distances of candidates other than the an-  tecedent (and the distance to non-referents  can be ignored):  P(so, d~a, 2~) o?	PT	phrase table	0
Given a PTicular choice of the antecedent  candidates, the distance is independent of  distances of candidates other than the an-  tecedent (and the distance to non-referents  can be ignored):  P(so, d~a, 2~) o?	PT	Portuguese	0
Given a PTicular choice of the antecedent  candidates, the distance is independent of  distances of candidates other than the an-  tecedent (and the distance to non-referents  can be ignored):  P(so, d~a, 2~) o?	PT	Parse Trees	0
Given a PTicular choice of the antecedent  candidates, the distance is independent of  distances of candidates other than the an-  tecedent (and the distance to non-referents  can be ignored):  P(so, d~a, 2~) o?	PT	part	1
Given a PTicular choice of the antecedent  candidates, the distance is independent of  distances of candidates other than the an-  tecedent (and the distance to non-referents  can be ignored):  P(so, d~a, 2~) o?	PT	Portugese	0
Given a PTicular choice of the antecedent  candidates, the distance is independent of  distances of candidates other than the an-  tecedent (and the distance to non-referents  can be ignored):  P(so, d~a, 2~) o?	PT	Prompts	0
Given a PTicular choice of the antecedent  candidates, the distance is independent of  distances of candidates other than the an-  tecedent (and the distance to non-referents  can be ignored):  P(so, d~a, 2~) o?	PT	Probable alignments	0
Given a PTicular choice of the antecedent  candidates, the distance is independent of  distances of candidates other than the an-  tecedent (and the distance to non-referents  can be ignored):  P(so, d~a, 2~) o?	PT	Parse tree	0
Given a PTicular choice of the antecedent  candidates, the distance is independent of  distances of candidates other than the an-  tecedent (and the distance to non-referents  can be ignored):  P(so, d~a, 2~) o?	PT	partial tree	0
Given a PTicular choice of the antecedent  candidates, the distance is independent of  distances of candidates other than the an-  tecedent (and the distance to non-referents  can be ignored):  P(so, d~a, 2~) o?	PT	Phrase Table	0
In PTicular, the scheme infers the gender of a  referent from the gender of the pronouns that  161  refer to it and selects referents using the pro-  noun anaphora program.	PT	probability threshold	0
In PTicular, the scheme infers the gender of a  referent from the gender of the pronouns that  161  refer to it and selects referents using the pro-  noun anaphora program.	PT	parse tree	0
In PTicular, the scheme infers the gender of a  referent from the gender of the pronouns that  161  refer to it and selects referents using the pro-  noun anaphora program.	PT	phrase table	0
In PTicular, the scheme infers the gender of a  referent from the gender of the pronouns that  161  refer to it and selects referents using the pro-  noun anaphora program.	PT	Portuguese	0
In PTicular, the scheme infers the gender of a  referent from the gender of the pronouns that  161  refer to it and selects referents using the pro-  noun anaphora program.	PT	Parse Trees	0
In PTicular, the scheme infers the gender of a  referent from the gender of the pronouns that  161  refer to it and selects referents using the pro-  noun anaphora program.	PT	part	1
In PTicular, the scheme infers the gender of a  referent from the gender of the pronouns that  161  refer to it and selects referents using the pro-  noun anaphora program.	PT	Portugese	0
In PTicular, the scheme infers the gender of a  referent from the gender of the pronouns that  161  refer to it and selects referents using the pro-  noun anaphora program.	PT	Prompts	0
In PTicular, the scheme infers the gender of a  referent from the gender of the pronouns that  161  refer to it and selects referents using the pro-  noun anaphora program.	PT	Probable alignments	0
In PTicular, the scheme infers the gender of a  referent from the gender of the pronouns that  161  refer to it and selects referents using the pro-  noun anaphora program.	PT	Parse tree	0
In PTicular, the scheme infers the gender of a  referent from the gender of the pronouns that  161  refer to it and selects referents using the pro-  noun anaphora program.	PT	partial tree	0
In PTicular, the scheme infers the gender of a  referent from the gender of the pronouns that  161  refer to it and selects referents using the pro-  noun anaphora program.	PT	Phrase Table	0
When viewed in this way, a can be regarded as  an index into these vectors that specifies which  value is relevant o the PTicular choice of an-  tecedent.	PT	probability threshold	0
When viewed in this way, a can be regarded as  an index into these vectors that specifies which  value is relevant o the PTicular choice of an-  tecedent.	PT	parse tree	0
When viewed in this way, a can be regarded as  an index into these vectors that specifies which  value is relevant o the PTicular choice of an-  tecedent.	PT	phrase table	0
When viewed in this way, a can be regarded as  an index into these vectors that specifies which  value is relevant o the PTicular choice of an-  tecedent.	PT	Portuguese	0
When viewed in this way, a can be regarded as  an index into these vectors that specifies which  value is relevant o the PTicular choice of an-  tecedent.	PT	Parse Trees	0
When viewed in this way, a can be regarded as  an index into these vectors that specifies which  value is relevant o the PTicular choice of an-  tecedent.	PT	part	1
When viewed in this way, a can be regarded as  an index into these vectors that specifies which  value is relevant o the PTicular choice of an-  tecedent.	PT	Portugese	0
When viewed in this way, a can be regarded as  an index into these vectors that specifies which  value is relevant o the PTicular choice of an-  tecedent.	PT	Prompts	0
When viewed in this way, a can be regarded as  an index into these vectors that specifies which  value is relevant o the PTicular choice of an-  tecedent.	PT	Probable alignments	0
When viewed in this way, a can be regarded as  an index into these vectors that specifies which  value is relevant o the PTicular choice of an-  tecedent.	PT	Parse tree	0
When viewed in this way, a can be regarded as  an index into these vectors that specifies which  value is relevant o the PTicular choice of an-  tecedent.	PT	partial tree	0
When viewed in this way, a can be regarded as  an index into these vectors that specifies which  value is relevant o the PTicular choice of an-  tecedent.	PT	Phrase Table	0
These questions were machine translated to Spanish, French and Brazilian PT using IBM?s n.	PT	probability threshold	0
These questions were machine translated to Spanish, French and Brazilian PT using IBM?s n.	PT	parse tree	0
These questions were machine translated to Spanish, French and Brazilian PT using IBM?s n.	PT	phrase table	0
These questions were machine translated to Spanish, French and Brazilian PT using IBM?s n.	PT	Portuguese	0
These questions were machine translated to Spanish, French and Brazilian PT using IBM?s n.	PT	Parse Trees	0
These questions were machine translated to Spanish, French and Brazilian PT using IBM?s n.	PT	part	0
These questions were machine translated to Spanish, French and Brazilian PT using IBM?s n.	PT	Portugese	1
These questions were machine translated to Spanish, French and Brazilian PT using IBM?s n.	PT	Prompts	0
These questions were machine translated to Spanish, French and Brazilian PT using IBM?s n.	PT	Probable alignments	0
These questions were machine translated to Spanish, French and Brazilian PT using IBM?s n.	PT	Parse tree	0
These questions were machine translated to Spanish, French and Brazilian PT using IBM?s n.	PT	partial tree	0
These questions were machine translated to Spanish, French and Brazilian PT using IBM?s n.	PT	Phrase Table	0
We will initially focus on the case of English/Brazillian-PT but we intend our work to be generalizable to other language pairs.	PT	probability threshold	0
We will initially focus on the case of English/Brazillian-PT but we intend our work to be generalizable to other language pairs.	PT	parse tree	0
We will initially focus on the case of English/Brazillian-PT but we intend our work to be generalizable to other language pairs.	PT	phrase table	0
We will initially focus on the case of English/Brazillian-PT but we intend our work to be generalizable to other language pairs.	PT	Portuguese	0
We will initially focus on the case of English/Brazillian-PT but we intend our work to be generalizable to other language pairs.	PT	Parse Trees	0
We will initially focus on the case of English/Brazillian-PT but we intend our work to be generalizable to other language pairs.	PT	part	0
We will initially focus on the case of English/Brazillian-PT but we intend our work to be generalizable to other language pairs.	PT	Portugese	1
We will initially focus on the case of English/Brazillian-PT but we intend our work to be generalizable to other language pairs.	PT	Prompts	0
We will initially focus on the case of English/Brazillian-PT but we intend our work to be generalizable to other language pairs.	PT	Probable alignments	0
We will initially focus on the case of English/Brazillian-PT but we intend our work to be generalizable to other language pairs.	PT	Parse tree	0
We will initially focus on the case of English/Brazillian-PT but we intend our work to be generalizable to other language pairs.	PT	partial tree	0
We will initially focus on the case of English/Brazillian-PT but we intend our work to be generalizable to other language pairs.	PT	Phrase Table	0
UAS with out preprocessing: Arabic 65.19 77.74 79.02 76.74  Chinese 84.27 89.46 86.42 90.03  Czech 76.24 83.4 83.52 82.88  Danish 81.72 88.64 86.11 88.45  Dutch 71.77 75.49 75.83 74.97  German 84.11 87.66 90.67 87.53  Japanese 89.91 93.12 92.40 92.99  PT 85.07 90.3 88.00 90.21  Slovene 71.42 81.14 80.96 80.43  Spanish 80.46 85.15 88.90 85.19  Swedish 81.08 88.57 83.99 88.83  Turkish 61.22 74.49 73.91 74.3  AV: 77.7 84.6 84.1 84.38  SD: 8.67 6.15 5.78 6.42  Bulgarian 86.34 91.3 89.27 91.44  Table 1: Results  195  An Alternative Conception of  Tree-Adjoining Derivation  Yves Schabes*  Mitsubishi Electric Research Laboratory  Stuart M. Shie	PT	probability threshold	0
UAS with out preprocessing: Arabic 65.19 77.74 79.02 76.74  Chinese 84.27 89.46 86.42 90.03  Czech 76.24 83.4 83.52 82.88  Danish 81.72 88.64 86.11 88.45  Dutch 71.77 75.49 75.83 74.97  German 84.11 87.66 90.67 87.53  Japanese 89.91 93.12 92.40 92.99  PT 85.07 90.3 88.00 90.21  Slovene 71.42 81.14 80.96 80.43  Spanish 80.46 85.15 88.90 85.19  Swedish 81.08 88.57 83.99 88.83  Turkish 61.22 74.49 73.91 74.3  AV: 77.7 84.6 84.1 84.38  SD: 8.67 6.15 5.78 6.42  Bulgarian 86.34 91.3 89.27 91.44  Table 1: Results  195  An Alternative Conception of  Tree-Adjoining Derivation  Yves Schabes*  Mitsubishi Electric Research Laboratory  Stuart M. Shie	PT	parse tree	0
UAS with out preprocessing: Arabic 65.19 77.74 79.02 76.74  Chinese 84.27 89.46 86.42 90.03  Czech 76.24 83.4 83.52 82.88  Danish 81.72 88.64 86.11 88.45  Dutch 71.77 75.49 75.83 74.97  German 84.11 87.66 90.67 87.53  Japanese 89.91 93.12 92.40 92.99  PT 85.07 90.3 88.00 90.21  Slovene 71.42 81.14 80.96 80.43  Spanish 80.46 85.15 88.90 85.19  Swedish 81.08 88.57 83.99 88.83  Turkish 61.22 74.49 73.91 74.3  AV: 77.7 84.6 84.1 84.38  SD: 8.67 6.15 5.78 6.42  Bulgarian 86.34 91.3 89.27 91.44  Table 1: Results  195  An Alternative Conception of  Tree-Adjoining Derivation  Yves Schabes*  Mitsubishi Electric Research Laboratory  Stuart M. Shie	PT	phrase table	0
UAS with out preprocessing: Arabic 65.19 77.74 79.02 76.74  Chinese 84.27 89.46 86.42 90.03  Czech 76.24 83.4 83.52 82.88  Danish 81.72 88.64 86.11 88.45  Dutch 71.77 75.49 75.83 74.97  German 84.11 87.66 90.67 87.53  Japanese 89.91 93.12 92.40 92.99  PT 85.07 90.3 88.00 90.21  Slovene 71.42 81.14 80.96 80.43  Spanish 80.46 85.15 88.90 85.19  Swedish 81.08 88.57 83.99 88.83  Turkish 61.22 74.49 73.91 74.3  AV: 77.7 84.6 84.1 84.38  SD: 8.67 6.15 5.78 6.42  Bulgarian 86.34 91.3 89.27 91.44  Table 1: Results  195  An Alternative Conception of  Tree-Adjoining Derivation  Yves Schabes*  Mitsubishi Electric Research Laboratory  Stuart M. Shie	PT	Portuguese	0
UAS with out preprocessing: Arabic 65.19 77.74 79.02 76.74  Chinese 84.27 89.46 86.42 90.03  Czech 76.24 83.4 83.52 82.88  Danish 81.72 88.64 86.11 88.45  Dutch 71.77 75.49 75.83 74.97  German 84.11 87.66 90.67 87.53  Japanese 89.91 93.12 92.40 92.99  PT 85.07 90.3 88.00 90.21  Slovene 71.42 81.14 80.96 80.43  Spanish 80.46 85.15 88.90 85.19  Swedish 81.08 88.57 83.99 88.83  Turkish 61.22 74.49 73.91 74.3  AV: 77.7 84.6 84.1 84.38  SD: 8.67 6.15 5.78 6.42  Bulgarian 86.34 91.3 89.27 91.44  Table 1: Results  195  An Alternative Conception of  Tree-Adjoining Derivation  Yves Schabes*  Mitsubishi Electric Research Laboratory  Stuart M. Shie	PT	Parse Trees	0
UAS with out preprocessing: Arabic 65.19 77.74 79.02 76.74  Chinese 84.27 89.46 86.42 90.03  Czech 76.24 83.4 83.52 82.88  Danish 81.72 88.64 86.11 88.45  Dutch 71.77 75.49 75.83 74.97  German 84.11 87.66 90.67 87.53  Japanese 89.91 93.12 92.40 92.99  PT 85.07 90.3 88.00 90.21  Slovene 71.42 81.14 80.96 80.43  Spanish 80.46 85.15 88.90 85.19  Swedish 81.08 88.57 83.99 88.83  Turkish 61.22 74.49 73.91 74.3  AV: 77.7 84.6 84.1 84.38  SD: 8.67 6.15 5.78 6.42  Bulgarian 86.34 91.3 89.27 91.44  Table 1: Results  195  An Alternative Conception of  Tree-Adjoining Derivation  Yves Schabes*  Mitsubishi Electric Research Laboratory  Stuart M. Shie	PT	part	0
UAS with out preprocessing: Arabic 65.19 77.74 79.02 76.74  Chinese 84.27 89.46 86.42 90.03  Czech 76.24 83.4 83.52 82.88  Danish 81.72 88.64 86.11 88.45  Dutch 71.77 75.49 75.83 74.97  German 84.11 87.66 90.67 87.53  Japanese 89.91 93.12 92.40 92.99  PT 85.07 90.3 88.00 90.21  Slovene 71.42 81.14 80.96 80.43  Spanish 80.46 85.15 88.90 85.19  Swedish 81.08 88.57 83.99 88.83  Turkish 61.22 74.49 73.91 74.3  AV: 77.7 84.6 84.1 84.38  SD: 8.67 6.15 5.78 6.42  Bulgarian 86.34 91.3 89.27 91.44  Table 1: Results  195  An Alternative Conception of  Tree-Adjoining Derivation  Yves Schabes*  Mitsubishi Electric Research Laboratory  Stuart M. Shie	PT	Portugese	1
UAS with out preprocessing: Arabic 65.19 77.74 79.02 76.74  Chinese 84.27 89.46 86.42 90.03  Czech 76.24 83.4 83.52 82.88  Danish 81.72 88.64 86.11 88.45  Dutch 71.77 75.49 75.83 74.97  German 84.11 87.66 90.67 87.53  Japanese 89.91 93.12 92.40 92.99  PT 85.07 90.3 88.00 90.21  Slovene 71.42 81.14 80.96 80.43  Spanish 80.46 85.15 88.90 85.19  Swedish 81.08 88.57 83.99 88.83  Turkish 61.22 74.49 73.91 74.3  AV: 77.7 84.6 84.1 84.38  SD: 8.67 6.15 5.78 6.42  Bulgarian 86.34 91.3 89.27 91.44  Table 1: Results  195  An Alternative Conception of  Tree-Adjoining Derivation  Yves Schabes*  Mitsubishi Electric Research Laboratory  Stuart M. Shie	PT	Prompts	0
UAS with out preprocessing: Arabic 65.19 77.74 79.02 76.74  Chinese 84.27 89.46 86.42 90.03  Czech 76.24 83.4 83.52 82.88  Danish 81.72 88.64 86.11 88.45  Dutch 71.77 75.49 75.83 74.97  German 84.11 87.66 90.67 87.53  Japanese 89.91 93.12 92.40 92.99  PT 85.07 90.3 88.00 90.21  Slovene 71.42 81.14 80.96 80.43  Spanish 80.46 85.15 88.90 85.19  Swedish 81.08 88.57 83.99 88.83  Turkish 61.22 74.49 73.91 74.3  AV: 77.7 84.6 84.1 84.38  SD: 8.67 6.15 5.78 6.42  Bulgarian 86.34 91.3 89.27 91.44  Table 1: Results  195  An Alternative Conception of  Tree-Adjoining Derivation  Yves Schabes*  Mitsubishi Electric Research Laboratory  Stuart M. Shie	PT	Probable alignments	0
UAS with out preprocessing: Arabic 65.19 77.74 79.02 76.74  Chinese 84.27 89.46 86.42 90.03  Czech 76.24 83.4 83.52 82.88  Danish 81.72 88.64 86.11 88.45  Dutch 71.77 75.49 75.83 74.97  German 84.11 87.66 90.67 87.53  Japanese 89.91 93.12 92.40 92.99  PT 85.07 90.3 88.00 90.21  Slovene 71.42 81.14 80.96 80.43  Spanish 80.46 85.15 88.90 85.19  Swedish 81.08 88.57 83.99 88.83  Turkish 61.22 74.49 73.91 74.3  AV: 77.7 84.6 84.1 84.38  SD: 8.67 6.15 5.78 6.42  Bulgarian 86.34 91.3 89.27 91.44  Table 1: Results  195  An Alternative Conception of  Tree-Adjoining Derivation  Yves Schabes*  Mitsubishi Electric Research Laboratory  Stuart M. Shie	PT	Parse tree	0
UAS with out preprocessing: Arabic 65.19 77.74 79.02 76.74  Chinese 84.27 89.46 86.42 90.03  Czech 76.24 83.4 83.52 82.88  Danish 81.72 88.64 86.11 88.45  Dutch 71.77 75.49 75.83 74.97  German 84.11 87.66 90.67 87.53  Japanese 89.91 93.12 92.40 92.99  PT 85.07 90.3 88.00 90.21  Slovene 71.42 81.14 80.96 80.43  Spanish 80.46 85.15 88.90 85.19  Swedish 81.08 88.57 83.99 88.83  Turkish 61.22 74.49 73.91 74.3  AV: 77.7 84.6 84.1 84.38  SD: 8.67 6.15 5.78 6.42  Bulgarian 86.34 91.3 89.27 91.44  Table 1: Results  195  An Alternative Conception of  Tree-Adjoining Derivation  Yves Schabes*  Mitsubishi Electric Research Laboratory  Stuart M. Shie	PT	partial tree	0
UAS with out preprocessing: Arabic 65.19 77.74 79.02 76.74  Chinese 84.27 89.46 86.42 90.03  Czech 76.24 83.4 83.52 82.88  Danish 81.72 88.64 86.11 88.45  Dutch 71.77 75.49 75.83 74.97  German 84.11 87.66 90.67 87.53  Japanese 89.91 93.12 92.40 92.99  PT 85.07 90.3 88.00 90.21  Slovene 71.42 81.14 80.96 80.43  Spanish 80.46 85.15 88.90 85.19  Swedish 81.08 88.57 83.99 88.83  Turkish 61.22 74.49 73.91 74.3  AV: 77.7 84.6 84.1 84.38  SD: 8.67 6.15 5.78 6.42  Bulgarian 86.34 91.3 89.27 91.44  Table 1: Results  195  An Alternative Conception of  Tree-Adjoining Derivation  Yves Schabes*  Mitsubishi Electric Research Laboratory  Stuart M. Shie	PT	Phrase Table	0
We enumerate  them below as a key: Arabic (ar), Danish (da),  German (de), English (en), Spanish (es), French  (fr), Indonesian (Malay) (id), Italian (it), Japanese  (ja), Korean (ko), Malaysian (Malay) (ms), Dutch  (nl), PT (pt), Russian (ru), Thai (th), Viet- namese (vi) and Chinese (zh).	PT	probability threshold	0
We enumerate  them below as a key: Arabic (ar), Danish (da),  German (de), English (en), Spanish (es), French  (fr), Indonesian (Malay) (id), Italian (it), Japanese  (ja), Korean (ko), Malaysian (Malay) (ms), Dutch  (nl), PT (pt), Russian (ru), Thai (th), Viet- namese (vi) and Chinese (zh).	PT	parse tree	0
We enumerate  them below as a key: Arabic (ar), Danish (da),  German (de), English (en), Spanish (es), French  (fr), Indonesian (Malay) (id), Italian (it), Japanese  (ja), Korean (ko), Malaysian (Malay) (ms), Dutch  (nl), PT (pt), Russian (ru), Thai (th), Viet- namese (vi) and Chinese (zh).	PT	phrase table	0
We enumerate  them below as a key: Arabic (ar), Danish (da),  German (de), English (en), Spanish (es), French  (fr), Indonesian (Malay) (id), Italian (it), Japanese  (ja), Korean (ko), Malaysian (Malay) (ms), Dutch  (nl), PT (pt), Russian (ru), Thai (th), Viet- namese (vi) and Chinese (zh).	PT	Portuguese	0
We enumerate  them below as a key: Arabic (ar), Danish (da),  German (de), English (en), Spanish (es), French  (fr), Indonesian (Malay) (id), Italian (it), Japanese  (ja), Korean (ko), Malaysian (Malay) (ms), Dutch  (nl), PT (pt), Russian (ru), Thai (th), Viet- namese (vi) and Chinese (zh).	PT	Parse Trees	0
We enumerate  them below as a key: Arabic (ar), Danish (da),  German (de), English (en), Spanish (es), French  (fr), Indonesian (Malay) (id), Italian (it), Japanese  (ja), Korean (ko), Malaysian (Malay) (ms), Dutch  (nl), PT (pt), Russian (ru), Thai (th), Viet- namese (vi) and Chinese (zh).	PT	part	0
We enumerate  them below as a key: Arabic (ar), Danish (da),  German (de), English (en), Spanish (es), French  (fr), Indonesian (Malay) (id), Italian (it), Japanese  (ja), Korean (ko), Malaysian (Malay) (ms), Dutch  (nl), PT (pt), Russian (ru), Thai (th), Viet- namese (vi) and Chinese (zh).	PT	Portugese	1
We enumerate  them below as a key: Arabic (ar), Danish (da),  German (de), English (en), Spanish (es), French  (fr), Indonesian (Malay) (id), Italian (it), Japanese  (ja), Korean (ko), Malaysian (Malay) (ms), Dutch  (nl), PT (pt), Russian (ru), Thai (th), Viet- namese (vi) and Chinese (zh).	PT	Prompts	0
We enumerate  them below as a key: Arabic (ar), Danish (da),  German (de), English (en), Spanish (es), French  (fr), Indonesian (Malay) (id), Italian (it), Japanese  (ja), Korean (ko), Malaysian (Malay) (ms), Dutch  (nl), PT (pt), Russian (ru), Thai (th), Viet- namese (vi) and Chinese (zh).	PT	Probable alignments	0
We enumerate  them below as a key: Arabic (ar), Danish (da),  German (de), English (en), Spanish (es), French  (fr), Indonesian (Malay) (id), Italian (it), Japanese  (ja), Korean (ko), Malaysian (Malay) (ms), Dutch  (nl), PT (pt), Russian (ru), Thai (th), Viet- namese (vi) and Chinese (zh).	PT	Parse tree	0
We enumerate  them below as a key: Arabic (ar), Danish (da),  German (de), English (en), Spanish (es), French  (fr), Indonesian (Malay) (id), Italian (it), Japanese  (ja), Korean (ko), Malaysian (Malay) (ms), Dutch  (nl), PT (pt), Russian (ru), Thai (th), Viet- namese (vi) and Chinese (zh).	PT	partial tree	0
We enumerate  them below as a key: Arabic (ar), Danish (da),  German (de), English (en), Spanish (es), French  (fr), Indonesian (Malay) (id), Italian (it), Japanese  (ja), Korean (ko), Malaysian (Malay) (ms), Dutch  (nl), PT (pt), Russian (ru), Thai (th), Viet- namese (vi) and Chinese (zh).	PT	Phrase Table	0
Our initial investigations using this set for English, Spanish, French, Brazilian PT and German text, suggest min- imal modification is required to adapt for use in question answering.	PT	probability threshold	0
Our initial investigations using this set for English, Spanish, French, Brazilian PT and German text, suggest min- imal modification is required to adapt for use in question answering.	PT	parse tree	0
Our initial investigations using this set for English, Spanish, French, Brazilian PT and German text, suggest min- imal modification is required to adapt for use in question answering.	PT	phrase table	0
Our initial investigations using this set for English, Spanish, French, Brazilian PT and German text, suggest min- imal modification is required to adapt for use in question answering.	PT	Portuguese	0
Our initial investigations using this set for English, Spanish, French, Brazilian PT and German text, suggest min- imal modification is required to adapt for use in question answering.	PT	Parse Trees	0
Our initial investigations using this set for English, Spanish, French, Brazilian PT and German text, suggest min- imal modification is required to adapt for use in question answering.	PT	part	0
Our initial investigations using this set for English, Spanish, French, Brazilian PT and German text, suggest min- imal modification is required to adapt for use in question answering.	PT	Portugese	1
Our initial investigations using this set for English, Spanish, French, Brazilian PT and German text, suggest min- imal modification is required to adapt for use in question answering.	PT	Prompts	0
Our initial investigations using this set for English, Spanish, French, Brazilian PT and German text, suggest min- imal modification is required to adapt for use in question answering.	PT	Probable alignments	0
Our initial investigations using this set for English, Spanish, French, Brazilian PT and German text, suggest min- imal modification is required to adapt for use in question answering.	PT	Parse tree	0
Our initial investigations using this set for English, Spanish, French, Brazilian PT and German text, suggest min- imal modification is required to adapt for use in question answering.	PT	partial tree	0
Our initial investigations using this set for English, Spanish, French, Brazilian PT and German text, suggest min- imal modification is required to adapt for use in question answering.	PT	Phrase Table	0
workshop on joint evaluation of computational processing of PT at PorTAL 2002.	PT	probability threshold	0
workshop on joint evaluation of computational processing of PT at PorTAL 2002.	PT	parse tree	0
workshop on joint evaluation of computational processing of PT at PorTAL 2002.	PT	phrase table	0
workshop on joint evaluation of computational processing of PT at PorTAL 2002.	PT	Portuguese	0
workshop on joint evaluation of computational processing of PT at PorTAL 2002.	PT	Parse Trees	0
workshop on joint evaluation of computational processing of PT at PorTAL 2002.	PT	part	0
workshop on joint evaluation of computational processing of PT at PorTAL 2002.	PT	Portugese	1
workshop on joint evaluation of computational processing of PT at PorTAL 2002.	PT	Prompts	0
workshop on joint evaluation of computational processing of PT at PorTAL 2002.	PT	Probable alignments	0
workshop on joint evaluation of computational processing of PT at PorTAL 2002.	PT	Parse tree	0
workshop on joint evaluation of computational processing of PT at PorTAL 2002.	PT	partial tree	0
workshop on joint evaluation of computational processing of PT at PorTAL 2002.	PT	Phrase Table	0
PT the user to interactively enter sam- ple documents (used primarily for testing, or entering queries).	PT	probability threshold	0
PT the user to interactively enter sam- ple documents (used primarily for testing, or entering queries).	PT	parse tree	0
PT the user to interactively enter sam- ple documents (used primarily for testing, or entering queries).	PT	phrase table	0
PT the user to interactively enter sam- ple documents (used primarily for testing, or entering queries).	PT	Portuguese	0
PT the user to interactively enter sam- ple documents (used primarily for testing, or entering queries).	PT	Parse Trees	0
PT the user to interactively enter sam- ple documents (used primarily for testing, or entering queries).	PT	part	0
PT the user to interactively enter sam- ple documents (used primarily for testing, or entering queries).	PT	Portugese	0
PT the user to interactively enter sam- ple documents (used primarily for testing, or entering queries).	PT	Prompts	1
PT the user to interactively enter sam- ple documents (used primarily for testing, or entering queries).	PT	Probable alignments	0
PT the user to interactively enter sam- ple documents (used primarily for testing, or entering queries).	PT	Parse tree	0
PT the user to interactively enter sam- ple documents (used primarily for testing, or entering queries).	PT	partial tree	0
PT the user to interactively enter sam- ple documents (used primarily for testing, or entering queries).	PT	Phrase Table	0
4.2 Automatic Generation of Corrective PT In this study, not all prompts were modified to match the user?s choice of words.	PT	probability threshold	0
4.2 Automatic Generation of Corrective PT In this study, not all prompts were modified to match the user?s choice of words.	PT	parse tree	0
4.2 Automatic Generation of Corrective PT In this study, not all prompts were modified to match the user?s choice of words.	PT	phrase table	0
4.2 Automatic Generation of Corrective PT In this study, not all prompts were modified to match the user?s choice of words.	PT	Portuguese	0
4.2 Automatic Generation of Corrective PT In this study, not all prompts were modified to match the user?s choice of words.	PT	Parse Trees	0
4.2 Automatic Generation of Corrective PT In this study, not all prompts were modified to match the user?s choice of words.	PT	part	0
4.2 Automatic Generation of Corrective PT In this study, not all prompts were modified to match the user?s choice of words.	PT	Portugese	0
4.2 Automatic Generation of Corrective PT In this study, not all prompts were modified to match the user?s choice of words.	PT	Prompts	1
4.2 Automatic Generation of Corrective PT In this study, not all prompts were modified to match the user?s choice of words.	PT	Probable alignments	0
4.2 Automatic Generation of Corrective PT In this study, not all prompts were modified to match the user?s choice of words.	PT	Parse tree	0
4.2 Automatic Generation of Corrective PT In this study, not all prompts were modified to match the user?s choice of words.	PT	partial tree	0
4.2 Automatic Generation of Corrective PT In this study, not all prompts were modified to match the user?s choice of words.	PT	Phrase Table	0
PT.	PT	probability threshold	0
PT.	PT	parse tree	0
PT.	PT	phrase table	0
PT.	PT	Portuguese	0
PT.	PT	Parse Trees	0
PT.	PT	part	0
PT.	PT	Portugese	0
PT.	PT	Prompts	1
PT.	PT	Probable alignments	0
PT.	PT	Parse tree	0
PT.	PT	partial tree	0
PT.	PT	Phrase Table	0
Of course, the role of PT and SI is not surprising, although interest- ingly they are significant only in association with certain tutor moves.	PT	probability threshold	0
Of course, the role of PT and SI is not surprising, although interest- ingly they are significant only in association with certain tutor moves.	PT	parse tree	0
Of course, the role of PT and SI is not surprising, although interest- ingly they are significant only in association with certain tutor moves.	PT	phrase table	0
Of course, the role of PT and SI is not surprising, although interest- ingly they are significant only in association with certain tutor moves.	PT	Portuguese	0
Of course, the role of PT and SI is not surprising, although interest- ingly they are significant only in association with certain tutor moves.	PT	Parse Trees	0
Of course, the role of PT and SI is not surprising, although interest- ingly they are significant only in association with certain tutor moves.	PT	part	0
Of course, the role of PT and SI is not surprising, although interest- ingly they are significant only in association with certain tutor moves.	PT	Portugese	0
Of course, the role of PT and SI is not surprising, although interest- ingly they are significant only in association with certain tutor moves.	PT	Prompts	1
Of course, the role of PT and SI is not surprising, although interest- ingly they are significant only in association with certain tutor moves.	PT	Probable alignments	0
Of course, the role of PT and SI is not surprising, although interest- ingly they are significant only in association with certain tutor moves.	PT	Parse tree	0
Of course, the role of PT and SI is not surprising, although interest- ingly they are significant only in association with certain tutor moves.	PT	partial tree	0
Of course, the role of PT and SI is not surprising, although interest- ingly they are significant only in association with certain tutor moves.	PT	Phrase Table	0
4PT are short texts that present an argument or issue and ask test takers to respond to it, either by analyzing the given argument or taking a stance on the given issue.	PT	probability threshold	0
4PT are short texts that present an argument or issue and ask test takers to respond to it, either by analyzing the given argument or taking a stance on the given issue.	PT	parse tree	0
4PT are short texts that present an argument or issue and ask test takers to respond to it, either by analyzing the given argument or taking a stance on the given issue.	PT	phrase table	0
4PT are short texts that present an argument or issue and ask test takers to respond to it, either by analyzing the given argument or taking a stance on the given issue.	PT	Portuguese	0
4PT are short texts that present an argument or issue and ask test takers to respond to it, either by analyzing the given argument or taking a stance on the given issue.	PT	Parse Trees	0
4PT are short texts that present an argument or issue and ask test takers to respond to it, either by analyzing the given argument or taking a stance on the given issue.	PT	part	0
4PT are short texts that present an argument or issue and ask test takers to respond to it, either by analyzing the given argument or taking a stance on the given issue.	PT	Portugese	0
4PT are short texts that present an argument or issue and ask test takers to respond to it, either by analyzing the given argument or taking a stance on the given issue.	PT	Prompts	1
4PT are short texts that present an argument or issue and ask test takers to respond to it, either by analyzing the given argument or taking a stance on the given issue.	PT	Probable alignments	0
4PT are short texts that present an argument or issue and ask test takers to respond to it, either by analyzing the given argument or taking a stance on the given issue.	PT	Parse tree	0
4PT are short texts that present an argument or issue and ask test takers to respond to it, either by analyzing the given argument or taking a stance on the given issue.	PT	partial tree	0
4PT are short texts that present an argument or issue and ask test takers to respond to it, either by analyzing the given argument or taking a stance on the given issue.	PT	Phrase Table	0
For example, since Student Initiatives (SI) are not as frequent, we needed to double code more sessions to find a number of SI?s high enough to compute a meaningful Kappa (in our whole corpus, there are 1157 SIs but e.g. 4957 PT).	PT	probability threshold	0
For example, since Student Initiatives (SI) are not as frequent, we needed to double code more sessions to find a number of SI?s high enough to compute a meaningful Kappa (in our whole corpus, there are 1157 SIs but e.g. 4957 PT).	PT	parse tree	0
For example, since Student Initiatives (SI) are not as frequent, we needed to double code more sessions to find a number of SI?s high enough to compute a meaningful Kappa (in our whole corpus, there are 1157 SIs but e.g. 4957 PT).	PT	phrase table	0
For example, since Student Initiatives (SI) are not as frequent, we needed to double code more sessions to find a number of SI?s high enough to compute a meaningful Kappa (in our whole corpus, there are 1157 SIs but e.g. 4957 PT).	PT	Portuguese	0
For example, since Student Initiatives (SI) are not as frequent, we needed to double code more sessions to find a number of SI?s high enough to compute a meaningful Kappa (in our whole corpus, there are 1157 SIs but e.g. 4957 PT).	PT	Parse Trees	0
For example, since Student Initiatives (SI) are not as frequent, we needed to double code more sessions to find a number of SI?s high enough to compute a meaningful Kappa (in our whole corpus, there are 1157 SIs but e.g. 4957 PT).	PT	part	0
For example, since Student Initiatives (SI) are not as frequent, we needed to double code more sessions to find a number of SI?s high enough to compute a meaningful Kappa (in our whole corpus, there are 1157 SIs but e.g. 4957 PT).	PT	Portugese	0
For example, since Student Initiatives (SI) are not as frequent, we needed to double code more sessions to find a number of SI?s high enough to compute a meaningful Kappa (in our whole corpus, there are 1157 SIs but e.g. 4957 PT).	PT	Prompts	1
For example, since Student Initiatives (SI) are not as frequent, we needed to double code more sessions to find a number of SI?s high enough to compute a meaningful Kappa (in our whole corpus, there are 1157 SIs but e.g. 4957 PT).	PT	Probable alignments	0
For example, since Student Initiatives (SI) are not as frequent, we needed to double code more sessions to find a number of SI?s high enough to compute a meaningful Kappa (in our whole corpus, there are 1157 SIs but e.g. 4957 PT).	PT	Parse tree	0
For example, since Student Initiatives (SI) are not as frequent, we needed to double code more sessions to find a number of SI?s high enough to compute a meaningful Kappa (in our whole corpus, there are 1157 SIs but e.g. 4957 PT).	PT	partial tree	0
For example, since Student Initiatives (SI) are not as frequent, we needed to double code more sessions to find a number of SI?s high enough to compute a meaningful Kappa (in our whole corpus, there are 1157 SIs but e.g. 4957 PT).	PT	Phrase Table	0
The only evaluation set where Romanian-English data leads to better performance is the PT set.	PT	probability threshold	0
The only evaluation set where Romanian-English data leads to better performance is the PT set.	PT	parse tree	0
The only evaluation set where Romanian-English data leads to better performance is the PT set.	PT	phrase table	0
The only evaluation set where Romanian-English data leads to better performance is the PT set.	PT	Portuguese	0
The only evaluation set where Romanian-English data leads to better performance is the PT set.	PT	Parse Trees	0
The only evaluation set where Romanian-English data leads to better performance is the PT set.	PT	part	0
The only evaluation set where Romanian-English data leads to better performance is the PT set.	PT	Portugese	0
The only evaluation set where Romanian-English data leads to better performance is the PT set.	PT	Prompts	0
The only evaluation set where Romanian-English data leads to better performance is the PT set.	PT	Probable alignments	1
The only evaluation set where Romanian-English data leads to better performance is the PT set.	PT	Parse tree	0
The only evaluation set where Romanian-English data leads to better performance is the PT set.	PT	partial tree	0
The only evaluation set where Romanian-English data leads to better performance is the PT set.	PT	Phrase Table	0
That is,   ff  ff	           fi         (4) The F?measure is the harmonic mean of precision and recall:            ff  ff	    fi   ff  ff	   fi (5) AER is defined by (Och and Ney, 2000a) and accounts for both Sure and PT in scoring.	PT	probability threshold	0
That is,   ff  ff	           fi         (4) The F?measure is the harmonic mean of precision and recall:            ff  ff	    fi   ff  ff	   fi (5) AER is defined by (Och and Ney, 2000a) and accounts for both Sure and PT in scoring.	PT	parse tree	0
That is,   ff  ff	           fi         (4) The F?measure is the harmonic mean of precision and recall:            ff  ff	    fi   ff  ff	   fi (5) AER is defined by (Och and Ney, 2000a) and accounts for both Sure and PT in scoring.	PT	phrase table	0
That is,   ff  ff	           fi         (4) The F?measure is the harmonic mean of precision and recall:            ff  ff	    fi   ff  ff	   fi (5) AER is defined by (Och and Ney, 2000a) and accounts for both Sure and PT in scoring.	PT	Portuguese	0
That is,   ff  ff	           fi         (4) The F?measure is the harmonic mean of precision and recall:            ff  ff	    fi   ff  ff	   fi (5) AER is defined by (Och and Ney, 2000a) and accounts for both Sure and PT in scoring.	PT	Parse Trees	0
That is,   ff  ff	           fi         (4) The F?measure is the harmonic mean of precision and recall:            ff  ff	    fi   ff  ff	   fi (5) AER is defined by (Och and Ney, 2000a) and accounts for both Sure and PT in scoring.	PT	part	0
That is,   ff  ff	           fi         (4) The F?measure is the harmonic mean of precision and recall:            ff  ff	    fi   ff  ff	   fi (5) AER is defined by (Och and Ney, 2000a) and accounts for both Sure and PT in scoring.	PT	Portugese	0
That is,   ff  ff	           fi         (4) The F?measure is the harmonic mean of precision and recall:            ff  ff	    fi   ff  ff	   fi (5) AER is defined by (Och and Ney, 2000a) and accounts for both Sure and PT in scoring.	PT	Prompts	0
That is,   ff  ff	           fi         (4) The F?measure is the harmonic mean of precision and recall:            ff  ff	    fi   ff  ff	   fi (5) AER is defined by (Och and Ney, 2000a) and accounts for both Sure and PT in scoring.	PT	Probable alignments	1
That is,   ff  ff	           fi         (4) The F?measure is the harmonic mean of precision and recall:            ff  ff	    fi   ff  ff	   fi (5) AER is defined by (Och and Ney, 2000a) and accounts for both Sure and PT in scoring.	PT	Parse tree	0
That is,   ff  ff	           fi         (4) The F?measure is the harmonic mean of precision and recall:            ff  ff	    fi   ff  ff	   fi (5) AER is defined by (Och and Ney, 2000a) and accounts for both Sure and PT in scoring.	PT	partial tree	0
That is,   ff  ff	           fi         (4) The F?measure is the harmonic mean of precision and recall:            ff  ff	    fi   ff  ff	   fi (5) AER is defined by (Och and Ney, 2000a) and accounts for both Sure and PT in scoring.	PT	Phrase Table	0
PT are large blocks of words which the annotator was uncertain of how to align.	PT	probability threshold	0
PT are large blocks of words which the annotator was uncertain of how to align.	PT	parse tree	0
PT are large blocks of words which the annotator was uncertain of how to align.	PT	phrase table	0
PT are large blocks of words which the annotator was uncertain of how to align.	PT	Portuguese	0
PT are large blocks of words which the annotator was uncertain of how to align.	PT	Parse Trees	0
PT are large blocks of words which the annotator was uncertain of how to align.	PT	part	0
PT are large blocks of words which the annotator was uncertain of how to align.	PT	Portugese	0
PT are large blocks of words which the annotator was uncertain of how to align.	PT	Prompts	0
PT are large blocks of words which the annotator was uncertain of how to align.	PT	Probable alignments	1
PT are large blocks of words which the annotator was uncertain of how to align.	PT	Parse tree	0
PT are large blocks of words which the annotator was uncertain of how to align.	PT	partial tree	0
PT are large blocks of words which the annotator was uncertain of how to align.	PT	Phrase Table	0
The intersection of the Sure alignments pro- duced by the two annotators led to the final Sure aligned set, while the reunion of the PT led to the final Probable aligned set.	PT	probability threshold	0
The intersection of the Sure alignments pro- duced by the two annotators led to the final Sure aligned set, while the reunion of the PT led to the final Probable aligned set.	PT	parse tree	0
The intersection of the Sure alignments pro- duced by the two annotators led to the final Sure aligned set, while the reunion of the PT led to the final Probable aligned set.	PT	phrase table	0
The intersection of the Sure alignments pro- duced by the two annotators led to the final Sure aligned set, while the reunion of the PT led to the final Probable aligned set.	PT	Portuguese	0
The intersection of the Sure alignments pro- duced by the two annotators led to the final Sure aligned set, while the reunion of the PT led to the final Probable aligned set.	PT	Parse Trees	0
The intersection of the Sure alignments pro- duced by the two annotators led to the final Sure aligned set, while the reunion of the PT led to the final Probable aligned set.	PT	part	0
The intersection of the Sure alignments pro- duced by the two annotators led to the final Sure aligned set, while the reunion of the PT led to the final Probable aligned set.	PT	Portugese	0
The intersection of the Sure alignments pro- duced by the two annotators led to the final Sure aligned set, while the reunion of the PT led to the final Probable aligned set.	PT	Prompts	0
The intersection of the Sure alignments pro- duced by the two annotators led to the final Sure aligned set, while the reunion of the PT led to the final Probable aligned set.	PT	Probable alignments	1
The intersection of the Sure alignments pro- duced by the two annotators led to the final Sure aligned set, while the reunion of the PT led to the final Probable aligned set.	PT	Parse tree	0
The intersection of the Sure alignments pro- duced by the two annotators led to the final Sure aligned set, while the reunion of the PT led to the final Probable aligned set.	PT	partial tree	0
The intersection of the Sure alignments pro- duced by the two annotators led to the final Sure aligned set, while the reunion of the PT led to the final Probable aligned set.	PT	Phrase Table	0
Given an alignment , and a gold standard alignment , each such alignment set eventually consisting of two sets   ,   , and   ,   corresponding to Sure and PT, the following measures are defined (where  is the alignment type, and can be set to either S or P).	PT	probability threshold	0
Given an alignment , and a gold standard alignment , each such alignment set eventually consisting of two sets   ,   , and   ,   corresponding to Sure and PT, the following measures are defined (where  is the alignment type, and can be set to either S or P).	PT	parse tree	0
Given an alignment , and a gold standard alignment , each such alignment set eventually consisting of two sets   ,   , and   ,   corresponding to Sure and PT, the following measures are defined (where  is the alignment type, and can be set to either S or P).	PT	phrase table	0
Given an alignment , and a gold standard alignment , each such alignment set eventually consisting of two sets   ,   , and   ,   corresponding to Sure and PT, the following measures are defined (where  is the alignment type, and can be set to either S or P).	PT	Portuguese	0
Given an alignment , and a gold standard alignment , each such alignment set eventually consisting of two sets   ,   , and   ,   corresponding to Sure and PT, the following measures are defined (where  is the alignment type, and can be set to either S or P).	PT	Parse Trees	0
Given an alignment , and a gold standard alignment , each such alignment set eventually consisting of two sets   ,   , and   ,   corresponding to Sure and PT, the following measures are defined (where  is the alignment type, and can be set to either S or P).	PT	part	0
Given an alignment , and a gold standard alignment , each such alignment set eventually consisting of two sets   ,   , and   ,   corresponding to Sure and PT, the following measures are defined (where  is the alignment type, and can be set to either S or P).	PT	Portugese	0
Given an alignment , and a gold standard alignment , each such alignment set eventually consisting of two sets   ,   , and   ,   corresponding to Sure and PT, the following measures are defined (where  is the alignment type, and can be set to either S or P).	PT	Prompts	0
Given an alignment , and a gold standard alignment , each such alignment set eventually consisting of two sets   ,   , and   ,   corresponding to Sure and PT, the following measures are defined (where  is the alignment type, and can be set to either S or P).	PT	Probable alignments	1
Given an alignment , and a gold standard alignment , each such alignment set eventually consisting of two sets   ,   , and   ,   corresponding to Sure and PT, the following measures are defined (where  is the alignment type, and can be set to either S or P).	PT	Parse tree	0
Given an alignment , and a gold standard alignment , each such alignment set eventually consisting of two sets   ,   , and   ,   corresponding to Sure and PT, the following measures are defined (where  is the alignment type, and can be set to either S or P).	PT	partial tree	0
Given an alignment , and a gold standard alignment , each such alignment set eventually consisting of two sets   ,   , and   ,   corresponding to Sure and PT, the following measures are defined (where  is the alignment type, and can be set to either S or P).	PT	Phrase Table	0
While this may seem contradictory, AER factors in both Sure and PT into is scoring while only the English?French data included such alignments in its gold standard.	PT	probability threshold	0
While this may seem contradictory, AER factors in both Sure and PT into is scoring while only the English?French data included such alignments in its gold standard.	PT	parse tree	0
While this may seem contradictory, AER factors in both Sure and PT into is scoring while only the English?French data included such alignments in its gold standard.	PT	phrase table	0
While this may seem contradictory, AER factors in both Sure and PT into is scoring while only the English?French data included such alignments in its gold standard.	PT	Portuguese	0
While this may seem contradictory, AER factors in both Sure and PT into is scoring while only the English?French data included such alignments in its gold standard.	PT	Parse Trees	0
While this may seem contradictory, AER factors in both Sure and PT into is scoring while only the English?French data included such alignments in its gold standard.	PT	part	0
While this may seem contradictory, AER factors in both Sure and PT into is scoring while only the English?French data included such alignments in its gold standard.	PT	Portugese	0
While this may seem contradictory, AER factors in both Sure and PT into is scoring while only the English?French data included such alignments in its gold standard.	PT	Prompts	0
While this may seem contradictory, AER factors in both Sure and PT into is scoring while only the English?French data included such alignments in its gold standard.	PT	Probable alignments	1
While this may seem contradictory, AER factors in both Sure and PT into is scoring while only the English?French data included such alignments in its gold standard.	PT	Parse tree	0
While this may seem contradictory, AER factors in both Sure and PT into is scoring while only the English?French data included such alignments in its gold standard.	PT	partial tree	0
While this may seem contradictory, AER factors in both Sure and PT into is scoring while only the English?French data included such alignments in its gold standard.	PT	Phrase Table	0
Sen -- sentence branch     full_propernoun -- proper noun phrase propernoun JOHN [Sexed]     neg_copula -- copula verb phrase asCopulaN RETIRE [Fut] comnoun CHAIRMAN [Sing,Per3] Figure 3: PT for "John will retire as chairman".	PT	probability threshold	0
Sen -- sentence branch     full_propernoun -- proper noun phrase propernoun JOHN [Sexed]     neg_copula -- copula verb phrase asCopulaN RETIRE [Fut] comnoun CHAIRMAN [Sing,Per3] Figure 3: PT for "John will retire as chairman".	PT	parse tree	0
Sen -- sentence branch     full_propernoun -- proper noun phrase propernoun JOHN [Sexed]     neg_copula -- copula verb phrase asCopulaN RETIRE [Fut] comnoun CHAIRMAN [Sing,Per3] Figure 3: PT for "John will retire as chairman".	PT	phrase table	0
Sen -- sentence branch     full_propernoun -- proper noun phrase propernoun JOHN [Sexed]     neg_copula -- copula verb phrase asCopulaN RETIRE [Fut] comnoun CHAIRMAN [Sing,Per3] Figure 3: PT for "John will retire as chairman".	PT	Portuguese	0
Sen -- sentence branch     full_propernoun -- proper noun phrase propernoun JOHN [Sexed]     neg_copula -- copula verb phrase asCopulaN RETIRE [Fut] comnoun CHAIRMAN [Sing,Per3] Figure 3: PT for "John will retire as chairman".	PT	Parse Trees	0
Sen -- sentence branch     full_propernoun -- proper noun phrase propernoun JOHN [Sexed]     neg_copula -- copula verb phrase asCopulaN RETIRE [Fut] comnoun CHAIRMAN [Sing,Per3] Figure 3: PT for "John will retire as chairman".	PT	part	0
Sen -- sentence branch     full_propernoun -- proper noun phrase propernoun JOHN [Sexed]     neg_copula -- copula verb phrase asCopulaN RETIRE [Fut] comnoun CHAIRMAN [Sing,Per3] Figure 3: PT for "John will retire as chairman".	PT	Portugese	0
Sen -- sentence branch     full_propernoun -- proper noun phrase propernoun JOHN [Sexed]     neg_copula -- copula verb phrase asCopulaN RETIRE [Fut] comnoun CHAIRMAN [Sing,Per3] Figure 3: PT for "John will retire as chairman".	PT	Prompts	0
Sen -- sentence branch     full_propernoun -- proper noun phrase propernoun JOHN [Sexed]     neg_copula -- copula verb phrase asCopulaN RETIRE [Fut] comnoun CHAIRMAN [Sing,Per3] Figure 3: PT for "John will retire as chairman".	PT	Probable alignments	0
Sen -- sentence branch     full_propernoun -- proper noun phrase propernoun JOHN [Sexed]     neg_copula -- copula verb phrase asCopulaN RETIRE [Fut] comnoun CHAIRMAN [Sing,Per3] Figure 3: PT for "John will retire as chairman".	PT	Parse tree	1
Sen -- sentence branch     full_propernoun -- proper noun phrase propernoun JOHN [Sexed]     neg_copula -- copula verb phrase asCopulaN RETIRE [Fut] comnoun CHAIRMAN [Sing,Per3] Figure 3: PT for "John will retire as chairman".	PT	partial tree	0
Sen -- sentence branch     full_propernoun -- proper noun phrase propernoun JOHN [Sexed]     neg_copula -- copula verb phrase asCopulaN RETIRE [Fut] comnoun CHAIRMAN [Sing,Per3] Figure 3: PT for "John will retire as chairman".	PT	Phrase Table	0
I I l l  I i I l i   ' t i l t   ADJ'* "a"  "fish"  PREP "with"  DET ADJ*  NOUN* "fork"  t ta t '   Tree I. PT for a syntactically ambiguous PP attachment  DECL NP  VERB*  NP  PUNC  ' t i "  PRON*  ttwant"  DET  NOUN*  PP  I t  t,   ADJ*  "book"  PREP  DET  NOUN*  ~LCL   "the"  "by"  ADJ'*  "uncle"  NP  VERB*  PP  "my"  PRON* "that"  'tlSt'  PREP "on"  DET ADJ*  NOUN* "shelf"  "the"  Tree 2.	PT	probability threshold	0
I I l l  I i I l i   ' t i l t   ADJ'* "a"  "fish"  PREP "with"  DET ADJ*  NOUN* "fork"  t ta t '   Tree I. PT for a syntactically ambiguous PP attachment  DECL NP  VERB*  NP  PUNC  ' t i "  PRON*  ttwant"  DET  NOUN*  PP  I t  t,   ADJ*  "book"  PREP  DET  NOUN*  ~LCL   "the"  "by"  ADJ'*  "uncle"  NP  VERB*  PP  "my"  PRON* "that"  'tlSt'  PREP "on"  DET ADJ*  NOUN* "shelf"  "the"  Tree 2.	PT	parse tree	0
I I l l  I i I l i   ' t i l t   ADJ'* "a"  "fish"  PREP "with"  DET ADJ*  NOUN* "fork"  t ta t '   Tree I. PT for a syntactically ambiguous PP attachment  DECL NP  VERB*  NP  PUNC  ' t i "  PRON*  ttwant"  DET  NOUN*  PP  I t  t,   ADJ*  "book"  PREP  DET  NOUN*  ~LCL   "the"  "by"  ADJ'*  "uncle"  NP  VERB*  PP  "my"  PRON* "that"  'tlSt'  PREP "on"  DET ADJ*  NOUN* "shelf"  "the"  Tree 2.	PT	phrase table	0
I I l l  I i I l i   ' t i l t   ADJ'* "a"  "fish"  PREP "with"  DET ADJ*  NOUN* "fork"  t ta t '   Tree I. PT for a syntactically ambiguous PP attachment  DECL NP  VERB*  NP  PUNC  ' t i "  PRON*  ttwant"  DET  NOUN*  PP  I t  t,   ADJ*  "book"  PREP  DET  NOUN*  ~LCL   "the"  "by"  ADJ'*  "uncle"  NP  VERB*  PP  "my"  PRON* "that"  'tlSt'  PREP "on"  DET ADJ*  NOUN* "shelf"  "the"  Tree 2.	PT	Portuguese	0
I I l l  I i I l i   ' t i l t   ADJ'* "a"  "fish"  PREP "with"  DET ADJ*  NOUN* "fork"  t ta t '   Tree I. PT for a syntactically ambiguous PP attachment  DECL NP  VERB*  NP  PUNC  ' t i "  PRON*  ttwant"  DET  NOUN*  PP  I t  t,   ADJ*  "book"  PREP  DET  NOUN*  ~LCL   "the"  "by"  ADJ'*  "uncle"  NP  VERB*  PP  "my"  PRON* "that"  'tlSt'  PREP "on"  DET ADJ*  NOUN* "shelf"  "the"  Tree 2.	PT	Parse Trees	0
I I l l  I i I l i   ' t i l t   ADJ'* "a"  "fish"  PREP "with"  DET ADJ*  NOUN* "fork"  t ta t '   Tree I. PT for a syntactically ambiguous PP attachment  DECL NP  VERB*  NP  PUNC  ' t i "  PRON*  ttwant"  DET  NOUN*  PP  I t  t,   ADJ*  "book"  PREP  DET  NOUN*  ~LCL   "the"  "by"  ADJ'*  "uncle"  NP  VERB*  PP  "my"  PRON* "that"  'tlSt'  PREP "on"  DET ADJ*  NOUN* "shelf"  "the"  Tree 2.	PT	part	0
I I l l  I i I l i   ' t i l t   ADJ'* "a"  "fish"  PREP "with"  DET ADJ*  NOUN* "fork"  t ta t '   Tree I. PT for a syntactically ambiguous PP attachment  DECL NP  VERB*  NP  PUNC  ' t i "  PRON*  ttwant"  DET  NOUN*  PP  I t  t,   ADJ*  "book"  PREP  DET  NOUN*  ~LCL   "the"  "by"  ADJ'*  "uncle"  NP  VERB*  PP  "my"  PRON* "that"  'tlSt'  PREP "on"  DET ADJ*  NOUN* "shelf"  "the"  Tree 2.	PT	Portugese	0
I I l l  I i I l i   ' t i l t   ADJ'* "a"  "fish"  PREP "with"  DET ADJ*  NOUN* "fork"  t ta t '   Tree I. PT for a syntactically ambiguous PP attachment  DECL NP  VERB*  NP  PUNC  ' t i "  PRON*  ttwant"  DET  NOUN*  PP  I t  t,   ADJ*  "book"  PREP  DET  NOUN*  ~LCL   "the"  "by"  ADJ'*  "uncle"  NP  VERB*  PP  "my"  PRON* "that"  'tlSt'  PREP "on"  DET ADJ*  NOUN* "shelf"  "the"  Tree 2.	PT	Prompts	0
I I l l  I i I l i   ' t i l t   ADJ'* "a"  "fish"  PREP "with"  DET ADJ*  NOUN* "fork"  t ta t '   Tree I. PT for a syntactically ambiguous PP attachment  DECL NP  VERB*  NP  PUNC  ' t i "  PRON*  ttwant"  DET  NOUN*  PP  I t  t,   ADJ*  "book"  PREP  DET  NOUN*  ~LCL   "the"  "by"  ADJ'*  "uncle"  NP  VERB*  PP  "my"  PRON* "that"  'tlSt'  PREP "on"  DET ADJ*  NOUN* "shelf"  "the"  Tree 2.	PT	Probable alignments	0
I I l l  I i I l i   ' t i l t   ADJ'* "a"  "fish"  PREP "with"  DET ADJ*  NOUN* "fork"  t ta t '   Tree I. PT for a syntactically ambiguous PP attachment  DECL NP  VERB*  NP  PUNC  ' t i "  PRON*  ttwant"  DET  NOUN*  PP  I t  t,   ADJ*  "book"  PREP  DET  NOUN*  ~LCL   "the"  "by"  ADJ'*  "uncle"  NP  VERB*  PP  "my"  PRON* "that"  'tlSt'  PREP "on"  DET ADJ*  NOUN* "shelf"  "the"  Tree 2.	PT	Parse tree	1
I I l l  I i I l i   ' t i l t   ADJ'* "a"  "fish"  PREP "with"  DET ADJ*  NOUN* "fork"  t ta t '   Tree I. PT for a syntactically ambiguous PP attachment  DECL NP  VERB*  NP  PUNC  ' t i "  PRON*  ttwant"  DET  NOUN*  PP  I t  t,   ADJ*  "book"  PREP  DET  NOUN*  ~LCL   "the"  "by"  ADJ'*  "uncle"  NP  VERB*  PP  "my"  PRON* "that"  'tlSt'  PREP "on"  DET ADJ*  NOUN* "shelf"  "the"  Tree 2.	PT	partial tree	0
I I l l  I i I l i   ' t i l t   ADJ'* "a"  "fish"  PREP "with"  DET ADJ*  NOUN* "fork"  t ta t '   Tree I. PT for a syntactically ambiguous PP attachment  DECL NP  VERB*  NP  PUNC  ' t i "  PRON*  ttwant"  DET  NOUN*  PP  I t  t,   ADJ*  "book"  PREP  DET  NOUN*  ~LCL   "the"  "by"  ADJ'*  "uncle"  NP  VERB*  PP  "my"  PRON* "that"  'tlSt'  PREP "on"  DET ADJ*  NOUN* "shelf"  "the"  Tree 2.	PT	Phrase Table	0
PT for "About which very important topic has John talked on Sunday?"	PT	probability threshold	0
PT for "About which very important topic has John talked on Sunday?"	PT	parse tree	0
PT for "About which very important topic has John talked on Sunday?"	PT	phrase table	0
PT for "About which very important topic has John talked on Sunday?"	PT	Portuguese	0
PT for "About which very important topic has John talked on Sunday?"	PT	Parse Trees	0
PT for "About which very important topic has John talked on Sunday?"	PT	part	0
PT for "About which very important topic has John talked on Sunday?"	PT	Portugese	0
PT for "About which very important topic has John talked on Sunday?"	PT	Prompts	0
PT for "About which very important topic has John talked on Sunday?"	PT	Probable alignments	0
PT for "About which very important topic has John talked on Sunday?"	PT	Parse tree	1
PT for "About which very important topic has John talked on Sunday?"	PT	partial tree	0
PT for "About which very important topic has John talked on Sunday?"	PT	Phrase Table	0
PT for "Bill gives the woman a book which	PT	probability threshold	0
PT for "Bill gives the woman a book which	PT	parse tree	0
PT for "Bill gives the woman a book which	PT	phrase table	0
PT for "Bill gives the woman a book which	PT	Portuguese	0
PT for "Bill gives the woman a book which	PT	Parse Trees	0
PT for "Bill gives the woman a book which	PT	part	0
PT for "Bill gives the woman a book which	PT	Portugese	0
PT for "Bill gives the woman a book which	PT	Prompts	0
PT for "Bill gives the woman a book which	PT	Probable alignments	0
PT for "Bill gives the woman a book which	PT	Parse tree	1
PT for "Bill gives the woman a book which	PT	partial tree	0
PT for "Bill gives the woman a book which	PT	Phrase Table	0
The best TB2 feature set is approximately the same as the best FN feature set in spite of the dif- ferences between the datasets (PTs: TB2 ?	PT	probability threshold	0
The best TB2 feature set is approximately the same as the best FN feature set in spite of the dif- ferences between the datasets (PTs: TB2 ?	PT	parse tree	0
The best TB2 feature set is approximately the same as the best FN feature set in spite of the dif- ferences between the datasets (PTs: TB2 ?	PT	phrase table	0
The best TB2 feature set is approximately the same as the best FN feature set in spite of the dif- ferences between the datasets (PTs: TB2 ?	PT	Portuguese	0
The best TB2 feature set is approximately the same as the best FN feature set in spite of the dif- ferences between the datasets (PTs: TB2 ?	PT	Parse Trees	0
The best TB2 feature set is approximately the same as the best FN feature set in spite of the dif- ferences between the datasets (PTs: TB2 ?	PT	part	0
The best TB2 feature set is approximately the same as the best FN feature set in spite of the dif- ferences between the datasets (PTs: TB2 ?	PT	Portugese	0
The best TB2 feature set is approximately the same as the best FN feature set in spite of the dif- ferences between the datasets (PTs: TB2 ?	PT	Prompts	0
The best TB2 feature set is approximately the same as the best FN feature set in spite of the dif- ferences between the datasets (PTs: TB2 ?	PT	Probable alignments	0
The best TB2 feature set is approximately the same as the best FN feature set in spite of the dif- ferences between the datasets (PTs: TB2 ?	PT	Parse tree	1
The best TB2 feature set is approximately the same as the best FN feature set in spite of the dif- ferences between the datasets (PTs: TB2 ?	PT	partial tree	0
The best TB2 feature set is approximately the same as the best FN feature set in spite of the dif- ferences between the datasets (PTs: TB2 ?	PT	Phrase Table	0
PT for "Which man's woman will he persuade to date him?"	PT	probability threshold	0
PT for "Which man's woman will he persuade to date him?"	PT	parse tree	0
PT for "Which man's woman will he persuade to date him?"	PT	phrase table	0
PT for "Which man's woman will he persuade to date him?"	PT	Portuguese	0
PT for "Which man's woman will he persuade to date him?"	PT	Parse Trees	0
PT for "Which man's woman will he persuade to date him?"	PT	part	0
PT for "Which man's woman will he persuade to date him?"	PT	Portugese	0
PT for "Which man's woman will he persuade to date him?"	PT	Prompts	0
PT for "Which man's woman will he persuade to date him?"	PT	Probable alignments	0
PT for "Which man's woman will he persuade to date him?"	PT	Parse tree	1
PT for "Which man's woman will he persuade to date him?"	PT	partial tree	0
PT for "Which man's woman will he persuade to date him?"	PT	Phrase Table	0
edu/?mgo031000/ppa/ 274 FN TB2 Source FrameNet annotation samples (British National Corpus) Penn Treebank-II (WSJ articles) Instance identifica- tion Semantic-centered (related to Frame Elements) Syntactic-centered (related to the structure of the parse tree) PTs Automatically generated (Charniak) Gold standard Total size 27,421 instances 60,699 instances Distribution statistics 70.28% ambiguous verb attachments 2.36:1 v-attch:n-attch 35.71% ambiguous verb attachments 1:1.8 v-attch:n-attch Training / test sets 90% - 10% ?	PT	probability threshold	0
edu/?mgo031000/ppa/ 274 FN TB2 Source FrameNet annotation samples (British National Corpus) Penn Treebank-II (WSJ articles) Instance identifica- tion Semantic-centered (related to Frame Elements) Syntactic-centered (related to the structure of the parse tree) PTs Automatically generated (Charniak) Gold standard Total size 27,421 instances 60,699 instances Distribution statistics 70.28% ambiguous verb attachments 2.36:1 v-attch:n-attch 35.71% ambiguous verb attachments 1:1.8 v-attch:n-attch Training / test sets 90% - 10% ?	PT	parse tree	0
edu/?mgo031000/ppa/ 274 FN TB2 Source FrameNet annotation samples (British National Corpus) Penn Treebank-II (WSJ articles) Instance identifica- tion Semantic-centered (related to Frame Elements) Syntactic-centered (related to the structure of the parse tree) PTs Automatically generated (Charniak) Gold standard Total size 27,421 instances 60,699 instances Distribution statistics 70.28% ambiguous verb attachments 2.36:1 v-attch:n-attch 35.71% ambiguous verb attachments 1:1.8 v-attch:n-attch Training / test sets 90% - 10% ?	PT	phrase table	0
edu/?mgo031000/ppa/ 274 FN TB2 Source FrameNet annotation samples (British National Corpus) Penn Treebank-II (WSJ articles) Instance identifica- tion Semantic-centered (related to Frame Elements) Syntactic-centered (related to the structure of the parse tree) PTs Automatically generated (Charniak) Gold standard Total size 27,421 instances 60,699 instances Distribution statistics 70.28% ambiguous verb attachments 2.36:1 v-attch:n-attch 35.71% ambiguous verb attachments 1:1.8 v-attch:n-attch Training / test sets 90% - 10% ?	PT	Portuguese	0
edu/?mgo031000/ppa/ 274 FN TB2 Source FrameNet annotation samples (British National Corpus) Penn Treebank-II (WSJ articles) Instance identifica- tion Semantic-centered (related to Frame Elements) Syntactic-centered (related to the structure of the parse tree) PTs Automatically generated (Charniak) Gold standard Total size 27,421 instances 60,699 instances Distribution statistics 70.28% ambiguous verb attachments 2.36:1 v-attch:n-attch 35.71% ambiguous verb attachments 1:1.8 v-attch:n-attch Training / test sets 90% - 10% ?	PT	Parse Trees	0
edu/?mgo031000/ppa/ 274 FN TB2 Source FrameNet annotation samples (British National Corpus) Penn Treebank-II (WSJ articles) Instance identifica- tion Semantic-centered (related to Frame Elements) Syntactic-centered (related to the structure of the parse tree) PTs Automatically generated (Charniak) Gold standard Total size 27,421 instances 60,699 instances Distribution statistics 70.28% ambiguous verb attachments 2.36:1 v-attch:n-attch 35.71% ambiguous verb attachments 1:1.8 v-attch:n-attch Training / test sets 90% - 10% ?	PT	part	0
edu/?mgo031000/ppa/ 274 FN TB2 Source FrameNet annotation samples (British National Corpus) Penn Treebank-II (WSJ articles) Instance identifica- tion Semantic-centered (related to Frame Elements) Syntactic-centered (related to the structure of the parse tree) PTs Automatically generated (Charniak) Gold standard Total size 27,421 instances 60,699 instances Distribution statistics 70.28% ambiguous verb attachments 2.36:1 v-attch:n-attch 35.71% ambiguous verb attachments 1:1.8 v-attch:n-attch Training / test sets 90% - 10% ?	PT	Portugese	0
edu/?mgo031000/ppa/ 274 FN TB2 Source FrameNet annotation samples (British National Corpus) Penn Treebank-II (WSJ articles) Instance identifica- tion Semantic-centered (related to Frame Elements) Syntactic-centered (related to the structure of the parse tree) PTs Automatically generated (Charniak) Gold standard Total size 27,421 instances 60,699 instances Distribution statistics 70.28% ambiguous verb attachments 2.36:1 v-attch:n-attch 35.71% ambiguous verb attachments 1:1.8 v-attch:n-attch Training / test sets 90% - 10% ?	PT	Prompts	0
edu/?mgo031000/ppa/ 274 FN TB2 Source FrameNet annotation samples (British National Corpus) Penn Treebank-II (WSJ articles) Instance identifica- tion Semantic-centered (related to Frame Elements) Syntactic-centered (related to the structure of the parse tree) PTs Automatically generated (Charniak) Gold standard Total size 27,421 instances 60,699 instances Distribution statistics 70.28% ambiguous verb attachments 2.36:1 v-attch:n-attch 35.71% ambiguous verb attachments 1:1.8 v-attch:n-attch Training / test sets 90% - 10% ?	PT	Probable alignments	0
edu/?mgo031000/ppa/ 274 FN TB2 Source FrameNet annotation samples (British National Corpus) Penn Treebank-II (WSJ articles) Instance identifica- tion Semantic-centered (related to Frame Elements) Syntactic-centered (related to the structure of the parse tree) PTs Automatically generated (Charniak) Gold standard Total size 27,421 instances 60,699 instances Distribution statistics 70.28% ambiguous verb attachments 2.36:1 v-attch:n-attch 35.71% ambiguous verb attachments 1:1.8 v-attch:n-attch Training / test sets 90% - 10% ?	PT	Parse tree	1
edu/?mgo031000/ppa/ 274 FN TB2 Source FrameNet annotation samples (British National Corpus) Penn Treebank-II (WSJ articles) Instance identifica- tion Semantic-centered (related to Frame Elements) Syntactic-centered (related to the structure of the parse tree) PTs Automatically generated (Charniak) Gold standard Total size 27,421 instances 60,699 instances Distribution statistics 70.28% ambiguous verb attachments 2.36:1 v-attch:n-attch 35.71% ambiguous verb attachments 1:1.8 v-attch:n-attch Training / test sets 90% - 10% ?	PT	partial tree	0
edu/?mgo031000/ppa/ 274 FN TB2 Source FrameNet annotation samples (British National Corpus) Penn Treebank-II (WSJ articles) Instance identifica- tion Semantic-centered (related to Frame Elements) Syntactic-centered (related to the structure of the parse tree) PTs Automatically generated (Charniak) Gold standard Total size 27,421 instances 60,699 instances Distribution statistics 70.28% ambiguous verb attachments 2.36:1 v-attch:n-attch 35.71% ambiguous verb attachments 1:1.8 v-attch:n-attch Training / test sets 90% - 10% ?	PT	Phrase Table	0
Further, the de-  sign of a graphical user-interface to enter the  queries is planned, allowing to specify queries  by drawing PTs instead of typing in  the expressions in the query language.	PT	probability threshold	0
Further, the de-  sign of a graphical user-interface to enter the  queries is planned, allowing to specify queries  by drawing PTs instead of typing in  the expressions in the query language.	PT	parse tree	0
Further, the de-  sign of a graphical user-interface to enter the  queries is planned, allowing to specify queries  by drawing PTs instead of typing in  the expressions in the query language.	PT	phrase table	0
Further, the de-  sign of a graphical user-interface to enter the  queries is planned, allowing to specify queries  by drawing PTs instead of typing in  the expressions in the query language.	PT	Portuguese	0
Further, the de-  sign of a graphical user-interface to enter the  queries is planned, allowing to specify queries  by drawing PTs instead of typing in  the expressions in the query language.	PT	Parse Trees	0
Further, the de-  sign of a graphical user-interface to enter the  queries is planned, allowing to specify queries  by drawing PTs instead of typing in  the expressions in the query language.	PT	part	0
Further, the de-  sign of a graphical user-interface to enter the  queries is planned, allowing to specify queries  by drawing PTs instead of typing in  the expressions in the query language.	PT	Portugese	0
Further, the de-  sign of a graphical user-interface to enter the  queries is planned, allowing to specify queries  by drawing PTs instead of typing in  the expressions in the query language.	PT	Prompts	0
Further, the de-  sign of a graphical user-interface to enter the  queries is planned, allowing to specify queries  by drawing PTs instead of typing in  the expressions in the query language.	PT	Probable alignments	0
Further, the de-  sign of a graphical user-interface to enter the  queries is planned, allowing to specify queries  by drawing PTs instead of typing in  the expressions in the query language.	PT	Parse tree	0
Further, the de-  sign of a graphical user-interface to enter the  queries is planned, allowing to specify queries  by drawing PTs instead of typing in  the expressions in the query language.	PT	partial tree	1
Further, the de-  sign of a graphical user-interface to enter the  queries is planned, allowing to specify queries  by drawing PTs instead of typing in  the expressions in the query language.	PT	Phrase Table	0
Here in incremental dependency parsing we define the loss function between a gold tree y and an incorrect PT z as the number of incorrect edges in z, plus the number of correct edges in y which are already ruled out by z. This MIRA extension results in slightly higher accuracy of 92.36, which we will use as the pure online learn- ing baseline in the comparisons below.	PT	probability threshold	0
Here in incremental dependency parsing we define the loss function between a gold tree y and an incorrect PT z as the number of incorrect edges in z, plus the number of correct edges in y which are already ruled out by z. This MIRA extension results in slightly higher accuracy of 92.36, which we will use as the pure online learn- ing baseline in the comparisons below.	PT	parse tree	0
Here in incremental dependency parsing we define the loss function between a gold tree y and an incorrect PT z as the number of incorrect edges in z, plus the number of correct edges in y which are already ruled out by z. This MIRA extension results in slightly higher accuracy of 92.36, which we will use as the pure online learn- ing baseline in the comparisons below.	PT	phrase table	0
Here in incremental dependency parsing we define the loss function between a gold tree y and an incorrect PT z as the number of incorrect edges in z, plus the number of correct edges in y which are already ruled out by z. This MIRA extension results in slightly higher accuracy of 92.36, which we will use as the pure online learn- ing baseline in the comparisons below.	PT	Portuguese	0
Here in incremental dependency parsing we define the loss function between a gold tree y and an incorrect PT z as the number of incorrect edges in z, plus the number of correct edges in y which are already ruled out by z. This MIRA extension results in slightly higher accuracy of 92.36, which we will use as the pure online learn- ing baseline in the comparisons below.	PT	Parse Trees	0
Here in incremental dependency parsing we define the loss function between a gold tree y and an incorrect PT z as the number of incorrect edges in z, plus the number of correct edges in y which are already ruled out by z. This MIRA extension results in slightly higher accuracy of 92.36, which we will use as the pure online learn- ing baseline in the comparisons below.	PT	part	0
Here in incremental dependency parsing we define the loss function between a gold tree y and an incorrect PT z as the number of incorrect edges in z, plus the number of correct edges in y which are already ruled out by z. This MIRA extension results in slightly higher accuracy of 92.36, which we will use as the pure online learn- ing baseline in the comparisons below.	PT	Portugese	0
Here in incremental dependency parsing we define the loss function between a gold tree y and an incorrect PT z as the number of incorrect edges in z, plus the number of correct edges in y which are already ruled out by z. This MIRA extension results in slightly higher accuracy of 92.36, which we will use as the pure online learn- ing baseline in the comparisons below.	PT	Prompts	0
Here in incremental dependency parsing we define the loss function between a gold tree y and an incorrect PT z as the number of incorrect edges in z, plus the number of correct edges in y which are already ruled out by z. This MIRA extension results in slightly higher accuracy of 92.36, which we will use as the pure online learn- ing baseline in the comparisons below.	PT	Probable alignments	0
Here in incremental dependency parsing we define the loss function between a gold tree y and an incorrect PT z as the number of incorrect edges in z, plus the number of correct edges in y which are already ruled out by z. This MIRA extension results in slightly higher accuracy of 92.36, which we will use as the pure online learn- ing baseline in the comparisons below.	PT	Parse tree	0
Here in incremental dependency parsing we define the loss function between a gold tree y and an incorrect PT z as the number of incorrect edges in z, plus the number of correct edges in y which are already ruled out by z. This MIRA extension results in slightly higher accuracy of 92.36, which we will use as the pure online learn- ing baseline in the comparisons below.	PT	partial tree	1
Here in incremental dependency parsing we define the loss function between a gold tree y and an incorrect PT z as the number of incorrect edges in z, plus the number of correct edges in y which are already ruled out by z. This MIRA extension results in slightly higher accuracy of 92.36, which we will use as the pure online learn- ing baseline in the comparisons below.	PT	Phrase Table	0
Horeover~ a PT in the whole parse  tree plays a role of adjusting semantic and syntactic  interpretation.	PT	probability threshold	0
Horeover~ a PT in the whole parse  tree plays a role of adjusting semantic and syntactic  interpretation.	PT	parse tree	0
Horeover~ a PT in the whole parse  tree plays a role of adjusting semantic and syntactic  interpretation.	PT	phrase table	0
Horeover~ a PT in the whole parse  tree plays a role of adjusting semantic and syntactic  interpretation.	PT	Portuguese	0
Horeover~ a PT in the whole parse  tree plays a role of adjusting semantic and syntactic  interpretation.	PT	Parse Trees	0
Horeover~ a PT in the whole parse  tree plays a role of adjusting semantic and syntactic  interpretation.	PT	part	0
Horeover~ a PT in the whole parse  tree plays a role of adjusting semantic and syntactic  interpretation.	PT	Portugese	0
Horeover~ a PT in the whole parse  tree plays a role of adjusting semantic and syntactic  interpretation.	PT	Prompts	0
Horeover~ a PT in the whole parse  tree plays a role of adjusting semantic and syntactic  interpretation.	PT	Probable alignments	0
Horeover~ a PT in the whole parse  tree plays a role of adjusting semantic and syntactic  interpretation.	PT	Parse tree	0
Horeover~ a PT in the whole parse  tree plays a role of adjusting semantic and syntactic  interpretation.	PT	partial tree	1
Horeover~ a PT in the whole parse  tree plays a role of adjusting semantic and syntactic  interpretation.	PT	Phrase Table	0
On the basis of these derivation trees, we selected several features for training our disam- biguation models: local trees of depth 1, several lev- els of grandparenting, i.e. inclusion of grandpar- ent node (GP 2), great-grandparent node (GP 3) and great-great-grandparent node (GP 4), PTs of depth 1 (+AE).	PT	probability threshold	0
On the basis of these derivation trees, we selected several features for training our disam- biguation models: local trees of depth 1, several lev- els of grandparenting, i.e. inclusion of grandpar- ent node (GP 2), great-grandparent node (GP 3) and great-great-grandparent node (GP 4), PTs of depth 1 (+AE).	PT	parse tree	0
On the basis of these derivation trees, we selected several features for training our disam- biguation models: local trees of depth 1, several lev- els of grandparenting, i.e. inclusion of grandpar- ent node (GP 2), great-grandparent node (GP 3) and great-great-grandparent node (GP 4), PTs of depth 1 (+AE).	PT	phrase table	0
On the basis of these derivation trees, we selected several features for training our disam- biguation models: local trees of depth 1, several lev- els of grandparenting, i.e. inclusion of grandpar- ent node (GP 2), great-grandparent node (GP 3) and great-great-grandparent node (GP 4), PTs of depth 1 (+AE).	PT	Portuguese	0
On the basis of these derivation trees, we selected several features for training our disam- biguation models: local trees of depth 1, several lev- els of grandparenting, i.e. inclusion of grandpar- ent node (GP 2), great-grandparent node (GP 3) and great-great-grandparent node (GP 4), PTs of depth 1 (+AE).	PT	Parse Trees	0
On the basis of these derivation trees, we selected several features for training our disam- biguation models: local trees of depth 1, several lev- els of grandparenting, i.e. inclusion of grandpar- ent node (GP 2), great-grandparent node (GP 3) and great-great-grandparent node (GP 4), PTs of depth 1 (+AE).	PT	part	0
On the basis of these derivation trees, we selected several features for training our disam- biguation models: local trees of depth 1, several lev- els of grandparenting, i.e. inclusion of grandpar- ent node (GP 2), great-grandparent node (GP 3) and great-great-grandparent node (GP 4), PTs of depth 1 (+AE).	PT	Portugese	0
On the basis of these derivation trees, we selected several features for training our disam- biguation models: local trees of depth 1, several lev- els of grandparenting, i.e. inclusion of grandpar- ent node (GP 2), great-grandparent node (GP 3) and great-great-grandparent node (GP 4), PTs of depth 1 (+AE).	PT	Prompts	0
On the basis of these derivation trees, we selected several features for training our disam- biguation models: local trees of depth 1, several lev- els of grandparenting, i.e. inclusion of grandpar- ent node (GP 2), great-grandparent node (GP 3) and great-great-grandparent node (GP 4), PTs of depth 1 (+AE).	PT	Probable alignments	0
On the basis of these derivation trees, we selected several features for training our disam- biguation models: local trees of depth 1, several lev- els of grandparenting, i.e. inclusion of grandpar- ent node (GP 2), great-grandparent node (GP 3) and great-great-grandparent node (GP 4), PTs of depth 1 (+AE).	PT	Parse tree	0
On the basis of these derivation trees, we selected several features for training our disam- biguation models: local trees of depth 1, several lev- els of grandparenting, i.e. inclusion of grandpar- ent node (GP 2), great-grandparent node (GP 3) and great-great-grandparent node (GP 4), PTs of depth 1 (+AE).	PT	partial tree	1
On the basis of these derivation trees, we selected several features for training our disam- biguation models: local trees of depth 1, several lev- els of grandparenting, i.e. inclusion of grandpar- ent node (GP 2), great-grandparent node (GP 3) and great-great-grandparent node (GP 4), PTs of depth 1 (+AE).	PT	Phrase Table	0
The idea is that the supertags might give a more fine-grained definition of struc- ture, using PTs rather than parts of speech.	PT	probability threshold	0
The idea is that the supertags might give a more fine-grained definition of struc- ture, using PTs rather than parts of speech.	PT	parse tree	0
The idea is that the supertags might give a more fine-grained definition of struc- ture, using PTs rather than parts of speech.	PT	phrase table	0
The idea is that the supertags might give a more fine-grained definition of struc- ture, using PTs rather than parts of speech.	PT	Portuguese	0
The idea is that the supertags might give a more fine-grained definition of struc- ture, using PTs rather than parts of speech.	PT	Parse Trees	0
The idea is that the supertags might give a more fine-grained definition of struc- ture, using PTs rather than parts of speech.	PT	part	0
The idea is that the supertags might give a more fine-grained definition of struc- ture, using PTs rather than parts of speech.	PT	Portugese	0
The idea is that the supertags might give a more fine-grained definition of struc- ture, using PTs rather than parts of speech.	PT	Prompts	0
The idea is that the supertags might give a more fine-grained definition of struc- ture, using PTs rather than parts of speech.	PT	Probable alignments	0
The idea is that the supertags might give a more fine-grained definition of struc- ture, using PTs rather than parts of speech.	PT	Parse tree	0
The idea is that the supertags might give a more fine-grained definition of struc- ture, using PTs rather than parts of speech.	PT	partial tree	1
The idea is that the supertags might give a more fine-grained definition of struc- ture, using PTs rather than parts of speech.	PT	Phrase Table	0
Bottom-up generation locally applies the translation model to a PT.	PT	probability threshold	0
Bottom-up generation locally applies the translation model to a PT.	PT	parse tree	0
Bottom-up generation locally applies the translation model to a PT.	PT	phrase table	0
Bottom-up generation locally applies the translation model to a PT.	PT	Portuguese	0
Bottom-up generation locally applies the translation model to a PT.	PT	Parse Trees	0
Bottom-up generation locally applies the translation model to a PT.	PT	part	0
Bottom-up generation locally applies the translation model to a PT.	PT	Portugese	0
Bottom-up generation locally applies the translation model to a PT.	PT	Prompts	0
Bottom-up generation locally applies the translation model to a PT.	PT	Probable alignments	0
Bottom-up generation locally applies the translation model to a PT.	PT	Parse tree	0
Bottom-up generation locally applies the translation model to a PT.	PT	partial tree	1
Bottom-up generation locally applies the translation model to a PT.	PT	Phrase Table	0
using Europarl LM & PT (max phrase length 7) ?	PT	probability threshold	0
using Europarl LM & PT (max phrase length 7) ?	PT	parse tree	0
using Europarl LM & PT (max phrase length 7) ?	PT	phrase table	0
using Europarl LM & PT (max phrase length 7) ?	PT	Portuguese	0
using Europarl LM & PT (max phrase length 7) ?	PT	Parse Trees	0
using Europarl LM & PT (max phrase length 7) ?	PT	part	0
using Europarl LM & PT (max phrase length 7) ?	PT	Portugese	0
using Europarl LM & PT (max phrase length 7) ?	PT	Prompts	0
using Europarl LM & PT (max phrase length 7) ?	PT	Probable alignments	0
using Europarl LM & PT (max phrase length 7) ?	PT	Parse tree	0
using Europarl LM & PT (max phrase length 7) ?	PT	partial tree	0
using Europarl LM & PT (max phrase length 7) ?	PT	Phrase Table	1
using Europarl LM & PT (max phrase length 10) ?	PT	probability threshold	0
using Europarl LM & PT (max phrase length 10) ?	PT	parse tree	0
using Europarl LM & PT (max phrase length 10) ?	PT	phrase table	0
using Europarl LM & PT (max phrase length 10) ?	PT	Portuguese	0
using Europarl LM & PT (max phrase length 10) ?	PT	Parse Trees	0
using Europarl LM & PT (max phrase length 10) ?	PT	part	0
using Europarl LM & PT (max phrase length 10) ?	PT	Portugese	0
using Europarl LM & PT (max phrase length 10) ?	PT	Prompts	0
using Europarl LM & PT (max phrase length 10) ?	PT	Probable alignments	0
using Europarl LM & PT (max phrase length 10) ?	PT	Parse tree	0
using Europarl LM & PT (max phrase length 10) ?	PT	partial tree	0
using Europarl LM & PT (max phrase length 10) ?	PT	Phrase Table	1
2 PT Pruning Phrase table pruning algorithms are important in translation, since they efficiently reduce the size of the translation model, without having a large nega- tive impact in the translation quality.	PT	probability threshold	0
2 PT Pruning Phrase table pruning algorithms are important in translation, since they efficiently reduce the size of the translation model, without having a large nega- tive impact in the translation quality.	PT	parse tree	0
2 PT Pruning Phrase table pruning algorithms are important in translation, since they efficiently reduce the size of the translation model, without having a large nega- tive impact in the translation quality.	PT	phrase table	0
2 PT Pruning Phrase table pruning algorithms are important in translation, since they efficiently reduce the size of the translation model, without having a large nega- tive impact in the translation quality.	PT	Portuguese	0
2 PT Pruning Phrase table pruning algorithms are important in translation, since they efficiently reduce the size of the translation model, without having a large nega- tive impact in the translation quality.	PT	Parse Trees	0
2 PT Pruning Phrase table pruning algorithms are important in translation, since they efficiently reduce the size of the translation model, without having a large nega- tive impact in the translation quality.	PT	part	0
2 PT Pruning Phrase table pruning algorithms are important in translation, since they efficiently reduce the size of the translation model, without having a large nega- tive impact in the translation quality.	PT	Portugese	0
2 PT Pruning Phrase table pruning algorithms are important in translation, since they efficiently reduce the size of the translation model, without having a large nega- tive impact in the translation quality.	PT	Prompts	0
2 PT Pruning Phrase table pruning algorithms are important in translation, since they efficiently reduce the size of the translation model, without having a large nega- tive impact in the translation quality.	PT	Probable alignments	0
2 PT Pruning Phrase table pruning algorithms are important in translation, since they efficiently reduce the size of the translation model, without having a large nega- tive impact in the translation quality.	PT	Parse tree	0
2 PT Pruning Phrase table pruning algorithms are important in translation, since they efficiently reduce the size of the translation model, without having a large nega- tive impact in the translation quality.	PT	partial tree	0
2 PT Pruning Phrase table pruning algorithms are important in translation, since they efficiently reduce the size of the translation model, without having a large nega- tive impact in the translation quality.	PT	Phrase Table	1
A Systematic Comparison of PT Pruning Techniques.	PT	probability threshold	0
A Systematic Comparison of PT Pruning Techniques.	PT	parse tree	0
A Systematic Comparison of PT Pruning Techniques.	PT	phrase table	0
A Systematic Comparison of PT Pruning Techniques.	PT	Portuguese	0
A Systematic Comparison of PT Pruning Techniques.	PT	Parse Trees	0
A Systematic Comparison of PT Pruning Techniques.	PT	part	0
A Systematic Comparison of PT Pruning Techniques.	PT	Portugese	0
A Systematic Comparison of PT Pruning Techniques.	PT	Prompts	0
A Systematic Comparison of PT Pruning Techniques.	PT	Probable alignments	0
A Systematic Comparison of PT Pruning Techniques.	PT	Parse tree	0
A Systematic Comparison of PT Pruning Techniques.	PT	partial tree	0
A Systematic Comparison of PT Pruning Techniques.	PT	Phrase Table	1
3.3 Additional PT of bilingual MWEs Wu et al (2008) proposed a method to construct a phrase table by a manually-made translation dic- tionary.	PT	probability threshold	0
3.3 Additional PT of bilingual MWEs Wu et al (2008) proposed a method to construct a phrase table by a manually-made translation dic- tionary.	PT	parse tree	0
3.3 Additional PT of bilingual MWEs Wu et al (2008) proposed a method to construct a phrase table by a manually-made translation dic- tionary.	PT	phrase table	0
3.3 Additional PT of bilingual MWEs Wu et al (2008) proposed a method to construct a phrase table by a manually-made translation dic- tionary.	PT	Portuguese	0
3.3 Additional PT of bilingual MWEs Wu et al (2008) proposed a method to construct a phrase table by a manually-made translation dic- tionary.	PT	Parse Trees	0
3.3 Additional PT of bilingual MWEs Wu et al (2008) proposed a method to construct a phrase table by a manually-made translation dic- tionary.	PT	part	0
3.3 Additional PT of bilingual MWEs Wu et al (2008) proposed a method to construct a phrase table by a manually-made translation dic- tionary.	PT	Portugese	0
3.3 Additional PT of bilingual MWEs Wu et al (2008) proposed a method to construct a phrase table by a manually-made translation dic- tionary.	PT	Prompts	0
3.3 Additional PT of bilingual MWEs Wu et al (2008) proposed a method to construct a phrase table by a manually-made translation dic- tionary.	PT	Probable alignments	0
3.3 Additional PT of bilingual MWEs Wu et al (2008) proposed a method to construct a phrase table by a manually-made translation dic- tionary.	PT	Parse tree	0
3.3 Additional PT of bilingual MWEs Wu et al (2008) proposed a method to construct a phrase table by a manually-made translation dic- tionary.	PT	partial tree	0
3.3 Additional PT of bilingual MWEs Wu et al (2008) proposed a method to construct a phrase table by a manually-made translation dic- tionary.	PT	Phrase Table	1
or, How to  Extract Paraphrases from PTs   1Roland Kuhn, 1Boxing Chen, 1George Foster and  2Evan Stratford  1National Research Council of Canada  2University of Waterloo  1First.Last@nrc.gc.ca; 2evan.stratford@gmail.com    Abstract  This paper describes how to cluster to- gether the phrases of a phrase-based sta- tistical machine translation (SMT) sys- tem, using information in the phrase table  itself.	PT	probability threshold	0
or, How to  Extract Paraphrases from PTs   1Roland Kuhn, 1Boxing Chen, 1George Foster and  2Evan Stratford  1National Research Council of Canada  2University of Waterloo  1First.Last@nrc.gc.ca; 2evan.stratford@gmail.com    Abstract  This paper describes how to cluster to- gether the phrases of a phrase-based sta- tistical machine translation (SMT) sys- tem, using information in the phrase table  itself.	PT	parse tree	0
or, How to  Extract Paraphrases from PTs   1Roland Kuhn, 1Boxing Chen, 1George Foster and  2Evan Stratford  1National Research Council of Canada  2University of Waterloo  1First.Last@nrc.gc.ca; 2evan.stratford@gmail.com    Abstract  This paper describes how to cluster to- gether the phrases of a phrase-based sta- tistical machine translation (SMT) sys- tem, using information in the phrase table  itself.	PT	phrase table	0
or, How to  Extract Paraphrases from PTs   1Roland Kuhn, 1Boxing Chen, 1George Foster and  2Evan Stratford  1National Research Council of Canada  2University of Waterloo  1First.Last@nrc.gc.ca; 2evan.stratford@gmail.com    Abstract  This paper describes how to cluster to- gether the phrases of a phrase-based sta- tistical machine translation (SMT) sys- tem, using information in the phrase table  itself.	PT	Portuguese	0
or, How to  Extract Paraphrases from PTs   1Roland Kuhn, 1Boxing Chen, 1George Foster and  2Evan Stratford  1National Research Council of Canada  2University of Waterloo  1First.Last@nrc.gc.ca; 2evan.stratford@gmail.com    Abstract  This paper describes how to cluster to- gether the phrases of a phrase-based sta- tistical machine translation (SMT) sys- tem, using information in the phrase table  itself.	PT	Parse Trees	0
or, How to  Extract Paraphrases from PTs   1Roland Kuhn, 1Boxing Chen, 1George Foster and  2Evan Stratford  1National Research Council of Canada  2University of Waterloo  1First.Last@nrc.gc.ca; 2evan.stratford@gmail.com    Abstract  This paper describes how to cluster to- gether the phrases of a phrase-based sta- tistical machine translation (SMT) sys- tem, using information in the phrase table  itself.	PT	part	0
or, How to  Extract Paraphrases from PTs   1Roland Kuhn, 1Boxing Chen, 1George Foster and  2Evan Stratford  1National Research Council of Canada  2University of Waterloo  1First.Last@nrc.gc.ca; 2evan.stratford@gmail.com    Abstract  This paper describes how to cluster to- gether the phrases of a phrase-based sta- tistical machine translation (SMT) sys- tem, using information in the phrase table  itself.	PT	Portugese	0
or, How to  Extract Paraphrases from PTs   1Roland Kuhn, 1Boxing Chen, 1George Foster and  2Evan Stratford  1National Research Council of Canada  2University of Waterloo  1First.Last@nrc.gc.ca; 2evan.stratford@gmail.com    Abstract  This paper describes how to cluster to- gether the phrases of a phrase-based sta- tistical machine translation (SMT) sys- tem, using information in the phrase table  itself.	PT	Prompts	0
or, How to  Extract Paraphrases from PTs   1Roland Kuhn, 1Boxing Chen, 1George Foster and  2Evan Stratford  1National Research Council of Canada  2University of Waterloo  1First.Last@nrc.gc.ca; 2evan.stratford@gmail.com    Abstract  This paper describes how to cluster to- gether the phrases of a phrase-based sta- tistical machine translation (SMT) sys- tem, using information in the phrase table  itself.	PT	Probable alignments	0
or, How to  Extract Paraphrases from PTs   1Roland Kuhn, 1Boxing Chen, 1George Foster and  2Evan Stratford  1National Research Council of Canada  2University of Waterloo  1First.Last@nrc.gc.ca; 2evan.stratford@gmail.com    Abstract  This paper describes how to cluster to- gether the phrases of a phrase-based sta- tistical machine translation (SMT) sys- tem, using information in the phrase table  itself.	PT	Parse tree	0
or, How to  Extract Paraphrases from PTs   1Roland Kuhn, 1Boxing Chen, 1George Foster and  2Evan Stratford  1National Research Council of Canada  2University of Waterloo  1First.Last@nrc.gc.ca; 2evan.stratford@gmail.com    Abstract  This paper describes how to cluster to- gether the phrases of a phrase-based sta- tistical machine translation (SMT) sys- tem, using information in the phrase table  itself.	PT	partial tree	0
or, How to  Extract Paraphrases from PTs   1Roland Kuhn, 1Boxing Chen, 1George Foster and  2Evan Stratford  1National Research Council of Canada  2University of Waterloo  1First.Last@nrc.gc.ca; 2evan.stratford@gmail.com    Abstract  This paper describes how to cluster to- gether the phrases of a phrase-based sta- tistical machine translation (SMT) sys- tem, using information in the phrase table  itself.	PT	Phrase Table	1
As future work we aim at allowing for the comparison at fragment level, where a fragment is considered a part of a function, a GR of functions.	GR	group	1
As future work we aim at allowing for the comparison at fragment level, where a fragment is considered a part of a function, a GR of functions.	GR	or	0
As future work we aim at allowing for the comparison at fragment level, where a fragment is considered a part of a function, a GR of functions.	GR	Grammatical Relation	0
As future work we aim at allowing for the comparison at fragment level, where a fragment is considered a part of a function, a GR of functions.	GR	grammatical relations	0
As future work we aim at allowing for the comparison at fragment level, where a fragment is considered a part of a function, a GR of functions.	GR	Grammatical relation	0
As future work we aim at allowing for the comparison at fragment level, where a fragment is considered a part of a function, a GR of functions.	GR	gri|SCF	0
As future work we aim at allowing for the comparison at fragment level, where a fragment is considered a part of a function, a GR of functions.	GR	Greetin	0
As future work we aim at allowing for the comparison at fragment level, where a fragment is considered a part of a function, a GR of functions.	GR	glue rule	0
9 Acknowledgements   The authors would like to thank Mark Johnson  and other members of the Brown NLP GR  for many useful ideas and NSF and ONR for  support (NSF grants IRI-9319516 and SBR-  9720368, ONR grant N0014-96-1-0549).	GR	group	1
9 Acknowledgements   The authors would like to thank Mark Johnson  and other members of the Brown NLP GR  for many useful ideas and NSF and ONR for  support (NSF grants IRI-9319516 and SBR-  9720368, ONR grant N0014-96-1-0549).	GR	or	0
9 Acknowledgements   The authors would like to thank Mark Johnson  and other members of the Brown NLP GR  for many useful ideas and NSF and ONR for  support (NSF grants IRI-9319516 and SBR-  9720368, ONR grant N0014-96-1-0549).	GR	Grammatical Relation	0
9 Acknowledgements   The authors would like to thank Mark Johnson  and other members of the Brown NLP GR  for many useful ideas and NSF and ONR for  support (NSF grants IRI-9319516 and SBR-  9720368, ONR grant N0014-96-1-0549).	GR	grammatical relations	0
9 Acknowledgements   The authors would like to thank Mark Johnson  and other members of the Brown NLP GR  for many useful ideas and NSF and ONR for  support (NSF grants IRI-9319516 and SBR-  9720368, ONR grant N0014-96-1-0549).	GR	Grammatical relation	0
9 Acknowledgements   The authors would like to thank Mark Johnson  and other members of the Brown NLP GR  for many useful ideas and NSF and ONR for  support (NSF grants IRI-9319516 and SBR-  9720368, ONR grant N0014-96-1-0549).	GR	gri|SCF	0
9 Acknowledgements   The authors would like to thank Mark Johnson  and other members of the Brown NLP GR  for many useful ideas and NSF and ONR for  support (NSF grants IRI-9319516 and SBR-  9720368, ONR grant N0014-96-1-0549).	GR	Greetin	0
9 Acknowledgements   The authors would like to thank Mark Johnson  and other members of the Brown NLP GR  for many useful ideas and NSF and ONR for  support (NSF grants IRI-9319516 and SBR-  9720368, ONR grant N0014-96-1-0549).	GR	glue rule	0
In clustering, objects are GRed together according to their feature value distribution, not to a predefined classification (as is the case when using supervised techniques), so that we achieve a better guarantee that we are learning a structure already present in the data.	GR	group	1
In clustering, objects are GRed together according to their feature value distribution, not to a predefined classification (as is the case when using supervised techniques), so that we achieve a better guarantee that we are learning a structure already present in the data.	GR	or	0
In clustering, objects are GRed together according to their feature value distribution, not to a predefined classification (as is the case when using supervised techniques), so that we achieve a better guarantee that we are learning a structure already present in the data.	GR	Grammatical Relation	0
In clustering, objects are GRed together according to their feature value distribution, not to a predefined classification (as is the case when using supervised techniques), so that we achieve a better guarantee that we are learning a structure already present in the data.	GR	grammatical relations	0
In clustering, objects are GRed together according to their feature value distribution, not to a predefined classification (as is the case when using supervised techniques), so that we achieve a better guarantee that we are learning a structure already present in the data.	GR	Grammatical relation	0
In clustering, objects are GRed together according to their feature value distribution, not to a predefined classification (as is the case when using supervised techniques), so that we achieve a better guarantee that we are learning a structure already present in the data.	GR	gri|SCF	0
In clustering, objects are GRed together according to their feature value distribution, not to a predefined classification (as is the case when using supervised techniques), so that we achieve a better guarantee that we are learning a structure already present in the data.	GR	Greetin	0
In clustering, objects are GRed together according to their feature value distribution, not to a predefined classification (as is the case when using supervised techniques), so that we achieve a better guarantee that we are learning a structure already present in the data.	GR	glue rule	0
3.1 The  gender /an imat ic i ty  stat ist ics   After we have identified the correct antecedents  it is a simple counting procedure to compute  P(p\[wa) where wa is in the correct antecedent  for the pronoun p (Note the pronouns are  GRed by their gender):  \[ wain the antecedent for p \[  P(pl o) =  When there are multiple relevant words in the  antecedent we apply the likelihood test designed  by Dunning (1993) on all the words in the candi-  date NP.	GR	group	1
3.1 The  gender /an imat ic i ty  stat ist ics   After we have identified the correct antecedents  it is a simple counting procedure to compute  P(p\[wa) where wa is in the correct antecedent  for the pronoun p (Note the pronouns are  GRed by their gender):  \[ wain the antecedent for p \[  P(pl o) =  When there are multiple relevant words in the  antecedent we apply the likelihood test designed  by Dunning (1993) on all the words in the candi-  date NP.	GR	or	0
3.1 The  gender /an imat ic i ty  stat ist ics   After we have identified the correct antecedents  it is a simple counting procedure to compute  P(p\[wa) where wa is in the correct antecedent  for the pronoun p (Note the pronouns are  GRed by their gender):  \[ wain the antecedent for p \[  P(pl o) =  When there are multiple relevant words in the  antecedent we apply the likelihood test designed  by Dunning (1993) on all the words in the candi-  date NP.	GR	Grammatical Relation	0
3.1 The  gender /an imat ic i ty  stat ist ics   After we have identified the correct antecedents  it is a simple counting procedure to compute  P(p\[wa) where wa is in the correct antecedent  for the pronoun p (Note the pronouns are  GRed by their gender):  \[ wain the antecedent for p \[  P(pl o) =  When there are multiple relevant words in the  antecedent we apply the likelihood test designed  by Dunning (1993) on all the words in the candi-  date NP.	GR	grammatical relations	0
3.1 The  gender /an imat ic i ty  stat ist ics   After we have identified the correct antecedents  it is a simple counting procedure to compute  P(p\[wa) where wa is in the correct antecedent  for the pronoun p (Note the pronouns are  GRed by their gender):  \[ wain the antecedent for p \[  P(pl o) =  When there are multiple relevant words in the  antecedent we apply the likelihood test designed  by Dunning (1993) on all the words in the candi-  date NP.	GR	Grammatical relation	0
3.1 The  gender /an imat ic i ty  stat ist ics   After we have identified the correct antecedents  it is a simple counting procedure to compute  P(p\[wa) where wa is in the correct antecedent  for the pronoun p (Note the pronouns are  GRed by their gender):  \[ wain the antecedent for p \[  P(pl o) =  When there are multiple relevant words in the  antecedent we apply the likelihood test designed  by Dunning (1993) on all the words in the candi-  date NP.	GR	gri|SCF	0
3.1 The  gender /an imat ic i ty  stat ist ics   After we have identified the correct antecedents  it is a simple counting procedure to compute  P(p\[wa) where wa is in the correct antecedent  for the pronoun p (Note the pronouns are  GRed by their gender):  \[ wain the antecedent for p \[  P(pl o) =  When there are multiple relevant words in the  antecedent we apply the likelihood test designed  by Dunning (1993) on all the words in the candi-  date NP.	GR	Greetin	0
3.1 The  gender /an imat ic i ty  stat ist ics   After we have identified the correct antecedents  it is a simple counting procedure to compute  P(p\[wa) where wa is in the correct antecedent  for the pronoun p (Note the pronouns are  GRed by their gender):  \[ wain the antecedent for p \[  P(pl o) =  When there are multiple relevant words in the  antecedent we apply the likelihood test designed  by Dunning (1993) on all the words in the candi-  date NP.	GR	glue rule	0
To make sure that the resulting system is practically useful, several user GRs have been identified, who drive the interface development process by providing practical use cases.	GR	group	1
To make sure that the resulting system is practically useful, several user GRs have been identified, who drive the interface development process by providing practical use cases.	GR	or	0
To make sure that the resulting system is practically useful, several user GRs have been identified, who drive the interface development process by providing practical use cases.	GR	Grammatical Relation	0
To make sure that the resulting system is practically useful, several user GRs have been identified, who drive the interface development process by providing practical use cases.	GR	grammatical relations	0
To make sure that the resulting system is practically useful, several user GRs have been identified, who drive the interface development process by providing practical use cases.	GR	Grammatical relation	0
To make sure that the resulting system is practically useful, several user GRs have been identified, who drive the interface development process by providing practical use cases.	GR	gri|SCF	0
To make sure that the resulting system is practically useful, several user GRs have been identified, who drive the interface development process by providing practical use cases.	GR	Greetin	0
To make sure that the resulting system is practically useful, several user GRs have been identified, who drive the interface development process by providing practical use cases.	GR	glue rule	0
Instead of computing the proba-  bUity for each one of them we GR them into  "buckets", so that rrt a iS the bucket for the num-  ber of times that a is mentioned.	GR	group	1
Instead of computing the proba-  bUity for each one of them we GR them into  "buckets", so that rrt a iS the bucket for the num-  ber of times that a is mentioned.	GR	or	0
Instead of computing the proba-  bUity for each one of them we GR them into  "buckets", so that rrt a iS the bucket for the num-  ber of times that a is mentioned.	GR	Grammatical Relation	0
Instead of computing the proba-  bUity for each one of them we GR them into  "buckets", so that rrt a iS the bucket for the num-  ber of times that a is mentioned.	GR	grammatical relations	0
Instead of computing the proba-  bUity for each one of them we GR them into  "buckets", so that rrt a iS the bucket for the num-  ber of times that a is mentioned.	GR	Grammatical relation	0
Instead of computing the proba-  bUity for each one of them we GR them into  "buckets", so that rrt a iS the bucket for the num-  ber of times that a is mentioned.	GR	gri|SCF	0
Instead of computing the proba-  bUity for each one of them we GR them into  "buckets", so that rrt a iS the bucket for the num-  ber of times that a is mentioned.	GR	Greetin	0
Instead of computing the proba-  bUity for each one of them we GR them into  "buckets", so that rrt a iS the bucket for the num-  ber of times that a is mentioned.	GR	glue rule	0
A Statistical Approach to AnaphGRa Resolution  Niyu  Ge, John  Hale and Eugene Charn iak   Dept.	GR	group	0
A Statistical Approach to AnaphGRa Resolution  Niyu  Ge, John  Hale and Eugene Charn iak   Dept.	GR	or	1
A Statistical Approach to AnaphGRa Resolution  Niyu  Ge, John  Hale and Eugene Charn iak   Dept.	GR	Grammatical Relation	0
A Statistical Approach to AnaphGRa Resolution  Niyu  Ge, John  Hale and Eugene Charn iak   Dept.	GR	grammatical relations	0
A Statistical Approach to AnaphGRa Resolution  Niyu  Ge, John  Hale and Eugene Charn iak   Dept.	GR	Grammatical relation	0
A Statistical Approach to AnaphGRa Resolution  Niyu  Ge, John  Hale and Eugene Charn iak   Dept.	GR	gri|SCF	0
A Statistical Approach to AnaphGRa Resolution  Niyu  Ge, John  Hale and Eugene Charn iak   Dept.	GR	Greetin	0
A Statistical Approach to AnaphGRa Resolution  Niyu  Ge, John  Hale and Eugene Charn iak   Dept.	GR	glue rule	0
brown, edu  Abst ract   This paper presents an algGRithm fGR identi-  fying pronominal anaphGRa and two experi-  ments based upon this algGRithm.	GR	group	0
brown, edu  Abst ract   This paper presents an algGRithm fGR identi-  fying pronominal anaphGRa and two experi-  ments based upon this algGRithm.	GR	or	1
brown, edu  Abst ract   This paper presents an algGRithm fGR identi-  fying pronominal anaphGRa and two experi-  ments based upon this algGRithm.	GR	Grammatical Relation	0
brown, edu  Abst ract   This paper presents an algGRithm fGR identi-  fying pronominal anaphGRa and two experi-  ments based upon this algGRithm.	GR	grammatical relations	0
brown, edu  Abst ract   This paper presents an algGRithm fGR identi-  fying pronominal anaphGRa and two experi-  ments based upon this algGRithm.	GR	Grammatical relation	0
brown, edu  Abst ract   This paper presents an algGRithm fGR identi-  fying pronominal anaphGRa and two experi-  ments based upon this algGRithm.	GR	gri|SCF	0
brown, edu  Abst ract   This paper presents an algGRithm fGR identi-  fying pronominal anaphGRa and two experi-  ments based upon this algGRithm.	GR	Greetin	0
brown, edu  Abst ract   This paper presents an algGRithm fGR identi-  fying pronominal anaphGRa and two experi-  ments based upon this algGRithm.	GR	glue rule	0
We incGRpo-  rate multiple anaphGRa resolution factGRs into  a statistical framewGRk - -  specifically the dis-  tance between the pronoun and the proposed  antecedent, gender/number/animaticity of the  proposed antecedent, governing head infGRma-  tion and noun phrase repetition.	GR	group	0
We incGRpo-  rate multiple anaphGRa resolution factGRs into  a statistical framewGRk - -  specifically the dis-  tance between the pronoun and the proposed  antecedent, gender/number/animaticity of the  proposed antecedent, governing head infGRma-  tion and noun phrase repetition.	GR	or	1
We incGRpo-  rate multiple anaphGRa resolution factGRs into  a statistical framewGRk - -  specifically the dis-  tance between the pronoun and the proposed  antecedent, gender/number/animaticity of the  proposed antecedent, governing head infGRma-  tion and noun phrase repetition.	GR	Grammatical Relation	0
We incGRpo-  rate multiple anaphGRa resolution factGRs into  a statistical framewGRk - -  specifically the dis-  tance between the pronoun and the proposed  antecedent, gender/number/animaticity of the  proposed antecedent, governing head infGRma-  tion and noun phrase repetition.	GR	grammatical relations	0
We incGRpo-  rate multiple anaphGRa resolution factGRs into  a statistical framewGRk - -  specifically the dis-  tance between the pronoun and the proposed  antecedent, gender/number/animaticity of the  proposed antecedent, governing head infGRma-  tion and noun phrase repetition.	GR	Grammatical relation	0
We incGRpo-  rate multiple anaphGRa resolution factGRs into  a statistical framewGRk - -  specifically the dis-  tance between the pronoun and the proposed  antecedent, gender/number/animaticity of the  proposed antecedent, governing head infGRma-  tion and noun phrase repetition.	GR	gri|SCF	0
We incGRpo-  rate multiple anaphGRa resolution factGRs into  a statistical framewGRk - -  specifically the dis-  tance between the pronoun and the proposed  antecedent, gender/number/animaticity of the  proposed antecedent, governing head infGRma-  tion and noun phrase repetition.	GR	Greetin	0
We incGRpo-  rate multiple anaphGRa resolution factGRs into  a statistical framewGRk - -  specifically the dis-  tance between the pronoun and the proposed  antecedent, gender/number/animaticity of the  proposed antecedent, governing head infGRma-  tion and noun phrase repetition.	GR	glue rule	0
We incGRpo-  rate multiple anaphGRa resolution factGRs into  a statistical framewGRk - -  specifically the dis-  tance be	GR	group	0
We incGRpo-  rate multiple anaphGRa resolution factGRs into  a statistical framewGRk - -  specifically the dis-  tance be	GR	or	1
We incGRpo-  rate multiple anaphGRa resolution factGRs into  a statistical framewGRk - -  specifically the dis-  tance be	GR	Grammatical Relation	0
We incGRpo-  rate multiple anaphGRa resolution factGRs into  a statistical framewGRk - -  specifically the dis-  tance be	GR	grammatical relations	0
We incGRpo-  rate multiple anaphGRa resolution factGRs into  a statistical framewGRk - -  specifically the dis-  tance be	GR	Grammatical relation	0
We incGRpo-  rate multiple anaphGRa resolution factGRs into  a statistical framewGRk - -  specifically the dis-  tance be	GR	gri|SCF	0
We incGRpo-  rate multiple anaphGRa resolution factGRs into  a statistical framewGRk - -  specifically the dis-  tance be	GR	Greetin	0
We incGRpo-  rate multiple anaphGRa resolution factGRs into  a statistical framewGRk - -  specifically the dis-  tance be	GR	glue rule	0
In the Mental Representation of GRs (Joan Bresnan ed.),	GR	group	0
In the Mental Representation of GRs (Joan Bresnan ed.),	GR	or	0
In the Mental Representation of GRs (Joan Bresnan ed.),	GR	Grammatical Relation	1
In the Mental Representation of GRs (Joan Bresnan ed.),	GR	grammatical relations	0
In the Mental Representation of GRs (Joan Bresnan ed.),	GR	Grammatical relation	0
In the Mental Representation of GRs (Joan Bresnan ed.),	GR	gri|SCF	0
In the Mental Representation of GRs (Joan Bresnan ed.),	GR	Greetin	0
In the Mental Representation of GRs (Joan Bresnan ed.),	GR	glue rule	0
The Mental Representation of GRs, The MIT Press, Cambridge, Mass. J. Maxwell and R. Kaplan, 1991. "	GR	group	0
The Mental Representation of GRs, The MIT Press, Cambridge, Mass. J. Maxwell and R. Kaplan, 1991. "	GR	or	0
The Mental Representation of GRs, The MIT Press, Cambridge, Mass. J. Maxwell and R. Kaplan, 1991. "	GR	Grammatical Relation	1
The Mental Representation of GRs, The MIT Press, Cambridge, Mass. J. Maxwell and R. Kaplan, 1991. "	GR	grammatical relations	0
The Mental Representation of GRs, The MIT Press, Cambridge, Mass. J. Maxwell and R. Kaplan, 1991. "	GR	Grammatical relation	0
The Mental Representation of GRs, The MIT Press, Cambridge, Mass. J. Maxwell and R. Kaplan, 1991. "	GR	gri|SCF	0
The Mental Representation of GRs, The MIT Press, Cambridge, Mass. J. Maxwell and R. Kaplan, 1991. "	GR	Greetin	0
The Mental Representation of GRs, The MIT Press, Cambridge, Mass. J. Maxwell and R. Kaplan, 1991. "	GR	glue rule	0
In Joan Bresnan, editor, The Mental Repre- sentation of GRs, pages 173?281.	GR	group	0
In Joan Bresnan, editor, The Mental Repre- sentation of GRs, pages 173?281.	GR	or	0
In Joan Bresnan, editor, The Mental Repre- sentation of GRs, pages 173?281.	GR	Grammatical Relation	1
In Joan Bresnan, editor, The Mental Repre- sentation of GRs, pages 173?281.	GR	grammatical relations	0
In Joan Bresnan, editor, The Mental Repre- sentation of GRs, pages 173?281.	GR	Grammatical relation	0
In Joan Bresnan, editor, The Mental Repre- sentation of GRs, pages 173?281.	GR	gri|SCF	0
In Joan Bresnan, editor, The Mental Repre- sentation of GRs, pages 173?281.	GR	Greetin	0
In Joan Bresnan, editor, The Mental Repre- sentation of GRs, pages 173?281.	GR	glue rule	0
The Mental  Computational Linguistics, Volume 15, Number 1, March 1989  Representation fGRs.	GR	group	0
The Mental  Computational Linguistics, Volume 15, Number 1, March 1989  Representation fGRs.	GR	or	0
The Mental  Computational Linguistics, Volume 15, Number 1, March 1989  Representation fGRs.	GR	Grammatical Relation	1
The Mental  Computational Linguistics, Volume 15, Number 1, March 1989  Representation fGRs.	GR	grammatical relations	0
The Mental  Computational Linguistics, Volume 15, Number 1, March 1989  Representation fGRs.	GR	Grammatical relation	0
The Mental  Computational Linguistics, Volume 15, Number 1, March 1989  Representation fGRs.	GR	gri|SCF	0
The Mental  Computational Linguistics, Volume 15, Number 1, March 1989  Representation fGRs.	GR	Greetin	0
The Mental  Computational Linguistics, Volume 15, Number 1, March 1989  Representation fGRs.	GR	glue rule	0
In Joan Bresnan, editor, The Mental Representation of GRs, pages 173?281.	GR	group	0
In Joan Bresnan, editor, The Mental Representation of GRs, pages 173?281.	GR	or	0
In Joan Bresnan, editor, The Mental Representation of GRs, pages 173?281.	GR	Grammatical Relation	1
In Joan Bresnan, editor, The Mental Representation of GRs, pages 173?281.	GR	grammatical relations	0
In Joan Bresnan, editor, The Mental Representation of GRs, pages 173?281.	GR	Grammatical relation	0
In Joan Bresnan, editor, The Mental Representation of GRs, pages 173?281.	GR	gri|SCF	0
In Joan Bresnan, editor, The Mental Representation of GRs, pages 173?281.	GR	Greetin	0
In Joan Bresnan, editor, The Mental Representation of GRs, pages 173?281.	GR	glue rule	0
The Mental Representation fGRs.	GR	group	0
The Mental Representation fGRs.	GR	or	0
The Mental Representation fGRs.	GR	Grammatical Relation	1
The Mental Representation fGRs.	GR	grammatical relations	0
The Mental Representation fGRs.	GR	Grammatical relation	0
The Mental Representation fGRs.	GR	gri|SCF	0
The Mental Representation fGRs.	GR	Greetin	0
The Mental Representation fGRs.	GR	glue rule	0
Booch G. 1994, Analyse et conception orientees  objets , Addison-Wesley, Reading Mass.  Bresnan Joan and Ronald Kaplan 1981, Lexical  functional grammars ; a formal system for gram-  105  matical representation, The mental representation  of GR, MIT Press, Cambridge,  Mass.  Delmonte R. 1990, Semantic Parsing with LFG and  Conceptual Representations, Computers and the  Humanities, Kluwer Academic Publishers, 24 , p.  461-488.	GR	group	0
Booch G. 1994, Analyse et conception orientees  objets , Addison-Wesley, Reading Mass.  Bresnan Joan and Ronald Kaplan 1981, Lexical  functional grammars ; a formal system for gram-  105  matical representation, The mental representation  of GR, MIT Press, Cambridge,  Mass.  Delmonte R. 1990, Semantic Parsing with LFG and  Conceptual Representations, Computers and the  Humanities, Kluwer Academic Publishers, 24 , p.  461-488.	GR	or	0
Booch G. 1994, Analyse et conception orientees  objets , Addison-Wesley, Reading Mass.  Bresnan Joan and Ronald Kaplan 1981, Lexical  functional grammars ; a formal system for gram-  105  matical representation, The mental representation  of GR, MIT Press, Cambridge,  Mass.  Delmonte R. 1990, Semantic Parsing with LFG and  Conceptual Representations, Computers and the  Humanities, Kluwer Academic Publishers, 24 , p.  461-488.	GR	Grammatical Relation	0
Booch G. 1994, Analyse et conception orientees  objets , Addison-Wesley, Reading Mass.  Bresnan Joan and Ronald Kaplan 1981, Lexical  functional grammars ; a formal system for gram-  105  matical representation, The mental representation  of GR, MIT Press, Cambridge,  Mass.  Delmonte R. 1990, Semantic Parsing with LFG and  Conceptual Representations, Computers and the  Humanities, Kluwer Academic Publishers, 24 , p.  461-488.	GR	grammatical relations	1
Booch G. 1994, Analyse et conception orientees  objets , Addison-Wesley, Reading Mass.  Bresnan Joan and Ronald Kaplan 1981, Lexical  functional grammars ; a formal system for gram-  105  matical representation, The mental representation  of GR, MIT Press, Cambridge,  Mass.  Delmonte R. 1990, Semantic Parsing with LFG and  Conceptual Representations, Computers and the  Humanities, Kluwer Academic Publishers, 24 , p.  461-488.	GR	Grammatical relation	0
Booch G. 1994, Analyse et conception orientees  objets , Addison-Wesley, Reading Mass.  Bresnan Joan and Ronald Kaplan 1981, Lexical  functional grammars ; a formal system for gram-  105  matical representation, The mental representation  of GR, MIT Press, Cambridge,  Mass.  Delmonte R. 1990, Semantic Parsing with LFG and  Conceptual Representations, Computers and the  Humanities, Kluwer Academic Publishers, 24 , p.  461-488.	GR	gri|SCF	0
Booch G. 1994, Analyse et conception orientees  objets , Addison-Wesley, Reading Mass.  Bresnan Joan and Ronald Kaplan 1981, Lexical  functional grammars ; a formal system for gram-  105  matical representation, The mental representation  of GR, MIT Press, Cambridge,  Mass.  Delmonte R. 1990, Semantic Parsing with LFG and  Conceptual Representations, Computers and the  Humanities, Kluwer Academic Publishers, 24 , p.  461-488.	GR	Greetin	0
Booch G. 1994, Analyse et conception orientees  objets , Addison-Wesley, Reading Mass.  Bresnan Joan and Ronald Kaplan 1981, Lexical  functional grammars ; a formal system for gram-  105  matical representation, The mental representation  of GR, MIT Press, Cambridge,  Mass.  Delmonte R. 1990, Semantic Parsing with LFG and  Conceptual Representations, Computers and the  Humanities, Kluwer Academic Publishers, 24 , p.  461-488.	GR	glue rule	0
Pairs of syntactic units connected by GR are extracted from the parse trees.	GR	group	0
Pairs of syntactic units connected by GR are extracted from the parse trees.	GR	or	0
Pairs of syntactic units connected by GR are extracted from the parse trees.	GR	Grammatical Relation	0
Pairs of syntactic units connected by GR are extracted from the parse trees.	GR	grammatical relations	1
Pairs of syntactic units connected by GR are extracted from the parse trees.	GR	Grammatical relation	0
Pairs of syntactic units connected by GR are extracted from the parse trees.	GR	gri|SCF	0
Pairs of syntactic units connected by GR are extracted from the parse trees.	GR	Greetin	0
Pairs of syntactic units connected by GR are extracted from the parse trees.	GR	glue rule	0
We therefore designed the lexical entries for the case markers so that they specify information about what GR they attach to and what se- mantic information is needed in the clausal analysis.	GR	group	0
We therefore designed the lexical entries for the case markers so that they specify information about what GR they attach to and what se- mantic information is needed in the clausal analysis.	GR	or	0
We therefore designed the lexical entries for the case markers so that they specify information about what GR they attach to and what se- mantic information is needed in the clausal analysis.	GR	Grammatical Relation	0
We therefore designed the lexical entries for the case markers so that they specify information about what GR they attach to and what se- mantic information is needed in the clausal analysis.	GR	grammatical relations	1
We therefore designed the lexical entries for the case markers so that they specify information about what GR they attach to and what se- mantic information is needed in the clausal analysis.	GR	Grammatical relation	0
We therefore designed the lexical entries for the case markers so that they specify information about what GR they attach to and what se- mantic information is needed in the clausal analysis.	GR	gri|SCF	0
We therefore designed the lexical entries for the case markers so that they specify information about what GR they attach to and what se- mantic information is needed in the clausal analysis.	GR	Greetin	0
We therefore designed the lexical entries for the case markers so that they specify information about what GR they attach to and what se- mantic information is needed in the clausal analysis.	GR	glue rule	0
The most important differences are the use of manually defined rules and the inclusion of GR from a parser as critical fea- tures.	GR	group	0
The most important differences are the use of manually defined rules and the inclusion of GR from a parser as critical fea- tures.	GR	or	0
The most important differences are the use of manually defined rules and the inclusion of GR from a parser as critical fea- tures.	GR	Grammatical Relation	0
The most important differences are the use of manually defined rules and the inclusion of GR from a parser as critical fea- tures.	GR	grammatical relations	1
The most important differences are the use of manually defined rules and the inclusion of GR from a parser as critical fea- tures.	GR	Grammatical relation	0
The most important differences are the use of manually defined rules and the inclusion of GR from a parser as critical fea- tures.	GR	gri|SCF	0
The most important differences are the use of manually defined rules and the inclusion of GR from a parser as critical fea- tures.	GR	Greetin	0
The most important differences are the use of manually defined rules and the inclusion of GR from a parser as critical fea- tures.	GR	glue rule	0
Discriminative re- ordering with Chinese GR fea- tures.	GR	group	0
Discriminative re- ordering with Chinese GR fea- tures.	GR	or	0
Discriminative re- ordering with Chinese GR fea- tures.	GR	Grammatical Relation	0
Discriminative re- ordering with Chinese GR fea- tures.	GR	grammatical relations	1
Discriminative re- ordering with Chinese GR fea- tures.	GR	Grammatical relation	0
Discriminative re- ordering with Chinese GR fea- tures.	GR	gri|SCF	0
Discriminative re- ordering with Chinese GR fea- tures.	GR	Greetin	0
Discriminative re- ordering with Chinese GR fea- tures.	GR	glue rule	0
Discriminative re- ordering with Chinese GR features.	GR	group	0
Discriminative re- ordering with Chinese GR features.	GR	or	0
Discriminative re- ordering with Chinese GR features.	GR	Grammatical Relation	0
Discriminative re- ordering with Chinese GR features.	GR	grammatical relations	1
Discriminative re- ordering with Chinese GR features.	GR	Grammatical relation	0
Discriminative re- ordering with Chinese GR features.	GR	gri|SCF	0
Discriminative re- ordering with Chinese GR features.	GR	Greetin	0
Discriminative re- ordering with Chinese GR features.	GR	glue rule	0
GRs are in italics (con- junction and nominal-subject).	GR	group	0
GRs are in italics (con- junction and nominal-subject).	GR	or	0
GRs are in italics (con- junction and nominal-subject).	GR	Grammatical Relation	0
GRs are in italics (con- junction and nominal-subject).	GR	grammatical relations	0
GRs are in italics (con- junction and nominal-subject).	GR	Grammatical relation	1
GRs are in italics (con- junction and nominal-subject).	GR	gri|SCF	0
GRs are in italics (con- junction and nominal-subject).	GR	Greetin	0
GRs are in italics (con- junction and nominal-subject).	GR	glue rule	0
GRs  and Montague Grammar.	GR	group	0
GRs  and Montague Grammar.	GR	or	0
GRs  and Montague Grammar.	GR	Grammatical Relation	0
GRs  and Montague Grammar.	GR	grammatical relations	0
GRs  and Montague Grammar.	GR	Grammatical relation	1
GRs  and Montague Grammar.	GR	gri|SCF	0
GRs  and Montague Grammar.	GR	Greetin	0
GRs  and Montague Grammar.	GR	glue rule	0
GRs are arranged in a  hierarchy and are usually referred to by  numbers: 1, which is the highest, corresponds  to SUBJECT, 2 to DIRECT OBJEC~I ', 3 to  INDIRECT OBJECT.	GR	group	0
GRs are arranged in a  hierarchy and are usually referred to by  numbers: 1, which is the highest, corresponds  to SUBJECT, 2 to DIRECT OBJEC~I ', 3 to  INDIRECT OBJECT.	GR	or	0
GRs are arranged in a  hierarchy and are usually referred to by  numbers: 1, which is the highest, corresponds  to SUBJECT, 2 to DIRECT OBJEC~I ', 3 to  INDIRECT OBJECT.	GR	Grammatical Relation	0
GRs are arranged in a  hierarchy and are usually referred to by  numbers: 1, which is the highest, corresponds  to SUBJECT, 2 to DIRECT OBJEC~I ', 3 to  INDIRECT OBJECT.	GR	grammatical relations	0
GRs are arranged in a  hierarchy and are usually referred to by  numbers: 1, which is the highest, corresponds  to SUBJECT, 2 to DIRECT OBJEC~I ', 3 to  INDIRECT OBJECT.	GR	Grammatical relation	1
GRs are arranged in a  hierarchy and are usually referred to by  numbers: 1, which is the highest, corresponds  to SUBJECT, 2 to DIRECT OBJEC~I ', 3 to  INDIRECT OBJECT.	GR	gri|SCF	0
GRs are arranged in a  hierarchy and are usually referred to by  numbers: 1, which is the highest, corresponds  to SUBJECT, 2 to DIRECT OBJEC~I ', 3 to  INDIRECT OBJECT.	GR	Greetin	0
GRs are arranged in a  hierarchy and are usually referred to by  numbers: 1, which is the highest, corresponds  to SUBJECT, 2 to DIRECT OBJEC~I ', 3 to  INDIRECT OBJECT.	GR	glue rule	0
To do this, we Figure 8 GRs output for metaphorical expressions.	GR	group	0
To do this, we Figure 8 GRs output for metaphorical expressions.	GR	or	0
To do this, we Figure 8 GRs output for metaphorical expressions.	GR	Grammatical Relation	0
To do this, we Figure 8 GRs output for metaphorical expressions.	GR	grammatical relations	0
To do this, we Figure 8 GRs output for metaphorical expressions.	GR	Grammatical relation	1
To do this, we Figure 8 GRs output for metaphorical expressions.	GR	gri|SCF	0
To do this, we Figure 8 GRs output for metaphorical expressions.	GR	Greetin	0
To do this, we Figure 8 GRs output for metaphorical expressions.	GR	glue rule	0
GRs  This kind of features refers to the grammatical  relation themselves.	GR	group	0
GRs  This kind of features refers to the grammatical  relation themselves.	GR	or	0
GRs  This kind of features refers to the grammatical  relation themselves.	GR	Grammatical Relation	0
GRs  This kind of features refers to the grammatical  relation themselves.	GR	grammatical relations	0
GRs  This kind of features refers to the grammatical  relation themselves.	GR	Grammatical relation	1
GRs  This kind of features refers to the grammatical  relation themselves.	GR	gri|SCF	0
GRs  This kind of features refers to the grammatical  relation themselves.	GR	Greetin	0
GRs  This kind of features refers to the grammatical  relation themselves.	GR	glue rule	0
DP-HWC(i)c-l GRships ?	GR	group	0
DP-HWC(i)c-l GRships ?	GR	or	0
DP-HWC(i)c-l GRships ?	GR	Grammatical Relation	0
DP-HWC(i)c-l GRships ?	GR	grammatical relations	0
DP-HWC(i)c-l GRships ?	GR	Grammatical relation	1
DP-HWC(i)c-l GRships ?	GR	gri|SCF	0
DP-HWC(i)c-l GRships ?	GR	Greetin	0
DP-HWC(i)c-l GRships ?	GR	glue rule	0
72  THIRTEENTH ANNUAL MEETING  THE AS$OCIATtON FOR COMPUTATIONAL UNGUISTICS  Sheraton Boston Hotel  Boston, Massachusef t s   October $0-November 1, 1975  Thursday, October 30, 197.5  S&SSION 1: i,AhrCUAGE C/IVn ERSTAArDlllFG SYSTElhf S  Session Chairman: Yorick Wdks - h r v e r s l t y  of Edtnburgh  990 A.M. GRgs and Irrtroductory Rernarks  9: 15 A.M. PE'DfiGI,OT and Underrl art d i n g  Natural 1,nnguagr Procr t sing  Willtarn Fabens - Rutgers University  9:40 AM A Syrtcm /or Gencral Scmankc Analysis And l t , q  Use  I n  Drawirt g A l  apa from Dircctiotts  Jerry R. t.lobbs - The C ~ t y  College of CUNY  tO:OS A.M An Adaprivo Nutural Lartguago Parser  Ferry t. M~ller - M I T.  1030 AM.	GR	group	0
72  THIRTEENTH ANNUAL MEETING  THE AS$OCIATtON FOR COMPUTATIONAL UNGUISTICS  Sheraton Boston Hotel  Boston, Massachusef t s   October $0-November 1, 1975  Thursday, October 30, 197.5  S&SSION 1: i,AhrCUAGE C/IVn ERSTAArDlllFG SYSTElhf S  Session Chairman: Yorick Wdks - h r v e r s l t y  of Edtnburgh  990 A.M. GRgs and Irrtroductory Rernarks  9: 15 A.M. PE'DfiGI,OT and Underrl art d i n g  Natural 1,nnguagr Procr t sing  Willtarn Fabens - Rutgers University  9:40 AM A Syrtcm /or Gencral Scmankc Analysis And l t , q  Use  I n  Drawirt g A l  apa from Dircctiotts  Jerry R. t.lobbs - The C ~ t y  College of CUNY  tO:OS A.M An Adaprivo Nutural Lartguago Parser  Ferry t. M~ller - M I T.  1030 AM.	GR	or	0
72  THIRTEENTH ANNUAL MEETING  THE AS$OCIATtON FOR COMPUTATIONAL UNGUISTICS  Sheraton Boston Hotel  Boston, Massachusef t s   October $0-November 1, 1975  Thursday, October 30, 197.5  S&SSION 1: i,AhrCUAGE C/IVn ERSTAArDlllFG SYSTElhf S  Session Chairman: Yorick Wdks - h r v e r s l t y  of Edtnburgh  990 A.M. GRgs and Irrtroductory Rernarks  9: 15 A.M. PE'DfiGI,OT and Underrl art d i n g  Natural 1,nnguagr Procr t sing  Willtarn Fabens - Rutgers University  9:40 AM A Syrtcm /or Gencral Scmankc Analysis And l t , q  Use  I n  Drawirt g A l  apa from Dircctiotts  Jerry R. t.lobbs - The C ~ t y  College of CUNY  tO:OS A.M An Adaprivo Nutural Lartguago Parser  Ferry t. M~ller - M I T.  1030 AM.	GR	Grammatical Relation	0
72  THIRTEENTH ANNUAL MEETING  THE AS$OCIATtON FOR COMPUTATIONAL UNGUISTICS  Sheraton Boston Hotel  Boston, Massachusef t s   October $0-November 1, 1975  Thursday, October 30, 197.5  S&SSION 1: i,AhrCUAGE C/IVn ERSTAArDlllFG SYSTElhf S  Session Chairman: Yorick Wdks - h r v e r s l t y  of Edtnburgh  990 A.M. GRgs and Irrtroductory Rernarks  9: 15 A.M. PE'DfiGI,OT and Underrl art d i n g  Natural 1,nnguagr Procr t sing  Willtarn Fabens - Rutgers University  9:40 AM A Syrtcm /or Gencral Scmankc Analysis And l t , q  Use  I n  Drawirt g A l  apa from Dircctiotts  Jerry R. t.lobbs - The C ~ t y  College of CUNY  tO:OS A.M An Adaprivo Nutural Lartguago Parser  Ferry t. M~ller - M I T.  1030 AM.	GR	grammatical relations	0
72  THIRTEENTH ANNUAL MEETING  THE AS$OCIATtON FOR COMPUTATIONAL UNGUISTICS  Sheraton Boston Hotel  Boston, Massachusef t s   October $0-November 1, 1975  Thursday, October 30, 197.5  S&SSION 1: i,AhrCUAGE C/IVn ERSTAArDlllFG SYSTElhf S  Session Chairman: Yorick Wdks - h r v e r s l t y  of Edtnburgh  990 A.M. GRgs and Irrtroductory Rernarks  9: 15 A.M. PE'DfiGI,OT and Underrl art d i n g  Natural 1,nnguagr Procr t sing  Willtarn Fabens - Rutgers University  9:40 AM A Syrtcm /or Gencral Scmankc Analysis And l t , q  Use  I n  Drawirt g A l  apa from Dircctiotts  Jerry R. t.lobbs - The C ~ t y  College of CUNY  tO:OS A.M An Adaprivo Nutural Lartguago Parser  Ferry t. M~ller - M I T.  1030 AM.	GR	Grammatical relation	0
72  THIRTEENTH ANNUAL MEETING  THE AS$OCIATtON FOR COMPUTATIONAL UNGUISTICS  Sheraton Boston Hotel  Boston, Massachusef t s   October $0-November 1, 1975  Thursday, October 30, 197.5  S&SSION 1: i,AhrCUAGE C/IVn ERSTAArDlllFG SYSTElhf S  Session Chairman: Yorick Wdks - h r v e r s l t y  of Edtnburgh  990 A.M. GRgs and Irrtroductory Rernarks  9: 15 A.M. PE'DfiGI,OT and Underrl art d i n g  Natural 1,nnguagr Procr t sing  Willtarn Fabens - Rutgers University  9:40 AM A Syrtcm /or Gencral Scmankc Analysis And l t , q  Use  I n  Drawirt g A l  apa from Dircctiotts  Jerry R. t.lobbs - The C ~ t y  College of CUNY  tO:OS A.M An Adaprivo Nutural Lartguago Parser  Ferry t. M~ller - M I T.  1030 AM.	GR	gri|SCF	0
72  THIRTEENTH ANNUAL MEETING  THE AS$OCIATtON FOR COMPUTATIONAL UNGUISTICS  Sheraton Boston Hotel  Boston, Massachusef t s   October $0-November 1, 1975  Thursday, October 30, 197.5  S&SSION 1: i,AhrCUAGE C/IVn ERSTAArDlllFG SYSTElhf S  Session Chairman: Yorick Wdks - h r v e r s l t y  of Edtnburgh  990 A.M. GRgs and Irrtroductory Rernarks  9: 15 A.M. PE'DfiGI,OT and Underrl art d i n g  Natural 1,nnguagr Procr t sing  Willtarn Fabens - Rutgers University  9:40 AM A Syrtcm /or Gencral Scmankc Analysis And l t , q  Use  I n  Drawirt g A l  apa from Dircctiotts  Jerry R. t.lobbs - The C ~ t y  College of CUNY  tO:OS A.M An Adaprivo Nutural Lartguago Parser  Ferry t. M~ller - M I T.  1030 AM.	GR	Greetin	1
72  THIRTEENTH ANNUAL MEETING  THE AS$OCIATtON FOR COMPUTATIONAL UNGUISTICS  Sheraton Boston Hotel  Boston, Massachusef t s   October $0-November 1, 1975  Thursday, October 30, 197.5  S&SSION 1: i,AhrCUAGE C/IVn ERSTAArDlllFG SYSTElhf S  Session Chairman: Yorick Wdks - h r v e r s l t y  of Edtnburgh  990 A.M. GRgs and Irrtroductory Rernarks  9: 15 A.M. PE'DfiGI,OT and Underrl art d i n g  Natural 1,nnguagr Procr t sing  Willtarn Fabens - Rutgers University  9:40 AM A Syrtcm /or Gencral Scmankc Analysis And l t , q  Use  I n  Drawirt g A l  apa from Dircctiotts  Jerry R. t.lobbs - The C ~ t y  College of CUNY  tO:OS A.M An Adaprivo Nutural Lartguago Parser  Ferry t. M~ller - M I T.  1030 AM.	GR	glue rule	0
GRgs Contact.	GR	group	0
GRgs Contact.	GR	or	0
GRgs Contact.	GR	Grammatical Relation	0
GRgs Contact.	GR	grammatical relations	0
GRgs Contact.	GR	Grammatical relation	0
GRgs Contact.	GR	gri|SCF	0
GRgs Contact.	GR	Greetin	1
GRgs Contact.	GR	glue rule	0
GRg SYS2 Saikin no oishii mono ni tsuite kikasete kudasai Q-Plan Topic-Inducing (Tell me about delicious food that you?ve had recently) USR2 Karei ni hamatterunda! (	GR	group	0
GRg SYS2 Saikin no oishii mono ni tsuite kikasete kudasai Q-Plan Topic-Inducing (Tell me about delicious food that you?ve had recently) USR2 Karei ni hamatterunda! (	GR	or	0
GRg SYS2 Saikin no oishii mono ni tsuite kikasete kudasai Q-Plan Topic-Inducing (Tell me about delicious food that you?ve had recently) USR2 Karei ni hamatterunda! (	GR	Grammatical Relation	0
GRg SYS2 Saikin no oishii mono ni tsuite kikasete kudasai Q-Plan Topic-Inducing (Tell me about delicious food that you?ve had recently) USR2 Karei ni hamatterunda! (	GR	grammatical relations	0
GRg SYS2 Saikin no oishii mono ni tsuite kikasete kudasai Q-Plan Topic-Inducing (Tell me about delicious food that you?ve had recently) USR2 Karei ni hamatterunda! (	GR	Grammatical relation	0
GRg SYS2 Saikin no oishii mono ni tsuite kikasete kudasai Q-Plan Topic-Inducing (Tell me about delicious food that you?ve had recently) USR2 Karei ni hamatterunda! (	GR	gri|SCF	0
GRg SYS2 Saikin no oishii mono ni tsuite kikasete kudasai Q-Plan Topic-Inducing (Tell me about delicious food that you?ve had recently) USR2 Karei ni hamatterunda! (	GR	Greetin	1
GRg SYS2 Saikin no oishii mono ni tsuite kikasete kudasai Q-Plan Topic-Inducing (Tell me about delicious food that you?ve had recently) USR2 Karei ni hamatterunda! (	GR	glue rule	0
930 Utterance (English translation by the authors) DA Gen. Module SYS1 Doumo desu (Hi) GRg Initial prompt USR1 Doumo.	GR	group	0
930 Utterance (English translation by the authors) DA Gen. Module SYS1 Doumo desu (Hi) GRg Initial prompt USR1 Doumo.	GR	or	0
930 Utterance (English translation by the authors) DA Gen. Module SYS1 Doumo desu (Hi) GRg Initial prompt USR1 Doumo.	GR	Grammatical Relation	0
930 Utterance (English translation by the authors) DA Gen. Module SYS1 Doumo desu (Hi) GRg Initial prompt USR1 Doumo.	GR	grammatical relations	0
930 Utterance (English translation by the authors) DA Gen. Module SYS1 Doumo desu (Hi) GRg Initial prompt USR1 Doumo.	GR	Grammatical relation	0
930 Utterance (English translation by the authors) DA Gen. Module SYS1 Doumo desu (Hi) GRg Initial prompt USR1 Doumo.	GR	gri|SCF	0
930 Utterance (English translation by the authors) DA Gen. Module SYS1 Doumo desu (Hi) GRg Initial prompt USR1 Doumo.	GR	Greetin	1
930 Utterance (English translation by the authors) DA Gen. Module SYS1 Doumo desu (Hi) GRg Initial prompt USR1 Doumo.	GR	glue rule	0
JP) Table 3: Templates for topic types (translated by authors) Dialogue act Example GRgs Hello.	GR	group	0
JP) Table 3: Templates for topic types (translated by authors) Dialogue act Example GRgs Hello.	GR	or	0
JP) Table 3: Templates for topic types (translated by authors) Dialogue act Example GRgs Hello.	GR	Grammatical Relation	0
JP) Table 3: Templates for topic types (translated by authors) Dialogue act Example GRgs Hello.	GR	grammatical relations	0
JP) Table 3: Templates for topic types (translated by authors) Dialogue act Example GRgs Hello.	GR	Grammatical relation	0
JP) Table 3: Templates for topic types (translated by authors) Dialogue act Example GRgs Hello.	GR	gri|SCF	0
JP) Table 3: Templates for topic types (translated by authors) Dialogue act Example GRgs Hello.	GR	Greetin	1
JP) Table 3: Templates for topic types (translated by authors) Dialogue act Example GRgs Hello.	GR	glue rule	0
Features extracted from the alignments and used in translation are in common use: target lan- guage model, source-to-target and target-to-source phrase translation models, word and rule penalties, number of usages of the GR, source-to-target and target-to-source lexical models, and three rule Figure 1: Spreading neighborhood exploration within a cube, just before and after extraction of the item C. Grey squares represent the fron- tier queue; black squares are candidates already extracted.	GR	group	0
Features extracted from the alignments and used in translation are in common use: target lan- guage model, source-to-target and target-to-source phrase translation models, word and rule penalties, number of usages of the GR, source-to-target and target-to-source lexical models, and three rule Figure 1: Spreading neighborhood exploration within a cube, just before and after extraction of the item C. Grey squares represent the fron- tier queue; black squares are candidates already extracted.	GR	or	0
Features extracted from the alignments and used in translation are in common use: target lan- guage model, source-to-target and target-to-source phrase translation models, word and rule penalties, number of usages of the GR, source-to-target and target-to-source lexical models, and three rule Figure 1: Spreading neighborhood exploration within a cube, just before and after extraction of the item C. Grey squares represent the fron- tier queue; black squares are candidates already extracted.	GR	Grammatical Relation	0
Features extracted from the alignments and used in translation are in common use: target lan- guage model, source-to-target and target-to-source phrase translation models, word and rule penalties, number of usages of the GR, source-to-target and target-to-source lexical models, and three rule Figure 1: Spreading neighborhood exploration within a cube, just before and after extraction of the item C. Grey squares represent the fron- tier queue; black squares are candidates already extracted.	GR	grammatical relations	0
Features extracted from the alignments and used in translation are in common use: target lan- guage model, source-to-target and target-to-source phrase translation models, word and rule penalties, number of usages of the GR, source-to-target and target-to-source lexical models, and three rule Figure 1: Spreading neighborhood exploration within a cube, just before and after extraction of the item C. Grey squares represent the fron- tier queue; black squares are candidates already extracted.	GR	Grammatical relation	0
Features extracted from the alignments and used in translation are in common use: target lan- guage model, source-to-target and target-to-source phrase translation models, word and rule penalties, number of usages of the GR, source-to-target and target-to-source lexical models, and three rule Figure 1: Spreading neighborhood exploration within a cube, just before and after extraction of the item C. Grey squares represent the fron- tier queue; black squares are candidates already extracted.	GR	gri|SCF	0
Features extracted from the alignments and used in translation are in common use: target lan- guage model, source-to-target and target-to-source phrase translation models, word and rule penalties, number of usages of the GR, source-to-target and target-to-source lexical models, and three rule Figure 1: Spreading neighborhood exploration within a cube, just before and after extraction of the item C. Grey squares represent the fron- tier queue; black squares are candidates already extracted.	GR	Greetin	0
Features extracted from the alignments and used in translation are in common use: target lan- guage model, source-to-target and target-to-source phrase translation models, word and rule penalties, number of usages of the GR, source-to-target and target-to-source lexical models, and three rule Figure 1: Spreading neighborhood exploration within a cube, just before and after extraction of the item C. Grey squares represent the fron- tier queue; black squares are candidates already extracted.	GR	glue rule	1
Glue(r): exp(1) if rule is a GR ? ?	GR	group	0
Glue(r): exp(1) if rule is a GR ? ?	GR	or	0
Glue(r): exp(1) if rule is a GR ? ?	GR	Grammatical Relation	0
Glue(r): exp(1) if rule is a GR ? ?	GR	grammatical relations	0
Glue(r): exp(1) if rule is a GR ? ?	GR	Grammatical relation	0
Glue(r): exp(1) if rule is a GR ? ?	GR	gri|SCF	0
Glue(r): exp(1) if rule is a GR ? ?	GR	Greetin	0
Glue(r): exp(1) if rule is a GR ? ?	GR	glue rule	1
To pro- duce the result, the joint decoder made use of 8,114 hierarchical phrase pairs learned from train- ing data, 6,800 GRs connecting partial trans- lations monotonically, and 16,554 tree-to-string rules.	GR	group	0
To pro- duce the result, the joint decoder made use of 8,114 hierarchical phrase pairs learned from train- ing data, 6,800 GRs connecting partial trans- lations monotonically, and 16,554 tree-to-string rules.	GR	or	0
To pro- duce the result, the joint decoder made use of 8,114 hierarchical phrase pairs learned from train- ing data, 6,800 GRs connecting partial trans- lations monotonically, and 16,554 tree-to-string rules.	GR	Grammatical Relation	0
To pro- duce the result, the joint decoder made use of 8,114 hierarchical phrase pairs learned from train- ing data, 6,800 GRs connecting partial trans- lations monotonically, and 16,554 tree-to-string rules.	GR	grammatical relations	0
To pro- duce the result, the joint decoder made use of 8,114 hierarchical phrase pairs learned from train- ing data, 6,800 GRs connecting partial trans- lations monotonically, and 16,554 tree-to-string rules.	GR	Grammatical relation	0
To pro- duce the result, the joint decoder made use of 8,114 hierarchical phrase pairs learned from train- ing data, 6,800 GRs connecting partial trans- lations monotonically, and 16,554 tree-to-string rules.	GR	gri|SCF	0
To pro- duce the result, the joint decoder made use of 8,114 hierarchical phrase pairs learned from train- ing data, 6,800 GRs connecting partial trans- lations monotonically, and 16,554 tree-to-string rules.	GR	Greetin	0
To pro- duce the result, the joint decoder made use of 8,114 hierarchical phrase pairs learned from train- ing data, 6,800 GRs connecting partial trans- lations monotonically, and 16,554 tree-to-string rules.	GR	glue rule	1
Since concatenation is already possible under the general GR, rules with this pattern are redundant.	GR	group	0
Since concatenation is already possible under the general GR, rules with this pattern are redundant.	GR	or	0
Since concatenation is already possible under the general GR, rules with this pattern are redundant.	GR	Grammatical Relation	0
Since concatenation is already possible under the general GR, rules with this pattern are redundant.	GR	grammatical relations	0
Since concatenation is already possible under the general GR, rules with this pattern are redundant.	GR	Grammatical relation	0
Since concatenation is already possible under the general GR, rules with this pattern are redundant.	GR	gri|SCF	0
Since concatenation is already possible under the general GR, rules with this pattern are redundant.	GR	Greetin	0
Since concatenation is already possible under the general GR, rules with this pattern are redundant.	GR	glue rule	1
The Hiero decoder can easily be made to implement MJ1 reordering by allowing only a restricted set of reordering rules in addition to the usual GR, as shown in left-hand column of Table 1, where T is the set of terminals.	GR	group	0
The Hiero decoder can easily be made to implement MJ1 reordering by allowing only a restricted set of reordering rules in addition to the usual GR, as shown in left-hand column of Table 1, where T is the set of terminals.	GR	or	0
The Hiero decoder can easily be made to implement MJ1 reordering by allowing only a restricted set of reordering rules in addition to the usual GR, as shown in left-hand column of Table 1, where T is the set of terminals.	GR	Grammatical Relation	0
The Hiero decoder can easily be made to implement MJ1 reordering by allowing only a restricted set of reordering rules in addition to the usual GR, as shown in left-hand column of Table 1, where T is the set of terminals.	GR	grammatical relations	0
The Hiero decoder can easily be made to implement MJ1 reordering by allowing only a restricted set of reordering rules in addition to the usual GR, as shown in left-hand column of Table 1, where T is the set of terminals.	GR	Grammatical relation	0
The Hiero decoder can easily be made to implement MJ1 reordering by allowing only a restricted set of reordering rules in addition to the usual GR, as shown in left-hand column of Table 1, where T is the set of terminals.	GR	gri|SCF	0
The Hiero decoder can easily be made to implement MJ1 reordering by allowing only a restricted set of reordering rules in addition to the usual GR, as shown in left-hand column of Table 1, where T is the set of terminals.	GR	Greetin	0
The Hiero decoder can easily be made to implement MJ1 reordering by allowing only a restricted set of reordering rules in addition to the usual GR, as shown in left-hand column of Table 1, where T is the set of terminals.	GR	glue rule	1
T)+ Table 1: Hierarchical grammars (not including GRs).	GR	group	0
T)+ Table 1: Hierarchical grammars (not including GRs).	GR	or	0
T)+ Table 1: Hierarchical grammars (not including GRs).	GR	Grammatical Relation	0
T)+ Table 1: Hierarchical grammars (not including GRs).	GR	grammatical relations	0
T)+ Table 1: Hierarchical grammars (not including GRs).	GR	Grammatical relation	0
T)+ Table 1: Hierarchical grammars (not including GRs).	GR	gri|SCF	0
T)+ Table 1: Hierarchical grammars (not including GRs).	GR	Greetin	0
T)+ Table 1: Hierarchical grammars (not including GRs).	GR	glue rule	1
This class of models includes popular tag- ging models for named entities such as conditional random fields, MEMMs and max-margin Markov networks.	MEMM	maximum entropy Markov model	1
This class of models includes popular tag- ging models for named entities such as conditional random fields, MEMMs and max-margin Markov networks.	MEMM	Maximum Entropy Markov Model	0
We use a MEMM to estimate these probabilities, to pre-annotate in- stances, and to evaluate accuracy.	MEMM	maximum entropy Markov model	1
We use a MEMM to estimate these probabilities, to pre-annotate in- stances, and to evaluate accuracy.	MEMM	Maximum Entropy Markov Model	0
OSCAR (Open-Source Chemistry Analysis Rou- tines) (Corbett and Murray-Rust, 2006; Jessop et al 2011) extracts mentions of a wide range of chemi- cals using a MEMM (Mc- Callum et al 2000).	MEMM	maximum entropy Markov model	1
OSCAR (Open-Source Chemistry Analysis Rou- tines) (Corbett and Murray-Rust, 2006; Jessop et al 2011) extracts mentions of a wide range of chemi- cals using a MEMM (Mc- Callum et al 2000).	MEMM	Maximum Entropy Markov Model	0
Our features were implemented  in a MEMM.	MEMM	maximum entropy Markov model	1
Our features were implemented  in a MEMM.	MEMM	Maximum Entropy Markov Model	0
UBC-UPC: Sequential SRL using selectional preferences: an approach with MEMMs.	MEMM	maximum entropy Markov model	1
UBC-UPC: Sequential SRL using selectional preferences: an approach with MEMMs.	MEMM	Maximum Entropy Markov Model	0
It is based on maxent models, MEMMs and linear-chain CRF and proposes various optimization and regularization methods to improve both the computational complexity and the prediction performance of standard models.	MEMM	maximum entropy Markov model	1
It is based on maxent models, MEMMs and linear-chain CRF and proposes various optimization and regularization methods to improve both the computational complexity and the prediction performance of standard models.	MEMM	Maximum Entropy Markov Model	0
An Approach with MEMMs.	MEMM	maximum entropy Markov model	0
An Approach with MEMMs.	MEMM	Maximum Entropy Markov Model	1
68  CoNLL 2008: Proceedings of the 12th Conference on Computational Natural Language Learning, pages 243?247 Manchester, August 2008 The Integration of Dependency Relation Classification and Semantic Role Labeling Using Bilayer MEMMs Weiwei Sun and Hongzhan Li and Zhifang Sui Institute of Computational Linguistics Peking University {weiwsun, lihongzhan.pku}@gmail.com, szf@pku.edu.cn Abstract This paper describes a system to solve the joint learning of syntactic and seman- tic dependencies.	MEMM	maximum entropy Markov model	0
68  CoNLL 2008: Proceedings of the 12th Conference on Computational Natural Language Learning, pages 243?247 Manchester, August 2008 The Integration of Dependency Relation Classification and Semantic Role Labeling Using Bilayer MEMMs Weiwei Sun and Hongzhan Li and Zhifang Sui Institute of Computational Linguistics Peking University {weiwsun, lihongzhan.pku}@gmail.com, szf@pku.edu.cn Abstract This paper describes a system to solve the joint learning of syntactic and seman- tic dependencies.	MEMM	Maximum Entropy Markov Model	1
Rudnick et al(2013) present a combina- tion of MEMMs and HMM to perform lexical selection in the sense of cross-lingual word sense disam- biguation (i.e. by choice from the set of trans- lation alternatives).	MEMM	maximum entropy Markov model	0
Rudnick et al(2013) present a combina- tion of MEMMs and HMM to perform lexical selection in the sense of cross-lingual word sense disam- biguation (i.e. by choice from the set of trans- lation alternatives).	MEMM	Maximum Entropy Markov Model	1
An aproach with MEMMs Ben?at Zapirain, Eneko Agirre IXA NLP Group University of the Basque Country Donostia, Basque Country {benat.zapirain,e.agirre}@ehu.es Llu??s Ma`rquez TALP Research Center Technical University of Catalonia Barcelona, Catalonia lluism@lsi.upc.edu Abstract We present a sequential Semantic Role La- beling system that describes the tagging problem as a Maximum Entropy Mar	MEMM	maximum entropy Markov model	0
An aproach with MEMMs Ben?at Zapirain, Eneko Agirre IXA NLP Group University of the Basque Country Donostia, Basque Country {benat.zapirain,e.agirre}@ehu.es Llu??s Ma`rquez TALP Research Center Technical University of Catalonia Barcelona, Catalonia lluism@lsi.upc.edu Abstract We present a sequential Semantic Role La- beling system that describes the tagging problem as a Maximum Entropy Mar	MEMM	Maximum Entropy Markov Model	1
To provide part-of-speech tags to MSTParser, we use the MElt tagger (Denis and Sagot, 2009), a MEMM tagger en- riched with information from a large-scale dictio- nary.5 The tagger was trained on the training set to provide POS tags for the dev and test sets, and we used 10-way jackkni?ng to generate tags for the training set.	MEMM	maximum entropy Markov model	0
To provide part-of-speech tags to MSTParser, we use the MElt tagger (Denis and Sagot, 2009), a MEMM tagger en- riched with information from a large-scale dictio- nary.5 The tagger was trained on the training set to provide POS tags for the dev and test sets, and we used 10-way jackkni?ng to generate tags for the training set.	MEMM	Maximum Entropy Markov Model	1
To estimate the probability distribution of each arc and do inference, we im- plement a MEMM (Mc- Callum et al, 2000).	MEMM	maximum entropy Markov model	0
To estimate the probability distribution of each arc and do inference, we im- plement a MEMM (Mc- Callum et al, 2000).	MEMM	Maximum Entropy Markov Model	1
Using AWN to disambiguate par-  allel texts allows us to calculate the intersection  among the synsets accessible from an English text  through the English WoRDNET and the synsets  accessible from the parallel Italian text through  the Italian WORDNET.	AWN	Arabic Wordnet	0
Using AWN to disambiguate par-  allel texts allows us to calculate the intersection  among the synsets accessible from an English text  through the English WoRDNET and the synsets  accessible from the parallel Italian text through  the Italian WORDNET.	AWN	aligned wordnets	1
Using AWN to disambiguate par-  allel texts allows us to calculate the intersection  among the synsets accessible from an English text  through the English WoRDNET and the synsets  accessible from the parallel Italian text through  the Italian WORDNET.	AWN	Arabic WordNet	0
TufiS, D., Ion, R., Ide, N.: Fine-grained word  sense disambiguation based on parallel corpo- ra, word alignment, word clustering and  AWN.	AWN	Arabic Wordnet	0
TufiS, D., Ion, R., Ide, N.: Fine-grained word  sense disambiguation based on parallel corpo- ra, word alignment, word clustering and  AWN.	AWN	aligned wordnets	1
TufiS, D., Ion, R., Ide, N.: Fine-grained word  sense disambiguation based on parallel corpo- ra, word alignment, word clustering and  AWN.	AWN	Arabic WordNet	0
in (2004) presented a method that exploits word  clustering based on automatic extraction of trans- lation equivalents, being supported by available  AWN.	AWN	Arabic Wordnet	0
in (2004) presented a method that exploits word  clustering based on automatic extraction of trans- lation equivalents, being supported by available  AWN.	AWN	aligned wordnets	1
in (2004) presented a method that exploits word  clustering based on automatic extraction of trans- lation equivalents, being supported by available  AWN.	AWN	Arabic WordNet	0
lexical resources (e.g., dictionaries,  AWN), or  ?	AWN	Arabic Wordnet	0
lexical resources (e.g., dictionaries,  AWN), or  ?	AWN	aligned wordnets	1
lexical resources (e.g., dictionaries,  AWN), or  ?	AWN	Arabic WordNet	0
In this way the  AWN can be used to help each other and  derive a more compatible and consistent structure.	AWN	Arabic Wordnet	0
In this way the  AWN can be used to help each other and  derive a more compatible and consistent structure.	AWN	aligned wordnets	1
In this way the  AWN can be used to help each other and  derive a more compatible and consistent structure.	AWN	Arabic WordNet	0
There are then two general ways in which the  AWN can be accessed:  ?	AWN	Arabic Wordnet	0
There are then two general ways in which the  AWN can be accessed:  ?	AWN	aligned wordnets	1
There are then two general ways in which the  AWN can be accessed:  ?	AWN	Arabic WordNet	0
Automatically Extending Named Entities coverage  of AWN using Wikipedia.	AWN	Arabic Wordnet	0
Automatically Extending Named Entities coverage  of AWN using Wikipedia.	AWN	aligned wordnets	0
Automatically Extending Named Entities coverage  of AWN using Wikipedia.	AWN	Arabic WordNet	1
2010) proposed a system  that uses AWN to enhance Arabic  question/answering.	AWN	Arabic Wordnet	0
2010) proposed a system  that uses AWN to enhance Arabic  question/answering.	AWN	aligned wordnets	0
2010) proposed a system  that uses AWN to enhance Arabic  question/answering.	AWN	Arabic WordNet	1
Our corpus will be further extended to include more text, and all lexical annotations (i.e., Lemmas) will be linked with existing Arabic ontology resources such as the AWN (Black et al.,	AWN	Arabic Wordnet	0
Our corpus will be further extended to include more text, and all lexical annotations (i.e., Lemmas) will be linked with existing Arabic ontology resources such as the AWN (Black et al.,	AWN	aligned wordnets	0
Our corpus will be further extended to include more text, and all lexical annotations (i.e., Lemmas) will be linked with existing Arabic ontology resources such as the AWN (Black et al.,	AWN	Arabic WordNet	1
As in (Alkhalifa and Rodrguez,  2008), they proposed an automatic technique for  extending Named Entities of AWN  using Wikipedia.	AWN	Arabic Wordnet	0
As in (Alkhalifa and Rodrguez,  2008), they proposed an automatic technique for  extending Named Entities of AWN  using Wikipedia.	AWN	aligned wordnets	0
As in (Alkhalifa and Rodrguez,  2008), they proposed an automatic technique for  extending Named Entities of AWN  using Wikipedia.	AWN	Arabic WordNet	1
A Proposed Model for Quranic AWN.	AWN	Arabic Wordnet	0
A Proposed Model for Quranic AWN.	AWN	aligned wordnets	0
A Proposed Model for Quranic AWN.	AWN	Arabic WordNet	1
AWN: current state  and future extensions.	AWN	Arabic Wordnet	0
AWN: current state  and future extensions.	AWN	aligned wordnets	0
AWN: current state  and future extensions.	AWN	Arabic WordNet	1
Thus, the FM only chooses roles from frames that are the best syntac- tic matches with the extracted argument set.	FM	frame matcher	1
Thus, the FM only chooses roles from frames that are the best syntac- tic matches with the extracted argument set.	FM	F-measures	0
Thus, the FM only chooses roles from frames that are the best syntac- tic matches with the extracted argument set.	FM	F -measure	0
The potential argument slots are sub- ject, object, indirect object, and PP-object, where the latter is specialized by the individual preposi- tion.1 Given chunked sentences with our verbs, the FM uses VerbNet both to restrict the list of candidate roles for each slot, and to eliminate some of the PP slots that are likely not arguments.	FM	frame matcher	1
The potential argument slots are sub- ject, object, indirect object, and PP-object, where the latter is specialized by the individual preposi- tion.1 Given chunked sentences with our verbs, the FM uses VerbNet both to restrict the list of candidate roles for each slot, and to eliminate some of the PP slots that are likely not arguments.	FM	F-measures	0
The potential argument slots are sub- ject, object, indirect object, and PP-object, where the latter is specialized by the individual preposi- tion.1 Given chunked sentences with our verbs, the FM uses VerbNet both to restrict the list of candidate roles for each slot, and to eliminate some of the PP slots that are likely not arguments.	FM	F -measure	0
3.2 Adjustments to the Role Mapping We further extend the FM, which has ex- tensive knowledge of VerbNet, for the separate task of helping to eliminate some of the inconsistencies that are introduced by our role mapping procedure.	FM	frame matcher	1
3.2 Adjustments to the Role Mapping We further extend the FM, which has ex- tensive knowledge of VerbNet, for the separate task of helping to eliminate some of the inconsistencies that are introduced by our role mapping procedure.	FM	F-measures	0
3.2 Adjustments to the Role Mapping We further extend the FM, which has ex- tensive knowledge of VerbNet, for the separate task of helping to eliminate some of the inconsistencies that are introduced by our role mapping procedure.	FM	F -measure	0
3.1 Initialization of Candidate Roles The FM construes extracted arguments from the parsed sentence as being in one of the four main types of syntactic positions (or slots) used by VerbNet frames: subject, object, indirect object, and PP-object.3 Additionally, we specialize the lat- ter by the individual preposition, such as ?	FM	frame matcher	1
3.1 Initialization of Candidate Roles The FM construes extracted arguments from the parsed sentence as being in one of the four main types of syntactic positions (or slots) used by VerbNet frames: subject, object, indirect object, and PP-object.3 Additionally, we specialize the lat- ter by the individual preposition, such as ?	FM	F-measures	0
3.1 Initialization of Candidate Roles The FM construes extracted arguments from the parsed sentence as being in one of the four main types of syntactic positions (or slots) used by VerbNet frames: subject, object, indirect object, and PP-object.3 Additionally, we specialize the lat- ter by the individual preposition, such as ?	FM	F -measure	0
4 The Bootstrapping Algorithm We have described the FM that produces a set of slots with candidate role lists (some unam- biguous), and our backoff probability model.	FM	frame matcher	1
4 The Bootstrapping Algorithm We have described the FM that produces a set of slots with candidate role lists (some unam- biguous), and our backoff probability model.	FM	F-measures	0
4 The Bootstrapping Algorithm We have described the FM that produces a set of slots with candidate role lists (some unam- biguous), and our backoff probability model.	FM	F -measure	0
The automatic FM aligns arguments extracted from an automatically parsed sentence with the frames in VerbNet for the target verb in the sentence.	FM	frame matcher	1
The automatic FM aligns arguments extracted from an automatically parsed sentence with the frames in VerbNet for the target verb in the sentence.	FM	F-measures	0
The automatic FM aligns arguments extracted from an automatically parsed sentence with the frames in VerbNet for the target verb in the sentence.	FM	F -measure	0
FM for our categories range from .61 (TEXTUAL) and .52 (AIM) to .45 (BACKGROUND), .38 (BASIS), and .26 (CONTRAST).	FM	frame matcher	0
FM for our categories range from .61 (TEXTUAL) and .52 (AIM) to .45 (BACKGROUND), .38 (BASIS), and .26 (CONTRAST).	FM	F-measures	1
FM for our categories range from .61 (TEXTUAL) and .52 (AIM) to .45 (BACKGROUND), .38 (BASIS), and .26 (CONTRAST).	FM	F -measure	0
Tables 3 and 4 show the improvement of C-E and A-E alignment FM with the confidence-based alignment link filtering (ALF).	FM	frame matcher	0
Tables 3 and 4 show the improvement of C-E and A-E alignment FM with the confidence-based alignment link filtering (ALF).	FM	F-measures	1
Tables 3 and 4 show the improvement of C-E and A-E alignment FM with the confidence-based alignment link filtering (ALF).	FM	F -measure	0
Performing 5-fold cross validation on the nwire and bnews corpora, (Jiang and Zhai, 2007) and (Chan and Roth, 2010) reported FM of 71.5 and 71.2, respectively.	FM	frame matcher	0
Performing 5-fold cross validation on the nwire and bnews corpora, (Jiang and Zhai, 2007) and (Chan and Roth, 2010) reported FM of 71.5 and 71.2, respectively.	FM	F-measures	1
Performing 5-fold cross validation on the nwire and bnews corpora, (Jiang and Zhai, 2007) and (Chan and Roth, 2010) reported FM of 71.5 and 71.2, respectively.	FM	F -measure	0
Macro-F is the mean of the FM of all seven categories.	FM	frame matcher	0
Macro-F is the mean of the FM of all seven categories.	FM	F-measures	1
Macro-F is the mean of the FM of all seven categories.	FM	F -measure	0
Table 4 shows the NER results obtained by the meth- ods without considering NER-level rejection (i.e., to = 0), using threshold tw = 0.4 for Conf-A, Proposed, and Conf-Reject, which resulted in the best NER FM (see Table 5).	FM	frame matcher	0
Table 4 shows the NER results obtained by the meth- ods without considering NER-level rejection (i.e., to = 0), using threshold tw = 0.4 for Conf-A, Proposed, and Conf-Reject, which resulted in the best NER FM (see Table 5).	FM	F-measures	1
Table 4 shows the NER results obtained by the meth- ods without considering NER-level rejection (i.e., to = 0), using threshold tw = 0.4 for Conf-A, Proposed, and Conf-Reject, which resulted in the best NER FM (see Table 5).	FM	F -measure	0
The scored results show that our CWS can  increase the Bakeoff baseline system with 4.86%  and 5.04% FM for the CKIP and the CityU  word segmentation tasks, respectively.	FM	frame matcher	0
The scored results show that our CWS can  increase the Bakeoff baseline system with 4.86%  and 5.04% FM for the CKIP and the CityU  word segmentation tasks, respectively.	FM	F-measures	1
The scored results show that our CWS can  increase the Bakeoff baseline system with 4.86%  and 5.04% FM for the CKIP and the CityU  word segmentation tasks, respectively.	FM	F -measure	0
This system has achieved competitive results with an FM of 82.7 when trained on the seven main types of ACE data with access to wordnet and part-of-speech-tag informa- tion as well as output of other MD and named-entity recognizers (Zitouni and Florian, 2008).	FM	frame matcher	0
This system has achieved competitive results with an FM of 82.7 when trained on the seven main types of ACE data with access to wordnet and part-of-speech-tag informa- tion as well as output of other MD and named-entity recognizers (Zitouni and Florian, 2008).	FM	F-measures	0
This system has achieved competitive results with an FM of 82.7 when trained on the seven main types of ACE data with access to wordnet and part-of-speech-tag informa- tion as well as output of other MD and named-entity recognizers (Zitouni and Florian, 2008).	FM	F -measure	1
Precision/recall/FM re- sults are shown in Table 2.	FM	frame matcher	0
Precision/recall/FM re- sults are shown in Table 2.	FM	F-measures	0
Precision/recall/FM re- sults are shown in Table 2.	FM	F -measure	1
This module achieved an FM of 68% (Amaral et al, 2007).	FM	frame matcher	0
This module achieved an FM of 68% (Amaral et al, 2007).	FM	F-measures	0
This module achieved an FM of 68% (Amaral et al, 2007).	FM	F -measure	1
But system combination serves us well: it recovers all but 0.5 FM point of this loss, while also ac- tually performing better on the noisy data sets than the two classifiers specifically targeted toward them, as can be seen in Table 2.	FM	frame matcher	0
But system combination serves us well: it recovers all but 0.5 FM point of this loss, while also ac- tually performing better on the noisy data sets than the two classifiers specifically targeted toward them, as can be seen in Table 2.	FM	F-measures	0
But system combination serves us well: it recovers all but 0.5 FM point of this loss, while also ac- tually performing better on the noisy data sets than the two classifiers specifically targeted toward them, as can be seen in Table 2.	FM	F -measure	1
This module achieved an FM of 56% and SER (Slot Error Rate, the measure commonly used to evaluate this kind of task) of 0.74.	FM	frame matcher	0
This module achieved an FM of 56% and SER (Slot Error Rate, the measure commonly used to evaluate this kind of task) of 0.74.	FM	F-measures	0
This module achieved an FM of 56% and SER (Slot Error Rate, the measure commonly used to evaluate this kind of task) of 0.74.	FM	F -measure	1
However, because the mixed classifier, and moreso the gazetteer classifier, are oriented to noisy data, on clean data they suffer in performance by 2.5 and 5 FM points, respectively.	FM	frame matcher	0
However, because the mixed classifier, and moreso the gazetteer classifier, are oriented to noisy data, on clean data they suffer in performance by 2.5 and 5 FM points, respectively.	FM	F-measures	0
However, because the mixed classifier, and moreso the gazetteer classifier, are oriented to noisy data, on clean data they suffer in performance by 2.5 and 5 FM points, respectively.	FM	F -measure	1
Performance is presented in terms of Precision (P), Recall (R), and FM (F).	FM	frame matcher	0
Performance is presented in terms of Precision (P), Recall (R), and FM (F).	FM	F-measures	0
Performance is presented in terms of Precision (P), Recall (R), and FM (F).	FM	F -measure	1
7 Related Work Dawid and Skene (1979) investigated filtering annotations using the EM algorithm, estimating annotator-specific ERR in the context of patient medical records.	ERR	error rates	1
7 Related Work Dawid and Skene (1979) investigated filtering annotations using the EM algorithm, estimating annotator-specific ERR in the context of patient medical records.	ERR	error rate reductions	0
This shift did not nec- essarily translate to higher ERR, especially after optimizations.	ERR	error rates	1
This shift did not nec- essarily translate to higher ERR, especially after optimizations.	ERR	error rate reductions	0
Percentage denotes the ERR, i.e. the number of err	ERR	error rates	1
Percentage denotes the ERR, i.e. the number of err	ERR	error rate reductions	0
However, for sensitive applications where ERR must practically be zero, or other situations where speech recognition ERR are too high, we are currently developing a real-time correction interface.	ERR	error rates	1
However, for sensitive applications where ERR must practically be zero, or other situations where speech recognition ERR are too high, we are currently developing a real-time correction interface.	ERR	error rate reductions	0
The low ERR are the key reason the error cor- rection task is so difficult: it is quite challenging for a system to improve over a writer that already per- forms at the level of over 90%.	ERR	error rates	1
The low ERR are the key reason the error cor- rection task is so difficult: it is quite challenging for a system to improve over a writer that already per- forms at the level of over 90%.	ERR	error rate reductions	0
Table 1 shows the number of mistakes1 of each type and the ERR, i.e. the percentage of erroneous words by error type.	ERR	error rates	1
Table 1 shows the number of mistakes1 of each type and the ERR, i.e. the percentage of erroneous words by error type.	ERR	error rate reductions	0
Percentage denotes the ERR, i.e. the number of erroneous instances with respect to the to- tal number of relevant instances in the data.	ERR	error rates	1
Percentage denotes the ERR, i.e. the number of erroneous instances with respect to the to- tal number of relevant instances in the data.	ERR	error rate reductions	0
of mistakes1 of each type and the ERR, i.e. the percentage of erroneous words by error type.	ERR	error rates	1
of mistakes1 of each type and the ERR, i.e. the percentage of erroneous words by error type.	ERR	error rate reductions	0
In  section 5, we present word alignment results that  show significant alignment ERR  compared to the baseline HMM and IBM model 4.	ERR	error rates	0
In  section 5, we present word alignment results that  show significant alignment ERR  compared to the baseline HMM and IBM model 4.	ERR	error rate reductions	1
Even so, we are able to achieve ERR ranging from 6.52% to 63.41% for TWA, and from 3.33% to 34.62% for SEMEVAL.	ERR	error rates	0
Even so, we are able to achieve ERR ranging from 6.52% to 63.41% for TWA, and from 3.33% to 34.62% for SEMEVAL.	ERR	error rate reductions	1
We present experimen- tal results for heavily pruned backoff n- gram models, and demonstrate perplexity and word ERR when used with various baseline smoothing methods.	ERR	error rates	0
We present experimen- tal results for heavily pruned backoff n- gram models, and demonstrate perplexity and word ERR when used with various baseline smoothing methods.	ERR	error rate reductions	1
We translate the context of an ambiguous word in multiple languages, and show through experiments on standard datasets that by using a multilingual vector space we can obtain ERR of up to 25%, as compared to a monolingual classifier.	ERR	error rates	0
We translate the context of an ambiguous word in multiple languages, and show through experiments on standard datasets that by using a multilingual vector space we can obtain ERR of up to 25%, as compared to a monolingual classifier.	ERR	error rate reductions	1
We now look at the impacts on system perfor- mance we can achieve with these new models4, and whether the perplexity differences that we ob- serve translate to real ERR.	ERR	error rates	0
We now look at the impacts on system perfor- mance we can achieve with these new models4, and whether the perplexity differences that we ob- serve translate to real ERR.	ERR	error rate reductions	1
Using only low-level features capturing when partic- ipants choose to vocalize relative to one another, it attains relative ERR on unseen data of 37%, 67%, and 40% over chance on classifying role, leadership, and seniority, respectively.	ERR	error rates	0
Using only low-level features capturing when partic- ipants choose to vocalize relative to one another, it attains relative ERR on unseen data of 37%, 67%, and 40% over chance on classifying role, leadership, and seniority, respectively.	ERR	error rate reductions	1
Fortunately, work is being done on verb subcategorization frames in HI.5 We plan to incorporate this information into the Urdu grammar verb lexicon.	HI	Hindi	1
Fortunately, work is being done on verb subcategorization frames in HI.5 We plan to incorporate this information into the Urdu grammar verb lexicon.	HI	Harvard Inquirer	0
In addition, a guesser can be added to guess words that the morphology does not yet recognize (Chanod 4A web search on HI dictionary results in several promising sites.	HI	Hindi	1
In addition, a guesser can be added to guess words that the morphology does not yet recognize (Chanod 4A web search on HI dictionary results in several promising sites.	HI	Harvard Inquirer	0
While some morphological analyzers al- ready exist for HI,3 e.g., as part of the tools developed at the Language Technolo- gies Research Centre (LTRC), IIT Hyderabad (http://www.iiit.net/ltrc/index.html), they are not immediately compatible with the XLE grammar development platform, nor is it clear that the morphological analyses they produce conform to the standards and methods developed within the ParGram project.	HI	Hindi	1
While some morphological analyzers al- ready exist for HI,3 e.g., as part of the tools developed at the Language Technolo- gies Research Centre (LTRC), IIT Hyderabad (http://www.iiit.net/ltrc/index.html), they are not immediately compatible with the XLE grammar development platform, nor is it clear that the morphological analyses they produce conform to the standards and methods developed within the ParGram project.	HI	Harvard Inquirer	0
Even within a linguistic formalism, LFG for Par- Gram, there is often more than one way to ana- 5One significant effort is the HI Verb Project run by Prof. Alice Davison at the University of Iowa; further information is available via their web site.	HI	Hindi	1
Even within a linguistic formalism, LFG for Par- Gram, there is often more than one way to ana- 5One significant effort is the HI Verb Project run by Prof. Alice Davison at the University of Iowa; further information is available via their web site.	HI	Harvard Inquirer	0
5 Script One issue that has not been dealt with in the Urdu grammar is the different script systems used for Urdu and HI.	HI	Hindi	1
5 Script One issue that has not been dealt with in the Urdu grammar is the different script systems used for Urdu and HI.	HI	Harvard Inquirer	0
Transliteration by analogical learning has been attempted by Dandapat et al (2010) for an English-to-HI transliteration task.	HI	Hindi	1
Transliteration by analogical learning has been attempted by Dandapat et al (2010) for an English-to-HI transliteration task.	HI	Harvard Inquirer	0
scores for English dataset derived from the HI project.	HI	Hindi	0
scores for English dataset derived from the HI project.	HI	Harvard Inquirer	1
We then re- placed sentiment words with a sentiment cat- egory identifier using the sentiment lexica of the HI (Stone, 1966) and LIWC (Pennebaker et al, 2007).	HI	Hindi	0
We then re- placed sentiment words with a sentiment cat- egory identifier using the sentiment lexica of the HI (Stone, 1966) and LIWC (Pennebaker et al, 2007).	HI	Harvard Inquirer	1
Of the string-similarity features, we reused the LCS, Longest Common Subsequence (with and without normaliza- tion), and Greedy String Tiling measures.	LCS	Longest Common Substring	1
Of the string-similarity features, we reused the LCS, Longest Common Subsequence (with and without normaliza- tion), and Greedy String Tiling measures.	LCS	lexical conceptual structure	0
Of the string-similarity features, we reused the LCS, Longest Common Subsequence (with and without normaliza- tion), and Greedy String Tiling measures.	LCS	longest common substring	0
Of the string-similarity features, we reused the LCS, Longest Common Subsequence (with and without normaliza- tion), and Greedy String Tiling measures.	LCS	least common subsumer	0
Of the string-similarity features, we reused the LCS, Longest Common Subsequence (with and without normaliza- tion), and Greedy String Tiling measures.	LCS	Longest Common Subsequenc	0
Of the string-similarity features, we reused the LCS, Longest Common Subsequence (with and without normaliza- tion), and Greedy String Tiling measures.	LCS	Longest Common Sub	0
Of the string-similarity features, we reused the LCS, Longest Common Subsequence (with and without normaliza- tion), and Greedy String Tiling measures.	LCS	longest common subsequence	0
Of the string-similarity features, we reused the LCS, Longest Common Subsequence (with and without normaliza- tion), and Greedy String Tiling measures.	LCS	longer common sequence	0
Of the string-similarity features, we reused the LCS, Longest Common Subsequence (with and without normaliza- tion), and Greedy String Tiling measures.	LCS	lexical conccplual structure	0
LCSsComputed between Two Sentences 17.08.08 |  Computer Science Department | Ubiquitous Knowledge Processing Lab  |  75/206 Example of Tiling for Derived and Non-Derived Text (from Clough 2003) ?	LCS	Longest Common Substring	1
LCSsComputed between Two Sentences 17.08.08 |  Computer Science Department | Ubiquitous Knowledge Processing Lab  |  75/206 Example of Tiling for Derived and Non-Derived Text (from Clough 2003) ?	LCS	lexical conceptual structure	0
LCSsComputed between Two Sentences 17.08.08 |  Computer Science Department | Ubiquitous Knowledge Processing Lab  |  75/206 Example of Tiling for Derived and Non-Derived Text (from Clough 2003) ?	LCS	longest common substring	0
LCSsComputed between Two Sentences 17.08.08 |  Computer Science Department | Ubiquitous Knowledge Processing Lab  |  75/206 Example of Tiling for Derived and Non-Derived Text (from Clough 2003) ?	LCS	least common subsumer	0
LCSsComputed between Two Sentences 17.08.08 |  Computer Science Department | Ubiquitous Knowledge Processing Lab  |  75/206 Example of Tiling for Derived and Non-Derived Text (from Clough 2003) ?	LCS	Longest Common Subsequenc	0
LCSsComputed between Two Sentences 17.08.08 |  Computer Science Department | Ubiquitous Knowledge Processing Lab  |  75/206 Example of Tiling for Derived and Non-Derived Text (from Clough 2003) ?	LCS	Longest Common Sub	0
LCSsComputed between Two Sentences 17.08.08 |  Computer Science Department | Ubiquitous Knowledge Processing Lab  |  75/206 Example of Tiling for Derived and Non-Derived Text (from Clough 2003) ?	LCS	longest common subsequence	0
LCSsComputed between Two Sentences 17.08.08 |  Computer Science Department | Ubiquitous Knowledge Processing Lab  |  75/206 Example of Tiling for Derived and Non-Derived Text (from Clough 2003) ?	LCS	longer common sequence	0
LCSsComputed between Two Sentences 17.08.08 |  Computer Science Department | Ubiquitous Knowledge Processing Lab  |  75/206 Example of Tiling for Derived and Non-Derived Text (from Clough 2003) ?	LCS	lexical conccplual structure	0
LCSsComputed between Two Sentences 17.08.08 |  Computer Science Department | Ubiquitous Knowledge Processing Lab  |  74/206 ?	LCS	Longest Common Substring	1
LCSsComputed between Two Sentences 17.08.08 |  Computer Science Department | Ubiquitous Knowledge Processing Lab  |  74/206 ?	LCS	lexical conceptual structure	0
LCSsComputed between Two Sentences 17.08.08 |  Computer Science Department | Ubiquitous Knowledge Processing Lab  |  74/206 ?	LCS	longest common substring	0
LCSsComputed between Two Sentences 17.08.08 |  Computer Science Department | Ubiquitous Knowledge Processing Lab  |  74/206 ?	LCS	least common subsumer	0
LCSsComputed between Two Sentences 17.08.08 |  Computer Science Department | Ubiquitous Knowledge Processing Lab  |  74/206 ?	LCS	Longest Common Subsequenc	0
LCSsComputed between Two Sentences 17.08.08 |  Computer Science Department | Ubiquitous Knowledge Processing Lab  |  74/206 ?	LCS	Longest Common Sub	0
LCSsComputed between Two Sentences 17.08.08 |  Computer Science Department | Ubiquitous Knowledge Processing Lab  |  74/206 ?	LCS	longest common subsequence	0
LCSsComputed between Two Sentences 17.08.08 |  Computer Science Department | Ubiquitous Knowledge Processing Lab  |  74/206 ?	LCS	longer common sequence	0
LCSsComputed between Two Sentences 17.08.08 |  Computer Science Department | Ubiquitous Knowledge Processing Lab  |  74/206 ?	LCS	lexical conccplual structure	0
As  Dowry (1989), Jackendoff (1987, 1990) and oth-  ers have noted, LCS is  needed for correct semantic interpretation. (	LCS	Longest Common Substring	0
As  Dowry (1989), Jackendoff (1987, 1990) and oth-  ers have noted, LCS is  needed for correct semantic interpretation. (	LCS	lexical conceptual structure	1
As  Dowry (1989), Jackendoff (1987, 1990) and oth-  ers have noted, LCS is  needed for correct semantic interpretation. (	LCS	longest common substring	0
As  Dowry (1989), Jackendoff (1987, 1990) and oth-  ers have noted, LCS is  needed for correct semantic interpretation. (	LCS	least common subsumer	0
As  Dowry (1989), Jackendoff (1987, 1990) and oth-  ers have noted, LCS is  needed for correct semantic interpretation. (	LCS	Longest Common Subsequenc	0
As  Dowry (1989), Jackendoff (1987, 1990) and oth-  ers have noted, LCS is  needed for correct semantic interpretation. (	LCS	Longest Common Sub	0
As  Dowry (1989), Jackendoff (1987, 1990) and oth-  ers have noted, LCS is  needed for correct semantic interpretation. (	LCS	longest common subsequence	0
As  Dowry (1989), Jackendoff (1987, 1990) and oth-  ers have noted, LCS is  needed for correct semantic interpretation. (	LCS	longer common sequence	0
As  Dowry (1989), Jackendoff (1987, 1990) and oth-  ers have noted, LCS is  needed for correct semantic interpretation. (	LCS	lexical conccplual structure	0
This sentence is processed by the system  and the following LCS is  obtained:  [Event GO Loc   ([Thing JOHN],   [Path TO Loc ([Position AT Loc ([Thing JOHN],  [Property HOUSE])])],   [Manner RUNNINGLY])]  which is stored by the tutoring system and later  matched against the student?s answer.	LCS	Longest Common Substring	0
This sentence is processed by the system  and the following LCS is  obtained:  [Event GO Loc   ([Thing JOHN],   [Path TO Loc ([Position AT Loc ([Thing JOHN],  [Property HOUSE])])],   [Manner RUNNINGLY])]  which is stored by the tutoring system and later  matched against the student?s answer.	LCS	lexical conceptual structure	1
This sentence is processed by the system  and the following LCS is  obtained:  [Event GO Loc   ([Thing JOHN],   [Path TO Loc ([Position AT Loc ([Thing JOHN],  [Property HOUSE])])],   [Manner RUNNINGLY])]  which is stored by the tutoring system and later  matched against the student?s answer.	LCS	longest common substring	0
This sentence is processed by the system  and the following LCS is  obtained:  [Event GO Loc   ([Thing JOHN],   [Path TO Loc ([Position AT Loc ([Thing JOHN],  [Property HOUSE])])],   [Manner RUNNINGLY])]  which is stored by the tutoring system and later  matched against the student?s answer.	LCS	least common subsumer	0
This sentence is processed by the system  and the following LCS is  obtained:  [Event GO Loc   ([Thing JOHN],   [Path TO Loc ([Position AT Loc ([Thing JOHN],  [Property HOUSE])])],   [Manner RUNNINGLY])]  which is stored by the tutoring system and later  matched against the student?s answer.	LCS	Longest Common Subsequenc	0
This sentence is processed by the system  and the following LCS is  obtained:  [Event GO Loc   ([Thing JOHN],   [Path TO Loc ([Position AT Loc ([Thing JOHN],  [Property HOUSE])])],   [Manner RUNNINGLY])]  which is stored by the tutoring system and later  matched against the student?s answer.	LCS	Longest Common Sub	0
This sentence is processed by the system  and the following LCS is  obtained:  [Event GO Loc   ([Thing JOHN],   [Path TO Loc ([Position AT Loc ([Thing JOHN],  [Property HOUSE])])],   [Manner RUNNINGLY])]  which is stored by the tutoring system and later  matched against the student?s answer.	LCS	longest common subsequence	0
This sentence is processed by the system  and the following LCS is  obtained:  [Event GO Loc   ([Thing JOHN],   [Path TO Loc ([Position AT Loc ([Thing JOHN],  [Property HOUSE])])],   [Manner RUNNINGLY])]  which is stored by the tutoring system and later  matched against the student?s answer.	LCS	longer common sequence	0
This sentence is processed by the system  and the following LCS is  obtained:  [Event GO Loc   ([Thing JOHN],   [Path TO Loc ([Position AT Loc ([Thing JOHN],  [Property HOUSE])])],   [Manner RUNNINGLY])]  which is stored by the tutoring system and later  matched against the student?s answer.	LCS	lexical conccplual structure	0
predictions regarding complex event  nominals uch as those illustrated in (29), it fails  on nominals that do not represent complex events  as in (30)  (29) a. TREE felling  b. COOKIE baking  (30) a. HISTORY teacher  b. BIT guzzler  1 believe the distinction is a semantic one involv-  ing 0-participants in LCS  (Ics).	LCS	Longest Common Substring	0
predictions regarding complex event  nominals uch as those illustrated in (29), it fails  on nominals that do not represent complex events  as in (30)  (29) a. TREE felling  b. COOKIE baking  (30) a. HISTORY teacher  b. BIT guzzler  1 believe the distinction is a semantic one involv-  ing 0-participants in LCS  (Ics).	LCS	lexical conceptual structure	1
predictions regarding complex event  nominals uch as those illustrated in (29), it fails  on nominals that do not represent complex events  as in (30)  (29) a. TREE felling  b. COOKIE baking  (30) a. HISTORY teacher  b. BIT guzzler  1 believe the distinction is a semantic one involv-  ing 0-participants in LCS  (Ics).	LCS	longest common substring	0
predictions regarding complex event  nominals uch as those illustrated in (29), it fails  on nominals that do not represent complex events  as in (30)  (29) a. TREE felling  b. COOKIE baking  (30) a. HISTORY teacher  b. BIT guzzler  1 believe the distinction is a semantic one involv-  ing 0-participants in LCS  (Ics).	LCS	least common subsumer	0
predictions regarding complex event  nominals uch as those illustrated in (29), it fails  on nominals that do not represent complex events  as in (30)  (29) a. TREE felling  b. COOKIE baking  (30) a. HISTORY teacher  b. BIT guzzler  1 believe the distinction is a semantic one involv-  ing 0-participants in LCS  (Ics).	LCS	Longest Common Subsequenc	0
predictions regarding complex event  nominals uch as those illustrated in (29), it fails  on nominals that do not represent complex events  as in (30)  (29) a. TREE felling  b. COOKIE baking  (30) a. HISTORY teacher  b. BIT guzzler  1 believe the distinction is a semantic one involv-  ing 0-participants in LCS  (Ics).	LCS	Longest Common Sub	0
predictions regarding complex event  nominals uch as those illustrated in (29), it fails  on nominals that do not represent complex events  as in (30)  (29) a. TREE felling  b. COOKIE baking  (30) a. HISTORY teacher  b. BIT guzzler  1 believe the distinction is a semantic one involv-  ing 0-participants in LCS  (Ics).	LCS	longest common subsequence	0
predictions regarding complex event  nominals uch as those illustrated in (29), it fails  on nominals that do not represent complex events  as in (30)  (29) a. TREE felling  b. COOKIE baking  (30) a. HISTORY teacher  b. BIT guzzler  1 believe the distinction is a semantic one involv-  ing 0-participants in LCS  (Ics).	LCS	longer common sequence	0
predictions regarding complex event  nominals uch as those illustrated in (29), it fails  on nominals that do not represent complex events  as in (30)  (29) a. TREE felling  b. COOKIE baking  (30) a. HISTORY teacher  b. BIT guzzler  1 believe the distinction is a semantic one involv-  ing 0-participants in LCS  (Ics).	LCS	lexical conccplual structure	0
Generation from LCS.	LCS	Longest Common Substring	0
Generation from LCS.	LCS	lexical conceptual structure	1
Generation from LCS.	LCS	longest common substring	0
Generation from LCS.	LCS	least common subsumer	0
Generation from LCS.	LCS	Longest Common Subsequenc	0
Generation from LCS.	LCS	Longest Common Sub	0
Generation from LCS.	LCS	longest common subsequence	0
Generation from LCS.	LCS	longer common sequence	0
Generation from LCS.	LCS	lexical conccplual structure	0
For while rule (16) would  make correct predictions regarding complex event  nominals uch as those illustrated in (29), it fails  on nominals that do not represent complex events  as in (30)  (29) a. TREE felling  b. COOKIE baking  (30) a. HISTORY teacher  b. BIT guzzler  1 believe the distinction is a semantic one involv-  ing 0-participants in LCS  (Ics).	LCS	Longest Common Substring	0
For while rule (16) would  make correct predictions regarding complex event  nominals uch as those illustrated in (29), it fails  on nominals that do not represent complex events  as in (30)  (29) a. TREE felling  b. COOKIE baking  (30) a. HISTORY teacher  b. BIT guzzler  1 believe the distinction is a semantic one involv-  ing 0-participants in LCS  (Ics).	LCS	lexical conceptual structure	1
For while rule (16) would  make correct predictions regarding complex event  nominals uch as those illustrated in (29), it fails  on nominals that do not represent complex events  as in (30)  (29) a. TREE felling  b. COOKIE baking  (30) a. HISTORY teacher  b. BIT guzzler  1 believe the distinction is a semantic one involv-  ing 0-participants in LCS  (Ics).	LCS	longest common substring	0
For while rule (16) would  make correct predictions regarding complex event  nominals uch as those illustrated in (29), it fails  on nominals that do not represent complex events  as in (30)  (29) a. TREE felling  b. COOKIE baking  (30) a. HISTORY teacher  b. BIT guzzler  1 believe the distinction is a semantic one involv-  ing 0-participants in LCS  (Ics).	LCS	least common subsumer	0
For while rule (16) would  make correct predictions regarding complex event  nominals uch as those illustrated in (29), it fails  on nominals that do not represent complex events  as in (30)  (29) a. TREE felling  b. COOKIE baking  (30) a. HISTORY teacher  b. BIT guzzler  1 believe the distinction is a semantic one involv-  ing 0-participants in LCS  (Ics).	LCS	Longest Common Subsequenc	0
For while rule (16) would  make correct predictions regarding complex event  nominals uch as those illustrated in (29), it fails  on nominals that do not represent complex events  as in (30)  (29) a. TREE felling  b. COOKIE baking  (30) a. HISTORY teacher  b. BIT guzzler  1 believe the distinction is a semantic one involv-  ing 0-participants in LCS  (Ics).	LCS	Longest Common Sub	0
For while rule (16) would  make correct predictions regarding complex event  nominals uch as those illustrated in (29), it fails  on nominals that do not represent complex events  as in (30)  (29) a. TREE felling  b. COOKIE baking  (30) a. HISTORY teacher  b. BIT guzzler  1 believe the distinction is a semantic one involv-  ing 0-participants in LCS  (Ics).	LCS	longest common subsequence	0
For while rule (16) would  make correct predictions regarding complex event  nominals uch as those illustrated in (29), it fails  on nominals that do not represent complex events  as in (30)  (29) a. TREE felling  b. COOKIE baking  (30) a. HISTORY teacher  b. BIT guzzler  1 believe the distinction is a semantic one involv-  ing 0-participants in LCS  (Ics).	LCS	longer common sequence	0
For while rule (16) would  make correct predictions regarding complex event  nominals uch as those illustrated in (29), it fails  on nominals that do not represent complex events  as in (30)  (29) a. TREE felling  b. COOKIE baking  (30) a. HISTORY teacher  b. BIT guzzler  1 believe the distinction is a semantic one involv-  ing 0-participants in LCS  (Ics).	LCS	lexical conccplual structure	0
Every verb and noun (including deverbal  nouns) has a LCS that in-  cludes the entities involved in the events or states  described (see, for example, Dowty (1989),  Fillmore (1968), and Jackendoff (1987, 1990)).	LCS	Longest Common Substring	0
Every verb and noun (including deverbal  nouns) has a LCS that in-  cludes the entities involved in the events or states  described (see, for example, Dowty (1989),  Fillmore (1968), and Jackendoff (1987, 1990)).	LCS	lexical conceptual structure	1
Every verb and noun (including deverbal  nouns) has a LCS that in-  cludes the entities involved in the events or states  described (see, for example, Dowty (1989),  Fillmore (1968), and Jackendoff (1987, 1990)).	LCS	longest common substring	0
Every verb and noun (including deverbal  nouns) has a LCS that in-  cludes the entities involved in the events or states  described (see, for example, Dowty (1989),  Fillmore (1968), and Jackendoff (1987, 1990)).	LCS	least common subsumer	0
Every verb and noun (including deverbal  nouns) has a LCS that in-  cludes the entities involved in the events or states  described (see, for example, Dowty (1989),  Fillmore (1968), and Jackendoff (1987, 1990)).	LCS	Longest Common Subsequenc	0
Every verb and noun (including deverbal  nouns) has a LCS that in-  cludes the entities involved in the events or states  described (see, for example, Dowty (1989),  Fillmore (1968), and Jackendoff (1987, 1990)).	LCS	Longest Common Sub	0
Every verb and noun (including deverbal  nouns) has a LCS that in-  cludes the entities involved in the events or states  described (see, for example, Dowty (1989),  Fillmore (1968), and Jackendoff (1987, 1990)).	LCS	longest common subsequence	0
Every verb and noun (including deverbal  nouns) has a LCS that in-  cludes the entities involved in the events or states  described (see, for example, Dowty (1989),  Fillmore (1968), and Jackendoff (1987, 1990)).	LCS	longer common sequence	0
Every verb and noun (including deverbal  nouns) has a LCS that in-  cludes the entities involved in the events or states  described (see, for example, Dowty (1989),  Fillmore (1968), and Jackendoff (1987, 1990)).	LCS	lexical conccplual structure	0
Resnik and Diab [2000], in which similarity is computed according to the structure, rather than content, of LCS representations of verbs; see Jackendoff [1983] and Dorr [1993]).	LCS	Longest Common Substring	0
Resnik and Diab [2000], in which similarity is computed according to the structure, rather than content, of LCS representations of verbs; see Jackendoff [1983] and Dorr [1993]).	LCS	lexical conceptual structure	1
Resnik and Diab [2000], in which similarity is computed according to the structure, rather than content, of LCS representations of verbs; see Jackendoff [1983] and Dorr [1993]).	LCS	longest common substring	0
Resnik and Diab [2000], in which similarity is computed according to the structure, rather than content, of LCS representations of verbs; see Jackendoff [1983] and Dorr [1993]).	LCS	least common subsumer	0
Resnik and Diab [2000], in which similarity is computed according to the structure, rather than content, of LCS representations of verbs; see Jackendoff [1983] and Dorr [1993]).	LCS	Longest Common Subsequenc	0
Resnik and Diab [2000], in which similarity is computed according to the structure, rather than content, of LCS representations of verbs; see Jackendoff [1983] and Dorr [1993]).	LCS	Longest Common Sub	0
Resnik and Diab [2000], in which similarity is computed according to the structure, rather than content, of LCS representations of verbs; see Jackendoff [1983] and Dorr [1993]).	LCS	longest common subsequence	0
Resnik and Diab [2000], in which similarity is computed according to the structure, rather than content, of LCS representations of verbs; see Jackendoff [1983] and Dorr [1993]).	LCS	longer common sequence	0
Resnik and Diab [2000], in which similarity is computed according to the structure, rather than content, of LCS representations of verbs; see Jackendoff [1983] and Dorr [1993]).	LCS	lexical conccplual structure	0
From the downloaded corpus, we extract sentences such as Dickens was born in 1812 Dickens (1812 - 1870) was an English writer Dickens (1812 - 1870) wrote Oliver Twist The system identifies that the contexts of the last two sentences are very similar and chooses their LCS to produce the follow- ing patterns: <hook> was born in <target> <hook> ( <target> - 1870 ) In order to measure the precision of the ex- tracted patterns, a new corpus is downloaded us- ing the hook Dickens as the only query word, and the system looks for appearances of the patterns in the corpus.	LCS	Longest Common Substring	0
From the downloaded corpus, we extract sentences such as Dickens was born in 1812 Dickens (1812 - 1870) was an English writer Dickens (1812 - 1870) wrote Oliver Twist The system identifies that the contexts of the last two sentences are very similar and chooses their LCS to produce the follow- ing patterns: <hook> was born in <target> <hook> ( <target> - 1870 ) In order to measure the precision of the ex- tracted patterns, a new corpus is downloaded us- ing the hook Dickens as the only query word, and the system looks for appearances of the patterns in the corpus.	LCS	lexical conceptual structure	0
From the downloaded corpus, we extract sentences such as Dickens was born in 1812 Dickens (1812 - 1870) was an English writer Dickens (1812 - 1870) wrote Oliver Twist The system identifies that the contexts of the last two sentences are very similar and chooses their LCS to produce the follow- ing patterns: <hook> was born in <target> <hook> ( <target> - 1870 ) In order to measure the precision of the ex- tracted patterns, a new corpus is downloaded us- ing the hook Dickens as the only query word, and the system looks for appearances of the patterns in the corpus.	LCS	longest common substring	1
From the downloaded corpus, we extract sentences such as Dickens was born in 1812 Dickens (1812 - 1870) was an English writer Dickens (1812 - 1870) wrote Oliver Twist The system identifies that the contexts of the last two sentences are very similar and chooses their LCS to produce the follow- ing patterns: <hook> was born in <target> <hook> ( <target> - 1870 ) In order to measure the precision of the ex- tracted patterns, a new corpus is downloaded us- ing the hook Dickens as the only query word, and the system looks for appearances of the patterns in the corpus.	LCS	least common subsumer	0
From the downloaded corpus, we extract sentences such as Dickens was born in 1812 Dickens (1812 - 1870) was an English writer Dickens (1812 - 1870) wrote Oliver Twist The system identifies that the contexts of the last two sentences are very similar and chooses their LCS to produce the follow- ing patterns: <hook> was born in <target> <hook> ( <target> - 1870 ) In order to measure the precision of the ex- tracted patterns, a new corpus is downloaded us- ing the hook Dickens as the only query word, and the system looks for appearances of the patterns in the corpus.	LCS	Longest Common Subsequenc	0
From the downloaded corpus, we extract sentences such as Dickens was born in 1812 Dickens (1812 - 1870) was an English writer Dickens (1812 - 1870) wrote Oliver Twist The system identifies that the contexts of the last two sentences are very similar and chooses their LCS to produce the follow- ing patterns: <hook> was born in <target> <hook> ( <target> - 1870 ) In order to measure the precision of the ex- tracted patterns, a new corpus is downloaded us- ing the hook Dickens as the only query word, and the system looks for appearances of the patterns in the corpus.	LCS	Longest Common Sub	0
From the downloaded corpus, we extract sentences such as Dickens was born in 1812 Dickens (1812 - 1870) was an English writer Dickens (1812 - 1870) wrote Oliver Twist The system identifies that the contexts of the last two sentences are very similar and chooses their LCS to produce the follow- ing patterns: <hook> was born in <target> <hook> ( <target> - 1870 ) In order to measure the precision of the ex- tracted patterns, a new corpus is downloaded us- ing the hook Dickens as the only query word, and the system looks for appearances of the patterns in the corpus.	LCS	longest common subsequence	0
From the downloaded corpus, we extract sentences such as Dickens was born in 1812 Dickens (1812 - 1870) was an English writer Dickens (1812 - 1870) wrote Oliver Twist The system identifies that the contexts of the last two sentences are very similar and chooses their LCS to produce the follow- ing patterns: <hook> was born in <target> <hook> ( <target> - 1870 ) In order to measure the precision of the ex- tracted patterns, a new corpus is downloaded us- ing the hook Dickens as the only query word, and the system looks for appearances of the patterns in the corpus.	LCS	longer common sequence	0
From the downloaded corpus, we extract sentences such as Dickens was born in 1812 Dickens (1812 - 1870) was an English writer Dickens (1812 - 1870) wrote Oliver Twist The system identifies that the contexts of the last two sentences are very similar and chooses their LCS to produce the follow- ing patterns: <hook> was born in <target> <hook> ( <target> - 1870 ) In order to measure the precision of the ex- tracted patterns, a new corpus is downloaded us- ing the hook Dickens as the only query word, and the system looks for appearances of the patterns in the corpus.	LCS	lexical conccplual structure	0
We use the Recall-Oriented Un- derstudy for Gisting Evaluation (ROUGE) based on the LCSs (ROUGE-L) as our evaluation metric.	LCS	Longest Common Substring	0
We use the Recall-Oriented Un- derstudy for Gisting Evaluation (ROUGE) based on the LCSs (ROUGE-L) as our evaluation metric.	LCS	lexical conceptual structure	0
We use the Recall-Oriented Un- derstudy for Gisting Evaluation (ROUGE) based on the LCSs (ROUGE-L) as our evaluation metric.	LCS	longest common substring	1
We use the Recall-Oriented Un- derstudy for Gisting Evaluation (ROUGE) based on the LCSs (ROUGE-L) as our evaluation metric.	LCS	least common subsumer	0
We use the Recall-Oriented Un- derstudy for Gisting Evaluation (ROUGE) based on the LCSs (ROUGE-L) as our evaluation metric.	LCS	Longest Common Subsequenc	0
We use the Recall-Oriented Un- derstudy for Gisting Evaluation (ROUGE) based on the LCSs (ROUGE-L) as our evaluation metric.	LCS	Longest Common Sub	0
We use the Recall-Oriented Un- derstudy for Gisting Evaluation (ROUGE) based on the LCSs (ROUGE-L) as our evaluation metric.	LCS	longest common subsequence	0
We use the Recall-Oriented Un- derstudy for Gisting Evaluation (ROUGE) based on the LCSs (ROUGE-L) as our evaluation metric.	LCS	longer common sequence	0
We use the Recall-Oriented Un- derstudy for Gisting Evaluation (ROUGE) based on the LCSs (ROUGE-L) as our evaluation metric.	LCS	lexical conccplual structure	0
From the downloaded corpus, we extract sentences such as Dickens was born in 1812 Dickens (1812 - 1870) was an English writer Dickens (1812 - 1870) wrote Oliver Twist The system identifies that the contexts of the last two sentences are very similar and chooses their LCS to produce the follow- ing patterns: <hook> was born in <target> <hook> ( <target> - 1870 ) The rote extractor needs to estimate automati- cally the precision of the extracted patterns, in or- der to keep the best ones.	LCS	Longest Common Substring	0
From the downloaded corpus, we extract sentences such as Dickens was born in 1812 Dickens (1812 - 1870) was an English writer Dickens (1812 - 1870) wrote Oliver Twist The system identifies that the contexts of the last two sentences are very similar and chooses their LCS to produce the follow- ing patterns: <hook> was born in <target> <hook> ( <target> - 1870 ) The rote extractor needs to estimate automati- cally the precision of the extracted patterns, in or- der to keep the best ones.	LCS	lexical conceptual structure	0
From the downloaded corpus, we extract sentences such as Dickens was born in 1812 Dickens (1812 - 1870) was an English writer Dickens (1812 - 1870) wrote Oliver Twist The system identifies that the contexts of the last two sentences are very similar and chooses their LCS to produce the follow- ing patterns: <hook> was born in <target> <hook> ( <target> - 1870 ) The rote extractor needs to estimate automati- cally the precision of the extracted patterns, in or- der to keep the best ones.	LCS	longest common substring	1
From the downloaded corpus, we extract sentences such as Dickens was born in 1812 Dickens (1812 - 1870) was an English writer Dickens (1812 - 1870) wrote Oliver Twist The system identifies that the contexts of the last two sentences are very similar and chooses their LCS to produce the follow- ing patterns: <hook> was born in <target> <hook> ( <target> - 1870 ) The rote extractor needs to estimate automati- cally the precision of the extracted patterns, in or- der to keep the best ones.	LCS	least common subsumer	0
From the downloaded corpus, we extract sentences such as Dickens was born in 1812 Dickens (1812 - 1870) was an English writer Dickens (1812 - 1870) wrote Oliver Twist The system identifies that the contexts of the last two sentences are very similar and chooses their LCS to produce the follow- ing patterns: <hook> was born in <target> <hook> ( <target> - 1870 ) The rote extractor needs to estimate automati- cally the precision of the extracted patterns, in or- der to keep the best ones.	LCS	Longest Common Subsequenc	0
From the downloaded corpus, we extract sentences such as Dickens was born in 1812 Dickens (1812 - 1870) was an English writer Dickens (1812 - 1870) wrote Oliver Twist The system identifies that the contexts of the last two sentences are very similar and chooses their LCS to produce the follow- ing patterns: <hook> was born in <target> <hook> ( <target> - 1870 ) The rote extractor needs to estimate automati- cally the precision of the extracted patterns, in or- der to keep the best ones.	LCS	Longest Common Sub	0
From the downloaded corpus, we extract sentences such as Dickens was born in 1812 Dickens (1812 - 1870) was an English writer Dickens (1812 - 1870) wrote Oliver Twist The system identifies that the contexts of the last two sentences are very similar and chooses their LCS to produce the follow- ing patterns: <hook> was born in <target> <hook> ( <target> - 1870 ) The rote extractor needs to estimate automati- cally the precision of the extracted patterns, in or- der to keep the best ones.	LCS	longest common subsequence	0
From the downloaded corpus, we extract sentences such as Dickens was born in 1812 Dickens (1812 - 1870) was an English writer Dickens (1812 - 1870) wrote Oliver Twist The system identifies that the contexts of the last two sentences are very similar and chooses their LCS to produce the follow- ing patterns: <hook> was born in <target> <hook> ( <target> - 1870 ) The rote extractor needs to estimate automati- cally the precision of the extracted patterns, in or- der to keep the best ones.	LCS	longer common sequence	0
From the downloaded corpus, we extract sentences such as Dickens was born in 1812 Dickens (1812 - 1870) was an English writer Dickens (1812 - 1870) wrote Oliver Twist The system identifies that the contexts of the last two sentences are very similar and chooses their LCS to produce the follow- ing patterns: <hook> was born in <target> <hook> ( <target> - 1870 ) The rote extractor needs to estimate automati- cally the precision of the extracted patterns, in or- der to keep the best ones.	LCS	lexical conccplual structure	0
This may just be the m characters to the left or to the right (Brin, 1998), the LCS of several contexts (Agichtein and Gravano, 2000), or all substrings obtained with a suf- fix tree constructor (Ravichandran and Hovy, 2002).	LCS	Longest Common Substring	0
This may just be the m characters to the left or to the right (Brin, 1998), the LCS of several contexts (Agichtein and Gravano, 2000), or all substrings obtained with a suf- fix tree constructor (Ravichandran and Hovy, 2002).	LCS	lexical conceptual structure	0
This may just be the m characters to the left or to the right (Brin, 1998), the LCS of several contexts (Agichtein and Gravano, 2000), or all substrings obtained with a suf- fix tree constructor (Ravichandran and Hovy, 2002).	LCS	longest common substring	1
This may just be the m characters to the left or to the right (Brin, 1998), the LCS of several contexts (Agichtein and Gravano, 2000), or all substrings obtained with a suf- fix tree constructor (Ravichandran and Hovy, 2002).	LCS	least common subsumer	0
This may just be the m characters to the left or to the right (Brin, 1998), the LCS of several contexts (Agichtein and Gravano, 2000), or all substrings obtained with a suf- fix tree constructor (Ravichandran and Hovy, 2002).	LCS	Longest Common Subsequenc	0
This may just be the m characters to the left or to the right (Brin, 1998), the LCS of several contexts (Agichtein and Gravano, 2000), or all substrings obtained with a suf- fix tree constructor (Ravichandran and Hovy, 2002).	LCS	Longest Common Sub	0
This may just be the m characters to the left or to the right (Brin, 1998), the LCS of several contexts (Agichtein and Gravano, 2000), or all substrings obtained with a suf- fix tree constructor (Ravichandran and Hovy, 2002).	LCS	longest common subsequence	0
This may just be the m characters to the left or to the right (Brin, 1998), the LCS of several contexts (Agichtein and Gravano, 2000), or all substrings obtained with a suf- fix tree constructor (Ravichandran and Hovy, 2002).	LCS	longer common sequence	0
This may just be the m characters to the left or to the right (Brin, 1998), the LCS of several contexts (Agichtein and Gravano, 2000), or all substrings obtained with a suf- fix tree constructor (Ravichandran and Hovy, 2002).	LCS	lexical conccplual structure	0
This may just be the m characters to the left or to the right, (Brin, 1998), the LCS of several contexts (Agichtein and Gravano, 2000), or all substrings obtained with a suf- fix tree constructor (Ravichandran and Hovy, 2002).	LCS	Longest Common Substring	0
This may just be the m characters to the left or to the right, (Brin, 1998), the LCS of several contexts (Agichtein and Gravano, 2000), or all substrings obtained with a suf- fix tree constructor (Ravichandran and Hovy, 2002).	LCS	lexical conceptual structure	0
This may just be the m characters to the left or to the right, (Brin, 1998), the LCS of several contexts (Agichtein and Gravano, 2000), or all substrings obtained with a suf- fix tree constructor (Ravichandran and Hovy, 2002).	LCS	longest common substring	1
This may just be the m characters to the left or to the right, (Brin, 1998), the LCS of several contexts (Agichtein and Gravano, 2000), or all substrings obtained with a suf- fix tree constructor (Ravichandran and Hovy, 2002).	LCS	least common subsumer	0
This may just be the m characters to the left or to the right, (Brin, 1998), the LCS of several contexts (Agichtein and Gravano, 2000), or all substrings obtained with a suf- fix tree constructor (Ravichandran and Hovy, 2002).	LCS	Longest Common Subsequenc	0
This may just be the m characters to the left or to the right, (Brin, 1998), the LCS of several contexts (Agichtein and Gravano, 2000), or all substrings obtained with a suf- fix tree constructor (Ravichandran and Hovy, 2002).	LCS	Longest Common Sub	0
This may just be the m characters to the left or to the right, (Brin, 1998), the LCS of several contexts (Agichtein and Gravano, 2000), or all substrings obtained with a suf- fix tree constructor (Ravichandran and Hovy, 2002).	LCS	longest common subsequence	0
This may just be the m characters to the left or to the right, (Brin, 1998), the LCS of several contexts (Agichtein and Gravano, 2000), or all substrings obtained with a suf- fix tree constructor (Ravichandran and Hovy, 2002).	LCS	longer common sequence	0
This may just be the m characters to the left or to the right, (Brin, 1998), the LCS of several contexts (Agichtein and Gravano, 2000), or all substrings obtained with a suf- fix tree constructor (Ravichandran and Hovy, 2002).	LCS	lexical conccplual structure	0
6As basic similarity measures, we use greedy string tiling (Wise, 1996) with n = 3, longest common subsequence and LCS (Allison and Dix, 1986), and word n-gram containment(Lyon et al 2001) with n = 2.	LCS	Longest Common Substring	0
6As basic similarity measures, we use greedy string tiling (Wise, 1996) with n = 3, longest common subsequence and LCS (Allison and Dix, 1986), and word n-gram containment(Lyon et al 2001) with n = 2.	LCS	lexical conceptual structure	0
6As basic similarity measures, we use greedy string tiling (Wise, 1996) with n = 3, longest common subsequence and LCS (Allison and Dix, 1986), and word n-gram containment(Lyon et al 2001) with n = 2.	LCS	longest common substring	1
6As basic similarity measures, we use greedy string tiling (Wise, 1996) with n = 3, longest common subsequence and LCS (Allison and Dix, 1986), and word n-gram containment(Lyon et al 2001) with n = 2.	LCS	least common subsumer	0
6As basic similarity measures, we use greedy string tiling (Wise, 1996) with n = 3, longest common subsequence and LCS (Allison and Dix, 1986), and word n-gram containment(Lyon et al 2001) with n = 2.	LCS	Longest Common Subsequenc	0
6As basic similarity measures, we use greedy string tiling (Wise, 1996) with n = 3, longest common subsequence and LCS (Allison and Dix, 1986), and word n-gram containment(Lyon et al 2001) with n = 2.	LCS	Longest Common Sub	0
6As basic similarity measures, we use greedy string tiling (Wise, 1996) with n = 3, longest common subsequence and LCS (Allison and Dix, 1986), and word n-gram containment(Lyon et al 2001) with n = 2.	LCS	longest common subsequence	0
6As basic similarity measures, we use greedy string tiling (Wise, 1996) with n = 3, longest common subsequence and LCS (Allison and Dix, 1986), and word n-gram containment(Lyon et al 2001) with n = 2.	LCS	longer common sequence	0
6As basic similarity measures, we use greedy string tiling (Wise, 1996) with n = 3, longest common subsequence and LCS (Allison and Dix, 1986), and word n-gram containment(Lyon et al 2001) with n = 2.	LCS	lexical conccplual structure	0
lin uses the information content (Resnik, 1995) of the LCS of concepts A and B. Information content (IC) indi- cates the specificity of a concept; the least com- mon subsumer of a concept A and B is the most specific concept from which A and B are inherited.	LCS	Longest Common Substring	0
lin uses the information content (Resnik, 1995) of the LCS of concepts A and B. Information content (IC) indi- cates the specificity of a concept; the least com- mon subsumer of a concept A and B is the most specific concept from which A and B are inherited.	LCS	lexical conceptual structure	0
lin uses the information content (Resnik, 1995) of the LCS of concepts A and B. Information content (IC) indi- cates the specificity of a concept; the least com- mon subsumer of a concept A and B is the most specific concept from which A and B are inherited.	LCS	longest common substring	0
lin uses the information content (Resnik, 1995) of the LCS of concepts A and B. Information content (IC) indi- cates the specificity of a concept; the least com- mon subsumer of a concept A and B is the most specific concept from which A and B are inherited.	LCS	least common subsumer	1
lin uses the information content (Resnik, 1995) of the LCS of concepts A and B. Information content (IC) indi- cates the specificity of a concept; the least com- mon subsumer of a concept A and B is the most specific concept from which A and B are inherited.	LCS	Longest Common Subsequenc	0
lin uses the information content (Resnik, 1995) of the LCS of concepts A and B. Information content (IC) indi- cates the specificity of a concept; the least com- mon subsumer of a concept A and B is the most specific concept from which A and B are inherited.	LCS	Longest Common Sub	0
lin uses the information content (Resnik, 1995) of the LCS of concepts A and B. Information content (IC) indi- cates the specificity of a concept; the least com- mon subsumer of a concept A and B is the most specific concept from which A and B are inherited.	LCS	longest common subsequence	0
lin uses the information content (Resnik, 1995) of the LCS of concepts A and B. Information content (IC) indi- cates the specificity of a concept; the least com- mon subsumer of a concept A and B is the most specific concept from which A and B are inherited.	LCS	longer common sequence	0
lin uses the information content (Resnik, 1995) of the LCS of concepts A and B. Information content (IC) indi- cates the specificity of a concept; the least com- mon subsumer of a concept A and B is the most specific concept from which A and B are inherited.	LCS	lexical conccplual structure	0
LCS?	LCS	Longest Common Substring	0
LCS?	LCS	lexical conceptual structure	0
LCS?	LCS	longest common substring	0
LCS?	LCS	least common subsumer	1
LCS?	LCS	Longest Common Subsequenc	0
LCS?	LCS	Longest Common Sub	0
LCS?	LCS	longest common subsequence	0
LCS?	LCS	longer common sequence	0
LCS?	LCS	lexical conccplual structure	0
Tracing for the information content measures (res, lin, jcn) includes both the paths between concepts as well as the LCS.	LCS	Longest Common Substring	0
Tracing for the information content measures (res, lin, jcn) includes both the paths between concepts as well as the LCS.	LCS	lexical conceptual structure	0
Tracing for the information content measures (res, lin, jcn) includes both the paths between concepts as well as the LCS.	LCS	longest common substring	0
Tracing for the information content measures (res, lin, jcn) includes both the paths between concepts as well as the LCS.	LCS	least common subsumer	1
Tracing for the information content measures (res, lin, jcn) includes both the paths between concepts as well as the LCS.	LCS	Longest Common Subsequenc	0
Tracing for the information content measures (res, lin, jcn) includes both the paths between concepts as well as the LCS.	LCS	Longest Common Sub	0
Tracing for the information content measures (res, lin, jcn) includes both the paths between concepts as well as the LCS.	LCS	longest common subsequence	0
Tracing for the information content measures (res, lin, jcn) includes both the paths between concepts as well as the LCS.	LCS	longer common sequence	0
Tracing for the information content measures (res, lin, jcn) includes both the paths between concepts as well as the LCS.	LCS	lexical conccplual structure	0
features (covering lexical items before, af- ter and between both mentions (of the event trigger and an argument) as described by Zhou and Zhang (2007)); second, chunking features (concerned with head words of the phrases between two mentions as described by Zhou and Zhang (2007)); third, de- pendency parse features (considering both the se- lected dependency levels of the arguments (parents and LCS) as discussed by Ka- trenko and Adriaans (2006), as well as a shortest de- pendency path structure between the arguments as used by Kim et al (2008b) for walk features).	LCS	Longest Common Substring	0
features (covering lexical items before, af- ter and between both mentions (of the event trigger and an argument) as described by Zhou and Zhang (2007)); second, chunking features (concerned with head words of the phrases between two mentions as described by Zhou and Zhang (2007)); third, de- pendency parse features (considering both the se- lected dependency levels of the arguments (parents and LCS) as discussed by Ka- trenko and Adriaans (2006), as well as a shortest de- pendency path structure between the arguments as used by Kim et al (2008b) for walk features).	LCS	lexical conceptual structure	0
features (covering lexical items before, af- ter and between both mentions (of the event trigger and an argument) as described by Zhou and Zhang (2007)); second, chunking features (concerned with head words of the phrases between two mentions as described by Zhou and Zhang (2007)); third, de- pendency parse features (considering both the se- lected dependency levels of the arguments (parents and LCS) as discussed by Ka- trenko and Adriaans (2006), as well as a shortest de- pendency path structure between the arguments as used by Kim et al (2008b) for walk features).	LCS	longest common substring	0
features (covering lexical items before, af- ter and between both mentions (of the event trigger and an argument) as described by Zhou and Zhang (2007)); second, chunking features (concerned with head words of the phrases between two mentions as described by Zhou and Zhang (2007)); third, de- pendency parse features (considering both the se- lected dependency levels of the arguments (parents and LCS) as discussed by Ka- trenko and Adriaans (2006), as well as a shortest de- pendency path structure between the arguments as used by Kim et al (2008b) for walk features).	LCS	least common subsumer	1
features (covering lexical items before, af- ter and between both mentions (of the event trigger and an argument) as described by Zhou and Zhang (2007)); second, chunking features (concerned with head words of the phrases between two mentions as described by Zhou and Zhang (2007)); third, de- pendency parse features (considering both the se- lected dependency levels of the arguments (parents and LCS) as discussed by Ka- trenko and Adriaans (2006), as well as a shortest de- pendency path structure between the arguments as used by Kim et al (2008b) for walk features).	LCS	Longest Common Subsequenc	0
features (covering lexical items before, af- ter and between both mentions (of the event trigger and an argument) as described by Zhou and Zhang (2007)); second, chunking features (concerned with head words of the phrases between two mentions as described by Zhou and Zhang (2007)); third, de- pendency parse features (considering both the se- lected dependency levels of the arguments (parents and LCS) as discussed by Ka- trenko and Adriaans (2006), as well as a shortest de- pendency path structure between the arguments as used by Kim et al (2008b) for walk features).	LCS	Longest Common Sub	0
features (covering lexical items before, af- ter and between both mentions (of the event trigger and an argument) as described by Zhou and Zhang (2007)); second, chunking features (concerned with head words of the phrases between two mentions as described by Zhou and Zhang (2007)); third, de- pendency parse features (considering both the se- lected dependency levels of the arguments (parents and LCS) as discussed by Ka- trenko and Adriaans (2006), as well as a shortest de- pendency path structure between the arguments as used by Kim et al (2008b) for walk features).	LCS	longest common subsequence	0
features (covering lexical items before, af- ter and between both mentions (of the event trigger and an argument) as described by Zhou and Zhang (2007)); second, chunking features (concerned with head words of the phrases between two mentions as described by Zhou and Zhang (2007)); third, de- pendency parse features (considering both the se- lected dependency levels of the arguments (parents and LCS) as discussed by Ka- trenko and Adriaans (2006), as well as a shortest de- pendency path structure between the arguments as used by Kim et al (2008b) for walk features).	LCS	longer common sequence	0
features (covering lexical items before, af- ter and between both mentions (of the event trigger and an argument) as described by Zhou and Zhang (2007)); second, chunking features (concerned with head words of the phrases between two mentions as described by Zhou and Zhang (2007)); third, de- pendency parse features (considering both the se- lected dependency levels of the arguments (parents and LCS) as discussed by Ka- trenko and Adriaans (2006), as well as a shortest de- pendency path structure between the arguments as used by Kim et al (2008b) for walk features).	LCS	lexical conccplual structure	0
lin similarity 5 returns the difference between two times of the IC of the LCS of A and B, and the sum of IC of both concepts.	LCS	Longest Common Substring	0
lin similarity 5 returns the difference between two times of the IC of the LCS of A and B, and the sum of IC of both concepts.	LCS	lexical conceptual structure	0
lin similarity 5 returns the difference between two times of the IC of the LCS of A and B, and the sum of IC of both concepts.	LCS	longest common substring	0
lin similarity 5 returns the difference between two times of the IC of the LCS of A and B, and the sum of IC of both concepts.	LCS	least common subsumer	1
lin similarity 5 returns the difference between two times of the IC of the LCS of A and B, and the sum of IC of both concepts.	LCS	Longest Common Subsequenc	0
lin similarity 5 returns the difference between two times of the IC of the LCS of A and B, and the sum of IC of both concepts.	LCS	Longest Common Sub	0
lin similarity 5 returns the difference between two times of the IC of the LCS of A and B, and the sum of IC of both concepts.	LCS	longest common subsequence	0
lin similarity 5 returns the difference between two times of the IC of the LCS of A and B, and the sum of IC of both concepts.	LCS	longer common sequence	0
lin similarity 5 returns the difference between two times of the IC of the LCS of A and B, and the sum of IC of both concepts.	LCS	lexical conccplual structure	0
Path search takes place as a depth-limited search of maximum depth of 4 for a LCS.	LCS	Longest Common Substring	0
Path search takes place as a depth-limited search of maximum depth of 4 for a LCS.	LCS	lexical conceptual structure	0
Path search takes place as a depth-limited search of maximum depth of 4 for a LCS.	LCS	longest common substring	0
Path search takes place as a depth-limited search of maximum depth of 4 for a LCS.	LCS	least common subsumer	1
Path search takes place as a depth-limited search of maximum depth of 4 for a LCS.	LCS	Longest Common Subsequenc	0
Path search takes place as a depth-limited search of maximum depth of 4 for a LCS.	LCS	Longest Common Sub	0
Path search takes place as a depth-limited search of maximum depth of 4 for a LCS.	LCS	longest common subsequence	0
Path search takes place as a depth-limited search of maximum depth of 4 for a LCS.	LCS	longer common sequence	0
Path search takes place as a depth-limited search of maximum depth of 4 for a LCS.	LCS	lexical conccplual structure	0
WUP finds the depth of the LCS of the words, and scales that by the sum of the depths of individual words.	LCS	Longest Common Substring	0
WUP finds the depth of the LCS of the words, and scales that by the sum of the depths of individual words.	LCS	lexical conceptual structure	0
WUP finds the depth of the LCS of the words, and scales that by the sum of the depths of individual words.	LCS	longest common substring	0
WUP finds the depth of the LCS of the words, and scales that by the sum of the depths of individual words.	LCS	least common subsumer	1
WUP finds the depth of the LCS of the words, and scales that by the sum of the depths of individual words.	LCS	Longest Common Subsequenc	0
WUP finds the depth of the LCS of the words, and scales that by the sum of the depths of individual words.	LCS	Longest Common Sub	0
WUP finds the depth of the LCS of the words, and scales that by the sum of the depths of individual words.	LCS	longest common subsequence	0
WUP finds the depth of the LCS of the words, and scales that by the sum of the depths of individual words.	LCS	longer common sequence	0
WUP finds the depth of the LCS of the words, and scales that by the sum of the depths of individual words.	LCS	lexical conccplual structure	0
LCSe is computed as follows: lcs(X,Y ) = (length(X) + length(Y )?	LCS	Longest Common Substring	0
LCSe is computed as follows: lcs(X,Y ) = (length(X) + length(Y )?	LCS	lexical conceptual structure	0
LCSe is computed as follows: lcs(X,Y ) = (length(X) + length(Y )?	LCS	longest common substring	0
LCSe is computed as follows: lcs(X,Y ) = (length(X) + length(Y )?	LCS	least common subsumer	0
LCSe is computed as follows: lcs(X,Y ) = (length(X) + length(Y )?	LCS	Longest Common Subsequenc	1
LCSe is computed as follows: lcs(X,Y ) = (length(X) + length(Y )?	LCS	Longest Common Sub	0
LCSe is computed as follows: lcs(X,Y ) = (length(X) + length(Y )?	LCS	longest common subsequence	0
LCSe is computed as follows: lcs(X,Y ) = (length(X) + length(Y )?	LCS	longer common sequence	0
LCSe is computed as follows: lcs(X,Y ) = (length(X) + length(Y )?	LCS	lexical conccplual structure	0
Auto- matic Evaluation of Machine Translation Quality Us- ing LCSe and Skip-Bigram Statistics.	LCS	Longest Common Substring	0
Auto- matic Evaluation of Machine Translation Quality Us- ing LCSe and Skip-Bigram Statistics.	LCS	lexical conceptual structure	0
Auto- matic Evaluation of Machine Translation Quality Us- ing LCSe and Skip-Bigram Statistics.	LCS	longest common substring	0
Auto- matic Evaluation of Machine Translation Quality Us- ing LCSe and Skip-Bigram Statistics.	LCS	least common subsumer	0
Auto- matic Evaluation of Machine Translation Quality Us- ing LCSe and Skip-Bigram Statistics.	LCS	Longest Common Subsequenc	1
Auto- matic Evaluation of Machine Translation Quality Us- ing LCSe and Skip-Bigram Statistics.	LCS	Longest Common Sub	0
Auto- matic Evaluation of Machine Translation Quality Us- ing LCSe and Skip-Bigram Statistics.	LCS	longest common subsequence	0
Auto- matic Evaluation of Machine Translation Quality Us- ing LCSe and Skip-Bigram Statistics.	LCS	longer common sequence	0
Auto- matic Evaluation of Machine Translation Quality Us- ing LCSe and Skip-Bigram Statistics.	LCS	lexical conccplual structure	0
Auto- matic Evaluation of Machine Translation Quality Us- ing LCSe and Skip-Bigram Statics.	LCS	Longest Common Substring	0
Auto- matic Evaluation of Machine Translation Quality Us- ing LCSe and Skip-Bigram Statics.	LCS	lexical conceptual structure	0
Auto- matic Evaluation of Machine Translation Quality Us- ing LCSe and Skip-Bigram Statics.	LCS	longest common substring	0
Auto- matic Evaluation of Machine Translation Quality Us- ing LCSe and Skip-Bigram Statics.	LCS	least common subsumer	0
Auto- matic Evaluation of Machine Translation Quality Us- ing LCSe and Skip-Bigram Statics.	LCS	Longest Common Subsequenc	1
Auto- matic Evaluation of Machine Translation Quality Us- ing LCSe and Skip-Bigram Statics.	LCS	Longest Common Sub	0
Auto- matic Evaluation of Machine Translation Quality Us- ing LCSe and Skip-Bigram Statics.	LCS	longest common subsequence	0
Auto- matic Evaluation of Machine Translation Quality Us- ing LCSe and Skip-Bigram Statics.	LCS	longer common sequence	0
Auto- matic Evaluation of Machine Translation Quality Us- ing LCSe and Skip-Bigram Statics.	LCS	lexical conccplual structure	0
Salience grading for candidate antecedents  Features Score  F1  recency  0, if in two sentences away from anaphor  1, if in one sentence away from anaphor  2, if in same sentence as anaphor 0-2  F2 Subject and Object Preference 1  F3 Grammatical function agreement 1  F4 Number Agreement 1  F5 Semantic LCSe 0 to 3  F6 Semantic Type Agreement -1 to +2  F7 Biomedical antecedent preference -2 if not or +2  The first feature F1 is recency which measures the distance between an anaphor  and candidate antecedents in number of sentences.	LCS	Longest Common Substring	0
Salience grading for candidate antecedents  Features Score  F1  recency  0, if in two sentences away from anaphor  1, if in one sentence away from anaphor  2, if in same sentence as anaphor 0-2  F2 Subject and Object Preference 1  F3 Grammatical function agreement 1  F4 Number Agreement 1  F5 Semantic LCSe 0 to 3  F6 Semantic Type Agreement -1 to +2  F7 Biomedical antecedent preference -2 if not or +2  The first feature F1 is recency which measures the distance between an anaphor  and candidate antecedents in number of sentences.	LCS	lexical conceptual structure	0
Salience grading for candidate antecedents  Features Score  F1  recency  0, if in two sentences away from anaphor  1, if in one sentence away from anaphor  2, if in same sentence as anaphor 0-2  F2 Subject and Object Preference 1  F3 Grammatical function agreement 1  F4 Number Agreement 1  F5 Semantic LCSe 0 to 3  F6 Semantic Type Agreement -1 to +2  F7 Biomedical antecedent preference -2 if not or +2  The first feature F1 is recency which measures the distance between an anaphor  and candidate antecedents in number of sentences.	LCS	longest common substring	0
Salience grading for candidate antecedents  Features Score  F1  recency  0, if in two sentences away from anaphor  1, if in one sentence away from anaphor  2, if in same sentence as anaphor 0-2  F2 Subject and Object Preference 1  F3 Grammatical function agreement 1  F4 Number Agreement 1  F5 Semantic LCSe 0 to 3  F6 Semantic Type Agreement -1 to +2  F7 Biomedical antecedent preference -2 if not or +2  The first feature F1 is recency which measures the distance between an anaphor  and candidate antecedents in number of sentences.	LCS	least common subsumer	0
Salience grading for candidate antecedents  Features Score  F1  recency  0, if in two sentences away from anaphor  1, if in one sentence away from anaphor  2, if in same sentence as anaphor 0-2  F2 Subject and Object Preference 1  F3 Grammatical function agreement 1  F4 Number Agreement 1  F5 Semantic LCSe 0 to 3  F6 Semantic Type Agreement -1 to +2  F7 Biomedical antecedent preference -2 if not or +2  The first feature F1 is recency which measures the distance between an anaphor  and candidate antecedents in number of sentences.	LCS	Longest Common Subsequenc	1
Salience grading for candidate antecedents  Features Score  F1  recency  0, if in two sentences away from anaphor  1, if in one sentence away from anaphor  2, if in same sentence as anaphor 0-2  F2 Subject and Object Preference 1  F3 Grammatical function agreement 1  F4 Number Agreement 1  F5 Semantic LCSe 0 to 3  F6 Semantic Type Agreement -1 to +2  F7 Biomedical antecedent preference -2 if not or +2  The first feature F1 is recency which measures the distance between an anaphor  and candidate antecedents in number of sentences.	LCS	Longest Common Sub	0
Salience grading for candidate antecedents  Features Score  F1  recency  0, if in two sentences away from anaphor  1, if in one sentence away from anaphor  2, if in same sentence as anaphor 0-2  F2 Subject and Object Preference 1  F3 Grammatical function agreement 1  F4 Number Agreement 1  F5 Semantic LCSe 0 to 3  F6 Semantic Type Agreement -1 to +2  F7 Biomedical antecedent preference -2 if not or +2  The first feature F1 is recency which measures the distance between an anaphor  and candidate antecedents in number of sentences.	LCS	longest common subsequence	0
Salience grading for candidate antecedents  Features Score  F1  recency  0, if in two sentences away from anaphor  1, if in one sentence away from anaphor  2, if in same sentence as anaphor 0-2  F2 Subject and Object Preference 1  F3 Grammatical function agreement 1  F4 Number Agreement 1  F5 Semantic LCSe 0 to 3  F6 Semantic Type Agreement -1 to +2  F7 Biomedical antecedent preference -2 if not or +2  The first feature F1 is recency which measures the distance between an anaphor  and candidate antecedents in number of sentences.	LCS	longer common sequence	0
Salience grading for candidate antecedents  Features Score  F1  recency  0, if in two sentences away from anaphor  1, if in one sentence away from anaphor  2, if in same sentence as anaphor 0-2  F2 Subject and Object Preference 1  F3 Grammatical function agreement 1  F4 Number Agreement 1  F5 Semantic LCSe 0 to 3  F6 Semantic Type Agreement -1 to +2  F7 Biomedical antecedent preference -2 if not or +2  The first feature F1 is recency which measures the distance between an anaphor  and candidate antecedents in number of sentences.	LCS	lexical conccplual structure	0
Automatic Evaluation of Machine Translation Quality Us- ing the LCSe and Skip- Bigram Statistics.	LCS	Longest Common Substring	0
Automatic Evaluation of Machine Translation Quality Us- ing the LCSe and Skip- Bigram Statistics.	LCS	lexical conceptual structure	0
Automatic Evaluation of Machine Translation Quality Us- ing the LCSe and Skip- Bigram Statistics.	LCS	longest common substring	0
Automatic Evaluation of Machine Translation Quality Us- ing the LCSe and Skip- Bigram Statistics.	LCS	least common subsumer	0
Automatic Evaluation of Machine Translation Quality Us- ing the LCSe and Skip- Bigram Statistics.	LCS	Longest Common Subsequenc	1
Automatic Evaluation of Machine Translation Quality Us- ing the LCSe and Skip- Bigram Statistics.	LCS	Longest Common Sub	0
Automatic Evaluation of Machine Translation Quality Us- ing the LCSe and Skip- Bigram Statistics.	LCS	longest common subsequence	0
Automatic Evaluation of Machine Translation Quality Us- ing the LCSe and Skip- Bigram Statistics.	LCS	longer common sequence	0
Automatic Evaluation of Machine Translation Quality Us- ing the LCSe and Skip- Bigram Statistics.	LCS	lexical conccplual structure	0
Au- tomatic Evaluation of Machine Translation Qual- ity Using LCSe and Skip- Bigram Statics.	LCS	Longest Common Substring	0
Au- tomatic Evaluation of Machine Translation Qual- ity Using LCSe and Skip- Bigram Statics.	LCS	lexical conceptual structure	0
Au- tomatic Evaluation of Machine Translation Qual- ity Using LCSe and Skip- Bigram Statics.	LCS	longest common substring	0
Au- tomatic Evaluation of Machine Translation Qual- ity Using LCSe and Skip- Bigram Statics.	LCS	least common subsumer	0
Au- tomatic Evaluation of Machine Translation Qual- ity Using LCSe and Skip- Bigram Statics.	LCS	Longest Common Subsequenc	1
Au- tomatic Evaluation of Machine Translation Qual- ity Using LCSe and Skip- Bigram Statics.	LCS	Longest Common Sub	0
Au- tomatic Evaluation of Machine Translation Qual- ity Using LCSe and Skip- Bigram Statics.	LCS	longest common subsequence	0
Au- tomatic Evaluation of Machine Translation Qual- ity Using LCSe and Skip- Bigram Statics.	LCS	longer common sequence	0
Au- tomatic Evaluation of Machine Translation Qual- ity Using LCSe and Skip- Bigram Statics.	LCS	lexical conccplual structure	0
Auto- matic Evaluation of Machine Translation Quality Us- ing LCSsequence and Skip-Bigram Statistics.	LCS	Longest Common Substring	0
Auto- matic Evaluation of Machine Translation Quality Us- ing LCSsequence and Skip-Bigram Statistics.	LCS	lexical conceptual structure	0
Auto- matic Evaluation of Machine Translation Quality Us- ing LCSsequence and Skip-Bigram Statistics.	LCS	longest common substring	0
Auto- matic Evaluation of Machine Translation Quality Us- ing LCSsequence and Skip-Bigram Statistics.	LCS	least common subsumer	0
Auto- matic Evaluation of Machine Translation Quality Us- ing LCSsequence and Skip-Bigram Statistics.	LCS	Longest Common Subsequenc	0
Auto- matic Evaluation of Machine Translation Quality Us- ing LCSsequence and Skip-Bigram Statistics.	LCS	Longest Common Sub	1
Auto- matic Evaluation of Machine Translation Quality Us- ing LCSsequence and Skip-Bigram Statistics.	LCS	longest common subsequence	0
Auto- matic Evaluation of Machine Translation Quality Us- ing LCSsequence and Skip-Bigram Statistics.	LCS	longer common sequence	0
Auto- matic Evaluation of Machine Translation Quality Us- ing LCSsequence and Skip-Bigram Statistics.	LCS	lexical conccplual structure	0
Auto- matic Evaluation of Machine Translation Quality Us- ing LCSsequence and Skip-Bigram Statics.	LCS	Longest Common Substring	0
Auto- matic Evaluation of Machine Translation Quality Us- ing LCSsequence and Skip-Bigram Statics.	LCS	lexical conceptual structure	0
Auto- matic Evaluation of Machine Translation Quality Us- ing LCSsequence and Skip-Bigram Statics.	LCS	longest common substring	0
Auto- matic Evaluation of Machine Translation Quality Us- ing LCSsequence and Skip-Bigram Statics.	LCS	least common subsumer	0
Auto- matic Evaluation of Machine Translation Quality Us- ing LCSsequence and Skip-Bigram Statics.	LCS	Longest Common Subsequenc	0
Auto- matic Evaluation of Machine Translation Quality Us- ing LCSsequence and Skip-Bigram Statics.	LCS	Longest Common Sub	1
Auto- matic Evaluation of Machine Translation Quality Us- ing LCSsequence and Skip-Bigram Statics.	LCS	longest common subsequence	0
Auto- matic Evaluation of Machine Translation Quality Us- ing LCSsequence and Skip-Bigram Statics.	LCS	longer common sequence	0
Auto- matic Evaluation of Machine Translation Quality Us- ing LCSsequence and Skip-Bigram Statics.	LCS	lexical conccplual structure	0
Salience grading for candidate antecedents  Features Score  F1  recency  0, if in two sentences away from anaphor  1, if in one sentence away from anaphor  2, if in same sentence as anaphor 0-2  F2 Subject and Object Preference 1  F3 Grammatical function agreement 1  F4 Number Agreement 1  F5 Semantic LCSsequence 0 to 3  F6 Semantic Type Agreement -1 to +2  F7 Biomedical antecedent preference -2 if not or +2  The first feature F1 is recency which measures the distance between an anaphor  and candidate antecedents in number of sentences.	LCS	Longest Common Substring	0
Salience grading for candidate antecedents  Features Score  F1  recency  0, if in two sentences away from anaphor  1, if in one sentence away from anaphor  2, if in same sentence as anaphor 0-2  F2 Subject and Object Preference 1  F3 Grammatical function agreement 1  F4 Number Agreement 1  F5 Semantic LCSsequence 0 to 3  F6 Semantic Type Agreement -1 to +2  F7 Biomedical antecedent preference -2 if not or +2  The first feature F1 is recency which measures the distance between an anaphor  and candidate antecedents in number of sentences.	LCS	lexical conceptual structure	0
Salience grading for candidate antecedents  Features Score  F1  recency  0, if in two sentences away from anaphor  1, if in one sentence away from anaphor  2, if in same sentence as anaphor 0-2  F2 Subject and Object Preference 1  F3 Grammatical function agreement 1  F4 Number Agreement 1  F5 Semantic LCSsequence 0 to 3  F6 Semantic Type Agreement -1 to +2  F7 Biomedical antecedent preference -2 if not or +2  The first feature F1 is recency which measures the distance between an anaphor  and candidate antecedents in number of sentences.	LCS	longest common substring	0
Salience grading for candidate antecedents  Features Score  F1  recency  0, if in two sentences away from anaphor  1, if in one sentence away from anaphor  2, if in same sentence as anaphor 0-2  F2 Subject and Object Preference 1  F3 Grammatical function agreement 1  F4 Number Agreement 1  F5 Semantic LCSsequence 0 to 3  F6 Semantic Type Agreement -1 to +2  F7 Biomedical antecedent preference -2 if not or +2  The first feature F1 is recency which measures the distance between an anaphor  and candidate antecedents in number of sentences.	LCS	least common subsumer	0
Salience grading for candidate antecedents  Features Score  F1  recency  0, if in two sentences away from anaphor  1, if in one sentence away from anaphor  2, if in same sentence as anaphor 0-2  F2 Subject and Object Preference 1  F3 Grammatical function agreement 1  F4 Number Agreement 1  F5 Semantic LCSsequence 0 to 3  F6 Semantic Type Agreement -1 to +2  F7 Biomedical antecedent preference -2 if not or +2  The first feature F1 is recency which measures the distance between an anaphor  and candidate antecedents in number of sentences.	LCS	Longest Common Subsequenc	0
Salience grading for candidate antecedents  Features Score  F1  recency  0, if in two sentences away from anaphor  1, if in one sentence away from anaphor  2, if in same sentence as anaphor 0-2  F2 Subject and Object Preference 1  F3 Grammatical function agreement 1  F4 Number Agreement 1  F5 Semantic LCSsequence 0 to 3  F6 Semantic Type Agreement -1 to +2  F7 Biomedical antecedent preference -2 if not or +2  The first feature F1 is recency which measures the distance between an anaphor  and candidate antecedents in number of sentences.	LCS	Longest Common Sub	1
Salience grading for candidate antecedents  Features Score  F1  recency  0, if in two sentences away from anaphor  1, if in one sentence away from anaphor  2, if in same sentence as anaphor 0-2  F2 Subject and Object Preference 1  F3 Grammatical function agreement 1  F4 Number Agreement 1  F5 Semantic LCSsequence 0 to 3  F6 Semantic Type Agreement -1 to +2  F7 Biomedical antecedent preference -2 if not or +2  The first feature F1 is recency which measures the distance between an anaphor  and candidate antecedents in number of sentences.	LCS	longest common subsequence	0
Salience grading for candidate antecedents  Features Score  F1  recency  0, if in two sentences away from anaphor  1, if in one sentence away from anaphor  2, if in same sentence as anaphor 0-2  F2 Subject and Object Preference 1  F3 Grammatical function agreement 1  F4 Number Agreement 1  F5 Semantic LCSsequence 0 to 3  F6 Semantic Type Agreement -1 to +2  F7 Biomedical antecedent preference -2 if not or +2  The first feature F1 is recency which measures the distance between an anaphor  and candidate antecedents in number of sentences.	LCS	longer common sequence	0
Salience grading for candidate antecedents  Features Score  F1  recency  0, if in two sentences away from anaphor  1, if in one sentence away from anaphor  2, if in same sentence as anaphor 0-2  F2 Subject and Object Preference 1  F3 Grammatical function agreement 1  F4 Number Agreement 1  F5 Semantic LCSsequence 0 to 3  F6 Semantic Type Agreement -1 to +2  F7 Biomedical antecedent preference -2 if not or +2  The first feature F1 is recency which measures the distance between an anaphor  and candidate antecedents in number of sentences.	LCS	lexical conccplual structure	0
Of the string-similarity features, we reused the LCSstring, LCSsequence (with and without normaliza- tion), and Greedy String Tiling measures.	LCS	Longest Common Substring	0
Of the string-similarity features, we reused the LCSstring, LCSsequence (with and without normaliza- tion), and Greedy String Tiling measures.	LCS	lexical conceptual structure	0
Of the string-similarity features, we reused the LCSstring, LCSsequence (with and without normaliza- tion), and Greedy String Tiling measures.	LCS	longest common substring	0
Of the string-similarity features, we reused the LCSstring, LCSsequence (with and without normaliza- tion), and Greedy String Tiling measures.	LCS	least common subsumer	0
Of the string-similarity features, we reused the LCSstring, LCSsequence (with and without normaliza- tion), and Greedy String Tiling measures.	LCS	Longest Common Subsequenc	0
Of the string-similarity features, we reused the LCSstring, LCSsequence (with and without normaliza- tion), and Greedy String Tiling measures.	LCS	Longest Common Sub	1
Of the string-similarity features, we reused the LCSstring, LCSsequence (with and without normaliza- tion), and Greedy String Tiling measures.	LCS	longest common subsequence	0
Of the string-similarity features, we reused the LCSstring, LCSsequence (with and without normaliza- tion), and Greedy String Tiling measures.	LCS	longer common sequence	0
Of the string-similarity features, we reused the LCSstring, LCSsequence (with and without normaliza- tion), and Greedy String Tiling measures.	LCS	lexical conccplual structure	0
Automatic Evaluation of Machine Translation Quality Us- ing the LCSsequence and Skip- Bigram Statistics.	LCS	Longest Common Substring	0
Automatic Evaluation of Machine Translation Quality Us- ing the LCSsequence and Skip- Bigram Statistics.	LCS	lexical conceptual structure	0
Automatic Evaluation of Machine Translation Quality Us- ing the LCSsequence and Skip- Bigram Statistics.	LCS	longest common substring	0
Automatic Evaluation of Machine Translation Quality Us- ing the LCSsequence and Skip- Bigram Statistics.	LCS	least common subsumer	0
Automatic Evaluation of Machine Translation Quality Us- ing the LCSsequence and Skip- Bigram Statistics.	LCS	Longest Common Subsequenc	0
Automatic Evaluation of Machine Translation Quality Us- ing the LCSsequence and Skip- Bigram Statistics.	LCS	Longest Common Sub	1
Automatic Evaluation of Machine Translation Quality Us- ing the LCSsequence and Skip- Bigram Statistics.	LCS	longest common subsequence	0
Automatic Evaluation of Machine Translation Quality Us- ing the LCSsequence and Skip- Bigram Statistics.	LCS	longer common sequence	0
Automatic Evaluation of Machine Translation Quality Us- ing the LCSsequence and Skip- Bigram Statistics.	LCS	lexical conccplual structure	0
Au- tomatic Evaluation of Machine Translation Qual- ity Using LCSsequence and Skip- Bigram Statics.	LCS	Longest Common Substring	0
Au- tomatic Evaluation of Machine Translation Qual- ity Using LCSsequence and Skip- Bigram Statics.	LCS	lexical conceptual structure	0
Au- tomatic Evaluation of Machine Translation Qual- ity Using LCSsequence and Skip- Bigram Statics.	LCS	longest common substring	0
Au- tomatic Evaluation of Machine Translation Qual- ity Using LCSsequence and Skip- Bigram Statics.	LCS	least common subsumer	0
Au- tomatic Evaluation of Machine Translation Qual- ity Using LCSsequence and Skip- Bigram Statics.	LCS	Longest Common Subsequenc	0
Au- tomatic Evaluation of Machine Translation Qual- ity Using LCSsequence and Skip- Bigram Statics.	LCS	Longest Common Sub	1
Au- tomatic Evaluation of Machine Translation Qual- ity Using LCSsequence and Skip- Bigram Statics.	LCS	longest common subsequence	0
Au- tomatic Evaluation of Machine Translation Qual- ity Using LCSsequence and Skip- Bigram Statics.	LCS	longer common sequence	0
Au- tomatic Evaluation of Machine Translation Qual- ity Using LCSsequence and Skip- Bigram Statics.	LCS	lexical conccplual structure	0
1) Additionally, we relax the constraint of requiring exact string matches between the two sentences by using the LCS (Allison and Dix, 1986) and greedy string tiling (Wise, 1996) al- gorithms.	LCS	Longest Common Substring	0
1) Additionally, we relax the constraint of requiring exact string matches between the two sentences by using the LCS (Allison and Dix, 1986) and greedy string tiling (Wise, 1996) al- gorithms.	LCS	lexical conceptual structure	0
1) Additionally, we relax the constraint of requiring exact string matches between the two sentences by using the LCS (Allison and Dix, 1986) and greedy string tiling (Wise, 1996) al- gorithms.	LCS	longest common substring	0
1) Additionally, we relax the constraint of requiring exact string matches between the two sentences by using the LCS (Allison and Dix, 1986) and greedy string tiling (Wise, 1996) al- gorithms.	LCS	least common subsumer	0
1) Additionally, we relax the constraint of requiring exact string matches between the two sentences by using the LCS (Allison and Dix, 1986) and greedy string tiling (Wise, 1996) al- gorithms.	LCS	Longest Common Subsequenc	0
1) Additionally, we relax the constraint of requiring exact string matches between the two sentences by using the LCS (Allison and Dix, 1986) and greedy string tiling (Wise, 1996) al- gorithms.	LCS	Longest Common Sub	0
1) Additionally, we relax the constraint of requiring exact string matches between the two sentences by using the LCS (Allison and Dix, 1986) and greedy string tiling (Wise, 1996) al- gorithms.	LCS	longest common subsequence	1
1) Additionally, we relax the constraint of requiring exact string matches between the two sentences by using the LCS (Allison and Dix, 1986) and greedy string tiling (Wise, 1996) al- gorithms.	LCS	longer common sequence	0
1) Additionally, we relax the constraint of requiring exact string matches between the two sentences by using the LCS (Allison and Dix, 1986) and greedy string tiling (Wise, 1996) al- gorithms.	LCS	lexical conccplual structure	0
Size of the LCS.	LCS	Longest Common Substring	0
Size of the LCS.	LCS	lexical conceptual structure	0
Size of the LCS.	LCS	longest common substring	0
Size of the LCS.	LCS	least common subsumer	0
Size of the LCS.	LCS	Longest Common Subsequenc	0
Size of the LCS.	LCS	Longest Common Sub	0
Size of the LCS.	LCS	longest common subsequence	1
Size of the LCS.	LCS	longer common sequence	0
Size of the LCS.	LCS	lexical conccplual structure	0
A LCS algorithm suitable for similar text strings.	LCS	Longest Common Substring	0
A LCS algorithm suitable for similar text strings.	LCS	lexical conceptual structure	0
A LCS algorithm suitable for similar text strings.	LCS	longest common substring	0
A LCS algorithm suitable for similar text strings.	LCS	least common subsumer	0
A LCS algorithm suitable for similar text strings.	LCS	Longest Common Subsequenc	0
A LCS algorithm suitable for similar text strings.	LCS	Longest Common Sub	0
A LCS algorithm suitable for similar text strings.	LCS	longest common subsequence	1
A LCS algorithm suitable for similar text strings.	LCS	longer common sequence	0
A LCS algorithm suitable for similar text strings.	LCS	lexical conccplual structure	0
We used the cosine similarity, LCS, and word n-gram similarity measures.	LCS	Longest Common Substring	0
We used the cosine similarity, LCS, and word n-gram similarity measures.	LCS	lexical conceptual structure	0
We used the cosine similarity, LCS, and word n-gram similarity measures.	LCS	longest common substring	0
We used the cosine similarity, LCS, and word n-gram similarity measures.	LCS	least common subsumer	0
We used the cosine similarity, LCS, and word n-gram similarity measures.	LCS	Longest Common Subsequenc	0
We used the cosine similarity, LCS, and word n-gram similarity measures.	LCS	Longest Common Sub	0
We used the cosine similarity, LCS, and word n-gram similarity measures.	LCS	longest common subsequence	1
We used the cosine similarity, LCS, and word n-gram similarity measures.	LCS	longer common sequence	0
We used the cosine similarity, LCS, and word n-gram similarity measures.	LCS	lexical conccplual structure	0
Size of the LCS, normalized by TGT length.	LCS	Longest Common Substring	0
Size of the LCS, normalized by TGT length.	LCS	lexical conceptual structure	0
Size of the LCS, normalized by TGT length.	LCS	longest common substring	0
Size of the LCS, normalized by TGT length.	LCS	least common subsumer	0
Size of the LCS, normalized by TGT length.	LCS	Longest Common Subsequenc	0
Size of the LCS, normalized by TGT length.	LCS	Longest Common Sub	0
Size of the LCS, normalized by TGT length.	LCS	longest common subsequence	1
Size of the LCS, normalized by TGT length.	LCS	longer common sequence	0
Size of the LCS, normalized by TGT length.	LCS	lexical conccplual structure	0
Cosine similarity, LCS, and word n-gram similarity were also applied to measure the similarity between the edit comment and the turn text as well as the similarity between the edit comment and the turn topic name.	LCS	Longest Common Substring	0
Cosine similarity, LCS, and word n-gram similarity were also applied to measure the similarity between the edit comment and the turn text as well as the similarity between the edit comment and the turn topic name.	LCS	lexical conceptual structure	0
Cosine similarity, LCS, and word n-gram similarity were also applied to measure the similarity between the edit comment and the turn text as well as the similarity between the edit comment and the turn topic name.	LCS	longest common substring	0
Cosine similarity, LCS, and word n-gram similarity were also applied to measure the similarity between the edit comment and the turn text as well as the similarity between the edit comment and the turn topic name.	LCS	least common subsumer	0
Cosine similarity, LCS, and word n-gram similarity were also applied to measure the similarity between the edit comment and the turn text as well as the similarity between the edit comment and the turn topic name.	LCS	Longest Common Subsequenc	0
Cosine similarity, LCS, and word n-gram similarity were also applied to measure the similarity between the edit comment and the turn text as well as the similarity between the edit comment and the turn topic name.	LCS	Longest Common Sub	0
Cosine similarity, LCS, and word n-gram similarity were also applied to measure the similarity between the edit comment and the turn text as well as the similarity between the edit comment and the turn topic name.	LCS	longest common subsequence	1
Cosine similarity, LCS, and word n-gram similarity were also applied to measure the similarity between the edit comment and the turn text as well as the similarity between the edit comment and the turn topic name.	LCS	longer common sequence	0
Cosine similarity, LCS, and word n-gram similarity were also applied to measure the similarity between the edit comment and the turn text as well as the similarity between the edit comment and the turn topic name.	LCS	lexical conccplual structure	0
With tile description length of a model de-  fined in the above manner, we wish to select a  model having the MDL and  output it as the result of clustering.	MDL	minimum description length	1
With tile description length of a model de-  fined in the above manner, we wish to select a  model having the MDL and  output it as the result of clustering.	MDL	mum description length	0
With tile description length of a model de-  fined in the above manner, we wish to select a  model having the MDL and  output it as the result of clustering.	MDL	Minimum Description Length	0
5 Experimental setup To test whether MDL is a good driver for unsupervised inversion transduc- tion induction, we implemented and executed the method described above.	MDL	minimum description length	1
5 Experimental setup To test whether MDL is a good driver for unsupervised inversion transduc- tion induction, we implemented and executed the method described above.	MDL	mum description length	0
5 Experimental setup To test whether MDL is a good driver for unsupervised inversion transduc- tion induction, we implemented and executed the method described above.	MDL	Minimum Description Length	0
Comparing the MDL principle and boosting in the automatic analysis of discourse.	MDL	minimum description length	1
Comparing the MDL principle and boosting in the automatic analysis of discourse.	MDL	mum description length	0
Comparing the MDL principle and boosting in the automatic analysis of discourse.	MDL	Minimum Description Length	0
7 Conclusions We have presented a minimalist, unsupervised learning model that induces relatively clean phrasal ITGs by iteratively splitting existing rules into smaller rules using a theoretically well- founded MDL objective.	MDL	minimum description length	1
7 Conclusions We have presented a minimalist, unsupervised learning model that induces relatively clean phrasal ITGs by iteratively splitting existing rules into smaller rules using a theoretically well- founded MDL objective.	MDL	mum description length	0
7 Conclusions We have presented a minimalist, unsupervised learning model that induces relatively clean phrasal ITGs by iteratively splitting existing rules into smaller rules using a theoretically well- founded MDL objective.	MDL	Minimum Description Length	0
successor count as a means to re- duce the search space of a more powerful al- gorithm based on MDL (Marcken, 1996).	MDL	minimum description length	1
successor count as a means to re- duce the search space of a more powerful al- gorithm based on MDL (Marcken, 1996).	MDL	mum description length	0
successor count as a means to re- duce the search space of a more powerful al- gorithm based on MDL (Marcken, 1996).	MDL	Minimum Description Length	0
Besides specifying the means for forming new ECG constructions, the acquisition model provides an overarching computational framework for converging on an optimal set of constructions, based on a MDL principle ( Rissanen 1978) that favors compactness in describing both the grammar and the statistical properties of the data.	MDL	minimum description length	1
Besides specifying the means for forming new ECG constructions, the acquisition model provides an overarching computational framework for converging on an optimal set of constructions, based on a MDL principle ( Rissanen 1978) that favors compactness in describing both the grammar and the statistical properties of the data.	MDL	mum description length	0
Besides specifying the means for forming new ECG constructions, the acquisition model provides an overarching computational framework for converging on an optimal set of constructions, based on a MDL principle ( Rissanen 1978) that favors compactness in describing both the grammar and the statistical properties of the data.	MDL	Minimum Description Length	0
Os-  borne, 1999) extended a definite clause grammar  with rules induced by a learner that was based upon  the maxiMDL principle.	MDL	minimum description length	0
Os-  borne, 1999) extended a definite clause grammar  with rules induced by a learner that was based upon  the maxiMDL principle.	MDL	mum description length	1
Os-  borne, 1999) extended a definite clause grammar  with rules induced by a learner that was based upon  the maxiMDL principle.	MDL	Minimum Description Length	0
The min- iMDL principle in coding and modeling.	MDL	minimum description length	0
The min- iMDL principle in coding and modeling.	MDL	mum description length	1
The min- iMDL principle in coding and modeling.	MDL	Minimum Description Length	0
In Figure 2, we present information about the length of the Wikipedia entity descriptions for English and for the language other than English with the maxiMDL.	MDL	minimum description length	0
In Figure 2, we present information about the length of the Wikipedia entity descriptions for English and for the language other than English with the maxiMDL.	MDL	mum description length	1
In Figure 2, we present information about the length of the Wikipedia entity descriptions for English and for the language other than English with the maxiMDL.	MDL	Minimum Description Length	0
Comparing the miniMDL principle and boosting in the automatic analysis of discourse.	MDL	minimum description length	0
Comparing the miniMDL principle and boosting in the automatic analysis of discourse.	MDL	mum description length	1
Comparing the miniMDL principle and boosting in the automatic analysis of discourse.	MDL	Minimum Description Length	0
successor count as a means to re- duce the search space of a more powerful al- gorithm based on miniMDL (Marcken, 1996).	MDL	minimum description length	0
successor count as a means to re- duce the search space of a more powerful al- gorithm based on miniMDL (Marcken, 1996).	MDL	mum description length	1
successor count as a means to re- duce the search space of a more powerful al- gorithm based on miniMDL (Marcken, 1996).	MDL	Minimum Description Length	0
Besides specifying the means for forming new ECG constructions, the acquisition model provides an overarching computational framework for converging on an optimal set of constructions, based on a miniMDL principle ( Rissanen 1978) that favors compactness in describing both the grammar and the statistical properties of the data.	MDL	minimum description length	0
Besides specifying the means for forming new ECG constructions, the acquisition model provides an overarching computational framework for converging on an optimal set of constructions, based on a miniMDL principle ( Rissanen 1978) that favors compactness in describing both the grammar and the statistical properties of the data.	MDL	mum description length	1
Besides specifying the means for forming new ECG constructions, the acquisition model provides an overarching computational framework for converging on an optimal set of constructions, based on a miniMDL principle ( Rissanen 1978) that favors compactness in describing both the grammar and the statistical properties of the data.	MDL	Minimum Description Length	0
Discovering Morphemic Suffixes: A Case Study in MDL Induction.	MDL	minimum description length	0
Discovering Morphemic Suffixes: A Case Study in MDL Induction.	MDL	mum description length	0
Discovering Morphemic Suffixes: A Case Study in MDL Induction.	MDL	Minimum Description Length	1
The model of Cartwright & Brent (1997) uses an algorithm which incrementally merges word clusters so that a MDL criterion for a template grammar is optimized.	MDL	minimum description length	0
The model of Cartwright & Brent (1997) uses an algorithm which incrementally merges word clusters so that a MDL criterion for a template grammar is optimized.	MDL	mum description length	0
The model of Cartwright & Brent (1997) uses an algorithm which incrementally merges word clusters so that a MDL criterion for a template grammar is optimized.	MDL	Minimum Description Length	1
Their idea is closely related to the classic MDL princi- ple for model selection (Barron et al.,	MDL	minimum description length	0
Their idea is closely related to the classic MDL princi- ple for model selection (Barron et al.,	MDL	mum description length	0
Their idea is closely related to the classic MDL princi- ple for model selection (Barron et al.,	MDL	Minimum Description Length	1
c?2013 Association for Computational Linguistics Unsupervised Transduction Grammar Induction via MDL Markus Saers and Karteek Addanki and Dekai Wu Human Language Technology Center Dept.	MDL	minimum description length	0
c?2013 Association for Computational Linguistics Unsupervised Transduction Grammar Induction via MDL Markus Saers and Karteek Addanki and Dekai Wu Human Language Technology Center Dept.	MDL	mum description length	0
c?2013 Association for Computational Linguistics Unsupervised Transduction Grammar Induction via MDL Markus Saers and Karteek Addanki and Dekai Wu Human Language Technology Center Dept.	MDL	Minimum Description Length	1
The MDL Principle in Cod- ing and Modeling.	MDL	minimum description length	0
The MDL Principle in Cod- ing and Modeling.	MDL	mum description length	0
The MDL Principle in Cod- ing and Modeling.	MDL	Minimum Description Length	1
1.1 Hierarchical versus flat lexicons From the viewpoint of data compression and following the two-part MDL prin- ciple (Rissanen, 1978), Morfessor tries to minimize the number of bits needed to encode both the model parameters and the training data.	MDL	minimum description length	0
1.1 Hierarchical versus flat lexicons From the viewpoint of data compression and following the two-part MDL prin- ciple (Rissanen, 1978), Morfessor tries to minimize the number of bits needed to encode both the model parameters and the training data.	MDL	mum description length	0
1.1 Hierarchical versus flat lexicons From the viewpoint of data compression and following the two-part MDL prin- ciple (Rissanen, 1978), Morfessor tries to minimize the number of bits needed to encode both the model parameters and the training data.	MDL	Minimum Description Length	1
The alterna-  t:ives here are: OW the respective piece  of information by the correct one and re-gen-  erate the whole morphosyntactic surface  structure; or exchange just a partial structure.	OW	Overwrite	1
The alterna-  t:ives here are: OW the respective piece  of information by the correct one and re-gen-  erate the whole morphosyntactic surface  structure; or exchange just a partial structure.	OW	OmegaWiki	0
OW and add conservatively are also highly restricted operations.	OW	Overwrite	1
OW and add conservatively are also highly restricted operations.	OW	OmegaWiki	0
Thus, following Matuschek et al (2014), we induced a clustering of WordNet senses by aligning WordNet to the more coarse-grained OW LSR.	OW	Overwrite	0
Thus, following Matuschek et al (2014), we induced a clustering of WordNet senses by aligning WordNet to the more coarse-grained OW LSR.	OW	OmegaWiki	1
OW OW is a collaborative multilingual dictionary based on a relational database.	OW	Overwrite	0
OW OW is a collaborative multilingual dictionary based on a relational database.	OW	OmegaWiki	1
It currently contains nine resources in two lan- guages: English WordNet, Wiktionary, Wikipedia, FrameNet and VerbNet, German Wikipedia, Wiktionary and GermaNet, and multilingual OW modeled according to the LMF standard.	OW	Overwrite	0
It currently contains nine resources in two lan- guages: English WordNet, Wiktionary, Wikipedia, FrameNet and VerbNet, German Wikipedia, Wiktionary and GermaNet, and multilingual OW modeled according to the LMF standard.	OW	OmegaWiki	1
We observe that translation 11http://www.statmt.org/moses/ 12OW consists of interlinked language- independent concepts to which lexicalizations in several languages are attached.	OW	Overwrite	0
We observe that translation 11http://www.statmt.org/moses/ 12OW consists of interlinked language- independent concepts to which lexicalizations in several languages are attached.	OW	OmegaWiki	1
To improve alignment coverage, we re- trained the alignment model by supplying GIZA++ with an English-French bilingual dictionary that we assembled using three online dictionary databases: OW, Wiktionary, and Universal Dictionary.	OW	Overwrite	0
To improve alignment coverage, we re- trained the alignment model by supplying GIZA++ with an English-French bilingual dictionary that we assembled using three online dictionary databases: OW, Wiktionary, and Universal Dictionary.	OW	OmegaWiki	1
Currently, we are using AT&T Bell  Laboratories' TTS System).	TTS	Text To Speech	1
Currently, we are using AT&T Bell  Laboratories' TTS System).	TTS	Text-to-speech	0
Currently, we are using AT&T Bell  Laboratories' TTS System).	TTS	text-to-speech synthesis	0
It handles ellipsis and refering expressions and also provides the  final prosodically annotated output to the TTS ('ITS) synthesiser.	TTS	Text To Speech	1
It handles ellipsis and refering expressions and also provides the  final prosodically annotated output to the TTS ('ITS) synthesiser.	TTS	Text-to-speech	0
It handles ellipsis and refering expressions and also provides the  final prosodically annotated output to the TTS ('ITS) synthesiser.	TTS	text-to-speech synthesis	0
The system uses a 14k vocabulary, automatically generated by the AT&T Labs NextGen TTS system.	TTS	Text To Speech	1
The system uses a 14k vocabulary, automatically generated by the AT&T Labs NextGen TTS system.	TTS	Text-to-speech	0
The system uses a 14k vocabulary, automatically generated by the AT&T Labs NextGen TTS system.	TTS	text-to-speech synthesis	0
The remaining predictions are sent to the TTS engine that articulates the top  CDC predictions at the user's request.	TTS	Text To Speech	1
The remaining predictions are sent to the TTS engine that articulates the top  CDC predictions at the user's request.	TTS	Text-to-speech	0
The remaining predictions are sent to the TTS engine that articulates the top  CDC predictions at the user's request.	TTS	text-to-speech synthesis	0
server generally implements a key system function including Speech Recognition, Frame Construction (language parsing), Context Tracking, Dialogue Management, Application Interface, Language Generation, and TTS.	TTS	Text To Speech	0
server generally implements a key system function including Speech Recognition, Frame Construction (language parsing), Context Tracking, Dialogue Management, Application Interface, Language Generation, and TTS.	TTS	Text-to-speech	1
server generally implements a key system function including Speech Recognition, Frame Construction (language parsing), Context Tracking, Dialogue Management, Application Interface, Language Generation, and TTS.	TTS	text-to-speech synthesis	0
1 System configuration  Interface  User submits anew query  by selecting one or more  specific terms displayed  on the interface (web  page)  User enters an initial query  .I Reformulate the L query \]~'  q, ,   I  Search over the \[  I database  ,  I  I Display search result /  Summarize individual  TTS i  conversion :  Fig.	TTS	Text To Speech	0
1 System configuration  Interface  User submits anew query  by selecting one or more  specific terms displayed  on the interface (web  page)  User enters an initial query  .I Reformulate the L query \]~'  q, ,   I  Search over the \[  I database  ,  I  I Display search result /  Summarize individual  TTS i  conversion :  Fig.	TTS	Text-to-speech	1
1 System configuration  Interface  User submits anew query  by selecting one or more  specific terms displayed  on the interface (web  page)  User enters an initial query  .I Reformulate the L query \]~'  q, ,   I  Search over the \[  I database  ,  I  I Display search result /  Summarize individual  TTS i  conversion :  Fig.	TTS	text-to-speech synthesis	0
Each server generally implements a key system function including Speech Recognition, Frame Construction (language parsing), Context Tracking, Dialogue Management, Application Interface, Language Generation, and TTS.	TTS	Text To Speech	0
Each server generally implements a key system function including Speech Recognition, Frame Construction (language parsing), Context Tracking, Dialogue Management, Application Interface, Language Generation, and TTS.	TTS	Text-to-speech	1
Each server generally implements a key system function including Speech Recognition, Frame Construction (language parsing), Context Tracking, Dialogue Management, Application Interface, Language Generation, and TTS.	TTS	text-to-speech synthesis	0
As a final  note, we suggest hat its fundamental rationale is  arguably also highly pertinent to TTS  systems, which, however, cannot be elaborated here.	TTS	Text To Speech	0
As a final  note, we suggest hat its fundamental rationale is  arguably also highly pertinent to TTS  systems, which, however, cannot be elaborated here.	TTS	Text-to-speech	1
As a final  note, we suggest hat its fundamental rationale is  arguably also highly pertinent to TTS  systems, which, however, cannot be elaborated here.	TTS	text-to-speech synthesis	0
2001 HRL Laboratories, LLC, All rights reserved Speech Recognition Language Generation Dialogue Management Application Back-end Context Tracking Frame Construction Audio Server TTS Conversion Hub Figure 1.	TTS	Text To Speech	0
2001 HRL Laboratories, LLC, All rights reserved Speech Recognition Language Generation Dialogue Management Application Back-end Context Tracking Frame Construction Audio Server TTS Conversion Hub Figure 1.	TTS	Text-to-speech	1
2001 HRL Laboratories, LLC, All rights reserved Speech Recognition Language Generation Dialogue Management Application Back-end Context Tracking Frame Construction Audio Server TTS Conversion Hub Figure 1.	TTS	text-to-speech synthesis	0
Build the Microvox TTS  synthesizer.	TTS	Text To Speech	0
Build the Microvox TTS  synthesizer.	TTS	Text-to-speech	1
Build the Microvox TTS  synthesizer.	TTS	text-to-speech synthesis	0
Computational Linguistics Volume 16, No.3, pp.155-  170 1990  \[3\] Bachenkn, J., and Fitzpatrick, E.  Parsing for Prosody: What a TTS system  needs from syntax  Proceedings of lE~E Artificial Intelligence Systems in  Government (AISIG), 1989  \[41 Bachenko, J., Fitzpatrick, E., and Wright, C.E.  The contribution of parsing to prosodic phrasing in an  experimental text-to-speech system  Proceedings of the 24th Annual Meeting of the  Association for Computational Linguistics, pp.	TTS	Text To Speech	0
Computational Linguistics Volume 16, No.3, pp.155-  170 1990  \[3\] Bachenkn, J., and Fitzpatrick, E.  Parsing for Prosody: What a TTS system  needs from syntax  Proceedings of lE~E Artificial Intelligence Systems in  Government (AISIG), 1989  \[41 Bachenko, J., Fitzpatrick, E., and Wright, C.E.  The contribution of parsing to prosodic phrasing in an  experimental text-to-speech system  Proceedings of the 24th Annual Meeting of the  Association for Computational Linguistics, pp.	TTS	Text-to-speech	1
Computational Linguistics Volume 16, No.3, pp.155-  170 1990  \[3\] Bachenkn, J., and Fitzpatrick, E.  Parsing for Prosody: What a TTS system  needs from syntax  Proceedings of lE~E Artificial Intelligence Systems in  Government (AISIG), 1989  \[41 Bachenko, J., Fitzpatrick, E., and Wright, C.E.  The contribution of parsing to prosodic phrasing in an  experimental text-to-speech system  Proceedings of the 24th Annual Meeting of the  Association for Computational Linguistics, pp.	TTS	text-to-speech synthesis	0
2 Previous Work on Stress Prediction Pronunciation prediction, of which stress pre- diction is a part, is important for many speech applications including automatic speech recog- nition, TTS, and translit- eration for, say, machine translation.	TTS	Text To Speech	0
2 Previous Work on Stress Prediction Pronunciation prediction, of which stress pre- diction is a part, is important for many speech applications including automatic speech recog- nition, TTS, and translit- eration for, say, machine translation.	TTS	Text-to-speech	0
2 Previous Work on Stress Prediction Pronunciation prediction, of which stress pre- diction is a part, is important for many speech applications including automatic speech recog- nition, TTS, and translit- eration for, say, machine translation.	TTS	text-to-speech synthesis	1
A large number of text process- ing applications have already employed techniques for automatic subjectivity analysis, including auto- matic expressive TTS (Alm et al.,	TTS	Text To Speech	0
A large number of text process- ing applications have already employed techniques for automatic subjectivity analysis, including auto- matic expressive TTS (Alm et al.,	TTS	Text-to-speech	0
A large number of text process- ing applications have already employed techniques for automatic subjectivity analysis, including auto- matic expressive TTS (Alm et al.,	TTS	text-to-speech synthesis	1
Multilingual TTS.	TTS	Text To Speech	0
Multilingual TTS.	TTS	Text-to-speech	0
Multilingual TTS.	TTS	text-to-speech synthesis	1
Multilingual text analy-  sis for TTS.	TTS	Text To Speech	0
Multilingual text analy-  sis for TTS.	TTS	Text-to-speech	0
Multilingual text analy-  sis for TTS.	TTS	text-to-speech synthesis	1
Word stress assignment in a TTS system for British English.	TTS	Text To Speech	0
Word stress assignment in a TTS system for British English.	TTS	Text-to-speech	0
Word stress assignment in a TTS system for British English.	TTS	text-to-speech synthesis	1
For these reasons, computational mod-  eling of prosodic phrases is important both for TTS and speech  understanding applications.	TTS	Text To Speech	0
For these reasons, computational mod-  eling of prosodic phrases is important both for TTS and speech  understanding applications.	TTS	Text-to-speech	0
For these reasons, computational mod-  eling of prosodic phrases is important both for TTS and speech  understanding applications.	TTS	text-to-speech synthesis	1
for  ident i fy ing  STA verbs, Many  instances of this pat tern  appear  to be  pass ives  or s tat ive use of normal ly  non-  s tat ive verbs.	STA	stat ive	1
for  ident i fy ing  STA verbs, Many  instances of this pat tern  appear  to be  pass ives  or s tat ive use of normal ly  non-  s tat ive verbs.	STA	stat ive(	0
for  ident i fy ing  STA verbs, Many  instances of this pat tern  appear  to be  pass ives  or s tat ive use of normal ly  non-  s tat ive verbs.	STA	stat	0
Act ion  verbs can appear in a number of  embedded sentences  where STAs cannot  be used.	STA	stat ive	1
Act ion  verbs can appear in a number of  embedded sentences  where STAs cannot  be used.	STA	stat ive(	0
Act ion  verbs can appear in a number of  embedded sentences  where STAs cannot  be used.	STA	stat	0
This paper descr ibes  methods  for f inding taxonomy and set -membersh ip   re lat ionships,  recogniz ing nouns that  ord inar i ly  represent  human beings, and  ident i fy ing act ive and STA verbs and  adject ives.	STA	stat ive	1
This paper descr ibes  methods  for f inding taxonomy and set -membersh ip   re lat ionships,  recogniz ing nouns that  ord inar i ly  represent  human beings, and  ident i fy ing act ive and STA verbs and  adject ives.	STA	stat ive(	0
This paper descr ibes  methods  for f inding taxonomy and set -membersh ip   re lat ionships,  recogniz ing nouns that  ord inar i ly  represent  human beings, and  ident i fy ing act ive and STA verbs and  adject ives.	STA	stat	0
In testing this  one should  i. always use sentences with a non-stative basic  proposit ion, for i~ the la t te r  is STA the  sentence can never be habitual (of.	STA	stat ive	1
In testing this  one should  i. always use sentences with a non-stative basic  proposit ion, for i~ the la t te r  is STA the  sentence can never be habitual (of.	STA	stat ive(	0
In testing this  one should  i. always use sentences with a non-stative basic  proposit ion, for i~ the la t te r  is STA the  sentence can never be habitual (of.	STA	stat	0
l+STAI+durative ..ADJECTIUE, DA, etc.	STA	stat ive	1
l+STAI+durative ..ADJECTIUE, DA, etc.	STA	stat ive(	0
l+STAI+durative ..ADJECTIUE, DA, etc.	STA	stat	0
IF Lexical Aspect=stative  THEN Situation is a state  AND its Time Argument is a period  AND this period is unbounded  As shown here, if the lexical aspect of a predica-  tion is STA,  its grammatical aspect is  irrelevant.	STA	stat ive	1
IF Lexical Aspect=stative  THEN Situation is a state  AND its Time Argument is a period  AND this period is unbounded  As shown here, if the lexical aspect of a predica-  tion is STA,  its grammatical aspect is  irrelevant.	STA	stat ive(	0
IF Lexical Aspect=stative  THEN Situation is a state  AND its Time Argument is a period  AND this period is unbounded  As shown here, if the lexical aspect of a predica-  tion is STA,  its grammatical aspect is  irrelevant.	STA	stat	0
1 In t roduct ion   We present a STAistical method for determin-  ing pronoun anaphora.	STA	stat ive	0
1 In t roduct ion   We present a STAistical method for determin-  ing pronoun anaphora.	STA	stat ive(	0
1 In t roduct ion   We present a STAistical method for determin-  ing pronoun anaphora.	STA	stat	1
We incorpo-  rate multiple anaphora resolution factors into  a STAistical framework - -  specifically the dis-  tance between the pronoun and the proposed  antecedent, gender/number/animaticity of the  proposed antecedent, governing head informa-  tion and noun phrase repetition.	STA	stat ive	0
We incorpo-  rate multiple anaphora resolution factors into  a STAistical framework - -  specifically the dis-  tance between the pronoun and the proposed  antecedent, gender/number/animaticity of the  proposed antecedent, governing head informa-  tion and noun phrase repetition.	STA	stat ive(	0
We incorpo-  rate multiple anaphora resolution factors into  a STAistical framework - -  specifically the dis-  tance between the pronoun and the proposed  antecedent, gender/number/animaticity of the  proposed antecedent, governing head informa-  tion and noun phrase repetition.	STA	stat	1
This equation is decomposed into pieces that  correspond to all the above factors but are more  STAistically manageable.	STA	stat ive	0
This equation is decomposed into pieces that  correspond to all the above factors but are more  STAistically manageable.	STA	stat ive(	0
This equation is decomposed into pieces that  correspond to all the above factors but are more  STAistically manageable.	STA	stat	1
From  this data, we collect the three STAistics detailed  ha the following subsections.	STA	stat ive	0
From  this data, we collect the three STAistics detailed  ha the following subsections.	STA	stat ive(	0
From  this data, we collect the three STAistics detailed  ha the following subsections.	STA	stat	1
In building a STAistical parser for the Penn  Tree-bank various STALstics have been collected  (Charniak, 1997), two of which are P(w~lh, t, l)  and P(w~lt , l).	STA	stat ive	0
In building a STAistical parser for the Penn  Tree-bank various STALstics have been collected  (Charniak, 1997), two of which are P(w~lh, t, l)  and P(w~lt , l).	STA	stat ive(	0
In building a STAistical parser for the Penn  Tree-bank various STALstics have been collected  (Charniak, 1997), two of which are P(w~lh, t, l)  and P(w~lt , l).	STA	stat	1
Currently, the target parsing engine is the Stuttgart  FST Tools (http://www.ims.uni-                                                   6 We have resisted the temptation to make our linguistics too  modern, since linguistic theories also have a short half-life.	FST	Finite State Transducer	1
Currently, the target parsing engine is the Stuttgart  FST Tools (http://www.ims.uni-                                                   6 We have resisted the temptation to make our linguistics too  modern, since linguistic theories also have a short half-life.	FST	finite state transducer	0
This exten-  sion treats phonological features as I/O tapes for  FSTs in a parallel sequential  incrementation (PSI) architecture; phonological  processes (e.g. assimilation) are seen as variants of  an elementary unification operation over feature  tapes (linear unification phonology, LUP).	FST	Finite State Transducer	1
This exten-  sion treats phonological features as I/O tapes for  FSTs in a parallel sequential  incrementation (PSI) architecture; phonological  processes (e.g. assimilation) are seen as variants of  an elementary unification operation over feature  tapes (linear unification phonology, LUP).	FST	finite state transducer	0
c?2012 Association for Computational Linguistics Automated Essay Scoring Based on FST: towards ASR Transcription of Oral English Speech Xingyuan Peng?,	FST	Finite State Transducer	1
c?2012 Association for Computational Linguistics Automated Essay Scoring Based on FST: towards ASR Transcription of Oral English Speech Xingyuan Peng?,	FST	finite state transducer	0
In contrast, Algorithm/Model Time Acc.(%) Complexity This paper Stack Dependency Analysis (SVMs)  89.56 Stack Dependency Analysis (linear SVMs)  87.36 KM02 Cascaded Chunking (SVMs)  89.29 KM00 Backward Beam Search (SVMs)  89.09 USI99 Backward Beam Search (ME)  87.14 Seki00 Deterministic FST  77.97 Table 2: Comparison to Related Work.	FST	Finite State Transducer	1
In contrast, Algorithm/Model Time Acc.(%) Complexity This paper Stack Dependency Analysis (SVMs)  89.56 Stack Dependency Analysis (linear SVMs)  87.36 KM02 Cascaded Chunking (SVMs)  89.29 KM00 Backward Beam Search (SVMs)  89.09 USI99 Backward Beam Search (ME)  87.14 Seki00 Deterministic FST  77.97 Table 2: Comparison to Related Work.	FST	finite state transducer	0
720  Japanese Dependency Analysis  using a Determinist ic FST  Satosh i  Sek ine   Computer  Science Depar tment   New York University  715 Broadway, 7th floor  New York, NY 10003, USA  Abst ract   A deternfinistic finite state transducer is a fast  device fbr analyzing strings.	FST	Finite State Transducer	1
720  Japanese Dependency Analysis  using a Determinist ic FST  Satosh i  Sek ine   Computer  Science Depar tment   New York University  715 Broadway, 7th floor  New York, NY 10003, USA  Abst ract   A deternfinistic finite state transducer is a fast  device fbr analyzing strings.	FST	finite state transducer	0
4 Implementing Voting Constraints with FSTs  The approach described above can also be implemented by finite state transducers.	FST	Finite State Transducer	1
4 Implementing Voting Constraints with FSTs  The approach described above can also be implemented by finite state transducers.	FST	finite state transducer	0
A weighted FST translation template model for statistical machine translation.	FST	Finite State Transducer	0
A weighted FST translation template model for statistical machine translation.	FST	finite state transducer	1
The morphological component of the plat- form is a FST that makes use of the  lexicon for traversing the arcs of the diagram  (adopted, with changes, from Kemal Oflazer?s work  on Turkish morphology (Oflazer 1993)) to associate  a character string with the list of meaning contribu- tions of its morphemes.	FST	Finite State Transducer	0
The morphological component of the plat- form is a FST that makes use of the  lexicon for traversing the arcs of the diagram  (adopted, with changes, from Kemal Oflazer?s work  on Turkish morphology (Oflazer 1993)) to associate  a character string with the list of meaning contribu- tions of its morphemes.	FST	finite state transducer	1
We  adopt a FST as a computing model  which governs the fundamental interpretation control.	FST	Finite State Transducer	0
We  adopt a FST as a computing model  which governs the fundamental interpretation control.	FST	finite state transducer	1
Head transducer models consist of collections of  weighted FSTs associated with  pairs of lexical items in a bilingual exicon.	FST	Finite State Transducer	0
Head transducer models consist of collections of  weighted FSTs associated with  pairs of lexical items in a bilingual exicon.	FST	finite state transducer	1
The Temporal Expression Tagger we have developed is based on a large coverage cascade of FSTs and our Event Tagger on a set of simple heuris- tics applied over local context in a chunked text.	FST	Finite State Transducer	0
The Temporal Expression Tagger we have developed is based on a large coverage cascade of FSTs and our Event Tagger on a set of simple heuris- tics applied over local context in a chunked text.	FST	finite state transducer	1
He begins with FSTs, which es- sentially implement a general preference for onsets.	FST	Finite State Transducer	0
He begins with FSTs, which es- sentially implement a general preference for onsets.	FST	finite state transducer	1
The European project IST ALERT: Alert sys- tem for SDI (http://www.fb9- ti.uni-duisburg.de/alert) aims to associate state- of-the-art speech recognition with audio and video segmentation and automatic topic index- ing to develop an automatic media monitoring demonstrator and evaluate it in the context of real world applications.	SDI	selective dissemination	1
The European project IST ALERT: Alert sys- tem for SDI (http://www.fb9- ti.uni-duisburg.de/alert) aims to associate state- of-the-art speech recognition with audio and video segmentation and automatic topic index- ing to develop an automatic media monitoring demonstrator and evaluate it in the context of real world applications.	SDI	Sense Data Item	0
A va- riety of near-term applications are possible such as audio data mining, SDI of information (News-on-Demand), media monitor- ing, content-based audio and video retrieval.	SDI	selective dissemination	1
A va- riety of near-term applications are possible such as audio data mining, SDI of information (News-on-Demand), media monitor- ing, content-based audio and video retrieval.	SDI	Sense Data Item	0
The term can also be a variable that is used in  statist ical c lassif icat ion or clustering processes of documents (\[BLO  92\] and \[STA 95a\]), or in SDI of information, in  which it is used to bring together a document to be disseminated and  its target \[STA 93\].	SDI	selective dissemination	1
The term can also be a variable that is used in  statist ical c lassif icat ion or clustering processes of documents (\[BLO  92\] and \[STA 95a\]), or in SDI of information, in  which it is used to bring together a document to be disseminated and  its target \[STA 93\].	SDI	Sense Data Item	0
Why not just include in each  Key Data Item a full list of pointers to the corresponding SDIs.	SDI	selective dissemination	0
Why not just include in each  Key Data Item a full list of pointers to the corresponding SDIs.	SDI	Sense Data Item	1
We use MERT to tune the best weights for each DL, and dmax = 10 performs the best on our dev set.	DL	distortion limit	1
We use MERT to tune the best weights for each DL, and dmax = 10 performs the best on our dev set.	DL	decision list	0
We use MERT to tune the best weights for each DL, and dmax = 10 performs the best on our dev set.	DL	Description Logics	0
We use MERT to tune the best weights for each DL, and dmax = 10 performs the best on our dev set.	DL	Description Logic	0
2), unless a small DL (say, d=5) further restricts the possi- ble set of reorderings to those local ones by ruling out any long-distance reorderings that have a ?	DL	distortion limit	1
2), unless a small DL (say, d=5) further restricts the possi- ble set of reorderings to those local ones by ruling out any long-distance reorderings that have a ?	DL	decision list	0
2), unless a small DL (say, d=5) further restricts the possi- ble set of reorderings to those local ones by ruling out any long-distance reorderings that have a ?	DL	Description Logics	0
2), unless a small DL (say, d=5) further restricts the possi- ble set of reorderings to those local ones by ruling out any long-distance reorderings that have a ?	DL	Description Logic	0
Moses is shown with various DLs (0, 6, 10, +?;	DL	distortion limit	1
Moses is shown with various DLs (0, 6, 10, +?;	DL	decision list	0
Moses is shown with various DLs (0, 6, 10, +?;	DL	Description Logics	0
Moses is shown with various DLs (0, 6, 10, +?;	DL	Description Logic	0
Consistent with the theoretical anal- ysis in Section 2, Moses with no DL (dmax = +?)	DL	distortion limit	1
Consistent with the theoretical anal- ysis in Section 2, Moses with no DL (dmax = +?)	DL	decision list	0
Consistent with the theoretical anal- ysis in Section 2, Moses with no DL (dmax = +?)	DL	Description Logics	0
Consistent with the theoretical anal- ysis in Section 2, Moses with no DL (dmax = +?)	DL	Description Logic	0
Our linear-time incremental decoder with the small beam of size b = 10 achieves a BLEU score of 29.54, compara- ble to Moses with the optimal DL of 10 (BLEU score 29.41).	DL	distortion limit	1
Our linear-time incremental decoder with the small beam of size b = 10 achieves a BLEU score of 29.54, compara- ble to Moses with the optimal DL of 10 (BLEU score 29.41).	DL	decision list	0
Our linear-time incremental decoder with the small beam of size b = 10 achieves a BLEU score of 29.54, compara- ble to Moses with the optimal DL of 10 (BLEU score 29.41).	DL	Description Logics	0
Our linear-time incremental decoder with the small beam of size b = 10 achieves a BLEU score of 29.54, compara- ble to Moses with the optimal DL of 10 (BLEU score 29.41).	DL	Description Logic	0
As a special case, phrase-based decoding with DL dmax is O(nbdmax). *:	DL	distortion limit	1
As a special case, phrase-based decoding with DL dmax is O(nbdmax). *:	DL	decision list	0
As a special case, phrase-based decoding with DL dmax is O(nbdmax). *:	DL	Description Logics	0
As a special case, phrase-based decoding with DL dmax is O(nbdmax). *:	DL	Description Logic	0
with Moses at various DLs (dmax=0, 6, 10, and +?).	DL	distortion limit	1
with Moses at various DLs (dmax=0, 6, 10, and +?).	DL	decision list	0
with Moses at various DLs (dmax=0, 6, 10, and +?).	DL	Description Logics	0
with Moses at various DLs (dmax=0, 6, 10, and +?).	DL	Description Logic	0
817 3 Distinguishing Mass and Count Nouns In the proposed method, DLs [13] are used to distinguish mass and count nouns.	DL	distortion limit	0
817 3 Distinguishing Mass and Count Nouns In the proposed method, DLs [13] are used to distinguish mass and count nouns.	DL	decision list	1
817 3 Distinguishing Mass and Count Nouns In the proposed method, DLs [13] are used to distinguish mass and count nouns.	DL	Description Logics	0
817 3 Distinguishing Mass and Count Nouns In the proposed method, DLs [13] are used to distinguish mass and count nouns.	DL	Description Logic	0
Section 3.3 explains the method for distinguishing mass and count nouns using the DLs.	DL	distortion limit	0
Section 3.3 explains the method for distinguishing mass and count nouns using the DLs.	DL	decision list	1
Section 3.3 explains the method for distinguishing mass and count nouns using the DLs.	DL	Description Logics	0
Section 3.3 explains the method for distinguishing mass and count nouns using the DLs.	DL	Description Logic	0
Exploring au- tomatic word sense disambiguation with DLs and the Web.	DL	distortion limit	0
Exploring au- tomatic word sense disambiguation with DLs and the Web.	DL	decision list	1
Exploring au- tomatic word sense disambiguation with DLs and the Web.	DL	Description Logics	0
Exploring au- tomatic word sense disambiguation with DLs and the Web.	DL	Description Logic	0
Detecting Article Errors Based on the Mass Count Distinction 817 3 Distinguishing Mass and Count Nouns In the proposed method, DLs [13] are used to distinguish mass and count nouns.	DL	distortion limit	0
Detecting Article Errors Based on the Mass Count Distinction 817 3 Distinguishing Mass and Count Nouns In the proposed method, DLs [13] are used to distinguish mass and count nouns.	DL	decision list	1
Detecting Article Errors Based on the Mass Count Distinction 817 3 Distinguishing Mass and Count Nouns In the proposed method, DLs [13] are used to distinguish mass and count nouns.	DL	Description Logics	0
Detecting Article Errors Based on the Mass Count Distinction 817 3 Distinguishing Mass and Count Nouns In the proposed method, DLs [13] are used to distinguish mass and count nouns.	DL	Description Logic	0
Section 3.2 describes how to learn DLs from the training data.	DL	distortion limit	0
Section 3.2 describes how to learn DLs from the training data.	DL	decision list	1
Section 3.2 describes how to learn DLs from the training data.	DL	Description Logics	0
Section 3.2 describes how to learn DLs from the training data.	DL	Description Logic	0
These in- clude a variety of Bayesian classifiers (Golding, 1995; Golding and Schabes, 1996), DLs (Golding, 1995) transformation-based learning (Mangu and Brill, 1997), Latent Semantic Analysis (LSA) (Jones and Martin, 1997), multiplicative weight update algorithms (Golding and Roth, 1999), and augmented mixture mod- els (Cucerzan and Yarowsky, 2002).	DL	distortion limit	0
These in- clude a variety of Bayesian classifiers (Golding, 1995; Golding and Schabes, 1996), DLs (Golding, 1995) transformation-based learning (Mangu and Brill, 1997), Latent Semantic Analysis (LSA) (Jones and Martin, 1997), multiplicative weight update algorithms (Golding and Roth, 1999), and augmented mixture mod- els (Cucerzan and Yarowsky, 2002).	DL	decision list	1
These in- clude a variety of Bayesian classifiers (Golding, 1995; Golding and Schabes, 1996), DLs (Golding, 1995) transformation-based learning (Mangu and Brill, 1997), Latent Semantic Analysis (LSA) (Jones and Martin, 1997), multiplicative weight update algorithms (Golding and Roth, 1999), and augmented mixture mod- els (Cucerzan and Yarowsky, 2002).	DL	Description Logics	0
These in- clude a variety of Bayesian classifiers (Golding, 1995; Golding and Schabes, 1996), DLs (Golding, 1995) transformation-based learning (Mangu and Brill, 1997), Latent Semantic Analysis (LSA) (Jones and Martin, 1997), multiplicative weight update algorithms (Golding and Roth, 1999), and augmented mixture mod- els (Cucerzan and Yarowsky, 2002).	DL	Description Logic	0
Generally, DLs are learned from a set of manually tagged training data.	DL	distortion limit	0
Generally, DLs are learned from a set of manually tagged training data.	DL	decision list	1
Generally, DLs are learned from a set of manually tagged training data.	DL	Description Logics	0
Generally, DLs are learned from a set of manually tagged training data.	DL	Description Logic	0
709  Text Authoring, Knowledge Acquisition and DL Marc Dymetman Xerox Research Centre Europe 6 chemin de Maupertuis 38240 Meylan email: marc.dymetman@xrce.xerox.com Abstract We present a principled approach to the problem of con- necting a controlled document authoring system with a knowledge base.	DL	distortion limit	0
709  Text Authoring, Knowledge Acquisition and DL Marc Dymetman Xerox Research Centre Europe 6 chemin de Maupertuis 38240 Meylan email: marc.dymetman@xrce.xerox.com Abstract We present a principled approach to the problem of con- necting a controlled document authoring system with a knowledge base.	DL	decision list	0
709  Text Authoring, Knowledge Acquisition and DL Marc Dymetman Xerox Research Centre Europe 6 chemin de Maupertuis 38240 Meylan email: marc.dymetman@xrce.xerox.com Abstract We present a principled approach to the problem of con- necting a controlled document authoring system with a knowledge base.	DL	Description Logics	1
709  Text Authoring, Knowledge Acquisition and DL Marc Dymetman Xerox Research Centre Europe 6 chemin de Maupertuis 38240 Meylan email: marc.dymetman@xrce.xerox.com Abstract We present a principled approach to the problem of con- necting a controlled document authoring system with a knowledge base.	DL	Description Logic	0
(5)In this paper we address the problem of query          answering using views for non-recursive data log            queries embedded in a DL          knowledge base.	DL	distortion limit	0
(5)In this paper we address the problem of query          answering using views for non-recursive data log            queries embedded in a DL          knowledge base.	DL	decision list	0
(5)In this paper we address the problem of query          answering using views for non-recursive data log            queries embedded in a DL          knowledge base.	DL	Description Logics	1
(5)In this paper we address the problem of query          answering using views for non-recursive data log            queries embedded in a DL          knowledge base.	DL	Description Logic	0
We have presented a formal ap- proach to closed-world authoring that shows a correspon- dence between life-death problems and conjunctive Dat- alog queries, as well as a formal approach to open-world document authoring based on DL.	DL	distortion limit	0
We have presented a formal ap- proach to closed-world authoring that shows a correspon- dence between life-death problems and conjunctive Dat- alog queries, as well as a formal approach to open-world document authoring based on DL.	DL	decision list	0
We have presented a formal ap- proach to closed-world authoring that shows a correspon- dence between life-death problems and conjunctive Dat- alog queries, as well as a formal approach to open-world document authoring based on DL.	DL	Description Logics	1
We have presented a formal ap- proach to closed-world authoring that shows a correspon- dence between life-death problems and conjunctive Dat- alog queries, as well as a formal approach to open-world document authoring based on DL.	DL	Description Logic	0
In addition, the top level part of the ontology (i.e., the Tbox in the DL terminology) is very 1http://nlp.cs.swarthmore.edu/semeval/ tasks/task10/description.shtml 248 often modified during the ontology engineering life- cycle, for example by introducing new concepts and restructuring the subclass of hierarchy according to the renewed application needs required by the evo- lution of the application domain.	DL	distortion limit	0
In addition, the top level part of the ontology (i.e., the Tbox in the DL terminology) is very 1http://nlp.cs.swarthmore.edu/semeval/ tasks/task10/description.shtml 248 often modified during the ontology engineering life- cycle, for example by introducing new concepts and restructuring the subclass of hierarchy according to the renewed application needs required by the evo- lution of the application domain.	DL	decision list	0
In addition, the top level part of the ontology (i.e., the Tbox in the DL terminology) is very 1http://nlp.cs.swarthmore.edu/semeval/ tasks/task10/description.shtml 248 often modified during the ontology engineering life- cycle, for example by introducing new concepts and restructuring the subclass of hierarchy according to the renewed application needs required by the evo- lution of the application domain.	DL	Description Logics	1
In addition, the top level part of the ontology (i.e., the Tbox in the DL terminology) is very 1http://nlp.cs.swarthmore.edu/semeval/ tasks/task10/description.shtml 248 often modified during the ontology engineering life- cycle, for example by introducing new concepts and restructuring the subclass of hierarchy according to the renewed application needs required by the evo- lution of the application domain.	DL	Description Logic	0
In DL.	DL	distortion limit	0
In DL.	DL	decision list	0
In DL.	DL	Description Logics	1
In DL.	DL	Description Logic	0
Optimising Tableaux Decision Procedures for DL.	DL	distortion limit	0
Optimising Tableaux Decision Procedures for DL.	DL	decision list	0
Optimising Tableaux Decision Procedures for DL.	DL	Description Logics	1
Optimising Tableaux Decision Procedures for DL.	DL	Description Logic	0
The DL Handbook: Theory, Implementation, and Applica- tions.	DL	distortion limit	0
The DL Handbook: Theory, Implementation, and Applica- tions.	DL	decision list	0
The DL Handbook: Theory, Implementation, and Applica- tions.	DL	Description Logics	0
The DL Handbook: Theory, Implementation, and Applica- tions.	DL	Description Logic	1
Baader F., D. Calvanese, D. L. McGuinness, D. Nardi, P. F. Patel-Schneider (2003) The DL Hand- book : Theory, Implementation, Applications.	DL	distortion limit	0
Baader F., D. Calvanese, D. L. McGuinness, D. Nardi, P. F. Patel-Schneider (2003) The DL Hand- book : Theory, Implementation, Applications.	DL	decision list	0
Baader F., D. Calvanese, D. L. McGuinness, D. Nardi, P. F. Patel-Schneider (2003) The DL Hand- book : Theory, Implementation, Applications.	DL	Description Logics	0
Baader F., D. Calvanese, D. L. McGuinness, D. Nardi, P. F. Patel-Schneider (2003) The DL Hand- book : Theory, Implementation, Applications.	DL	Description Logic	1
Polarity compositions on the nodes  2.3 Tree DL in IG  Another specification of IG is that syntactic  structures can be underspecified: these structures  are trees descriptions.	DL	distortion limit	0
Polarity compositions on the nodes  2.3 Tree DL in IG  Another specification of IG is that syntactic  structures can be underspecified: these structures  are trees descriptions.	DL	decision list	0
Polarity compositions on the nodes  2.3 Tree DL in IG  Another specification of IG is that syntactic  structures can be underspecified: these structures  are trees descriptions.	DL	Description Logics	0
Polarity compositions on the nodes  2.3 Tree DL in IG  Another specification of IG is that syntactic  structures can be underspecified: these structures  are trees descriptions.	DL	Description Logic	1
Higher order relations and DL  Although description logics on which OIL and  RACER are based allow only binary relations, we  use OIL and Racer in a way that also allows us to  employ arbitrary n-ary relations and higher-order  relations.	DL	distortion limit	0
Higher order relations and DL  Although description logics on which OIL and  RACER are based allow only binary relations, we  use OIL and Racer in a way that also allows us to  employ arbitrary n-ary relations and higher-order  relations.	DL	decision list	0
Higher order relations and DL  Although description logics on which OIL and  RACER are based allow only binary relations, we  use OIL and Racer in a way that also allows us to  employ arbitrary n-ary relations and higher-order  relations.	DL	Description Logics	0
Higher order relations and DL  Although description logics on which OIL and  RACER are based allow only binary relations, we  use OIL and Racer in a way that also allows us to  employ arbitrary n-ary relations and higher-order  relations.	DL	Description Logic	1
In addition, the top level part of the ontology (i.e., the Tbox in the DLs terminology) is very 1http://nlp.cs.swarthmore.edu/semeval/ tasks/task10/description.shtml 248 often modified during the ontology engineering life- cycle, for example by introducing new concepts and restructuring the subclass of hierarchy according to the renewed application needs required by the evo- lution of the application domain.	DL	distortion limit	0
In addition, the top level part of the ontology (i.e., the Tbox in the DLs terminology) is very 1http://nlp.cs.swarthmore.edu/semeval/ tasks/task10/description.shtml 248 often modified during the ontology engineering life- cycle, for example by introducing new concepts and restructuring the subclass of hierarchy according to the renewed application needs required by the evo- lution of the application domain.	DL	decision list	0
In addition, the top level part of the ontology (i.e., the Tbox in the DLs terminology) is very 1http://nlp.cs.swarthmore.edu/semeval/ tasks/task10/description.shtml 248 often modified during the ontology engineering life- cycle, for example by introducing new concepts and restructuring the subclass of hierarchy according to the renewed application needs required by the evo- lution of the application domain.	DL	Description Logics	0
In addition, the top level part of the ontology (i.e., the Tbox in the DLs terminology) is very 1http://nlp.cs.swarthmore.edu/semeval/ tasks/task10/description.shtml 248 often modified during the ontology engineering life- cycle, for example by introducing new concepts and restructuring the subclass of hierarchy according to the renewed application needs required by the evo- lution of the application domain.	DL	Description Logic	1
The two KBs codify, in DL (Baader et al, 2003), assertions and concepts relevant for a given game scenario.	DL	distortion limit	0
The two KBs codify, in DL (Baader et al, 2003), assertions and concepts relevant for a given game scenario.	DL	decision list	0
The two KBs codify, in DL (Baader et al, 2003), assertions and concepts relevant for a given game scenario.	DL	Description Logics	0
The two KBs codify, in DL (Baader et al, 2003), assertions and concepts relevant for a given game scenario.	DL	Description Logic	1
A derivation in OT consists of an ORIGal candidate  set produced by a \[unction called GI,;N, and the subsequent  application of constraints o reduce tile candidate set, elimi-  nating non-optimal candidates and I)reserving those with  Ihe greatest harmony.	ORIG	origin	1
A derivation in OT consists of an ORIGal candidate  set produced by a \[unction called GI,;N, and the subsequent  application of constraints o reduce tile candidate set, elimi-  nating non-optimal candidates and I)reserving those with  Ihe greatest harmony.	ORIG	Origin	0
Similarly, at lexical level, words of foreign ORIG  are used to form multi-word terms (e.g. redun- dantan atribut (Engl.	ORIG	origin	1
Similarly, at lexical level, words of foreign ORIG  are used to form multi-word terms (e.g. redun- dantan atribut (Engl.	ORIG	Origin	0
For  example, at morphological level, foreign suffixes,  mostly ORIGating from Latin and Greek, are of- ten ?	ORIG	origin	1
For  example, at morphological level, foreign suffixes,  mostly ORIGating from Latin and Greek, are of- ten ?	ORIG	Origin	0
We subsumed the information in the ORIGal mor- phological tags in order to have the minimal number of categories needed for our task, listed in Table 1.2 In order to further reduce the number of features in a linguistically principled way, we took phrase boundaries into account: All words beyond a POS considered to be a phrase boundary marker (see Ta- ble 1) were assigned the tag empty.	ORIG	origin	1
We subsumed the information in the ORIGal mor- phological tags in order to have the minimal number of categories needed for our task, listed in Table 1.2 In order to further reduce the number of features in a linguistically principled way, we took phrase boundaries into account: All words beyond a POS considered to be a phrase boundary marker (see Ta- ble 1) were assigned the tag empty.	ORIG	Origin	0
This classification was ORIGally devised for sys- tems using an external ontology (so that semantic representations are directly linked to concepts in the ontology), but it is also suitable for broader settings, as we argue in the rest of the Section.	ORIG	origin	1
This classification was ORIGally devised for sys- tems using an external ontology (so that semantic representations are directly linked to concepts in the ontology), but it is also suitable for broader settings, as we argue in the rest of the Section.	ORIG	Origin	0
We  compiled a large corpus of text (News stories) and  made a second smaller corpus from the ORIGal one  which contains only sentences which are relevant to  the IE task.	ORIG	origin	1
We  compiled a large corpus of text (News stories) and  made a second smaller corpus from the ORIGal one  which contains only sentences which are relevant to  the IE task.	ORIG	Origin	0
ORIG?	ORIG	origin	1
ORIG?	ORIG	Origin	0
In Annals of the New York Academy  of Sciences: Conferences on the ORIG and Devel- opment of Language and Speech, Volume 280: 20- 32.	ORIG	origin	0
In Annals of the New York Academy  of Sciences: Conferences on the ORIG and Devel- opment of Language and Speech, Volume 280: 20- 32.	ORIG	Origin	1
Annals of the New York Academy of Sciences: Conference on the ORIG and Develop- ment of Language and Speech, 280:20?32.	ORIG	origin	0
Annals of the New York Academy of Sciences: Conference on the ORIG and Develop- ment of Language and Speech, 280:20?32.	ORIG	Origin	1
This should be taken with a grain Baseline ORIGal Query Hybrid Tot Good Top 5 Tot Good Top 5 Tot Good Top 5 Poe 12 6.5 3 10 0.5 0.5 10 5.5 2.5 Romantics 10 0 0 15 0 0 10 3 3 Witch Hunts 10 8 3 14 2 1 10 8 5 US Wars 15 12 2 0 0 0 16 13 4 Sonnets 15 10 5 10 2 0 10 8 4 Presidents 15 2 2 15 0 0 15 2 2 Epics 10 7 4 10 5 3 10 7 4 Dec of Ind 10 2 0 0 0 0 10 5.5 2 Avr.	ORIG	origin	0
This should be taken with a grain Baseline ORIGal Query Hybrid Tot Good Top 5 Tot Good Top 5 Tot Good Top 5 Poe 12 6.5 3 10 0.5 0.5 10 5.5 2.5 Romantics 10 0 0 15 0 0 10 3 3 Witch Hunts 10 8 3 14 2 1 10 8 5 US Wars 15 12 2 0 0 0 16 13 4 Sonnets 15 10 5 10 2 0 10 8 4 Presidents 15 2 2 15 0 0 15 2 2 Epics 10 7 4 10 5 3 10 7 4 Dec of Ind 10 2 0 0 0 0 10 5.5 2 Avr.	ORIG	Origin	1
In Proceed- ings of the Fourteenth International Conference Baseline ORIGal Query Hybrid FPF FPF/Tot FPF FPF/Tot FPF FPF/Tot Poe 1 .08 1 .1 1 .1 Romantics 8 .8 15 1 2 .2 Witch Hunts 2 .2 1 .07 0 0 US Wars 3 .2 3 .19 Sonnets 5 .33 8 .8 2 .2 Presidents 5 .33 10 .67 2 .13 Epics 0 0 0 0 0 0 Dec of Ind 3 .3 2 .2 28.1% 44% 12.8% Table 4: False Positives Containing Figure on Computational Linguistics, Nantes, France, July.	ORIG	origin	0
In Proceed- ings of the Fourteenth International Conference Baseline ORIGal Query Hybrid FPF FPF/Tot FPF FPF/Tot FPF FPF/Tot Poe 1 .08 1 .1 1 .1 Romantics 8 .8 15 1 2 .2 Witch Hunts 2 .2 1 .07 0 0 US Wars 3 .2 3 .19 Sonnets 5 .33 8 .8 2 .2 Presidents 5 .33 10 .67 2 .13 Epics 0 0 0 0 0 0 Dec of Ind 3 .3 2 .2 28.1% 44% 12.8% Table 4: False Positives Containing Figure on Computational Linguistics, Nantes, France, July.	ORIG	Origin	1
Table 2 shows the word error rate (WER) 115 Source of alternates WER ORIGal closed-captions 5.8% Phoneme confusion matrix 4.4% Word confusion matrix 3.1% Combined 2.9% Table 2: Error rate for perfect correction.	ORIG	origin	0
Table 2 shows the word error rate (WER) 115 Source of alternates WER ORIGal closed-captions 5.8% Phoneme confusion matrix 4.4% Word confusion matrix 3.1% Combined 2.9% Table 2: Error rate for perfect correction.	ORIG	Origin	1
ORIGal form in source:  Where there is a source  for the entity or for some canonical form of the entity,  the original form is given.	ORIG	origin	0
ORIGal form in source:  Where there is a source  for the entity or for some canonical form of the entity,  the original form is given.	ORIG	Origin	1
We will be describing a project with  partners including the UK and  the Assistive Technology team at Barnsley District  General Hospital.	UK	University of Sheffield	1
We will be describing a project with  partners including the UK and  the Assistive Technology team at Barnsley District  General Hospital.	UK	University	0
We will be describing a project with  partners including the UK and  the Assistive Technology team at Barnsley District  General Hospital.	UK	University of Manchester	0
We will be describing a project with  partners including the UK and  the Assistive Technology team at Barnsley District  General Hospital.	UK	University of Sunderland	0
We will be describing a project with  partners including the UK and  the Assistive Technology team at Barnsley District  General Hospital.	UK	USussex	0
We will be describing a project with  partners including the UK and  the Assistive Technology team at Barnsley District  General Hospital.	UK	University of Oxford	0
8  The Web as a Baseline: Evaluating the Performance of Unsupervised Web-based Models for a Range of NLP Tasks Mirella Lapata Department of Computer Science UK 211 Portobello St., Sheffield S1 4DP mlap@dcs.shef.ac.uk Frank Keller School of Informatics University of Edinburgh 2 Buccleuch Pl.,	UK	University of Sheffield	1
8  The Web as a Baseline: Evaluating the Performance of Unsupervised Web-based Models for a Range of NLP Tasks Mirella Lapata Department of Computer Science UK 211 Portobello St., Sheffield S1 4DP mlap@dcs.shef.ac.uk Frank Keller School of Informatics University of Edinburgh 2 Buccleuch Pl.,	UK	University	0
8  The Web as a Baseline: Evaluating the Performance of Unsupervised Web-based Models for a Range of NLP Tasks Mirella Lapata Department of Computer Science UK 211 Portobello St., Sheffield S1 4DP mlap@dcs.shef.ac.uk Frank Keller School of Informatics University of Edinburgh 2 Buccleuch Pl.,	UK	University of Manchester	0
8  The Web as a Baseline: Evaluating the Performance of Unsupervised Web-based Models for a Range of NLP Tasks Mirella Lapata Department of Computer Science UK 211 Portobello St., Sheffield S1 4DP mlap@dcs.shef.ac.uk Frank Keller School of Informatics University of Edinburgh 2 Buccleuch Pl.,	UK	University of Sunderland	0
8  The Web as a Baseline: Evaluating the Performance of Unsupervised Web-based Models for a Range of NLP Tasks Mirella Lapata Department of Computer Science UK 211 Portobello St., Sheffield S1 4DP mlap@dcs.shef.ac.uk Frank Keller School of Informatics University of Edinburgh 2 Buccleuch Pl.,	UK	USussex	0
8  The Web as a Baseline: Evaluating the Performance of Unsupervised Web-based Models for a Range of NLP Tasks Mirella Lapata Department of Computer Science UK 211 Portobello St., Sheffield S1 4DP mlap@dcs.shef.ac.uk Frank Keller School of Informatics University of Edinburgh 2 Buccleuch Pl.,	UK	University of Oxford	0
c?2013 Association for Computational Linguistics A temporal model of text periodicities using Gaussian Processes Daniel Preot?iuc-Pietro, Trevor Cohn Department of Computer Science UK Regent Court, 211 Portobello Street Sheffield, S1 4DP, United Kingdom {daniel,t.cohn}@dcs.shef.ac.uk Abstract Temporal variations of text are usually ig- nored in NLP applications.	UK	University of Sheffield	1
c?2013 Association for Computational Linguistics A temporal model of text periodicities using Gaussian Processes Daniel Preot?iuc-Pietro, Trevor Cohn Department of Computer Science UK Regent Court, 211 Portobello Street Sheffield, S1 4DP, United Kingdom {daniel,t.cohn}@dcs.shef.ac.uk Abstract Temporal variations of text are usually ig- nored in NLP applications.	UK	University	0
c?2013 Association for Computational Linguistics A temporal model of text periodicities using Gaussian Processes Daniel Preot?iuc-Pietro, Trevor Cohn Department of Computer Science UK Regent Court, 211 Portobello Street Sheffield, S1 4DP, United Kingdom {daniel,t.cohn}@dcs.shef.ac.uk Abstract Temporal variations of text are usually ig- nored in NLP applications.	UK	University of Manchester	0
c?2013 Association for Computational Linguistics A temporal model of text periodicities using Gaussian Processes Daniel Preot?iuc-Pietro, Trevor Cohn Department of Computer Science UK Regent Court, 211 Portobello Street Sheffield, S1 4DP, United Kingdom {daniel,t.cohn}@dcs.shef.ac.uk Abstract Temporal variations of text are usually ig- nored in NLP applications.	UK	University of Sunderland	0
c?2013 Association for Computational Linguistics A temporal model of text periodicities using Gaussian Processes Daniel Preot?iuc-Pietro, Trevor Cohn Department of Computer Science UK Regent Court, 211 Portobello Street Sheffield, S1 4DP, United Kingdom {daniel,t.cohn}@dcs.shef.ac.uk Abstract Temporal variations of text are usually ig- nored in NLP applications.	UK	USussex	0
c?2013 Association for Computational Linguistics A temporal model of text periodicities using Gaussian Processes Daniel Preot?iuc-Pietro, Trevor Cohn Department of Computer Science UK Regent Court, 211 Portobello Street Sheffield, S1 4DP, United Kingdom {daniel,t.cohn}@dcs.shef.ac.uk Abstract Temporal variations of text are usually ig- nored in NLP applications.	UK	University of Oxford	0
of  Computer Science, UK.	UK	University of Sheffield	1
of  Computer Science, UK.	UK	University	0
of  Computer Science, UK.	UK	University of Manchester	0
of  Computer Science, UK.	UK	University of Sunderland	0
of  Computer Science, UK.	UK	USussex	0
of  Computer Science, UK.	UK	University of Oxford	0
c?2013 Association for Computational Linguistics DALE: A Word Sense Disambiguation System for Biomedical Documents Trained using Automatically Labeled Examples Judita Preiss and Mark Stevenson Department of Computer Science, UK Regent Court, 211 Portobello Sheffield S1 4DP, United Kingdom j.preiss,m.stevenson@dcs.shef.ac.uk Abstract Automatic interpretation of documents is hampered by the fact that language contains terms which have multiple meanings.	UK	University of Sheffield	1
c?2013 Association for Computational Linguistics DALE: A Word Sense Disambiguation System for Biomedical Documents Trained using Automatically Labeled Examples Judita Preiss and Mark Stevenson Department of Computer Science, UK Regent Court, 211 Portobello Sheffield S1 4DP, United Kingdom j.preiss,m.stevenson@dcs.shef.ac.uk Abstract Automatic interpretation of documents is hampered by the fact that language contains terms which have multiple meanings.	UK	University	0
c?2013 Association for Computational Linguistics DALE: A Word Sense Disambiguation System for Biomedical Documents Trained using Automatically Labeled Examples Judita Preiss and Mark Stevenson Department of Computer Science, UK Regent Court, 211 Portobello Sheffield S1 4DP, United Kingdom j.preiss,m.stevenson@dcs.shef.ac.uk Abstract Automatic interpretation of documents is hampered by the fact that language contains terms which have multiple meanings.	UK	University of Manchester	0
c?2013 Association for Computational Linguistics DALE: A Word Sense Disambiguation System for Biomedical Documents Trained using Automatically Labeled Examples Judita Preiss and Mark Stevenson Department of Computer Science, UK Regent Court, 211 Portobello Sheffield S1 4DP, United Kingdom j.preiss,m.stevenson@dcs.shef.ac.uk Abstract Automatic interpretation of documents is hampered by the fact that language contains terms which have multiple meanings.	UK	University of Sunderland	0
c?2013 Association for Computational Linguistics DALE: A Word Sense Disambiguation System for Biomedical Documents Trained using Automatically Labeled Examples Judita Preiss and Mark Stevenson Department of Computer Science, UK Regent Court, 211 Portobello Sheffield S1 4DP, United Kingdom j.preiss,m.stevenson@dcs.shef.ac.uk Abstract Automatic interpretation of documents is hampered by the fact that language contains terms which have multiple meanings.	UK	USussex	0
c?2013 Association for Computational Linguistics DALE: A Word Sense Disambiguation System for Biomedical Documents Trained using Automatically Labeled Examples Judita Preiss and Mark Stevenson Department of Computer Science, UK Regent Court, 211 Portobello Sheffield S1 4DP, United Kingdom j.preiss,m.stevenson@dcs.shef.ac.uk Abstract Automatic interpretation of documents is hampered by the fact that language contains terms which have multiple meanings.	UK	University of Oxford	0
UK Introduction The title of this piece refers to Newton?s only known modest remark: ?	UK	University of Sheffield	1
UK Introduction The title of this piece refers to Newton?s only known modest remark: ?	UK	University	0
UK Introduction The title of this piece refers to Newton?s only known modest remark: ?	UK	University of Manchester	0
UK Introduction The title of this piece refers to Newton?s only known modest remark: ?	UK	University of Sunderland	0
UK Introduction The title of this piece refers to Newton?s only known modest remark: ?	UK	USussex	0
UK Introduction The title of this piece refers to Newton?s only known modest remark: ?	UK	University of Oxford	0
Phi)  thesis, UK of I{dinburgh.	UK	University of Sheffield	0
Phi)  thesis, UK of I{dinburgh.	UK	University	1
Phi)  thesis, UK of I{dinburgh.	UK	University of Manchester	0
Phi)  thesis, UK of I{dinburgh.	UK	University of Sunderland	0
Phi)  thesis, UK of I{dinburgh.	UK	USussex	0
Phi)  thesis, UK of I{dinburgh.	UK	University of Oxford	0
School of ECE, Technical UK of Crete, Chania 73100, Greece ?	UK	University of Sheffield	0
School of ECE, Technical UK of Crete, Chania 73100, Greece ?	UK	University	1
School of ECE, Technical UK of Crete, Chania 73100, Greece ?	UK	University of Manchester	0
School of ECE, Technical UK of Crete, Chania 73100, Greece ?	UK	University of Sunderland	0
School of ECE, Technical UK of Crete, Chania 73100, Greece ?	UK	USussex	0
School of ECE, Technical UK of Crete, Chania 73100, Greece ?	UK	University of Oxford	0
Ph.D. thesis, UK of Illinois at Urbana-Champaign.	UK	University of Sheffield	0
Ph.D. thesis, UK of Illinois at Urbana-Champaign.	UK	University	1
Ph.D. thesis, UK of Illinois at Urbana-Champaign.	UK	University of Manchester	0
Ph.D. thesis, UK of Illinois at Urbana-Champaign.	UK	University of Sunderland	0
Ph.D. thesis, UK of Illinois at Urbana-Champaign.	UK	USussex	0
Ph.D. thesis, UK of Illinois at Urbana-Champaign.	UK	University of Oxford	0
.ambridge UK Press.	UK	University of Sheffield	0
.ambridge UK Press.	UK	University	1
.ambridge UK Press.	UK	University of Manchester	0
.ambridge UK Press.	UK	University of Sunderland	0
.ambridge UK Press.	UK	USussex	0
.ambridge UK Press.	UK	University of Oxford	0
of Computer Science,  Brown UK,  \[nge I j th \[ ec\] ~cs.	UK	University of Sheffield	0
of Computer Science,  Brown UK,  \[nge I j th \[ ec\] ~cs.	UK	University	1
of Computer Science,  Brown UK,  \[nge I j th \[ ec\] ~cs.	UK	University of Manchester	0
of Computer Science,  Brown UK,  \[nge I j th \[ ec\] ~cs.	UK	University of Sunderland	0
of Computer Science,  Brown UK,  \[nge I j th \[ ec\] ~cs.	UK	USussex	0
of Computer Science,  Brown UK,  \[nge I j th \[ ec\] ~cs.	UK	University of Oxford	0
Tech-  nical Rcl)ort 2, Center \['or Cognitive Science, Rutgcrs  UK.	UK	University of Sheffield	0
Tech-  nical Rcl)ort 2, Center \['or Cognitive Science, Rutgcrs  UK.	UK	University	1
Tech-  nical Rcl)ort 2, Center \['or Cognitive Science, Rutgcrs  UK.	UK	University of Manchester	0
Tech-  nical Rcl)ort 2, Center \['or Cognitive Science, Rutgcrs  UK.	UK	University of Sunderland	0
Tech-  nical Rcl)ort 2, Center \['or Cognitive Science, Rutgcrs  UK.	UK	USussex	0
Tech-  nical Rcl)ort 2, Center \['or Cognitive Science, Rutgcrs  UK.	UK	University of Oxford	0
16  Advances in domain independent linear text segmentat ion  Freddy Y. Y.  Cho i   Artificial Intell igence Group  Department  of Computer  Science  UK  Manchester,  England  choif@cs.man.ac.uk  Abst rac t   This paper describes a method for linear text seg-  mentation which is twice as accurate and over seven  times as fast as the state-of-the-art (Reynar, 1998).	UK	University of Sheffield	0
16  Advances in domain independent linear text segmentat ion  Freddy Y. Y.  Cho i   Artificial Intell igence Group  Department  of Computer  Science  UK  Manchester,  England  choif@cs.man.ac.uk  Abst rac t   This paper describes a method for linear text seg-  mentation which is twice as accurate and over seven  times as fast as the state-of-the-art (Reynar, 1998).	UK	University	0
16  Advances in domain independent linear text segmentat ion  Freddy Y. Y.  Cho i   Artificial Intell igence Group  Department  of Computer  Science  UK  Manchester,  England  choif@cs.man.ac.uk  Abst rac t   This paper describes a method for linear text seg-  mentation which is twice as accurate and over seven  times as fast as the state-of-the-art (Reynar, 1998).	UK	University of Manchester	1
16  Advances in domain independent linear text segmentat ion  Freddy Y. Y.  Cho i   Artificial Intell igence Group  Department  of Computer  Science  UK  Manchester,  England  choif@cs.man.ac.uk  Abst rac t   This paper describes a method for linear text seg-  mentation which is twice as accurate and over seven  times as fast as the state-of-the-art (Reynar, 1998).	UK	University of Sunderland	0
16  Advances in domain independent linear text segmentat ion  Freddy Y. Y.  Cho i   Artificial Intell igence Group  Department  of Computer  Science  UK  Manchester,  England  choif@cs.man.ac.uk  Abst rac t   This paper describes a method for linear text seg-  mentation which is twice as accurate and over seven  times as fast as the state-of-the-art (Reynar, 1998).	UK	USussex	0
16  Advances in domain independent linear text segmentat ion  Freddy Y. Y.  Cho i   Artificial Intell igence Group  Department  of Computer  Science  UK  Manchester,  England  choif@cs.man.ac.uk  Abst rac t   This paper describes a method for linear text seg-  mentation which is twice as accurate and over seven  times as fast as the state-of-the-art (Reynar, 1998).	UK	University of Oxford	0
c?2006 Association for Computational Linguistics Extremely Lexicalized Models for Accurate and Fast HPSG Parsing Takashi Ninomiya Information Technology Center University of Tokyo Takuya Matsuzaki Department of Computer Science University of Tokyo Yoshimasa Tsuruoka School of Informatics UK Yusuke Miyao Department of Computer Science University of Tokyo Jun?ichi Tsujii Department of Computer Science, University of Tokyo School of Informatics, UK SORST, Japan Science and Technology Agency Hongo 7-3-1, Bunkyo-ku, Tokyo, 113-0033, Japan {ninomi, matuzaki, tsuruoka, yusuke, tsujii}@is.s.u-tokyo.ac.jp Abstract This paper describes an extremel	UK	University of Sheffield	0
c?2006 Association for Computational Linguistics Extremely Lexicalized Models for Accurate and Fast HPSG Parsing Takashi Ninomiya Information Technology Center University of Tokyo Takuya Matsuzaki Department of Computer Science University of Tokyo Yoshimasa Tsuruoka School of Informatics UK Yusuke Miyao Department of Computer Science University of Tokyo Jun?ichi Tsujii Department of Computer Science, University of Tokyo School of Informatics, UK SORST, Japan Science and Technology Agency Hongo 7-3-1, Bunkyo-ku, Tokyo, 113-0033, Japan {ninomi, matuzaki, tsuruoka, yusuke, tsujii}@is.s.u-tokyo.ac.jp Abstract This paper describes an extremel	UK	University	0
c?2006 Association for Computational Linguistics Extremely Lexicalized Models for Accurate and Fast HPSG Parsing Takashi Ninomiya Information Technology Center University of Tokyo Takuya Matsuzaki Department of Computer Science University of Tokyo Yoshimasa Tsuruoka School of Informatics UK Yusuke Miyao Department of Computer Science University of Tokyo Jun?ichi Tsujii Department of Computer Science, University of Tokyo School of Informatics, UK SORST, Japan Science and Technology Agency Hongo 7-3-1, Bunkyo-ku, Tokyo, 113-0033, Japan {ninomi, matuzaki, tsuruoka, yusuke, tsujii}@is.s.u-tokyo.ac.jp Abstract This paper describes an extremel	UK	University of Manchester	1
c?2006 Association for Computational Linguistics Extremely Lexicalized Models for Accurate and Fast HPSG Parsing Takashi Ninomiya Information Technology Center University of Tokyo Takuya Matsuzaki Department of Computer Science University of Tokyo Yoshimasa Tsuruoka School of Informatics UK Yusuke Miyao Department of Computer Science University of Tokyo Jun?ichi Tsujii Department of Computer Science, University of Tokyo School of Informatics, UK SORST, Japan Science and Technology Agency Hongo 7-3-1, Bunkyo-ku, Tokyo, 113-0033, Japan {ninomi, matuzaki, tsuruoka, yusuke, tsujii}@is.s.u-tokyo.ac.jp Abstract This paper describes an extremel	UK	University of Sunderland	0
c?2006 Association for Computational Linguistics Extremely Lexicalized Models for Accurate and Fast HPSG Parsing Takashi Ninomiya Information Technology Center University of Tokyo Takuya Matsuzaki Department of Computer Science University of Tokyo Yoshimasa Tsuruoka School of Informatics UK Yusuke Miyao Department of Computer Science University of Tokyo Jun?ichi Tsujii Department of Computer Science, University of Tokyo School of Informatics, UK SORST, Japan Science and Technology Agency Hongo 7-3-1, Bunkyo-ku, Tokyo, 113-0033, Japan {ninomi, matuzaki, tsuruoka, yusuke, tsujii}@is.s.u-tokyo.ac.jp Abstract This paper describes an extremel	UK	USussex	0
c?2006 Association for Computational Linguistics Extremely Lexicalized Models for Accurate and Fast HPSG Parsing Takashi Ninomiya Information Technology Center University of Tokyo Takuya Matsuzaki Department of Computer Science University of Tokyo Yoshimasa Tsuruoka School of Informatics UK Yusuke Miyao Department of Computer Science University of Tokyo Jun?ichi Tsujii Department of Computer Science, University of Tokyo School of Informatics, UK SORST, Japan Science and Technology Agency Hongo 7-3-1, Bunkyo-ku, Tokyo, 113-0033, Japan {ninomi, matuzaki, tsuruoka, yusuke, tsujii}@is.s.u-tokyo.ac.jp Abstract This paper describes an extremel	UK	University of Oxford	0
c?2005 Association for Computational Linguistics Chunk Parsing Revisited Yoshimasa Tsuruoka   and Jun?ichi Tsujii     CREST, JST (Japan Science and Technology Corporation)  Department of Computer Science, University of Tokyo  School of Informatics, UK  tsuruoka,tsujii  @is.s.u-tokyo.ac.jp Abstract Chunk parsing is conceptually appealing but its performance has not been satis- factory for practical use.	UK	University of Sheffield	0
c?2005 Association for Computational Linguistics Chunk Parsing Revisited Yoshimasa Tsuruoka   and Jun?ichi Tsujii     CREST, JST (Japan Science and Technology Corporation)  Department of Computer Science, University of Tokyo  School of Informatics, UK  tsuruoka,tsujii  @is.s.u-tokyo.ac.jp Abstract Chunk parsing is conceptually appealing but its performance has not been satis- factory for practical use.	UK	University	0
c?2005 Association for Computational Linguistics Chunk Parsing Revisited Yoshimasa Tsuruoka   and Jun?ichi Tsujii     CREST, JST (Japan Science and Technology Corporation)  Department of Computer Science, University of Tokyo  School of Informatics, UK  tsuruoka,tsujii  @is.s.u-tokyo.ac.jp Abstract Chunk parsing is conceptually appealing but its performance has not been satis- factory for practical use.	UK	University of Manchester	1
c?2005 Association for Computational Linguistics Chunk Parsing Revisited Yoshimasa Tsuruoka   and Jun?ichi Tsujii     CREST, JST (Japan Science and Technology Corporation)  Department of Computer Science, University of Tokyo  School of Informatics, UK  tsuruoka,tsujii  @is.s.u-tokyo.ac.jp Abstract Chunk parsing is conceptually appealing but its performance has not been satis- factory for practical use.	UK	University of Sunderland	0
c?2005 Association for Computational Linguistics Chunk Parsing Revisited Yoshimasa Tsuruoka   and Jun?ichi Tsujii     CREST, JST (Japan Science and Technology Corporation)  Department of Computer Science, University of Tokyo  School of Informatics, UK  tsuruoka,tsujii  @is.s.u-tokyo.ac.jp Abstract Chunk parsing is conceptually appealing but its performance has not been satis- factory for practical use.	UK	USussex	0
c?2005 Association for Computational Linguistics Chunk Parsing Revisited Yoshimasa Tsuruoka   and Jun?ichi Tsujii     CREST, JST (Japan Science and Technology Corporation)  Department of Computer Science, University of Tokyo  School of Informatics, UK  tsuruoka,tsujii  @is.s.u-tokyo.ac.jp Abstract Chunk parsing is conceptually appealing but its performance has not been satis- factory for practical use.	UK	University of Oxford	0
d Models for Accurate and Fast HPSG Parsing Takashi Ninomiya Information Technology Center University of Tokyo Takuya Matsuzaki Department of Computer Science University of Tokyo Yoshimasa Tsuruoka School of Informatics UK Yusuke Miyao Department of Computer Science University of Tokyo Jun?ichi Tsujii Department of Computer Science, University of Tokyo School of Informatics, UK SORST, Japan Science and Technology Agency Hongo 7-3-1, Bunkyo-ku, Tokyo, 113-0033, Japan {ninomi, matuzaki, tsuruoka, yusuke, tsujii}@is.s.u-tokyo.ac.jp Abstract This paper describes an extremely lexi- calized probabilistic model for fast and accurate HPSG parsing.	UK	University of Sheffield	0
d Models for Accurate and Fast HPSG Parsing Takashi Ninomiya Information Technology Center University of Tokyo Takuya Matsuzaki Department of Computer Science University of Tokyo Yoshimasa Tsuruoka School of Informatics UK Yusuke Miyao Department of Computer Science University of Tokyo Jun?ichi Tsujii Department of Computer Science, University of Tokyo School of Informatics, UK SORST, Japan Science and Technology Agency Hongo 7-3-1, Bunkyo-ku, Tokyo, 113-0033, Japan {ninomi, matuzaki, tsuruoka, yusuke, tsujii}@is.s.u-tokyo.ac.jp Abstract This paper describes an extremely lexi- calized probabilistic model for fast and accurate HPSG parsing.	UK	University	0
d Models for Accurate and Fast HPSG Parsing Takashi Ninomiya Information Technology Center University of Tokyo Takuya Matsuzaki Department of Computer Science University of Tokyo Yoshimasa Tsuruoka School of Informatics UK Yusuke Miyao Department of Computer Science University of Tokyo Jun?ichi Tsujii Department of Computer Science, University of Tokyo School of Informatics, UK SORST, Japan Science and Technology Agency Hongo 7-3-1, Bunkyo-ku, Tokyo, 113-0033, Japan {ninomi, matuzaki, tsuruoka, yusuke, tsujii}@is.s.u-tokyo.ac.jp Abstract This paper describes an extremely lexi- calized probabilistic model for fast and accurate HPSG parsing.	UK	University of Manchester	1
d Models for Accurate and Fast HPSG Parsing Takashi Ninomiya Information Technology Center University of Tokyo Takuya Matsuzaki Department of Computer Science University of Tokyo Yoshimasa Tsuruoka School of Informatics UK Yusuke Miyao Department of Computer Science University of Tokyo Jun?ichi Tsujii Department of Computer Science, University of Tokyo School of Informatics, UK SORST, Japan Science and Technology Agency Hongo 7-3-1, Bunkyo-ku, Tokyo, 113-0033, Japan {ninomi, matuzaki, tsuruoka, yusuke, tsujii}@is.s.u-tokyo.ac.jp Abstract This paper describes an extremely lexi- calized probabilistic model for fast and accurate HPSG parsing.	UK	University of Sunderland	0
d Models for Accurate and Fast HPSG Parsing Takashi Ninomiya Information Technology Center University of Tokyo Takuya Matsuzaki Department of Computer Science University of Tokyo Yoshimasa Tsuruoka School of Informatics UK Yusuke Miyao Department of Computer Science University of Tokyo Jun?ichi Tsujii Department of Computer Science, University of Tokyo School of Informatics, UK SORST, Japan Science and Technology Agency Hongo 7-3-1, Bunkyo-ku, Tokyo, 113-0033, Japan {ninomi, matuzaki, tsuruoka, yusuke, tsujii}@is.s.u-tokyo.ac.jp Abstract This paper describes an extremely lexi- calized probabilistic model for fast and accurate HPSG parsing.	UK	USussex	0
d Models for Accurate and Fast HPSG Parsing Takashi Ninomiya Information Technology Center University of Tokyo Takuya Matsuzaki Department of Computer Science University of Tokyo Yoshimasa Tsuruoka School of Informatics UK Yusuke Miyao Department of Computer Science University of Tokyo Jun?ichi Tsujii Department of Computer Science, University of Tokyo School of Informatics, UK SORST, Japan Science and Technology Agency Hongo 7-3-1, Bunkyo-ku, Tokyo, 113-0033, Japan {ninomi, matuzaki, tsuruoka, yusuke, tsujii}@is.s.u-tokyo.ac.jp Abstract This paper describes an extremely lexi- calized probabilistic model for fast and accurate HPSG parsing.	UK	University of Oxford	0
168  The FINITE STRING Newslet ter   S i te  Repor t   Control l ing Complex  Systems of Linguistic  Rules  Rod Johnson  UK Institute of Science and  Technology, U.K.  Steven Krauwer  Rijksuniversiteit, Utrecht, Holland  Mike Rosner  Fondazione dalle Molle, ISSCO, University of Geneva,  Switzerland  Nino Varile  Commission of the European Communities,  Luxembourg  \[Most of the ideas in this paper evolved during work  on design studies for the EEC Machine Translation  Project Eurotra.	UK	University of Sheffield	0
168  The FINITE STRING Newslet ter   S i te  Repor t   Control l ing Complex  Systems of Linguistic  Rules  Rod Johnson  UK Institute of Science and  Technology, U.K.  Steven Krauwer  Rijksuniversiteit, Utrecht, Holland  Mike Rosner  Fondazione dalle Molle, ISSCO, University of Geneva,  Switzerland  Nino Varile  Commission of the European Communities,  Luxembourg  \[Most of the ideas in this paper evolved during work  on design studies for the EEC Machine Translation  Project Eurotra.	UK	University	0
168  The FINITE STRING Newslet ter   S i te  Repor t   Control l ing Complex  Systems of Linguistic  Rules  Rod Johnson  UK Institute of Science and  Technology, U.K.  Steven Krauwer  Rijksuniversiteit, Utrecht, Holland  Mike Rosner  Fondazione dalle Molle, ISSCO, University of Geneva,  Switzerland  Nino Varile  Commission of the European Communities,  Luxembourg  \[Most of the ideas in this paper evolved during work  on design studies for the EEC Machine Translation  Project Eurotra.	UK	University of Manchester	1
168  The FINITE STRING Newslet ter   S i te  Repor t   Control l ing Complex  Systems of Linguistic  Rules  Rod Johnson  UK Institute of Science and  Technology, U.K.  Steven Krauwer  Rijksuniversiteit, Utrecht, Holland  Mike Rosner  Fondazione dalle Molle, ISSCO, University of Geneva,  Switzerland  Nino Varile  Commission of the European Communities,  Luxembourg  \[Most of the ideas in this paper evolved during work  on design studies for the EEC Machine Translation  Project Eurotra.	UK	University of Sunderland	0
168  The FINITE STRING Newslet ter   S i te  Repor t   Control l ing Complex  Systems of Linguistic  Rules  Rod Johnson  UK Institute of Science and  Technology, U.K.  Steven Krauwer  Rijksuniversiteit, Utrecht, Holland  Mike Rosner  Fondazione dalle Molle, ISSCO, University of Geneva,  Switzerland  Nino Varile  Commission of the European Communities,  Luxembourg  \[Most of the ideas in this paper evolved during work  on design studies for the EEC Machine Translation  Project Eurotra.	UK	USussex	0
168  The FINITE STRING Newslet ter   S i te  Repor t   Control l ing Complex  Systems of Linguistic  Rules  Rod Johnson  UK Institute of Science and  Technology, U.K.  Steven Krauwer  Rijksuniversiteit, Utrecht, Holland  Mike Rosner  Fondazione dalle Molle, ISSCO, University of Geneva,  Switzerland  Nino Varile  Commission of the European Communities,  Luxembourg  \[Most of the ideas in this paper evolved during work  on design studies for the EEC Machine Translation  Project Eurotra.	UK	University of Oxford	0
Master's Thesis, UK (1981).	UK	University of Sheffield	0
Master's Thesis, UK (1981).	UK	University	0
Master's Thesis, UK (1981).	UK	University of Manchester	1
Master's Thesis, UK (1981).	UK	University of Sunderland	0
Master's Thesis, UK (1981).	UK	USussex	0
Master's Thesis, UK (1981).	UK	University of Oxford	0
Practical Considerations in Building a Multi-Lingual Authoring System for  Business Letters  John Tait, Huw Sanderson  School of Computing +Information Systems  UK  Sunderland SR6 ODD, U.K.  {John.	UK	University of Sheffield	0
Practical Considerations in Building a Multi-Lingual Authoring System for  Business Letters  John Tait, Huw Sanderson  School of Computing +Information Systems  UK  Sunderland SR6 ODD, U.K.  {John.	UK	University	0
Practical Considerations in Building a Multi-Lingual Authoring System for  Business Letters  John Tait, Huw Sanderson  School of Computing +Information Systems  UK  Sunderland SR6 ODD, U.K.  {John.	UK	University of Manchester	0
Practical Considerations in Building a Multi-Lingual Authoring System for  Business Letters  John Tait, Huw Sanderson  School of Computing +Information Systems  UK  Sunderland SR6 ODD, U.K.  {John.	UK	University of Sunderland	1
Practical Considerations in Building a Multi-Lingual Authoring System for  Business Letters  John Tait, Huw Sanderson  School of Computing +Information Systems  UK  Sunderland SR6 ODD, U.K.  {John.	UK	USussex	0
Practical Considerations in Building a Multi-Lingual Authoring System for  Business Letters  John Tait, Huw Sanderson  School of Computing +Information Systems  UK  Sunderland SR6 ODD, U.K.  {John.	UK	University of Oxford	0
thesis, UK.	UK	University of Sheffield	0
thesis, UK.	UK	University	0
thesis, UK.	UK	University of Manchester	0
thesis, UK.	UK	University of Sunderland	1
thesis, UK.	UK	USussex	0
thesis, UK.	UK	University of Oxford	0
Ph.D. thesis, UK.	UK	University of Sheffield	0
Ph.D. thesis, UK.	UK	University	0
Ph.D. thesis, UK.	UK	University of Manchester	0
Ph.D. thesis, UK.	UK	University of Sunderland	1
Ph.D. thesis, UK.	UK	USussex	0
Ph.D. thesis, UK.	UK	University of Oxford	0
241  Selforganizing classification on the Reuters news corpus    Stefan Wermter  The Informatics Centre   School of CET  UK  St. Peter?s Way, Sunderland SR6 0DD   United Kingdom  Stefan.wermter@sunderland.ac.uk  Chihli Hung1  The Informatics Centre   School of CET  UK  St. Peter?s Way, Sunderland SR6 0DD   United Kingdom  Chihli.hung@sunderland.ac.uk                                                           1 Hung is a lecturer of De Lin Institute of Technology as well.	UK	University of Sheffield	0
241  Selforganizing classification on the Reuters news corpus    Stefan Wermter  The Informatics Centre   School of CET  UK  St. Peter?s Way, Sunderland SR6 0DD   United Kingdom  Stefan.wermter@sunderland.ac.uk  Chihli Hung1  The Informatics Centre   School of CET  UK  St. Peter?s Way, Sunderland SR6 0DD   United Kingdom  Chihli.hung@sunderland.ac.uk                                                           1 Hung is a lecturer of De Lin Institute of Technology as well.	UK	University	0
241  Selforganizing classification on the Reuters news corpus    Stefan Wermter  The Informatics Centre   School of CET  UK  St. Peter?s Way, Sunderland SR6 0DD   United Kingdom  Stefan.wermter@sunderland.ac.uk  Chihli Hung1  The Informatics Centre   School of CET  UK  St. Peter?s Way, Sunderland SR6 0DD   United Kingdom  Chihli.hung@sunderland.ac.uk                                                           1 Hung is a lecturer of De Lin Institute of Technology as well.	UK	University of Manchester	0
241  Selforganizing classification on the Reuters news corpus    Stefan Wermter  The Informatics Centre   School of CET  UK  St. Peter?s Way, Sunderland SR6 0DD   United Kingdom  Stefan.wermter@sunderland.ac.uk  Chihli Hung1  The Informatics Centre   School of CET  UK  St. Peter?s Way, Sunderland SR6 0DD   United Kingdom  Chihli.hung@sunderland.ac.uk                                                           1 Hung is a lecturer of De Lin Institute of Technology as well.	UK	University of Sunderland	1
241  Selforganizing classification on the Reuters news corpus    Stefan Wermter  The Informatics Centre   School of CET  UK  St. Peter?s Way, Sunderland SR6 0DD   United Kingdom  Stefan.wermter@sunderland.ac.uk  Chihli Hung1  The Informatics Centre   School of CET  UK  St. Peter?s Way, Sunderland SR6 0DD   United Kingdom  Chihli.hung@sunderland.ac.uk                                                           1 Hung is a lecturer of De Lin Institute of Technology as well.	UK	USussex	0
241  Selforganizing classification on the Reuters news corpus    Stefan Wermter  The Informatics Centre   School of CET  UK  St. Peter?s Way, Sunderland SR6 0DD   United Kingdom  Stefan.wermter@sunderland.ac.uk  Chihli Hung1  The Informatics Centre   School of CET  UK  St. Peter?s Way, Sunderland SR6 0DD   United Kingdom  Chihli.hung@sunderland.ac.uk                                                           1 Hung is a lecturer of De Lin Institute of Technology as well.	UK	University of Oxford	0
12  D-70174 Stuttgart  Germany  Psychology  UK  South Parks Road  Oxford OX1 3UD  England  University of Edinburgh  2 Buccleuch Place  Edinburgh EH8 9LW  Scotland  {vogel,holly,ueh}~cogsci.ed.ac.uk  Abstract  Cross-serial dependencies in Dutdl  and  Swiss-German are the only known extra-  context fi'ee natural language syntactic  phenonmna.	UK	University of Sheffield	0
12  D-70174 Stuttgart  Germany  Psychology  UK  South Parks Road  Oxford OX1 3UD  England  University of Edinburgh  2 Buccleuch Place  Edinburgh EH8 9LW  Scotland  {vogel,holly,ueh}~cogsci.ed.ac.uk  Abstract  Cross-serial dependencies in Dutdl  and  Swiss-German are the only known extra-  context fi'ee natural language syntactic  phenonmna.	UK	University	0
12  D-70174 Stuttgart  Germany  Psychology  UK  South Parks Road  Oxford OX1 3UD  England  University of Edinburgh  2 Buccleuch Place  Edinburgh EH8 9LW  Scotland  {vogel,holly,ueh}~cogsci.ed.ac.uk  Abstract  Cross-serial dependencies in Dutdl  and  Swiss-German are the only known extra-  context fi'ee natural language syntactic  phenonmna.	UK	University of Manchester	0
12  D-70174 Stuttgart  Germany  Psychology  UK  South Parks Road  Oxford OX1 3UD  England  University of Edinburgh  2 Buccleuch Place  Edinburgh EH8 9LW  Scotland  {vogel,holly,ueh}~cogsci.ed.ac.uk  Abstract  Cross-serial dependencies in Dutdl  and  Swiss-German are the only known extra-  context fi'ee natural language syntactic  phenonmna.	UK	University of Sunderland	0
12  D-70174 Stuttgart  Germany  Psychology  UK  South Parks Road  Oxford OX1 3UD  England  University of Edinburgh  2 Buccleuch Place  Edinburgh EH8 9LW  Scotland  {vogel,holly,ueh}~cogsci.ed.ac.uk  Abstract  Cross-serial dependencies in Dutdl  and  Swiss-German are the only known extra-  context fi'ee natural language syntactic  phenonmna.	UK	USussex	0
12  D-70174 Stuttgart  Germany  Psychology  UK  South Parks Road  Oxford OX1 3UD  England  University of Edinburgh  2 Buccleuch Place  Edinburgh EH8 9LW  Scotland  {vogel,holly,ueh}~cogsci.ed.ac.uk  Abstract  Cross-serial dependencies in Dutdl  and  Swiss-German are the only known extra-  context fi'ee natural language syntactic  phenonmna.	UK	University of Oxford	1
c?2014 Association for Computational Linguistics Monads as a Solution for Generalized Opacity Gianluca Giorgolo UK Ash Asudeh UK and Carleton University {gianluca.giorgolo,ash.asudeh}@ling-phil.ox.ac.uk Abstract In this paper we discuss a conservative ex- tension of the simply-typed lambda calcu- lus in order to model a class of expres- sions that generalize the notion of opaque contexts.	UK	University of Sheffield	0
c?2014 Association for Computational Linguistics Monads as a Solution for Generalized Opacity Gianluca Giorgolo UK Ash Asudeh UK and Carleton University {gianluca.giorgolo,ash.asudeh}@ling-phil.ox.ac.uk Abstract In this paper we discuss a conservative ex- tension of the simply-typed lambda calcu- lus in order to model a class of expres- sions that generalize the notion of opaque contexts.	UK	University	0
c?2014 Association for Computational Linguistics Monads as a Solution for Generalized Opacity Gianluca Giorgolo UK Ash Asudeh UK and Carleton University {gianluca.giorgolo,ash.asudeh}@ling-phil.ox.ac.uk Abstract In this paper we discuss a conservative ex- tension of the simply-typed lambda calcu- lus in order to model a class of expres- sions that generalize the notion of opaque contexts.	UK	University of Manchester	0
c?2014 Association for Computational Linguistics Monads as a Solution for Generalized Opacity Gianluca Giorgolo UK Ash Asudeh UK and Carleton University {gianluca.giorgolo,ash.asudeh}@ling-phil.ox.ac.uk Abstract In this paper we discuss a conservative ex- tension of the simply-typed lambda calcu- lus in order to model a class of expres- sions that generalize the notion of opaque contexts.	UK	University of Sunderland	0
c?2014 Association for Computational Linguistics Monads as a Solution for Generalized Opacity Gianluca Giorgolo UK Ash Asudeh UK and Carleton University {gianluca.giorgolo,ash.asudeh}@ling-phil.ox.ac.uk Abstract In this paper we discuss a conservative ex- tension of the simply-typed lambda calcu- lus in order to model a class of expres- sions that generalize the notion of opaque contexts.	UK	USussex	0
c?2014 Association for Computational Linguistics Monads as a Solution for Generalized Opacity Gianluca Giorgolo UK Ash Asudeh UK and Carleton University {gianluca.giorgolo,ash.asudeh}@ling-phil.ox.ac.uk Abstract In this paper we discuss a conservative ex- tension of the simply-typed lambda calcu- lus in order to model a class of expres- sions that generalize the notion of opaque contexts.	UK	University of Oxford	1
S. G. Pulman, J. Boye UK sgp@clg.ox.ac.uk M. Cavazza, C. Smith Teesside University m.o.cavazza@tees.ac.uk R. S. de la Ca?mara Telefonica I+D e.rsai@tid.es Abstract We describe a ?	UK	University of Sheffield	0
S. G. Pulman, J. Boye UK sgp@clg.ox.ac.uk M. Cavazza, C. Smith Teesside University m.o.cavazza@tees.ac.uk R. S. de la Ca?mara Telefonica I+D e.rsai@tid.es Abstract We describe a ?	UK	University	0
S. G. Pulman, J. Boye UK sgp@clg.ox.ac.uk M. Cavazza, C. Smith Teesside University m.o.cavazza@tees.ac.uk R. S. de la Ca?mara Telefonica I+D e.rsai@tid.es Abstract We describe a ?	UK	University of Manchester	0
S. G. Pulman, J. Boye UK sgp@clg.ox.ac.uk M. Cavazza, C. Smith Teesside University m.o.cavazza@tees.ac.uk R. S. de la Ca?mara Telefonica I+D e.rsai@tid.es Abstract We describe a ?	UK	University of Sunderland	0
S. G. Pulman, J. Boye UK sgp@clg.ox.ac.uk M. Cavazza, C. Smith Teesside University m.o.cavazza@tees.ac.uk R. S. de la Ca?mara Telefonica I+D e.rsai@tid.es Abstract We describe a ?	UK	USussex	0
S. G. Pulman, J. Boye UK sgp@clg.ox.ac.uk M. Cavazza, C. Smith Teesside University m.o.cavazza@tees.ac.uk R. S. de la Ca?mara Telefonica I+D e.rsai@tid.es Abstract We describe a ?	UK	University of Oxford	1
c?2016 Association for Computational Linguistics Cross-Lingual Morphological Tagging for Low-Resource Languages Jan Buys Department of Computer Science UK jan.buys@cs.ox.ac.uk Jan A. Botha Google Inc. London jabot@google.com Abstract Morphologically rich languages often lack the annotated linguistic resources required to develop accurate natural language pro- cessing tools.	UK	University of Sheffield	0
c?2016 Association for Computational Linguistics Cross-Lingual Morphological Tagging for Low-Resource Languages Jan Buys Department of Computer Science UK jan.buys@cs.ox.ac.uk Jan A. Botha Google Inc. London jabot@google.com Abstract Morphologically rich languages often lack the annotated linguistic resources required to develop accurate natural language pro- cessing tools.	UK	University	0
c?2016 Association for Computational Linguistics Cross-Lingual Morphological Tagging for Low-Resource Languages Jan Buys Department of Computer Science UK jan.buys@cs.ox.ac.uk Jan A. Botha Google Inc. London jabot@google.com Abstract Morphologically rich languages often lack the annotated linguistic resources required to develop accurate natural language pro- cessing tools.	UK	University of Manchester	0
c?2016 Association for Computational Linguistics Cross-Lingual Morphological Tagging for Low-Resource Languages Jan Buys Department of Computer Science UK jan.buys@cs.ox.ac.uk Jan A. Botha Google Inc. London jabot@google.com Abstract Morphologically rich languages often lack the annotated linguistic resources required to develop accurate natural language pro- cessing tools.	UK	University of Sunderland	0
c?2016 Association for Computational Linguistics Cross-Lingual Morphological Tagging for Low-Resource Languages Jan Buys Department of Computer Science UK jan.buys@cs.ox.ac.uk Jan A. Botha Google Inc. London jabot@google.com Abstract Morphologically rich languages often lack the annotated linguistic resources required to develop accurate natural language pro- cessing tools.	UK	USussex	0
c?2016 Association for Computational Linguistics Cross-Lingual Morphological Tagging for Low-Resource Languages Jan Buys Department of Computer Science UK jan.buys@cs.ox.ac.uk Jan A. Botha Google Inc. London jabot@google.com Abstract Morphologically rich languages often lack the annotated linguistic resources required to develop accurate natural language pro- cessing tools.	UK	University of Oxford	1
published for the TEI Consortium by Hu- manities Computing Unit, UK.	UK	University of Sheffield	0
published for the TEI Consortium by Hu- manities Computing Unit, UK.	UK	University	0
published for the TEI Consortium by Hu- manities Computing Unit, UK.	UK	University of Manchester	0
published for the TEI Consortium by Hu- manities Computing Unit, UK.	UK	University of Sunderland	0
published for the TEI Consortium by Hu- manities Computing Unit, UK.	UK	USussex	0
published for the TEI Consortium by Hu- manities Computing Unit, UK.	UK	University of Oxford	1
4.3 TAGs The construction that allows to handle the tree adjoining grammars of Joshi (Joshi and Schabes, 1997) may be seen as a generalization of the con- struction that we have described for the context- free grammars.	TAGs	Tree adjoining grammars	1
4.3 TAGs The construction that allows to handle the tree adjoining grammars of Joshi (Joshi and Schabes, 1997) may be seen as a generalization of the con- struction that we have described for the context- free grammars.	TAGs	Tree Adjoinig Grammars	0
4.3 TAGs The construction that allows to handle the tree adjoining grammars of Joshi (Joshi and Schabes, 1997) may be seen as a generalization of the con- struction that we have described for the context- free grammars.	TAGs	Tree Adjoining Grammars	0
TAGs: How much contextsensitivity is required ro provide reasonable structural descriptions?	TAGs	Tree adjoining grammars	1
TAGs: How much contextsensitivity is required ro provide reasonable structural descriptions?	TAGs	Tree Adjoinig Grammars	0
TAGs: How much contextsensitivity is required ro provide reasonable structural descriptions?	TAGs	Tree Adjoining Grammars	0
TAGs.	TAGs	Tree adjoining grammars	1
TAGs.	TAGs	Tree Adjoinig Grammars	0
TAGs.	TAGs	Tree Adjoining Grammars	0
TAGs (Joshi and Levy, 1977) can also be viewed as a special kind of LCFRS with f = 2, since each auxil- iary tree generates two strings, and with r given by the maximum number of adjunction and sub- stitution sites in an elementary tree.	TAGs	Tree adjoining grammars	1
TAGs (Joshi and Levy, 1977) can also be viewed as a special kind of LCFRS with f = 2, since each auxil- iary tree generates two strings, and with r given by the maximum number of adjunction and sub- stitution sites in an elementary tree.	TAGs	Tree Adjoinig Grammars	0
TAGs (Joshi and Levy, 1977) can also be viewed as a special kind of LCFRS with f = 2, since each auxil- iary tree generates two strings, and with r given by the maximum number of adjunction and sub- stitution sites in an elementary tree.	TAGs	Tree Adjoining Grammars	0
TAGs (Joshi and Levy, 1977), or TAG for short, can be viewed as a special kind of LCFRS with f = 2, since each elementary tree generates two strings, and r given by the maximum number of adjunction sites in an elementary tree.	TAGs	Tree adjoining grammars	1
TAGs (Joshi and Levy, 1977), or TAG for short, can be viewed as a special kind of LCFRS with f = 2, since each elementary tree generates two strings, and r given by the maximum number of adjunction sites in an elementary tree.	TAGs	Tree Adjoinig Grammars	0
TAGs (Joshi and Levy, 1977), or TAG for short, can be viewed as a special kind of LCFRS with f = 2, since each elementary tree generates two strings, and r given by the maximum number of adjunction sites in an elementary tree.	TAGs	Tree Adjoining Grammars	0
How much context sensi- tivity is necessary for characterizing structural de- scriptions: TAGs.	TAGs	Tree adjoining grammars	1
How much context sensi- tivity is necessary for characterizing structural de- scriptions: TAGs.	TAGs	Tree Adjoinig Grammars	0
How much context sensi- tivity is necessary for characterizing structural de- scriptions: TAGs.	TAGs	Tree Adjoining Grammars	0
In Gardent, C. and A. Sarkar, editors, Proceedings of the 9th In- ternational Workshop on TAGs and Related Formalisms (TAG+?08), pages 141?	TAGs	Tree adjoining grammars	0
In Gardent, C. and A. Sarkar, editors, Proceedings of the 9th In- ternational Workshop on TAGs and Related Formalisms (TAG+?08), pages 141?	TAGs	Tree Adjoinig Grammars	0
In Gardent, C. and A. Sarkar, editors, Proceedings of the 9th In- ternational Workshop on TAGs and Related Formalisms (TAG+?08), pages 141?	TAGs	Tree Adjoining Grammars	1
In Proceedings of the 9th International Workshop on TAGs and Related For- malisms.	TAGs	Tree adjoining grammars	0
In Proceedings of the 9th International Workshop on TAGs and Related For- malisms.	TAGs	Tree Adjoinig Grammars	0
In Proceedings of the 9th International Workshop on TAGs and Related For- malisms.	TAGs	Tree Adjoining Grammars	1
Feature Struc- tures Based TAGs.	TAGs	Tree adjoining grammars	0
Feature Struc- tures Based TAGs.	TAGs	Tree Adjoinig Grammars	0
Feature Struc- tures Based TAGs.	TAGs	Tree Adjoining Grammars	1
rkar, editors, Proceedings of the 9th In- ternational Workshop on TAGs and Related Formalisms (TAG+?08), pages 141?	TAGs	Tree adjoining grammars	0
rkar, editors, Proceedings of the 9th In- ternational Workshop on TAGs and Related Formalisms (TAG+?08), pages 141?	TAGs	Tree Adjoinig Grammars	0
rkar, editors, Proceedings of the 9th In- ternational Workshop on TAGs and Related Formalisms (TAG+?08), pages 141?	TAGs	Tree Adjoining Grammars	1
In Proceedings of the 6th In- ternational Workshop on TAGs and Related Frameworks (TAG+ 6), pages 19?24.	TAGs	Tree adjoining grammars	0
In Proceedings of the 6th In- ternational Workshop on TAGs and Related Frameworks (TAG+ 6), pages 19?24.	TAGs	Tree Adjoinig Grammars	0
In Proceedings of the 6th In- ternational Workshop on TAGs and Related Frameworks (TAG+ 6), pages 19?24.	TAGs	Tree Adjoining Grammars	1
The methodology works as follows: given a set of topics (including intruder words), we compute the word association features for each of the top- N topic words of a topic, 3 and combine the fea- tures in a ranking SVR model (SVM rank : Joachims (2006)) to learn the intruder words.	SVR	support vector regression	1
The methodology works as follows: given a set of topics (including intruder words), we compute the word association features for each of the top- N topic words of a topic, 3 and combine the fea- tures in a ranking SVR model (SVM rank : Joachims (2006)) to learn the intruder words.	SVR	Support Vector Regression	0
We use SVR implemented in the SVMLight toolkit 10 with Radial Basis Function (RBF) kernel to build this baseline.	SVR	support vector regression	1
We use SVR implemented in the SVMLight toolkit 10 with Radial Basis Function (RBF) kernel to build this baseline.	SVR	Support Vector Regression	0
A tutorial on SVR.	SVR	support vector regression	1
A tutorial on SVR.	SVR	Support Vector Regression	0
We  propose to use the SVR  model to achieve this goal.	SVR	support vector regression	1
We  propose to use the SVR  model to achieve this goal.	SVR	Support Vector Regression	0
Shrinking the tube: a new SVR algorithm.	SVR	support vector regression	1
Shrinking the tube: a new SVR algorithm.	SVR	Support Vector Regression	0
Extractive Multi- Document Summarization with Integer Linear Pro- gramming and SVR.	SVR	support vector regression	0
Extractive Multi- Document Summarization with Integer Linear Pro- gramming and SVR.	SVR	Support Vector Regression	1
719 3.4 Other Baselines We train a SVR model (see Section 4), which is one of the most widely used approaches in text scoring.	SVR	support vector regression	0
719 3.4 Other Baselines We train a SVR model (see Section 4), which is one of the most widely used approaches in text scoring.	SVR	Support Vector Regression	1
We find that our manifold ranking approach outperforms several state-of-the- art learning baselines on this task, including trans- ductive SVR.	SVR	support vector regression	0
We find that our manifold ranking approach outperforms several state-of-the- art learning baselines on this task, including trans- ductive SVR.	SVR	Support Vector Regression	1
Extractive multi-document summarization with ILP and SVR.	SVR	support vector regression	0
Extractive multi-document summarization with ILP and SVR.	SVR	Support Vector Regression	1
Accurate Online SVR.	SVR	support vector regression	0
Accurate Online SVR.	SVR	Support Vector Regression	1
A Hierarchic AE  Logic.	AE	Autoepistemic	1
A Hierarchic AE  Logic.	AE	Answer Extraction	0
sssss???sssss,  Figure 1: A Hierarchic AE Theory  Hearer's beliefs as a consequence of the speech act:  in AI:  ^ (13)  -~L0--\[h/\]~b ^ ~Zo\[hy\]',\[Sl\]?~ ^  "~Lo\[hy\]"{si}\[hy\]~) D \[h/l~b  The asymmetry between Axioms 12 and 13 is a  consequence of the fact that a speech act has dif-  ferent effects on the speaker's and hearer's mental  states.	AE	Autoepistemic	1
sssss???sssss,  Figure 1: A Hierarchic AE Theory  Hearer's beliefs as a consequence of the speech act:  in AI:  ^ (13)  -~L0--\[h/\]~b ^ ~Zo\[hy\]',\[Sl\]?~ ^  "~Lo\[hy\]"{si}\[hy\]~) D \[h/l~b  The asymmetry between Axioms 12 and 13 is a  consequence of the fact that a speech act has dif-  ferent effects on the speaker's and hearer's mental  states.	AE	Answer Extraction	0
AE as Se- quence Tagging with Tree Edit Distance.	AE	Autoepistemic	0
AE as Se- quence Tagging with Tree Edit Distance.	AE	Answer Extraction	1
3.3 Personalized QA Algorithm The interaction between the UM component and the core QA component modifies the standard QA process at the AE phase, which is modified as follows: 3 The TF ?	AE	Autoepistemic	0
3.3 Personalized QA Algorithm The interaction between the UM component and the core QA component modifies the standard QA process at the AE phase, which is modified as follows: 3 The TF ?	AE	Answer Extraction	1
Diego Molla, Rolf Schwitter, Michael Hess, Rachel Fournier, 2000, Extrans, an AE System, Traitement Automatique des Langues, Hermes Science Publication, 41-2, 495-522.	AE	Autoepistemic	0
Diego Molla, Rolf Schwitter, Michael Hess, Rachel Fournier, 2000, Extrans, an AE System, Traitement Automatique des Langues, Hermes Science Publication, 41-2, 495-522.	AE	Answer Extraction	1
Where Q and T are sets of the bag-of-words  for the question relation and the triple relation  respectively, Lin(a,b) is a measure for the seman- tic similarity between a and b based on WordNet  (Lin, 1998), and L(x) is the number of elements  in the set x.  The AE: this component first  filters out the triples mismatching the expected  answer type.	AE	Autoepistemic	0
Where Q and T are sets of the bag-of-words  for the question relation and the triple relation  respectively, Lin(a,b) is a measure for the seman- tic similarity between a and b based on WordNet  (Lin, 1998), and L(x) is the number of elements  in the set x.  The AE: this component first  filters out the triples mismatching the expected  answer type.	AE	Answer Extraction	1
AE as Sequence Tagging with Tree Edit Distance.	AE	Autoepistemic	0
AE as Sequence Tagging with Tree Edit Distance.	AE	Answer Extraction	1
AE: 1.	AE	Autoepistemic	0
AE: 1.	AE	Answer Extraction	1
When analysing the corresponding NG, the  case frame for the head noun "liquid" is used at first.	NG	noun group	1
When analysing the corresponding NG, the  case frame for the head noun "liquid" is used at first.	NG	null grammar	0
When analysing the corresponding NG, the  case frame for the head noun "liquid" is used at first.	NG	ngrams	0
When analysing the corresponding NG, the  case frame for the head noun "liquid" is used at first.	NG	noun groups	0
The semanl;i(:al analysis of a sentence is based on its head  verb, while the analysis of a NG is be based on the  head noun and also on adjective descriptors, genitive deter-  miners and preposit ional phrases.	NG	noun group	1
The semanl;i(:al analysis of a sentence is based on its head  verb, while the analysis of a NG is be based on the  head noun and also on adjective descriptors, genitive deter-  miners and preposit ional phrases.	NG	null grammar	0
The semanl;i(:al analysis of a sentence is based on its head  verb, while the analysis of a NG is be based on the  head noun and also on adjective descriptors, genitive deter-  miners and preposit ional phrases.	NG	ngrams	0
The semanl;i(:al analysis of a sentence is based on its head  verb, while the analysis of a NG is be based on the  head noun and also on adjective descriptors, genitive deter-  miners and preposit ional phrases.	NG	noun groups	0
The  constraint of the NG (being a physical object) is full-  filled, thus the analysis proceeds.	NG	noun group	1
The  constraint of the NG (being a physical object) is full-  filled, thus the analysis proceeds.	NG	null grammar	0
The  constraint of the NG (being a physical object) is full-  filled, thus the analysis proceeds.	NG	ngrams	0
The  constraint of the NG (being a physical object) is full-  filled, thus the analysis proceeds.	NG	noun groups	0
67 Set Comment Size IN  The inflection set for NG.	NG	noun group	1
67 Set Comment Size IN  The inflection set for NG.	NG	null grammar	0
67 Set Comment Size IN  The inflection set for NG.	NG	ngrams	0
67 Set Comment Size IN  The inflection set for NG.	NG	noun groups	0
40  IP The inflection set for proNG.	NG	noun group	1
40  IP The inflection set for proNG.	NG	null grammar	0
40  IP The inflection set for proNG.	NG	ngrams	0
40  IP The inflection set for proNG.	NG	noun groups	0
In natural anguages, propositions  can be expressed not only by sentences, but also by other  syntactic structures uch as NGs, infinitive phrases  and embedded sentences.	NG	noun group	1
In natural anguages, propositions  can be expressed not only by sentences, but also by other  syntactic structures uch as NGs, infinitive phrases  and embedded sentences.	NG	null grammar	0
In natural anguages, propositions  can be expressed not only by sentences, but also by other  syntactic structures uch as NGs, infinitive phrases  and embedded sentences.	NG	ngrams	0
In natural anguages, propositions  can be expressed not only by sentences, but also by other  syntactic structures uch as NGs, infinitive phrases  and embedded sentences.	NG	noun groups	0
The prepositional phrase of the NG is analysed by  first selecting the case frame for the	NG	noun group	1
The prepositional phrase of the NG is analysed by  first selecting the case frame for the	NG	null grammar	0
The prepositional phrase of the NG is analysed by  first selecting the case frame for the	NG	ngrams	0
The prepositional phrase of the NG is analysed by  first selecting the case frame for the	NG	noun groups	0
1 MOTIVAT ION  In recent DARPA speech community-wide r cognition  system evaluations, the recognition systems have been  tested using two grammatical conditions: no grammar  (or NG), and the word-pair grammar.	NG	noun group	0
1 MOTIVAT ION  In recent DARPA speech community-wide r cognition  system evaluations, the recognition systems have been  tested using two grammatical conditions: no grammar  (or NG), and the word-pair grammar.	NG	null grammar	1
1 MOTIVAT ION  In recent DARPA speech community-wide r cognition  system evaluations, the recognition systems have been  tested using two grammatical conditions: no grammar  (or NG), and the word-pair grammar.	NG	ngrams	0
1 MOTIVAT ION  In recent DARPA speech community-wide r cognition  system evaluations, the recognition systems have been  tested using two grammatical conditions: no grammar  (or NG), and the word-pair grammar.	NG	noun groups	0
The word accuracies for various versions of  SPHINX with the word-pair grammar (perplexity 60) and  the NG (perplexity 991) are shown in Table 1.	NG	noun group	0
The word accuracies for various versions of  SPHINX with the word-pair grammar (perplexity 60) and  the NG (perplexity 991) are shown in Table 1.	NG	null grammar	1
The word accuracies for various versions of  SPHINX with the word-pair grammar (perplexity 60) and  the NG (perplexity 991) are shown in Table 1.	NG	ngrams	0
The word accuracies for various versions of  SPHINX with the word-pair grammar (perplexity 60) and  the NG (perplexity 991) are shown in Table 1.	NG	noun groups	0
The NG provides "only a worst -case  recognit ion test point", whi le  the word-pa i r  grammar not only  excludes "many reasonable word sequences", but the use of the word-  pair  grammar y ie lds such high recognit ion per formance that re l iable  measurement  of system improvements (i.e. s tat is t ica l ly  s igni f icant   inferences of improvements) cannot be obtained wi thout  use of	NG	noun group	0
The NG provides "only a worst -case  recognit ion test point", whi le  the word-pa i r  grammar not only  excludes "many reasonable word sequences", but the use of the word-  pair  grammar y ie lds such high recognit ion per formance that re l iable  measurement  of system improvements (i.e. s tat is t ica l ly  s igni f icant   inferences of improvements) cannot be obtained wi thout  use of	NG	null grammar	1
The NG provides "only a worst -case  recognit ion test point", whi le  the word-pa i r  grammar not only  excludes "many reasonable word sequences", but the use of the word-  pair  grammar y ie lds such high recognit ion per formance that re l iable  measurement  of system improvements (i.e. s tat is t ica l ly  s igni f icant   inferences of improvements) cannot be obtained wi thout  use of	NG	ngrams	0
The NG provides "only a worst -case  recognit ion test point", whi le  the word-pa i r  grammar not only  excludes "many reasonable word sequences", but the use of the word-  pair  grammar y ie lds such high recognit ion per formance that re l iable  measurement  of system improvements (i.e. s tat is t ica l ly  s igni f icant   inferences of improvements) cannot be obtained wi thout  use of	NG	noun groups	0
The NG simply forces the recognition sys-  tem to partition the input speech into whole-word units  without using any knowledge of the language to place  restrictions on the possible sequences of words that are  allowed.	NG	noun group	0
The NG simply forces the recognition sys-  tem to partition the input speech into whole-word units  without using any knowledge of the language to place  restrictions on the possible sequences of words that are  allowed.	NG	null grammar	1
The NG simply forces the recognition sys-  tem to partition the input speech into whole-word units  without using any knowledge of the language to place  restrictions on the possible sequences of words that are  allowed.	NG	ngrams	0
The NG simply forces the recognition sys-  tem to partition the input speech into whole-word units  without using any knowledge of the language to place  restrictions on the possible sequences of words that are  allowed.	NG	noun groups	0
Therefore, even in  a NG there is a great deal of a priori knowledge  being brought to bear.	NG	noun group	0
Therefore, even in  a NG there is a great deal of a priori knowledge  being brought to bear.	NG	null grammar	1
Therefore, even in  a NG there is a great deal of a priori knowledge  being brought to bear.	NG	ngrams	0
Therefore, even in  a NG there is a great deal of a priori knowledge  being brought to bear.	NG	noun groups	0
For example, we could use  a NG in the forward direction and a more com-  plex grammar in the backward search.	NG	noun group	0
For example, we could use  a NG in the forward direction and a more com-  plex grammar in the backward search.	NG	null grammar	1
For example, we could use  a NG in the forward direction and a more com-  plex grammar in the backward search.	NG	ngrams	0
For example, we could use  a NG in the forward direction and a more com-  plex grammar in the backward search.	NG	noun groups	0
Each instance is con- verted into a feature vector where the features are  10 NG (unigrams or bigrams) and each cell is  the frequency of occurrence of a unigram or bi- gram or the log-likelihood of a bigram occurring in that particular instance after applying a feature selection method.	NG	noun group	0
Each instance is con- verted into a feature vector where the features are  10 NG (unigrams or bigrams) and each cell is  the frequency of occurrence of a unigram or bi- gram or the log-likelihood of a bigram occurring in that particular instance after applying a feature selection method.	NG	null grammar	0
Each instance is con- verted into a feature vector where the features are  10 NG (unigrams or bigrams) and each cell is  the frequency of occurrence of a unigram or bi- gram or the log-likelihood of a bigram occurring in that particular instance after applying a feature selection method.	NG	ngrams	1
Each instance is con- verted into a feature vector where the features are  10 NG (unigrams or bigrams) and each cell is  the frequency of occurrence of a unigram or bi- gram or the log-likelihood of a bigram occurring in that particular instance after applying a feature selection method.	NG	noun groups	0
These features can be broken into four categories: NG: Ngrams of order to 1 to 3, found via Hap- pierFunTokenizer, and restricted to those used by at least 5% of users (resulting in 10,450 NG).	NG	noun group	0
These features can be broken into four categories: NG: Ngrams of order to 1 to 3, found via Hap- pierFunTokenizer, and restricted to those used by at least 5% of users (resulting in 10,450 NG).	NG	null grammar	0
These features can be broken into four categories: NG: Ngrams of order to 1 to 3, found via Hap- pierFunTokenizer, and restricted to those used by at least 5% of users (resulting in 10,450 NG).	NG	ngrams	1
These features can be broken into four categories: NG: Ngrams of order to 1 to 3, found via Hap- pierFunTokenizer, and restricted to those used by at least 5% of users (resulting in 10,450 NG).	NG	noun groups	0
4 Differential Language Analysis Figure 4 shows the 100 NG most highly cor- related with depression score across the 21,913 Facebook users in our dataset writing at least 1,000 words.	NG	noun group	0
4 Differential Language Analysis Figure 4 shows the 100 NG most highly cor- related with depression score across the 21,913 Facebook users in our dataset writing at least 1,000 words.	NG	null grammar	0
4 Differential Language Analysis Figure 4 shows the 100 NG most highly cor- related with depression score across the 21,913 Facebook users in our dataset writing at least 1,000 words.	NG	ngrams	1
4 Differential Language Analysis Figure 4 shows the 100 NG most highly cor- related with depression score across the 21,913 Facebook users in our dataset writing at least 1,000 words.	NG	noun groups	0
This provides a continuous value outcome, for which we fit a regression model based on NG, LDA topics, and lexica usage.	NG	noun group	0
This provides a continuous value outcome, for which we fit a regression model based on NG, LDA topics, and lexica usage.	NG	null grammar	0
This provides a continuous value outcome, for which we fit a regression model based on NG, LDA topics, and lexica usage.	NG	ngrams	1
This provides a continuous value outcome, for which we fit a regression model based on NG, LDA topics, and lexica usage.	NG	noun groups	0
It  offers a variety of lexical features (NG, col- locations, etc.)	NG	noun group	0
It  offers a variety of lexical features (NG, col- locations, etc.)	NG	null grammar	0
It  offers a variety of lexical features (NG, col- locations, etc.)	NG	ngrams	1
It  offers a variety of lexical features (NG, col- locations, etc.)	NG	noun groups	0
ence has been identified by the  parser, so that also the underlying dependency relations (valency positions) of  the complementations (to the governing verb) are known, the verb and all the  complementations are first assumed to be NB, i.e., to belong to the focus,  which we denote by f.  (b) If the verb occupies the rightmost position in the sentence and its subject is  (ba) definite (including NG with this, with oneofthe, etc.),	NG	noun group	0
ence has been identified by the  parser, so that also the underlying dependency relations (valency positions) of  the complementations (to the governing verb) are known, the verb and all the  complementations are first assumed to be NB, i.e., to belong to the focus,  which we denote by f.  (b) If the verb occupies the rightmost position in the sentence and its subject is  (ba) definite (including NG with this, with oneofthe, etc.),	NG	null grammar	0
ence has been identified by the  parser, so that also the underlying dependency relations (valency positions) of  the complementations (to the governing verb) are known, the verb and all the  complementations are first assumed to be NB, i.e., to belong to the focus,  which we denote by f.  (b) If the verb occupies the rightmost position in the sentence and its subject is  (ba) definite (including NG with this, with oneofthe, etc.),	NG	ngrams	0
ence has been identified by the  parser, so that also the underlying dependency relations (valency positions) of  the complementations (to the governing verb) are known, the verb and all the  complementations are first assumed to be NB, i.e., to belong to the focus,  which we denote by f.  (b) If the verb occupies the rightmost position in the sentence and its subject is  (ba) definite (including NG with this, with oneofthe, etc.),	NG	noun groups	1
5 The tools can be used to acquire non-clausal patterns as well, e.g. patterns for NG and complex noun phrases, to extend an existing pattern library.	NG	noun group	0
5 The tools can be used to acquire non-clausal patterns as well, e.g. patterns for NG and complex noun phrases, to extend an existing pattern library.	NG	null grammar	0
5 The tools can be used to acquire non-clausal patterns as well, e.g. patterns for NG and complex noun phrases, to extend an existing pattern library.	NG	ngrams	0
5 The tools can be used to acquire non-clausal patterns as well, e.g. patterns for NG and complex noun phrases, to extend an existing pattern library.	NG	noun groups	1
In natural anguages, propositions  can be expressed not only by sentences, but also by other  syntactic structures uch as NG, infinitive phrases  and embedded sentences.	NG	noun group	0
In natural anguages, propositions  can be expressed not only by sentences, but also by other  syntactic structures uch as NG, infinitive phrases  and embedded sentences.	NG	null grammar	0
In natural anguages, propositions  can be expressed not only by sentences, but also by other  syntactic structures uch as NG, infinitive phrases  and embedded sentences.	NG	ngrams	0
In natural anguages, propositions  can be expressed not only by sentences, but also by other  syntactic structures uch as NG, infinitive phrases  and embedded sentences.	NG	noun groups	1
Thus,  the semantic domain for NG is \[C-~S\].	NG	noun group	0
Thus,  the semantic domain for NG is \[C-~S\].	NG	null grammar	0
Thus,  the semantic domain for NG is \[C-~S\].	NG	ngrams	0
Thus,  the semantic domain for NG is \[C-~S\].	NG	noun groups	1
Above all, this  concerns the following points in which a more general procedure could be formulated:  (i) The procedure should also take into account deeper embedded sentence parts  (embedded verb clauses, modifiers in NG, etc.).	NG	noun group	0
Above all, this  concerns the following points in which a more general procedure could be formulated:  (i) The procedure should also take into account deeper embedded sentence parts  (embedded verb clauses, modifiers in NG, etc.).	NG	null grammar	0
Above all, this  concerns the following points in which a more general procedure could be formulated:  (i) The procedure should also take into account deeper embedded sentence parts  (embedded verb clauses, modifiers in NG, etc.).	NG	ngrams	0
Above all, this  concerns the following points in which a more general procedure could be formulated:  (i) The procedure should also take into account deeper embedded sentence parts  (embedded verb clauses, modifiers in NG, etc.).	NG	noun groups	1
5.3 Case markers Turkish, being a fairly scrambling language, uses case markers to denote the syntactic functions of nouns and NG.	NG	noun group	0
5.3 Case markers Turkish, being a fairly scrambling language, uses case markers to denote the syntactic functions of nouns and NG.	NG	null grammar	0
5.3 Case markers Turkish, being a fairly scrambling language, uses case markers to denote the syntactic functions of nouns and NG.	NG	ngrams	0
5.3 Case markers Turkish, being a fairly scrambling language, uses case markers to denote the syntactic functions of nouns and NG.	NG	noun groups	1
RM uh, I mean, ? ?? ?	RM	Reparandum	1
RM uh, I mean, ? ?? ?	RM	Resource Management	0
RM uh, I mean, ? ?? ?	RM	restricted memory	0
Words with italic font are RMs.	RM	Reparandum	1
Words with italic font are RMs.	RM	Resource Management	0
Words with italic font are RMs.	RM	restricted memory	0
RM uh ????	RM	Reparandum	1
RM uh ????	RM	Resource Management	0
RM uh ????	RM	restricted memory	0
Second, hu- man processing of spoken language is complex and mixes acoustic and syntactic indicators (Cutler et al, 1997), so an automatic system must employ fea- tures targeting all levels of the perceptual stack to in  the    upper     school                          upper  four   grades  Fluent Fluent Disfluent  RM  Repair   Figure 1: Example of a disfluency where the speaker corrected upper school.	RM	Reparandum	1
Second, hu- man processing of spoken language is complex and mixes acoustic and syntactic indicators (Cutler et al, 1997), so an automatic system must employ fea- tures targeting all levels of the perceptual stack to in  the    upper     school                          upper  four   grades  Fluent Fluent Disfluent  RM  Repair   Figure 1: Example of a disfluency where the speaker corrected upper school.	RM	Resource Management	0
Second, hu- man processing of spoken language is complex and mixes acoustic and syntactic indicators (Cutler et al, 1997), so an automatic system must employ fea- tures targeting all levels of the perceptual stack to in  the    upper     school                          upper  four   grades  Fluent Fluent Disfluent  RM  Repair   Figure 1: Example of a disfluency where the speaker corrected upper school.	RM	restricted memory	0
RM + uh, I mean, ? ?? ?	RM	Reparandum	1
RM + uh, I mean, ? ?? ?	RM	Resource Management	0
RM + uh, I mean, ? ?? ?	RM	restricted memory	0
RM : what precedes the interruption point.	RM	Reparandum	1
RM : what precedes the interruption point.	RM	Resource Management	0
RM : what precedes the interruption point.	RM	restricted memory	0
Many tests on the RM task were  performed with both context-independent (CI) PLUs (set size = 47) and context-dependent (CD)  PLUs (set sizes range from 638 to 2340).	RM	Reparandum	0
Many tests on the RM task were  performed with both context-independent (CI) PLUs (set size = 47) and context-dependent (CD)  PLUs (set sizes range from 638 to 2340).	RM	Resource Management	1
Many tests on the RM task were  performed with both context-independent (CI) PLUs (set size = 47) and context-dependent (CD)  PLUs (set sizes range from 638 to 2340).	RM	restricted memory	0
Murveit, H., J. Butzberger, and M. Weintraub,"Speech  Recognition in SRI's RM and ATIS  Systems," Proc.	RM	Reparandum	0
Murveit, H., J. Butzberger, and M. Weintraub,"Speech  Recognition in SRI's RM and ATIS  Systems," Proc.	RM	Resource Management	1
Murveit, H., J. Butzberger, and M. Weintraub,"Speech  Recognition in SRI's RM and ATIS  Systems," Proc.	RM	restricted memory	0
Lee discussed CMU's present progress,  including the use of semi-continuous hidden Markov models (SCHMMs) applied to the 1000-  word speaker-independent RM continuous peech recognition task.	RM	Reparandum	0
Lee discussed CMU's present progress,  including the use of semi-continuous hidden Markov models (SCHMMs) applied to the 1000-  word speaker-independent RM continuous peech recognition task.	RM	Resource Management	1
Lee discussed CMU's present progress,  including the use of semi-continuous hidden Markov models (SCHMMs) applied to the 1000-  word speaker-independent RM continuous peech recognition task.	RM	restricted memory	0
Lee discussed CMU's present progress,  including the use of semi-continuous hidden Markov models (SCHMMs) applied to the 1000-  word speaker-independent RM continuous peech recognition ta	RM	Reparandum	0
Lee discussed CMU's present progress,  including the use of semi-continuous hidden Markov models (SCHMMs) applied to the 1000-  word speaker-independent RM continuous peech recognition ta	RM	Resource Management	1
Lee discussed CMU's present progress,  including the use of semi-continuous hidden Markov models (SCHMMs) applied to the 1000-  word speaker-independent RM continuous peech recognition ta	RM	restricted memory	0
Although not all of these methods have yet been combined  into one system, the error rate on the May 1988 RM test set (using word-pair  grammar) has been halved.	RM	Reparandum	0
Although not all of these methods have yet been combined  into one system, the error rate on the May 1988 RM test set (using word-pair  grammar) has been halved.	RM	Resource Management	1
Although not all of these methods have yet been combined  into one system, the error rate on the May 1988 RM test set (using word-pair  grammar) has been halved.	RM	restricted memory	0
Encouraging test results were obtained using  RM speech data where "new" words were created simply by removing a  subset of in-vocabulary words from the standard system lexicon.	RM	Reparandum	0
Encouraging test results were obtained using  RM speech data where "new" words were created simply by removing a  subset of in-vocabulary words from the standard system lexicon.	RM	Resource Management	1
Encouraging test results were obtained using  RM speech data where "new" words were created simply by removing a  subset of in-vocabulary words from the standard system lexicon.	RM	restricted memory	0
H. Murveit, J. Butzberger, and M. Weintraub, "Speech  Recognition in SRI's RM and ATIS  Systems," 1991 DARPA Speech and Natural Language  Workshop, pp.	RM	Reparandum	0
H. Murveit, J. Butzberger, and M. Weintraub, "Speech  Recognition in SRI's RM and ATIS  Systems," 1991 DARPA Speech and Natural Language  Workshop, pp.	RM	Resource Management	1
H. Murveit, J. Butzberger, and M. Weintraub, "Speech  Recognition in SRI's RM and ATIS  Systems," 1991 DARPA Speech and Natural Language  Workshop, pp.	RM	restricted memory	0
% correct)  CBL Default i ~ -   Algorithm Strategy Heuristics  w/o  feature  set selection  76.2 74.3 80.5  In the sections below, we describe the recency  bias, the RM bias, and the subject  accessibility bias in turn.	RM	Reparandum	0
% correct)  CBL Default i ~ -   Algorithm Strategy Heuristics  w/o  feature  set selection  76.2 74.3 80.5  In the sections below, we describe the recency  bias, the RM bias, and the subject  accessibility bias in turn.	RM	Resource Management	0
% correct)  CBL Default i ~ -   Algorithm Strategy Heuristics  w/o  feature  set selection  76.2 74.3 80.5  In the sections below, we describe the recency  bias, the RM bias, and the subject  accessibility bias in turn.	RM	restricted memory	1
Our baseline case representation does not nec-  essarily make use of this RM bias,  however.	RM	Reparandum	0
Our baseline case representation does not nec-  essarily make use of this RM bias,  however.	RM	Resource Management	0
Our baseline case representation does not nec-  essarily make use of this RM bias,  however.	RM	restricted memory	1
In addition, the RM bias  alon	RM	Reparandum	0
In addition, the RM bias  alon	RM	Resource Management	0
In addition, the RM bias  alon	RM	restricted memory	1
117  Thus far, we have incorporated three such bi-  ases into the feature set selection algorithm: (1) a  recency bias, (2) a RM bias, and (3)  a subject accessibility bias.	RM	Reparandum	0
117  Thus far, we have incorporated three such bi-  ases into the feature set selection algorithm: (1) a  recency bias, (2) a RM bias, and (3)  a subject accessibility bias.	RM	Resource Management	0
117  Thus far, we have incorporated three such bi-  ases into the feature set selection algorithm: (1) a  recency bias, (2) a RM bias, and (3)  a subject accessibility bias.	RM	restricted memory	1
Unfortunately, incorporat-  ing the RM limitations into the case  representation is problematic.	RM	Reparandum	0
Unfortunately, incorporat-  ing the RM limitations into the case  representation is problematic.	RM	Resource Management	0
Unfortunately, incorporat-  ing the RM limitations into the case  representation is problematic.	RM	restricted memory	1
To apply the RM bias to the  baseline case representation, we let n represent the  memory limit and, in each of five runs, set n to one  of five, six, seven, eight, or nine.	RM	Reparandum	0
To apply the RM bias to the  baseline case representation, we let n represent the  memory limit and, in each of five runs, set n to one  of five, six, seven, eight, or nine.	RM	Resource Management	0
To apply the RM bias to the  baseline case representation, we let n represent the  memory limit and, in each of five runs, set n to one  of five, six, seven, eight, or nine.	RM	restricted memory	1
ately, incorporat-  ing the RM limitations into the case  representation is problematic.	RM	Reparandum	0
ately, incorporat-  ing the RM limitations into the case  representation is problematic.	RM	Resource Management	0
ately, incorporat-  ing the RM limitations into the case  representation is problematic.	RM	restricted memory	1
In addition, the RM bias  alone does not state which chunks, or features, to  keep and which to discard.	RM	Reparandum	0
In addition, the RM bias  alone does not state which chunks, or features, to  keep and which to discard.	RM	Resource Management	0
In addition, the RM bias  alone does not state which chunks, or features, to  keep and which to discard.	RM	restricted memory	1
We used two approaches for identifying story characters motivated by (Elson and McKeown, 2010): 1) named entity reCOG was used for identifying proper names, e.g., ?	COG	cognition	1
We used two approaches for identifying story characters motivated by (Elson and McKeown, 2010): 1) named entity reCOG was used for identifying proper names, e.g., ?	COG	cognitio	0
Unsupervised personality reCOG for social network sites.	COG	cognition	1
Unsupervised personality reCOG for social network sites.	COG	cognitio	0
3) We employed IE methods (including pattern sets  and Named Entity ReCOG) as initial extraction  steps.	COG	cognition	1
3) We employed IE methods (including pattern sets  and Named Entity ReCOG) as initial extraction  steps.	COG	cognitio	0
This includes (i) tokenization, (ii) sen- tence splitting and identification of paragraph boundaries, (iii) part-of-speech (POS) tagging, (iv) lemmatization, (v) named entity reCOG, (vi) dependency parsing, and (vii) co-reference analysis.	COG	cognition	1
This includes (i) tokenization, (ii) sen- tence splitting and identification of paragraph boundaries, (iii) part-of-speech (POS) tagging, (iv) lemmatization, (v) named entity reCOG, (vi) dependency parsing, and (vii) co-reference analysis.	COG	cognitio	0
Two broad approaches for the iden- tification of story characters were followed: (i) named entity reCOG, and (ii) identification of character nominals, e.g., ?	COG	cognition	1
Two broad approaches for the iden- tification of story characters were followed: (i) named entity reCOG, and (ii) identification of character nominals, e.g., ?	COG	cognitio	0
The approach incorporates a set of ge- neric morpho-syntactic filters for recogni- tion of term candidates, a method for  conflation of morphological variants and  a module for foreign word reCOG.	COG	cognition	1
The approach incorporates a set of ge- neric morpho-syntactic filters for recogni- tion of term candidates, a method for  conflation of morphological variants and  a module for foreign word reCOG.	COG	cognitio	0
We used two approaches for identifying story characters motivated by (Elson and McKeown, 2010): 1) named entity reCOGn was used for identifying proper names, e.g., ?	COG	cognition	0
We used two approaches for identifying story characters motivated by (Elson and McKeown, 2010): 1) named entity reCOGn was used for identifying proper names, e.g., ?	COG	cognitio	1
Unsupervised personality reCOGn for social network sites.	COG	cognition	0
Unsupervised personality reCOGn for social network sites.	COG	cognitio	1
3) We employed IE methods (including pattern sets  and Named Entity ReCOGn) as initial extraction  steps.	COG	cognition	0
3) We employed IE methods (including pattern sets  and Named Entity ReCOGn) as initial extraction  steps.	COG	cognitio	1
This includes (i) tokenization, (ii) sen- tence splitting and identification of paragraph boundaries, (iii) part-of-speech (POS) tagging, (iv) lemmatization, (v) named entity reCOGn, (vi) dependency parsing, and (vii) co-reference analysis.	COG	cognition	0
This includes (i) tokenization, (ii) sen- tence splitting and identification of paragraph boundaries, (iii) part-of-speech (POS) tagging, (iv) lemmatization, (v) named entity reCOGn, (vi) dependency parsing, and (vii) co-reference analysis.	COG	cognitio	1
Two broad approaches for the iden- tification of story characters were followed: (i) named entity reCOGn, and (ii) identification of character nominals, e.g., ?	COG	cognition	0
Two broad approaches for the iden- tification of story characters were followed: (i) named entity reCOGn, and (ii) identification of character nominals, e.g., ?	COG	cognitio	1
The approach incorporates a set of ge- neric morpho-syntactic filters for recogni- tion of term candidates, a method for  conflation of morphological variants and  a module for foreign word reCOGn.	COG	cognition	0
The approach incorporates a set of ge- neric morpho-syntactic filters for recogni- tion of term candidates, a method for  conflation of morphological variants and  a module for foreign word reCOGn.	COG	cognitio	1
ADDe.	ADD	ATR Dialogue Databas	1
ADDe.	ADD	additive	0
ADDe.	ADD	added	0
Activation by pRb and c-Myc is not ADD, suggesting that they act upon the same site, thereby perhaps blocking the binding of an unidentified inhibitor.	ADD	ATR Dialogue Databas	0
Activation by pRb and c-Myc is not ADD, suggesting that they act upon the same site, thereby perhaps blocking the binding of an unidentified inhibitor.	ADD	additive	1
Activation by pRb and c-Myc is not ADD, suggesting that they act upon the same site, thereby perhaps blocking the binding of an unidentified inhibitor.	ADD	added	0
One of the main facts of interaction M4 (Ta- ble 1) is Activation by pRb and c-Myc is not ADD . . .	ADD	ATR Dialogue Databas	0
One of the main facts of interaction M4 (Ta- ble 1) is Activation by pRb and c-Myc is not ADD . . .	ADD	additive	1
One of the main facts of interaction M4 (Ta- ble 1) is Activation by pRb and c-Myc is not ADD . . .	ADD	added	0
Moreover,  this deviation cannot be remedied later by sum- ming up individual termhoods, since C-value is  not an ADD measure.	ADD	ATR Dialogue Databas	0
Moreover,  this deviation cannot be remedied later by sum- ming up individual termhoods, since C-value is  not an ADD measure.	ADD	additive	1
Moreover,  this deviation cannot be remedied later by sum- ming up individual termhoods, since C-value is  not an ADD measure.	ADD	added	0
the positive effects of RB and c-Myc were not ADD. (	ADD	ATR Dialogue Databas	0
the positive effects of RB and c-Myc were not ADD. (	ADD	additive	1
the positive effects of RB and c-Myc were not ADD. (	ADD	added	0
M4 Subfact: Activation of E-cadherin by pRb and c-Myc is not ADD, suggesting they act on the same site a) However, the precise molecular mechanisms by which RB, Myc, and AP-2 cooperate to effect transcriptional activation of E-cadherin requires further study. . . .	ADD	ATR Dialogue Databas	0
M4 Subfact: Activation of E-cadherin by pRb and c-Myc is not ADD, suggesting they act on the same site a) However, the precise molecular mechanisms by which RB, Myc, and AP-2 cooperate to effect transcriptional activation of E-cadherin requires further study. . . .	ADD	additive	1
M4 Subfact: Activation of E-cadherin by pRb and c-Myc is not ADD, suggesting they act on the same site a) However, the precise molecular mechanisms by which RB, Myc, and AP-2 cooperate to effect transcriptional activation of E-cadherin requires further study. . . .	ADD	added	0
An instance supporting part of this fact, the subfact in Table 2 Activation of E-cadherin by pRb and c- Myc is not ADD . . . ,	ADD	ATR Dialogue Databas	0
An instance supporting part of this fact, the subfact in Table 2 Activation of E-cadherin by pRb and c- Myc is not ADD . . . ,	ADD	additive	1
An instance supporting part of this fact, the subfact in Table 2 Activation of E-cadherin by pRb and c- Myc is not ADD . . . ,	ADD	added	0
Sparse ADD generative models of text.	ADD	ATR Dialogue Databas	0
Sparse ADD generative models of text.	ADD	additive	1
Sparse ADD generative models of text.	ADD	added	0
relevant nodes (m=5 for our reported experiment) and  ADD them to the previous set of WordNet nodes.	ADD	ATR Dialogue Databas	0
relevant nodes (m=5 for our reported experiment) and  ADD them to the previous set of WordNet nodes.	ADD	additive	0
relevant nodes (m=5 for our reported experiment) and  ADD them to the previous set of WordNet nodes.	ADD	added	1
As before, the harmonic vahlations arc ADD during con-  catenation.	ADD	ATR Dialogue Databas	0
As before, the harmonic vahlations arc ADD during con-  catenation.	ADD	additive	0
As before, the harmonic vahlations arc ADD during con-  catenation.	ADD	added	1
We present some experiments il-  lustrating the accuracy of the method and note  that with this information ADD, our pronoun  resolution method achieves 84.2% accuracy.	ADD	ATR Dialogue Databas	0
We present some experiments il-  lustrating the accuracy of the method and note  that with this information ADD, our pronoun  resolution method achieves 84.2% accuracy.	ADD	additive	0
We present some experiments il-  lustrating the accuracy of the method and note  that with this information ADD, our pronoun  resolution method achieves 84.2% accuracy.	ADD	added	1
As a measure of the utility of these results, we  also ran our pronoun-anaphora program with  these statistics ADD.	ADD	ATR Dialogue Databas	0
As a measure of the utility of these results, we  also ran our pronoun-anaphora program with  these statistics ADD.	ADD	additive	0
As a measure of the utility of these results, we  also ran our pronoun-anaphora program with  these statistics ADD.	ADD	added	1
If ally of  them point to states with undelined Imrtnony values, tile  harlnony of the state being expanded, and o1' tile arc, are  used to calculate the lmtumny value of tile {}tiler state and  it is ADD to the list.	ADD	ATR Dialogue Databas	0
If ally of  them point to states with undelined Imrtnony values, tile  harlnony of the state being expanded, and o1' tile arc, are  used to calculate the lmtumny value of tile {}tiler state and  it is ADD to the list.	ADD	additive	0
If ally of  them point to states with undelined Imrtnony values, tile  harlnony of the state being expanded, and o1' tile arc, are  used to calculate the lmtumny value of tile {}tiler state and  it is ADD to the list.	ADD	added	1
We see a significant improvement after the  word knowledge is ADD to the program.	ADD	ATR Dialogue Databas	0
We see a significant improvement after the  word knowledge is ADD to the program.	ADD	additive	0
We see a significant improvement after the  word knowledge is ADD to the program.	ADD	added	1
Recent reports by (Nivre, 2007) delineated a class of richly-inflected languages with relatively free word-order (including Greek, Basque, and MSA) for which the parsers performed poorly, regardless of the parsing method used.	MSA	Modern Standard Arabic	1
Recent reports by (Nivre, 2007) delineated a class of richly-inflected languages with relatively free word-order (including Greek, Basque, and MSA) for which the parsers performed poorly, regardless of the parsing method used.	MSA	morphosyntactic analyzers	0
Quantitative ap- proaches to analyzing COME constructions in  MSA.	MSA	Modern Standard Arabic	1
Quantitative ap- proaches to analyzing COME constructions in  MSA.	MSA	morphosyntactic analyzers	0
c?2014 Association for Computational Linguistics Annotating corpus data for a quantitative, constructional analysis of  motion verbs in MSA     Dana Abdulrahim  University of Bahrain  darahim@uob.edu.bh         Abstract  This article proposes an annotation method of corpus  data for the purposes of providing a constructionist  account of lexical behavior.	MSA	Modern Standard Arabic	1
c?2014 Association for Computational Linguistics Annotating corpus data for a quantitative, constructional analysis of  motion verbs in MSA     Dana Abdulrahim  University of Bahrain  darahim@uob.edu.bh         Abstract  This article proposes an annotation method of corpus  data for the purposes of providing a constructionist  account of lexical behavior.	MSA	morphosyntactic analyzers	0
In MSA, the existence of  several verbs denoting the motion events COME  (at?, ???	MSA	Modern Standard Arabic	1
In MSA, the existence of  several verbs denoting the motion events COME  (at?, ???	MSA	morphosyntactic analyzers	0
A corpus study of basic  motion events in MSA.	MSA	Modern Standard Arabic	1
A corpus study of basic  motion events in MSA.	MSA	morphosyntactic analyzers	0
However, a closer examination of the length difference evident through the BLEU brevity penalty and the reference:system-output length ra- tio (columns 4-5 of Table 2), reveals that the dif- ferences are small and inconsistent; on average, the brevity penalty difference accounts for roughly 0.1 absolute BLEU points and 0.2 absolute lemmatized BLEU points of the respective differences.7 Last, MSA is a morphologi- cally rich language: It has many inflected forms for most verbs, and several inflected forms for nouns, adjectives and other parts of speech ?	MSA	Modern Standard Arabic	1
However, a closer examination of the length difference evident through the BLEU brevity penalty and the reference:system-output length ra- tio (columns 4-5 of Table 2), reveals that the dif- ferences are small and inconsistent; on average, the brevity penalty difference accounts for roughly 0.1 absolute BLEU points and 0.2 absolute lemmatized BLEU points of the respective differences.7 Last, MSA is a morphologi- cally rich language: It has many inflected forms for most verbs, and several inflected forms for nouns, adjectives and other parts of speech ?	MSA	morphosyntactic analyzers	0
PROMT parsers are rule-based multi-level  MSA.	MSA	Modern Standard Arabic	0
PROMT parsers are rule-based multi-level  MSA.	MSA	morphosyntactic analyzers	1
PROMT uses  MSA to analyze the source  sentence and transfer rules to translate the sentence  345 into the target language.	MSA	Modern Standard Arabic	0
PROMT uses  MSA to analyze the source  sentence and transfer rules to translate the sentence  345 into the target language.	MSA	morphosyntactic analyzers	1
However, the flexible sys- tem architecture also allows us to experiment with different MSA, such as TextMorfo (Kielikone Oy 1999) and Conexor FDG (Conexor Oy 1997-2000), and we plan to run them in parallel as separate competing agents to test and compare their applicability as well as the Jaspis architecture in the given task.	MSA	Modern Standard Arabic	0
However, the flexible sys- tem architecture also allows us to experiment with different MSA, such as TextMorfo (Kielikone Oy 1999) and Conexor FDG (Conexor Oy 1997-2000), and we plan to run them in parallel as separate competing agents to test and compare their applicability as well as the Jaspis architecture in the given task.	MSA	morphosyntactic analyzers	1
Un- like language analysis, for which different ex- isting Finnish MSA can be used, there are no readily available general- purpose Finnish language generators.	MSA	Modern Standard Arabic	0
Un- like language analysis, for which different ex- isting Finnish MSA can be used, there are no readily available general- purpose Finnish language generators.	MSA	morphosyntactic analyzers	1
QA systems have been evaluated at TREC QA-Track1 in U.S. and QAC (Q&A Challenge)2 in Japan.	Q&A	Question & Answering	1
QA systems have been evaluated at TREC QA-Track1 in U.S. and QAC (Q&A Challenge)2 in Japan.	Q&A	question answerin	0
QA systems have been evaluated at TREC QA-Track1 in U.S. and QAC (Q&A Challenge)2 in Japan.	Q&A	question answer	0
WordNet has been widely criticised for being a sense repository that often offers too fine?grained sense distinctions for higher level applications like Machine Translation or Q&A.	Q&A	Question & Answering	1
WordNet has been widely criticised for being a sense repository that often offers too fine?grained sense distinctions for higher level applications like Machine Translation or Q&A.	Q&A	question answerin	0
WordNet has been widely criticised for being a sense repository that often offers too fine?grained sense distinctions for higher level applications like Machine Translation or Q&A.	Q&A	question answer	0
Therefore, Short Message Service (SMS) is a  better way for giving knowledge service, espe- cially automatic interchange of short text mes- sages, by providing the information from an  automatic Q&A System.	Q&A	Question & Answering	1
Therefore, Short Message Service (SMS) is a  better way for giving knowledge service, espe- cially automatic interchange of short text mes- sages, by providing the information from an  automatic Q&A System.	Q&A	question answerin	0
Therefore, Short Message Service (SMS) is a  better way for giving knowledge service, espe- cially automatic interchange of short text mes- sages, by providing the information from an  automatic Q&A System.	Q&A	question answer	0
Issues, Tasks and Program Structures to Roadmap Research in Q&A.	Q&A	Question & Answering	1
Issues, Tasks and Program Structures to Roadmap Research in Q&A.	Q&A	question answerin	0
Issues, Tasks and Program Structures to Roadmap Research in Q&A.	Q&A	question answer	0
But, WordNet (WN) has been widely criticized for being a sense repository that often provides too fine?grained sense distinctions for higher level applications like Machine Translation or Q&A.	Q&A	Question & Answering	1
But, WordNet (WN) has been widely criticized for being a sense repository that often provides too fine?grained sense distinctions for higher level applications like Machine Translation or Q&A.	Q&A	question answerin	0
But, WordNet (WN) has been widely criticized for being a sense repository that often provides too fine?grained sense distinctions for higher level applications like Machine Translation or Q&A.	Q&A	question answer	0
The tasks set in these conferences have molded a specific kind of Q&Ag that is easy to evaluate and that focuses on the use of fast and shallow methods that are generally independent of the application domain.	Q&A	Question & Answering	0
The tasks set in these conferences have molded a specific kind of Q&Ag that is easy to evaluate and that focuses on the use of fast and shallow methods that are generally independent of the application domain.	Q&A	question answerin	1
The tasks set in these conferences have molded a specific kind of Q&Ag that is easy to evaluate and that focuses on the use of fast and shallow methods that are generally independent of the application domain.	Q&A	question answer	0
A main characteristic of Q&Ag in restricted domains is the integration of domain-specific information that is either developed for Q&Ag or that has been developed for other purposes.	Q&A	Question & Answering	0
A main characteristic of Q&Ag in restricted domains is the integration of domain-specific information that is either developed for Q&Ag or that has been developed for other purposes.	Q&A	question answerin	1
A main characteristic of Q&Ag in restricted domains is the integration of domain-specific information that is either developed for Q&Ag or that has been developed for other purposes.	Q&A	question answer	0
A main characteristic of Q&Ag in restricted domains is the integration of domain-specific information that is either developed for Q&Ag or that has been developed for other pur	Q&A	Question & Answering	0
A main characteristic of Q&Ag in restricted domains is the integration of domain-specific information that is either developed for Q&Ag or that has been developed for other pur	Q&A	question answerin	1
A main characteristic of Q&Ag in restricted domains is the integration of domain-specific information that is either developed for Q&Ag or that has been developed for other pur	Q&A	question answer	0
article is on the use of restricted domains for automated Q&Ag.	Q&A	Question & Answering	0
article is on the use of restricted domains for automated Q&Ag.	Q&A	question answerin	1
article is on the use of restricted domains for automated Q&Ag.	Q&A	question answer	0
From this perspective, Q&Ag focuses on finding text excerpts that contain the answer within large collections of documents.	Q&A	Question & Answering	0
From this perspective, Q&Ag focuses on finding text excerpts that contain the answer within large collections of documents.	Q&A	question answerin	1
From this perspective, Q&Ag focuses on finding text excerpts that contain the answer within large collections of documents.	Q&A	question answer	0
The focus of this article is on the use of restricted domains for automated Q&Ag.	Q&A	Question & Answering	0
The focus of this article is on the use of restricted domains for automated Q&Ag.	Q&A	question answerin	1
The focus of this article is on the use of restricted domains for automated Q&Ag.	Q&A	question answer	0
University of Alicante, Spain Automated Q&Ag has been a topic of research and development since the earliest AI applications.	Q&A	Question & Answering	0
University of Alicante, Spain Automated Q&Ag has been a topic of research and development since the earliest AI applications.	Q&A	question answerin	1
University of Alicante, Spain Automated Q&Ag has been a topic of research and development since the earliest AI applications.	Q&A	question answer	0
The setting is also appropriate for cases that may require making  global decisions that involve multiple components, possibly pre-designed or pre- learned, as in summarization, paraphrasing, textual entailment and Q&Ag.	Q&A	Question & Answering	0
The setting is also appropriate for cases that may require making  global decisions that involve multiple components, possibly pre-designed or pre- learned, as in summarization, paraphrasing, textual entailment and Q&Ag.	Q&A	question answerin	1
The setting is also appropriate for cases that may require making  global decisions that involve multiple components, possibly pre-designed or pre- learned, as in summarization, paraphrasing, textual entailment and Q&Ag.	Q&A	question answer	0
The tasks set in these conferences have molded a specific kind of Q&Aing that is easy to evaluate and that focuses on the use of fast and shallow methods that are generally independent of the application domain.	Q&A	Question & Answering	0
The tasks set in these conferences have molded a specific kind of Q&Aing that is easy to evaluate and that focuses on the use of fast and shallow methods that are generally independent of the application domain.	Q&A	question answerin	0
The tasks set in these conferences have molded a specific kind of Q&Aing that is easy to evaluate and that focuses on the use of fast and shallow methods that are generally independent of the application domain.	Q&A	question answer	1
A main characteristic of Q&Aing in restricted domains is the integration of domain-specific information that is either developed for Q&Aing or that has been developed for other purposes.	Q&A	Question & Answering	0
A main characteristic of Q&Aing in restricted domains is the integration of domain-specific information that is either developed for Q&Aing or that has been developed for other purposes.	Q&A	question answerin	0
A main characteristic of Q&Aing in restricted domains is the integration of domain-specific information that is either developed for Q&Aing or that has been developed for other purposes.	Q&A	question answer	1
A main characteristic of Q&Aing in restricted domains is the integration of domain-specific information that is either developed for Q&Aing or that has been developed for other pur	Q&A	Question & Answering	0
A main characteristic of Q&Aing in restricted domains is the integration of domain-specific information that is either developed for Q&Aing or that has been developed for other pur	Q&A	question answerin	0
A main characteristic of Q&Aing in restricted domains is the integration of domain-specific information that is either developed for Q&Aing or that has been developed for other pur	Q&A	question answer	1
article is on the use of restricted domains for automated Q&Aing.	Q&A	Question & Answering	0
article is on the use of restricted domains for automated Q&Aing.	Q&A	question answerin	0
article is on the use of restricted domains for automated Q&Aing.	Q&A	question answer	1
From this perspective, Q&Aing focuses on finding text excerpts that contain the answer within large collections of documents.	Q&A	Question & Answering	0
From this perspective, Q&Aing focuses on finding text excerpts that contain the answer within large collections of documents.	Q&A	question answerin	0
From this perspective, Q&Aing focuses on finding text excerpts that contain the answer within large collections of documents.	Q&A	question answer	1
The focus of this article is on the use of restricted domains for automated Q&Aing.	Q&A	Question & Answering	0
The focus of this article is on the use of restricted domains for automated Q&Aing.	Q&A	question answerin	0
The focus of this article is on the use of restricted domains for automated Q&Aing.	Q&A	question answer	1
University of Alicante, Spain Automated Q&Aing has been a topic of research and development since the earliest AI applications.	Q&A	Question & Answering	0
University of Alicante, Spain Automated Q&Aing has been a topic of research and development since the earliest AI applications.	Q&A	question answerin	0
University of Alicante, Spain Automated Q&Aing has been a topic of research and development since the earliest AI applications.	Q&A	question answer	1
The setting is also appropriate for cases that may require making  global decisions that involve multiple components, possibly pre-designed or pre- learned, as in summarization, paraphrasing, textual entailment and Q&Aing.	Q&A	Question & Answering	0
The setting is also appropriate for cases that may require making  global decisions that involve multiple components, possibly pre-designed or pre- learned, as in summarization, paraphrasing, textual entailment and Q&Aing.	Q&A	question answerin	0
The setting is also appropriate for cases that may require making  global decisions that involve multiple components, possibly pre-designed or pre- learned, as in summarization, paraphrasing, textual entailment and Q&Aing.	Q&A	question answer	1
"sCBt\] SUr Ile~ert, 2)   \] G_N_.~( ~unphot rca, i t er  ~ n ~ I ,mval;ur ore!mr1 t ~ 2 )  G_N_V(mvvsucwr.entm,I 1~ ub~,;~ i n .2 )   G. N. V{ meac, ur e~r.er, t ,  n ~. ~., ShO..,, ~ )  .~: G_N.H(&Cr.~e?.fc3ounCier',nl t~measur'onmn~;3)  G.H.H(pntltnna, n|  1 ~aea~'~.ent ,  3)  :~' G. N,.	CB	Ui	1
"sCBt\] SUr Ile~ert, 2)   \] G_N_.~( ~unphot rca, i t er  ~ n ~ I ,mval;ur ore!mr1 t ~ 2 )  G_N_V(mvvsucwr.entm,I 1~ ub~,;~ i n .2 )   G. N. V{ meac, ur e~r.er, t ,  n ~. ~., ShO..,, ~ )  .~: G_N.H(&Cr.~e?.fc3ounCier',nl t~measur'onmn~;3)  G.H.H(pntltnna, n|  1 ~aea~'~.ent ,  3)  :~' G. N,.	CB	Carpetbagge	0
"sCBt\] SUr Ile~ert, 2)   \] G_N_.~( ~unphot rca, i t er  ~ n ~ I ,mval;ur ore!mr1 t ~ 2 )  G_N_V(mvvsucwr.entm,I 1~ ub~,;~ i n .2 )   G. N. V{ meac, ur e~r.er, t ,  n ~. ~., ShO..,, ~ )  .~: G_N.H(&Cr.~e?.fc3ounCier',nl t~measur'onmn~;3)  G.H.H(pntltnna, n|  1 ~aea~'~.ent ,  3)  :~' G. N,.	CB	Carpetbagger	0
Juyup Sung, Jae-Gil Lee, and CBchin Lee.	CB	Ui	1
Juyup Sung, Jae-Gil Lee, and CBchin Lee.	CB	Carpetbagge	0
Juyup Sung, Jae-Gil Lee, and CBchin Lee.	CB	Carpetbagger	0
The HITs for the following languages were posted for a week and were never completed: Tigrinya, CBghur, Tibetan, Kyrgyz, and Kazakh.	CB	Ui	1
The HITs for the following languages were posted for a week and were never completed: Tigrinya, CBghur, Tibetan, Kyrgyz, and Kazakh.	CB	Carpetbagge	0
The HITs for the following languages were posted for a week and were never completed: Tigrinya, CBghur, Tibetan, Kyrgyz, and Kazakh.	CB	Carpetbagger	0
Instead of computing the proba-  bCBty for each one of them we group them into  "buckets", so that rrt a iS the bucket for the num-  ber of times that a is mentioned.	CB	Ui	1
Instead of computing the proba-  bCBty for each one of them we group them into  "buckets", so that rrt a iS the bucket for the num-  ber of times that a is mentioned.	CB	Carpetbagge	0
Instead of computing the proba-  bCBty for each one of them we group them into  "buckets", so that rrt a iS the bucket for the num-  ber of times that a is mentioned.	CB	Carpetbagger	0
Each word can thus be represented as a 108 Tigrinya 36 Punjabi 401 Kyrgyz 492 Somali 585 Nepali 1293 Tibetan 1358 CBghur 1814 Maltese 1896 Turkmen 3137 Kazakh 3470 Mongolian 4009 Tatar 4180 Kurdish 5059 Uzbek 5875 Kapampangan 6827 Urdu 7674 Irish 9859 Azeri 12568 Tamil 13470 Albanian 13714 Afrikaans 14315 Hindi 14824 Bangla 16026 Tagalog 17757 Latvian 22737 Bosnian 23144 Welsh 25292 Latin 31195 Basque 38594 Thai 40182 Farsi 58651 Bulgarian 68446 Serbian 71018 Indonesian 73962 Slovak 76421 Korean 84385 Turkish	CB	Ui	1
Each word can thus be represented as a 108 Tigrinya 36 Punjabi 401 Kyrgyz 492 Somali 585 Nepali 1293 Tibetan 1358 CBghur 1814 Maltese 1896 Turkmen 3137 Kazakh 3470 Mongolian 4009 Tatar 4180 Kurdish 5059 Uzbek 5875 Kapampangan 6827 Urdu 7674 Irish 9859 Azeri 12568 Tamil 13470 Albanian 13714 Afrikaans 14315 Hindi 14824 Bangla 16026 Tagalog 17757 Latvian 22737 Bosnian 23144 Welsh 25292 Latin 31195 Basque 38594 Thai 40182 Farsi 58651 Bulgarian 68446 Serbian 71018 Indonesian 73962 Slovak 76421 Korean 84385 Turkish	CB	Carpetbagge	0
Each word can thus be represented as a 108 Tigrinya 36 Punjabi 401 Kyrgyz 492 Somali 585 Nepali 1293 Tibetan 1358 CBghur 1814 Maltese 1896 Turkmen 3137 Kazakh 3470 Mongolian 4009 Tatar 4180 Kurdish 5059 Uzbek 5875 Kapampangan 6827 Urdu 7674 Irish 9859 Azeri 12568 Tamil 13470 Albanian 13714 Afrikaans 14315 Hindi 14824 Bangla 16026 Tagalog 17757 Latvian 22737 Bosnian 23144 Welsh 25292 Latin 31195 Basque 38594 Thai 40182 Farsi 58651 Bulgarian 68446 Serbian 71018 Indonesian 73962 Slovak 76421 Korean 84385 Turkish	CB	Carpetbagger	0
CBth  progress in automatic classification, another possibility  is being explored, that of creating new information rather  than merely gathering, maintaining or distributing the  results of human intellectual activities.	CB	Ui	1
CBth  progress in automatic classification, another possibility  is being explored, that of creating new information rather  than merely gathering, maintaining or distributing the  results of human intellectual activities.	CB	Carpetbagge	0
CBth  progress in automatic classification, another possibility  is being explored, that of creating new information rather  than merely gathering, maintaining or distributing the  results of human intellectual activities.	CB	Carpetbagger	0
We selected three blog sites from this dataset: the Right Wing News (right-ideology) ; the CBr, and Daily Kos as representatives 1144 palest inian is raeli peace year  polit ical  proces s   state  end  rig ht   g overnment  need  conflict way s ecurit y palest inian is raeli Peace polit ical  occupation  proces s end  s ecurit y   conflict   way  g overnment   people t ime year force  neg ot iation bush US pres ident  american sharon administ ration prime  s ett lem ent  pres	CB	Ui	0
We selected three blog sites from this dataset: the Right Wing News (right-ideology) ; the CBr, and Daily Kos as representatives 1144 palest inian is raeli peace year  polit ical  proces s   state  end  rig ht   g overnment  need  conflict way s ecurit y palest inian is raeli Peace polit ical  occupation  proces s end  s ecurit y   conflict   way  g overnment   people t ime year force  neg ot iation bush US pres ident  american sharon administ ration prime  s ett lem ent  pres	CB	Carpetbagge	1
We selected three blog sites from this dataset: the Right Wing News (right-ideology) ; the CBr, and Daily Kos as representatives 1144 palest inian is raeli peace year  polit ical  proces s   state  end  rig ht   g overnment  need  conflict way s ecurit y palest inian is raeli Peace polit ical  occupation  proces s end  s ecurit y   conflict   way  g overnment   people t ime year force  neg ot iation bush US pres ident  american sharon administ ration prime  s ett lem ent  pres	CB	Carpetbagger	0
We selected three blog sites from this dataset: the Right Wing News (right-ideology) ; the CB, and Daily Kos as representatives 1144 palest inian is raeli peace year  polit ical  proces s   state  end  rig ht   g overnment  need  conflict way s ecurit y palest inian is raeli Peace polit ical  occupation  proces s end  s ecurit y   conflict   way  g overnment   people t ime year force  neg ot iation bush US pres ident  american sharon administ ration prime  s ett lem ent  pres	CB	Ui	0
We selected three blog sites from this dataset: the Right Wing News (right-ideology) ; the CB, and Daily Kos as representatives 1144 palest inian is raeli peace year  polit ical  proces s   state  end  rig ht   g overnment  need  conflict way s ecurit y palest inian is raeli Peace polit ical  occupation  proces s end  s ecurit y   conflict   way  g overnment   people t ime year force  neg ot iation bush US pres ident  american sharon administ ration prime  s ett lem ent  pres	CB	Carpetbagge	0
We selected three blog sites from this dataset: the Right Wing News (right-ideology) ; the CB, and Daily Kos as representatives 1144 palest inian is raeli peace year  polit ical  proces s   state  end  rig ht   g overnment  need  conflict way s ecurit y palest inian is raeli Peace polit ical  occupation  proces s end  s ecurit y   conflict   way  g overnment   people t ime year force  neg ot iation bush US pres ident  american sharon administ ration prime  s ett lem ent  pres	CB	Carpetbagger	1
syntactically inte- grated in the sentence, i.e. located in the MF of a sentence.	MF	middle field	1
syntactically inte- grated in the sentence, i.e. located in the MF of a sentence.	MF	Mean Field	0
last cl .... \] == \[first: \[verbal_modifi~r \] l \[v~rb  L rest \[ verb_last_claus~ 11\]j  V  first: \[finite verb \] 1 est : \[nil \]  \[forefield \] == \[verbal modifier \]  \[MF and rest fields \] ==  - \[first: \[verhe~ modifier \] _7  L rest : \[middJe field and rest fields J v  \[nil \]  v  f irst:\[nonfini ..... b \] 1 est : \[after field \]  \[after field \] == \[niJ \]  V  r first : \[ verbal modifier 1  est : \[after field \]  \[verbal modifier \] == \[complement \]  V \[adjunct \] "  The f irst def in i t ion  in (i0) descr ibes   the fact that a clau	MF	middle field	1
last cl .... \] == \[first: \[verbal_modifi~r \] l \[v~rb  L rest \[ verb_last_claus~ 11\]j  V  first: \[finite verb \] 1 est : \[nil \]  \[forefield \] == \[verbal modifier \]  \[MF and rest fields \] ==  - \[first: \[verhe~ modifier \] _7  L rest : \[middJe field and rest fields J v  \[nil \]  v  f irst:\[nonfini ..... b \] 1 est : \[after field \]  \[after field \] == \[niJ \]  V  r first : \[ verbal modifier 1  est : \[after field \]  \[verbal modifier \] == \[complement \]  V \[adjunct \] "  The f irst def in i t ion  in (i0) descr ibes   the fact that a clau	MF	Mean Field	0
of the MF (middle), and the el-  ement of the initial field cannot be a relative  phrase (o~nore l  in \[4\]).	MF	middle field	1
of the MF (middle), and the el-  ement of the initial field cannot be a relative  phrase (o~nore l  in \[4\]).	MF	Mean Field	0
of the  MF, with no restrictions for the initial  field (relative clauses and non-relative verb-final  clauses are subordinated to the noun and con-  junction, resp.)	MF	middle field	1
of the  MF, with no restrictions for the initial  field (relative clauses and non-relative verb-final  clauses are subordinated to the noun and con-  junction, resp.)	MF	Mean Field	0
Orderings in the MF.	MF	middle field	1
Orderings in the MF.	MF	Mean Field	0
5 The German Clause  Traditionally, the German main clause is de-  scribed using three topological fields; the ini-  tial and MFs are separated by the fi-  nite (auxiliary) verb, and the middle and the  5The modality O~ can be viewed as an abbreviation  of o~ O~,  composed of a mapping from a word to its ith  order domain and from that domain to all its elements.	MF	middle field	1
5 The German Clause  Traditionally, the German main clause is de-  scribed using three topological fields; the ini-  tial and MFs are separated by the fi-  nite (auxiliary) verb, and the middle and the  5The modality O~ can be viewed as an abbreviation  of o~ O~,  composed of a mapping from a word to its ith  order domain and from that domain to all its elements.	MF	Mean Field	0
A Generalized  MF Algorithm for Variational Inference in  Exponential Families.	MF	middle field	0
A Generalized  MF Algorithm for Variational Inference in  Exponential Families.	MF	Mean Field	1
2.1 MF Approximation Methods The simplest example of a variation method is the mean field method, originally introduced in statis- tical mechanics and later applied to unsupervised neural networks in (Hinton et al, 1995).	MF	middle field	0
2.1 MF Approximation Methods The simplest example of a variation method is the mean field method, originally introduced in statis- tical mechanics and later applied to unsupervised neural networks in (Hinton et al, 1995).	MF	Mean Field	1
3 Spin Model and MF Approximation We give a brief introduction to the spin model and the mean field approximation, which are well- studied subjects both in the statistical mechanics and the machine learning communities (Geman and Geman, 1984; Inoue and Carlucci, 2001; Mackay, 2003).	MF	middle field	0
3 Spin Model and MF Approximation We give a brief introduction to the spin model and the mean field approximation, which are well- studied subjects both in the statistical mechanics and the machine learning communities (Geman and Geman, 1984; Inoue and Carlucci, 2001; Mackay, 2003).	MF	Mean Field	1
Structured MF ?	MF	middle field	0
Structured MF ?	MF	Mean Field	1
3.3 A MF Approximation This section proposes a more accurate way to ap- proximate ISBNs with mean field methods, which we will call the mean field approximation.	MF	middle field	0
3.3 A MF Approximation This section proposes a more accurate way to ap- proximate ISBNs with mean field methods, which we will call the mean field approximation.	MF	Mean Field	1
MF ?	MF	middle field	0
MF ?	MF	Mean Field	1
Marginalization of hidden structure is also funda- mental to other work, and featured most prominently in generative Bayesian LVMs (Teh et al2006).	LVM	latent variable model	1
Marginalization of hidden structure is also funda- mental to other work, and featured most prominently in generative Bayesian LVMs (Teh et al2006).	LVM	latent variable method	0
A LVM for geographic lexical variation.	LVM	latent variable model	1
A LVM for geographic lexical variation.	LVM	latent variable method	0
Posterior regularization for structured LVMs.	LVM	latent variable model	1
Posterior regularization for structured LVMs.	LVM	latent variable method	0
In recent years, great research has shown the strength of LVMs for natural lan- guage processing (Blunsom et al, 2008).	LVM	latent variable model	1
In recent years, great research has shown the strength of LVMs for natural lan- guage processing (Blunsom et al, 2008).	LVM	latent variable method	0
A discrimi- native LVM for statistical machine trans- lation.	LVM	latent variable model	1
A discrimi- native LVM for statistical machine trans- lation.	LVM	latent variable method	0
A LVM for geo- graphic lexical variation.	LVM	latent variable model	1
A LVM for geo- graphic lexical variation.	LVM	latent variable method	0
Finally, our focus on these methods is justified by their clear advantages over other classes of models: unlike token-based or LVMs, they are much simpler and require no parameter tuning.	LVM	latent variable model	0
Finally, our focus on these methods is justified by their clear advantages over other classes of models: unlike token-based or LVMs, they are much simpler and require no parameter tuning.	LVM	latent variable method	1
However, we have to note that this accuracy of the proposed method was computed using the unlabeled data classified by the LVM.	LVM	latent variable model	0
However, we have to note that this accuracy of the proposed method was computed using the unlabeled data classified by the LVM.	LVM	latent variable method	1
Therefore, we computed the accuracy for 6586 instances using the LVM and obtained 80.76 %.	LVM	latent variable model	0
Therefore, we computed the accuracy for 6586 instances using the LVM and obtained 80.76 %.	LVM	latent variable method	1
This comparison shows that our method is better than or at least comparable to the LVM.	LVM	latent variable model	0
This comparison shows that our method is better than or at least comparable to the LVM.	LVM	latent variable method	1
