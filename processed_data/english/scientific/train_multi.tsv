Dialogue fillers and acceptance words af- fect the accuracy of POS tagging.	POS	Part-Of-Speech$part-of-speech$Part of speech$positive instances$Possessive$Position$Part-of-Speech$Part Of Speec$part of speec$partof-speech$Parts of Speech$postag$	0
Another possible explanation is that as a set of spoken dialogue data, Loqui is inherently more difficult to process than written form, since some common tasks such POS tagging have lower accuracy for spoken data.	POS	Part-Of-Speech$part-of-speech$Part of speech$positive instances$Possessive$Position$Part-of-Speech$Part Of Speec$part of speec$partof-speech$Parts of Speech$postag$	0
POS tags: we chose the GENIA dependency parser for parsing the corpus for two reasons.	POS	Part-Of-Speech$part-of-speech$Part of speech$positive instances$Possessive$Position$Part-of-Speech$Part Of Speec$part of speec$partof-speech$Parts of Speech$postag$	0
We built up three systems based on Condi- tional Random Field for Chinese Word  Segmentation, Named Entity Recognition  and POS Tagging respectively.	POS	Part-Of-Speech$part-of-speech$Part of speech$positive instances$Possessive$Position$Part-of-Speech$Part Of Speec$part of speec$partof-speech$Parts of Speech$postag$	0
Our final system achieve a F-score of  94.18 at CTB, 92.86 at NCC, 94.59 at SXU  on Segmentation, 85.26 at MSRA on  Named Entity Recognition, and 90.65 at  PKU on POS Ta	POS	Part-Of-Speech$part-of-speech$Part of speech$positive instances$Possessive$Position$Part-of-Speech$Part Of Speec$part of speec$partof-speech$Parts of Speech$postag$	0
The corpus has been part of speech tagged and lemmatized with Stanford POS Tagger (Toutanova et al.,	POS	Part-Of-Speech$part-of-speech$Part of speech$positive instances$Possessive$Position$Part-of-Speech$Part Of Speec$part of speec$partof-speech$Parts of Speech$postag$	0
A Maximum Entropy Model  for POS Tagging.	POS	Part-Of-Speech$part-of-speech$Part of speech$positive instances$Possessive$Position$Part-of-Speech$Part Of Speec$part of speec$partof-speech$Parts of Speech$postag$	0
In fact, the only POS tags  necessary are those indicating nouns and pro-  nouns .	POS	Part-Of-Speech$part-of-speech$Part of speech$positive instances$Possessive$Position$Part-of-Speech$Part Of Speec$part of speec$partof-speech$Parts of Speech$postag$	1
The training attributes  consist of lexical word attributes (surface word, stem  form, POS, semantic ode, morphological  attributes) applied to the anaphor, antecedent can-  didate, and clause predicate.	POS	Part-Of-Speech$part-of-speech$Part of speech$positive instances$Possessive$Position$Part-of-Speech$Part Of Speec$part of speec$partof-speech$Parts of Speech$postag$	1
Antecedent candidates are  identified according to noun phrase POS  tags.	POS	Part-Of-Speech$part-of-speech$Part of speech$positive instances$Possessive$Position$Part-of-Speech$Part Of Speec$part of speec$partof-speech$Parts of Speech$postag$	1
Our preliminary experiments did not show noticeable improvements from bi- gram or character-based features, but it is pos- sible that higher-level features such as morpho- logical, POS or syntactic features could yield further performance gains.	POS	Part-Of-Speech$part-of-speech$Part of speech$positive instances$Possessive$Position$Part-of-Speech$Part Of Speec$part of speec$partof-speech$Parts of Speech$postag$	1
Including the disjunctive POS types, just under half (49%) of the types in the grammar are provided by the Matrix.	POS	Part-Of-Speech$part-of-speech$Part of speech$positive instances$Possessive$Position$Part-of-Speech$Part Of Speec$part of speec$partof-speech$Parts of Speech$postag$	1
This apparent parser overkill is a control  to ensure that the POS tags assigned  to words are the same when we use the previ-  ous noun heuristic and the Hobbs algorithm, to  which we wish to compare the previous noun  method.	POS	Part-Of-Speech$part-of-speech$Part of speech$positive instances$Possessive$Position$Part-of-Speech$Part Of Speec$part of speec$partof-speech$Parts of Speech$postag$	1
POS tags are not used in the n-gram system except to identify insertion points for missing prepositions and determiners.	POS	Part-Of-Speech$part-of-speech$Part of speech$positive instances$Possessive$Position$Part-of-Speech$Part Of Speec$part of speec$partof-speech$Parts of Speech$postag$	2
log ntypes log ntokens ) POS distribution The relative frequen- cies of different parts of speech also correlate with essay grade, although more weakly so than the re- lated measure of average word length.	POS	Part-Of-Speech$part-of-speech$Part of speech$positive instances$Possessive$Position$Part-of-Speech$Part Of Speec$part of speec$partof-speech$Parts of Speech$postag$	2
POS ambiguity,  3.	POS	Part-Of-Speech$part-of-speech$Part of speech$positive instances$Possessive$Position$Part-of-Speech$Part Of Speec$part of speec$partof-speech$Parts of Speech$postag$	2
POS tagging and chunking with hmm and crf.	POS	Part-Of-Speech$part-of-speech$Part of speech$positive instances$Possessive$Position$Part-of-Speech$Part Of Speec$part of speec$partof-speech$Parts of Speech$postag$	2
POS, however, is not balanced across our seed sets.	POS	Part-Of-Speech$part-of-speech$Part of speech$positive instances$Possessive$Position$Part-of-Speech$Part Of Speec$part of speec$partof-speech$Parts of Speech$postag$	2
POS, or "class."	POS	Part-Of-Speech$part-of-speech$Part of speech$positive instances$Possessive$Position$Part-of-Speech$Part Of Speec$part of speec$partof-speech$Parts of Speech$postag$	2
Recall = #POS classified correctly#POS Precision = #POS classified correctly#instances classified as positive The above metrics evaluate the capability of the learned classifier in identifying posi- tive instances3.	POS	Part-Of-Speech$part-of-speech$Part of speech$positive instances$Possessive$Position$Part-of-Speech$Part Of Speec$part of speec$partof-speech$Parts of Speech$postag$	3
These heuristics filter out redundant constituents  and raise the ratio of POS in the  dataset.	POS	Part-Of-Speech$part-of-speech$Part of speech$positive instances$Possessive$Position$Part-of-Speech$Part Of Speec$part of speec$partof-speech$Parts of Speech$postag$	3
DDI interaction 14.47% Interaction type effect 6.07% Interaction type advise 2.97% Interaction type mechanism 4.75% Interaction type int 0.68% Table 1: POS for the different class types The data is not balanced, as shown in table 1.	POS	Part-Of-Speech$part-of-speech$Part of speech$positive instances$Possessive$Position$Part-of-Speech$Part Of Speec$part of speec$partof-speech$Parts of Speech$postag$	3
Collection of POS As indicated be- fore, every sentence from the original documents matching a summary sentence that contains at least one SCU is considered a positive example.	POS	Part-Of-Speech$part-of-speech$Part of speech$positive instances$Possessive$Position$Part-of-Speech$Part Of Speec$part of speec$partof-speech$Parts of Speech$postag$	3
For instance, the number of constituents labeled to  arguments (POS) is much less than  the number of the rest (negative instances).	POS	Part-Of-Speech$part-of-speech$Part of speech$positive instances$Possessive$Position$Part-of-Speech$Part Of Speec$part of speec$partof-speech$Parts of Speech$postag$	3
Table1 shows the percentage of POS in the dataset.	POS	Part-Of-Speech$part-of-speech$Part of speech$positive instances$Possessive$Position$Part-of-Speech$Part Of Speec$part of speec$partof-speech$Parts of Speech$postag$	3
The definite article marker is placed on  the second word of the construction:  (2)    beit sefer / house-[const] book   School  (3)    beit ha-sefer / house-[const] the-book   The school    The construct form can also be embedded:  (4) 	      690 misrad ro$ ha-mem$ala   Office-[const poss] head-[const] the-government  The prime-minister?s office    POS: the smixut form can be used to indi- cate possession.	POS	Part-Of-Speech$part-of-speech$Part of speech$positive instances$Possessive$Position$Part-of-Speech$Part Of Speec$part of speec$partof-speech$Parts of Speech$postag$	4
The content of Figure 1 can be reconstructed  straightforwardly asa category structure subject o a set  of L c constraints (for a closely related analysis of this  10  {~ Animate  Question _ _  -- Subjective  Case Objective  Reflex ve  POS POS-Determ ner  _ I First  Personal ~_ .P__~ Second _ _ I Feminine  Ingu la r - -  J Neuter  f \[ | Plural  Demonstrative - - l~  Near  / Far  Figure 1: Systemic Network for English Pronouns  example, developed independently, see Mellish (1986).	POS	Part-Of-Speech$part-of-speech$Part of speech$positive instances$Possessive$Position$Part-of-Speech$Part Of Speec$part of speec$partof-speech$Parts of Speech$postag$	4
Premodifiers, POS, Preposition, For- mulaic and Verbal.	POS	Part-Of-Speech$part-of-speech$Part of speech$positive instances$Possessive$Position$Part-of-Speech$Part Of Speec$part of speec$partof-speech$Parts of Speech$postag$	4
Premodifier relations specify the proper adjective or proper noun premodifier and the following noun it modifies, e.g.: [the [Seattle] zoo] POS indicates that the first mention is in a possessive case, e.g.: [[California] ?	POS	Part-Of-Speech$part-of-speech$Part of speech$positive instances$Possessive$Position$Part-of-Speech$Part Of Speec$part of speec$partof-speech$Parts of Speech$postag$	4
% related PPs are  allowed:    5% of the sales    POS  - '$el' /  'of' - is not consid- ered a PP  Table 2.	POS	Part-Of-Speech$part-of-speech$Part of speech$positive instances$Possessive$Position$Part-of-Speech$Part Of Speec$part of speec$partof-speech$Parts of Speech$postag$	4
739 Supt Richard-Nixon Supt Abraham-Lincoln 50 Date of birth: 419 Government POSs Held: - Jan 9, 1913 - United States Representative Mar 4,1847-Mar 3,1849 108 Tracks Recorded: 558 Military Commands: - 23-73 Broadcast: End of the Vietnam War - American Civil War - United States of America 120 Works Written About This Topic: 810 Quotations: - Nearly all men can stand adversity, but if - Watergate you want to test a man?s character, give him power.	POS	Part-Of-Speech$part-of-speech$Part of speech$positive instances$Possessive$Position$Part-of-Speech$Part Of Speec$part of speec$partof-speech$Parts of Speech$postag$	5
POS of first occurrence : Important concepts are expected to be mentioned before less relevant ones.	POS	Part-Of-Speech$part-of-speech$Part of speech$positive instances$Possessive$Position$Part-of-Speech$Part Of Speec$part of speec$partof-speech$Parts of Speech$postag$	5
2 3 POS sets and binarizations Throughout this section we assume an LCFRS production p : A?	POS	Part-Of-Speech$part-of-speech$Part of speech$positive instances$Possessive$Position$Part-of-Speech$Part Of Speec$part of speec$partof-speech$Parts of Speech$postag$	5
POS of first appearance: the position  where a keyword first appears in a doc- ument, normalized by number of words  in the document.	POS	Part-Of-Speech$part-of-speech$Part of speech$positive instances$Possessive$Position$Part-of-Speech$Part Of Speec$part of speec$partof-speech$Parts of Speech$postag$	5
POS and scope of epis- temic phrases in planned and unplanned american english.	POS	Part-Of-Speech$part-of-speech$Part of speech$positive instances$Possessive$Position$Part-of-Speech$Part Of Speec$part of speec$partof-speech$Parts of Speech$postag$	5
- Martin Feldstein Reagan?s first National Security advisor was quoted as declaring... - Chief Economic Advisor Government POSs Held: 967 1981 Jan 20, Ronald Reagan was sworn in as president as 52 American hostages - President of the United States boarded a plane in Tehran and headed toward freedom.	POS	Part-Of-Speech$part-of-speech$Part of speech$positive instances$Possessive$Position$Part-of-Speech$Part Of Speec$part of speec$partof-speech$Parts of Speech$postag$	5
Helmut Schmid, 1994, Probabilistic POS Tag- ging Using Decision Trees, Intl.	POS	Part-Of-Speech$part-of-speech$Part of speech$positive instances$Possessive$Position$Part-of-Speech$Part Of Speec$part of speec$partof-speech$Parts of Speech$postag$	6
The 529 PoS tagging Lemmatisation PoS after Lemmas error rate error rate update after update French 4.50 % 2.29 % 1.92 % 1.22 % English 5.16 % 3.13 % 2.66 % 3.03 % Table 1: POS tagging and lemmatisation error rate on the test sentences average sentence length of a database entry is 9 words.	POS	Part-Of-Speech$part-of-speech$Part of speech$positive instances$Possessive$Position$Part-of-Speech$Part Of Speec$part of speec$partof-speech$Parts of Speech$postag$	6
Arabic Tok- enization, POS Tagging and Morphological Disambiguation in One Fell Swoop.	POS	Part-Of-Speech$part-of-speech$Part of speech$positive instances$Possessive$Position$Part-of-Speech$Part Of Speec$part of speec$partof-speech$Parts of Speech$postag$	6
l/,obust POS Tagging  Using a lIidden Markov Model.	POS	Part-Of-Speech$part-of-speech$Part of speech$positive instances$Possessive$Position$Part-of-Speech$Part Of Speec$part of speec$partof-speech$Parts of Speech$postag$	6
770  Learning POS Guessing Rules from Lexicon:  Extension to Non-Concatenat ive Operations*  Andre i  M ikheev   HCRC Language Techno logy  Group  Un ivers i ty  of Ed inburgh   2 Buccleuch Place  Edinburgh EH8 9LW, Scotland, UK  : Andrei.	POS	Part-Of-Speech$part-of-speech$Part of speech$positive instances$Possessive$Position$Part-of-Speech$Part Of Speec$part of speec$partof-speech$Parts of Speech$postag$	6
On the tech- nology side, the tutorial mainly covers Chinese  word segmentation and POS tagging.	POS	Part-Of-Speech$part-of-speech$Part of speech$positive instances$Possessive$Position$Part-of-Speech$Part Of Speec$part of speec$partof-speech$Parts of Speech$postag$	6
Transformation-Based Error-  Driven Learning and Natural Language  Processing: A Case Study in POSh  Tagging. (	POS	Part-Of-Speech$part-of-speech$Part of speech$positive instances$Possessive$Position$Part-of-Speech$Part Of Speec$part of speec$partof-speech$Parts of Speech$postag$	7
A Simple Rule-Based POSh Tagger (Workshop On Speech And Natural Language, 1992) As Figure 1 shows, probabilistic models seem to have arrived significantly before classifiers.	POS	Part-Of-Speech$part-of-speech$Part of speech$positive instances$Possessive$Position$Part-of-Speech$Part Of Speec$part of speec$partof-speech$Parts of Speech$postag$	7
7 FirstLevel Preprocessing Second Level Preprocessing Editors and Interfaces Models and Other Applications Higher Level Multilingual NLP Applications Text Language-Encoding Identification Encoding Converters Text Normalization Sentence Splitting Tokenization Morphological Analyzer Encoding Converter Generator Model of Scripts Spell Checker Model of Morphology POSh Tagger Other Specialized Interfaces Text Editor Annotation Interfaces Local Word Grouper or Chunker Figure 1: One view of the basic computational in- frastructure required for Natural Language Process- ing or Computational Linguistics.	POS	Part-Of-Speech$part-of-speech$Part of speech$positive instances$Possessive$Position$Part-of-Speech$Part Of Speec$part of speec$partof-speech$Parts of Speech$postag$	7
Linguistics features like POSh, Chunk, etc are also used.	POS	Part-Of-Speech$part-of-speech$Part of speech$positive instances$Possessive$Position$Part-of-Speech$Part Of Speec$part of speec$partof-speech$Parts of Speech$postag$	7
path through the parse  tree from the parse constituent  to  the predicate being classified  Position A binary feature identifying whether  the phrase is before or after the  predicate  Phrase Type The syntactic category of the phrase  corresponding to the argument  Phrase type of the  sibling to the left The syntactic category of the phrase  is sibling to the argument in the left Head Word and  POSh  The syntactic head of the phrase  First and last word  of the constituent  in focus  First and last word of phrase corre- sponding to the argument  Syntactic Frame The syntactic frame consists of the  NPs that surround the predicate  Table 1.	POS	Part-Of-Speech$part-of-speech$Part of speech$positive instances$Possessive$Position$Part-of-Speech$Part Of Speec$part of speec$partof-speech$Parts of Speech$postag$	7
WordNet (292) POSh (45) Precision (172) Probablistic CFG (43) Recall (167) FrameNet (38) Noun phrase (97) Conditional Random Field (29) Word sense disambiguation (60) Inverse document frequency (28) Support Vector Machine (60) PropBank (27) Hidden Markov Model (54) Context Free Grammar (25) Latent Semantic Analysis (57) Accuracy (20) Table 6: Subset of most frequently defined terms.	POS	Part-Of-Speech$part-of-speech$Part of speech$positive instances$Possessive$Position$Part-of-Speech$Part Of Speec$part of speec$partof-speech$Parts of Speech$postag$	7
MBT: A memory-based POSh tagger generator.	POS	Part-Of-Speech$part-of-speech$Part of speech$positive instances$Possessive$Position$Part-of-Speech$Part Of Speec$part of speec$partof-speech$Parts of Speech$postag$	8
Turning this around, if a word which is ambiguous between a preposition and an- other POSh is not followed by the respective form till the end of the sentence, it is safe to discard the prepositional reading in almost all non-idiomatic, non-coordinated cases.	POS	Part-Of-Speech$part-of-speech$Part of speech$positive instances$Possessive$Position$Part-of-Speech$Part Of Speec$part of speec$partof-speech$Parts of Speech$postag$	8
types, the powerset of 9 POSh types.	POS	Part-Of-Speech$part-of-speech$Part of speech$positive instances$Possessive$Position$Part-of-Speech$Part Of Speec$part of speec$partof-speech$Parts of Speech$postag$	8
Since the nine proposed POSh types have varying crosslinguistic validity (e.g., not all languages have conjunctions), it might be better to provide software support for creating the disjunctive types as the need arises, rather than predefining them.	POS	Part-Of-Speech$part-of-speech$Part of speech$positive instances$Possessive$Position$Part-of-Speech$Part Of Speec$part of speec$partof-speech$Parts of Speech$postag$	8
Setting aside the types for POSh disjunc- tions, 59% of the Matrix-provided types are invoked by the Wambaya-specific portion of the grammar.	POS	Part-Of-Speech$part-of-speech$Part of speech$positive instances$Possessive$Position$Part-of-Speech$Part Of Speec$part of speec$partof-speech$Parts of Speech$postag$	8
Our framework opens up the possibility of efficiently adding many other con- straints that are directly applicable to word alignments, such as preferring alignments that respect dependency tree structure, POSh tags, or syntactic boundaries.	POS	Part-Of-Speech$part-of-speech$Part of speech$positive instances$Possessive$Position$Part-of-Speech$Part Of Speec$part of speec$partof-speech$Parts of Speech$postag$	8
Chinese POS tagging: One-at-a-time or all-at-once?	POS	Part-Of-Speech$part-of-speech$Part of speech$positive instances$Possessive$Position$Part-of-Speech$Part Of Speec$part of speec$partof-speech$Parts of Speech$postag$	9
c?2009 Association for Computational Linguistics     Tagging Urdu Text with POS: A Tagger Comparison    Hassan Sajjad  Universit?t Stuttgart  Stuttgart.	POS	Part-Of-Speech$part-of-speech$Part of speech$positive instances$Possessive$Position$Part-of-Speech$Part Of Speec$part of speec$partof-speech$Parts of Speech$postag$	10
2.2 POS and Morphology  In traditional Sinhala grammar, several classifi- cations have been proposed for parts of speech.	POS	Part-Of-Speech$part-of-speech$Part of speech$positive instances$Possessive$Position$Part-of-Speech$Part Of Speec$part of speec$partof-speech$Parts of Speech$postag$	10
Development of a Web-scale Chinese Word N-gram Corpus  with POS Information.	POS	Part-Of-Speech$part-of-speech$Part of speech$positive instances$Possessive$Position$Part-of-Speech$Part Of Speec$part of speec$partof-speech$Parts of Speech$postag$	10
POS (PoS) extends the WoC model by appending each lexical feature with its part of speech.	POS	Part-Of-Speech$part-of-speech$Part of speech$positive instances$Possessive$Position$Part-of-Speech$Part Of Speec$part of speec$partof-speech$Parts of Speech$postag$	10
4.2 Extending the Approach to Other  POS  The two-step approach to generating verbal  morphology also presents advantages for the  inflectional morphology of nouns and  adjectives.	POS	Part-Of-Speech$part-of-speech$Part of speech$positive instances$Possessive$Position$Part-of-Speech$Part Of Speec$part of speec$partof-speech$Parts of Speech$postag$	10
POS Tagging.	POS	Part-Of-Speech$part-of-speech$Part of speech$positive instances$Possessive$Position$Part-of-Speech$Part Of Speec$part of speec$partof-speech$Parts of Speech$postag$	10
rel:POS=NN} ?{	POS	Part-Of-Speech$part-of-speech$Part of speech$positive instances$Possessive$Position$Part-of-Speech$Part Of Speec$part of speec$partof-speech$Parts of Speech$postag$	11
rel:POS=VBN} ?	POS	Part-Of-Speech$part-of-speech$Part of speech$positive instances$Possessive$Position$Part-of-Speech$Part Of Speec$part of speec$partof-speech$Parts of Speech$postag$	11
rel:POS=VBD} ?	POS	Part-Of-Speech$part-of-speech$Part of speech$positive instances$Possessive$Position$Part-of-Speech$Part Of Speec$part of speec$partof-speech$Parts of Speech$postag$	11
slot:POS=VBN;lex ?	POS	Part-Of-Speech$part-of-speech$Part of speech$positive instances$Possessive$Position$Part-of-Speech$Part Of Speec$part of speec$partof-speech$Parts of Speech$postag$	11
arg1; be {rel} of; arg2) {rel:POS=NN;type=Person} ?	POS	Part-Of-Speech$part-of-speech$Part of speech$positive instances$Possessive$Position$Part-of-Speech$Part Of Speec$part of speec$partof-speech$Parts of Speech$postag$	11
rel:POS=VBN} ?{	POS	Part-Of-Speech$part-of-speech$Part of speech$positive instances$Possessive$Position$Part-of-Speech$Part Of Speec$part of speec$partof-speech$Parts of Speech$postag$	11
The main bottlenecks have been sentence connectors and non-projective dependencies which could not be straightforwardly converted into projec- tive tree structures, requiring a mechanism similar to traces in the PTB.	PTB	Penn English Treebank$PennTreebank$	0
Our primary reference sets are derived from the PTB?s Wall Street Journal por- tion (Marcus et al, 1993): WSJ45 (sentences with fewer than 46 tokens) and Section 23 of WSJ? (	PTB	Penn English Treebank$PennTreebank$	0
3.2 Data Sets and Scoring We trained on the PTB?s Wall Street Journal portion (Marcus et al, 1993).	PTB	Penn English Treebank$PennTreebank$	0
The PTB (Marcus, Marcinkiewicz, and Santorini 1993) is used as the source for the syntactic labels and syntax trees are relabeled to improve translation quality.	PTB	Penn English Treebank$PennTreebank$	0
4 Experiments  The CoNLL 2000 provided the software4 to con- vert PTB II into the IOB tags  form.	PTB	Penn English Treebank$PennTreebank$	0
2.2 PropBank Like FrameNet, PropBank (Kingsbury et al, 2002) is a project aimed at semantic annotation, in this case of the PTB.4 The intent of PropBank is to provide for ?	PTB	Penn English Treebank$PennTreebank$	0
We used the English WSJ PTB corpus in our experiments.	PTB	Penn English Treebank$PennTreebank$	1
The POS tagset used by the POS-based features is that of the WSJ PTB (see Section 7).	PTB	Penn English Treebank$PennTreebank$	1
To illustrate this point, the distance between known|classic/JJ and old/JJ 2Note that, although our tagger produces the very detailed PTB labels, we consider that all nouns (NN, NNS, NNP and NNPS) belong to the same part-of-speech class, and the same for adjectives, verbs and adverbs.	PTB	Penn English Treebank$PennTreebank$	1
McClosky et al(2006a) use sections 2-21 of the WSJ PTB as seed data and between 50K to 2,500K unlabeled NANC corpus sentences as self-training data.	PTB	Penn English Treebank$PennTreebank$	1
The number of supertags is generally much larger than the number of labels used in other sequence labeling tasks; Comparing to 45 POS tags used in PTB, the HPSG grammar used in our experiments includes 2,308 supertags.	PTB	Penn English Treebank$PennTreebank$	1
4 Experiments and Analysis We conducted experiments on WSJ-HPSG tree- bank corpus (Miyao, 2006), which was semi- automatically converted from the WSJ portion of PTB.	PTB	Penn English Treebank$PennTreebank$	1
The SPE.	SPE	Sound Pattern of English$specific$	0
The  SPE.	SPE	Sound Pattern of English$specific$	0
Stedman's Medical Dic t ionary ,  Baltimore, 1961  Although some phonologis ts  would consider  both prevocal ic  t i l a n d  morph-  f i n a l  101 t o  be underlyingly s h o r t  ( l a x ) ,  and would apply a lengthening  ( te<sing)  r u l e  t o  them a f t e r  stress placement (N. Chomsky and M. Hal le ,   The SPE (New York, 1968),  p a  74; M. Ha l le  and S.J.  Keyser, Engl ish  Stress (New York, 1971)  p'.	SPE	Sound Pattern of English$specific$	0
N.. and Halle, M., The SPE.	SPE	Sound Pattern of English$specific$	0
We incorpo-  rate multiple anaphora resolution factors into  a statistical framework - -  SPEally the dis-  tance between the pronoun and the proposed  antecedent, gender/number/animaticity of the  proposed antecedent, governing head informa-  tion and noun phrase repetition.	SPE	Sound Pattern of English$specific$	1
42 3.3 Quote Attribution & Speaker Identification Here the goal is to attribute (or assign) each quote to a SPE story character from the set identified in the previous step.	SPE	Sound Pattern of English$specific$	1
This system performs the following story analysis tasks: identification of charac- ters in each story; attribution of quotes to SPE story characters; identification of character age, gender and other salient personality attributes; and finally, affective analysis of the quoted material.	SPE	Sound Pattern of English$specific$	1
Regarding the story-SPE information, the associations between characters and third person pronouns (identified via anaphora resolution) were counted.	SPE	Sound Pattern of English$specific$	1
We SPEally extract the character reference CH either from the dependency relation nsubj, which links a speech verb SV with a CH that is the syntactic subject of a clause, or from the dependency relation dobj, which links a SV with a CH that is the direct object of the speech verb, across a conjunct (e.g., and).	SPE	Sound Pattern of English$specific$	1
The system also con-  tains domain knowledge including the domain  concepts, SPE list of subjects and verbs, and  topic headings.	SPE	Sound Pattern of English$specific$	1
As a lower bound, we evaluated the 1220 most frequent candidates in our Monolingual corpus (FC).	FC	fludarabine and cyclophosphamide$Frequent Candidates$functional composition$foot condition$	1
This method 641 Recall-at-1220 Dev Test FC 17.0 19.3 B as el in e WordNet 3.0 Frequent 41.6 43.7 WordNet 3.0 Filtered 49.4 48.8 Monolingual Only 30.1 30.2 B oo st ed Bilingual Only 47.1 43.9 Monolingual+Bilingual 50.8 47.9 Table 3: Our boosted ranker combining monolingual and bilingual features (bottom) compared to three base- lines (top) gives comparable performance to the human- curated upper bound.	FC	fludarabine and cyclophosphamide$Frequent Candidates$functional composition$foot condition$	1
-variables to be type consistent, and forcing the resulting FC to be acyclic and fully connected (we refer the reader to (Clarke et al, 2010) for more details).	FC	fludarabine and cyclophosphamide$Frequent Candidates$functional composition$foot condition$	2
x) and concatena- tion (+) is defined to be FC (?	FC	fludarabine and cyclophosphamide$Frequent Candidates$functional composition$foot condition$	2
Essentially, we now com- bine partial dependency trees using forward and backward concatenation rather than combining se- mantic representations by FC and application.	FC	fludarabine and cyclophosphamide$Frequent Candidates$functional composition$foot condition$	2
Figure 1 lists a core set of commonly assumed rules, derived from functional application and the B combinator, which models FC.	FC	fludarabine and cyclophosphamide$Frequent Candidates$functional composition$foot condition$	2
Non-ellipsis-based approach is characterized by: (a) strong proof sys- tem (Lambek, 1958), and (b) FC and type raising that allow coordination of incom- plete constituents, such as CG (Ajdukiewicz, 1935; Bar-Hillel, 1953; Moortgat, 2002), CCG (Steed- man, 2000), and multimodal CCG (Baldridge and Kruijff, 2003).	FC	fludarabine and cyclophosphamide$Frequent Candidates$functional composition$foot condition$	2
The rules derive state- ments about triples w ` A : f , expressing that the substring w can be assigned the category A and the semantic representation f ; an entire string counts as grammatical if it can be assigned the start cat- egory s. In parallel to the combination of substrings by the combinatory rules, their semantic represent- ations are combined by FC.	FC	fludarabine and cyclophosphamide$Frequent Candidates$functional composition$foot condition$	2
To solve this and other problems, a tree is now defined  to be admissible only if each non-terminal node of the  tree satisfies the FC, which is related to the  original FFP of GPSG 85.	FC	fludarabine and cyclophosphamide$Frequent Candidates$functional composition$foot condition$	3
The FC is defined  as follows.	FC	fludarabine and cyclophosphamide$Frequent Candidates$functional composition$foot condition$	3
"LF and language  learning."	LF	Lexical functions$long form$Logical Forms$logical form$late fusion$Log frequency$labelled F$Logical Form$left$	0
LF: A tool for the description of lexical relations in a lexicon.	LF	Lexical functions$long form$Logical Forms$logical form$late fusion$Log frequency$labelled F$Logical Form$left$	0
LF and  knowledge representation.	LF	Lexical functions$long form$Logical Forms$logical form$late fusion$Log frequency$labelled F$Logical Form$left$	0
LF: a tool for the description of lexical relations in a lexicon.	LF	Lexical functions$long form$Logical Forms$logical form$late fusion$Log frequency$labelled F$Logical Form$left$	0
4.2 LF Except wh(N), which generates interrogatives as shown in the bottom line of Table 1, the relations we have so far implemented are lexical derivations.	LF	Lexical functions$long form$Logical Forms$logical form$late fusion$Log frequency$labelled F$Logical Form$left$	0
LF: a tool for the descrip- tion of lexical relations in a lexicon.	LF	Lexical functions$long form$Logical Forms$logical form$late fusion$Log frequency$labelled F$Logical Form$left$	0
A trivial adaptation also reveals which parts of an abbreviation (one or more characters) map to which parts of the LF (one token, one partial token.)	LF	Lexical functions$long form$Logical Forms$logical form$late fusion$Log frequency$labelled F$Logical Form$left$	1
at the end of a name (for in- stance, a noun phrase) hints on a family-member or 4The original algorithm decides whether a given short form can be explained by a given LF.	LF	Lexical functions$long form$Logical Forms$logical form$late fusion$Log frequency$labelled F$Logical Form$left$	1
has to match the first letter of the proposed LF.	LF	Lexical functions$long form$Logical Forms$logical form$late fusion$Log frequency$labelled F$Logical Form$left$	1
As S&H allows for miss- ing tokens in the LF, we can also add the pos- sibility for (few) characters in the abbreviation not being reflected in the LF.	LF	Lexical functions$long form$Logical Forms$logical form$late fusion$Log frequency$labelled F$Logical Form$left$	1
We changed some of the details so that, for instance, the first letter of the potential abbreviation has to match the first letter of the proposed LF.	LF	Lexical functions$long form$Logical Forms$logical form$late fusion$Log frequency$labelled F$Logical Form$left$	1
LF) for the FASLG gene.	LF	Lexical functions$long form$Logical Forms$logical form$late fusion$Log frequency$labelled F$Logical Form$left$	1
Working with LF Skolemization: Skolemization of an existential formula of type (some x R S), where x is a variable, R is a restrictor formula and S is the nuclear scope, is performed via the transduction (/ (some ! !	LF	Lexical functions$long form$Logical Forms$logical form$late fusion$Log frequency$labelled F$Logical Form$left$	2
Examples are Quasi LF (Al- shawi, 1990), Dynamic Predicate Logic (Groe- nendijk and Stokhof, 1991), and Underspecified Discourse Representation Theory (Reyle, 1993).	LF	Lexical functions$long form$Logical Forms$logical form$late fusion$Log frequency$labelled F$Logical Form$left$	2
Resolving Quasi LF,  Computational Linguistics, 6(13), pp.	LF	Lexical functions$long form$Logical Forms$logical form$late fusion$Log frequency$labelled F$Logical Form$left$	2
Deriving Database  Queries from LF by Abductive Definition  Expansion".	LF	Lexical functions$long form$Logical Forms$logical form$late fusion$Log frequency$labelled F$Logical Form$left$	2
5 Building LF  Based on entries in the Generative Lexicon and  on the context given by a sentence to be inter-  preted, appropriate logical forms can be built  that represent semantic relations involved more  explicitly than this is the case with previous  approaches.	LF	Lexical functions$long form$Logical Forms$logical form$late fusion$Log frequency$labelled F$Logical Form$left$	2
Generation of Paraphrases from  Ambiguous LF.	LF	Lexical functions$long form$Logical Forms$logical form$late fusion$Log frequency$labelled F$Logical Form$left$	2
To compute soft truth values for LFulas, Lukasiewicz?s re- laxation of conjunctions(?),	LF	Lexical functions$long form$Logical Forms$logical form$late fusion$Log frequency$labelled F$Logical Form$left$	3
Inflected queries are performed by expanding an n-gram into all its morphoLFs.	LF	Lexical functions$long form$Logical Forms$logical form$late fusion$Log frequency$labelled F$Logical Form$left$	3
The MLN constructed to determine the probability of a given entailment includes the LFs for both T and H as well as soft inference rules that are constructed from distributional information.	LF	Lexical functions$long form$Logical Forms$logical form$late fusion$Log frequency$labelled F$Logical Form$left$	3
Unifying LFs to instantiate variables in this way follows the ?	LF	Lexical functions$long form$Logical Forms$logical form$late fusion$Log frequency$labelled F$Logical Form$left$	3
Given a set of weighted LFulas, PSL builds a graphical model defining a probability distribution over the continuous space of values of the random variables in the model.	LF	Lexical functions$long form$Logical Forms$logical form$late fusion$Log frequency$labelled F$Logical Form$left$	3
This guessing is based on the morphoLF of the surface form.	LF	Lexical functions$long form$Logical Forms$logical form$late fusion$Log frequency$labelled F$Logical Form$left$	3
Experiments in both (Shriberg et al, 2000) and (Kim et al, 2004) find no conclusive win- ner among early fusion, additive LF, and multiplicative LF.	LF	Lexical functions$long form$Logical Forms$logical form$late fusion$Log frequency$labelled F$Logical Form$left$	4
6 Results and Discussion In this section we present the results of our empiri- cal evaluation designed to test the three main char- acteristics of the LMDE model: (1) integration of multiple sources of information, (2) LF ap- proach and (3) latent variable which models the hidden dynamic between experts.	LF	Lexical functions$long form$Logical Forms$logical form$late fusion$Log frequency$labelled F$Logical Form$left$	4
As in LF, m	LF	Lexical functions$long form$Logical Forms$logical form$late fusion$Log frequency$labelled F$Logical Form$left$	4
As in LF, modality- specific classifiers are trained independently.	LF	Lexical functions$long form$Logical Forms$logical form$late fusion$Log frequency$labelled F$Logical Form$left$	4
Due to the very different nature of the models we opt to not fuse them at the feature level, using a LF scheme instead.	LF	Lexical functions$long form$Logical Forms$logical form$late fusion$Log frequency$labelled F$Logical Form$left$	4
LF.?	LF	Lexical functions$long form$Logical Forms$logical form$late fusion$Log frequency$labelled F$Logical Form$left$	4
0  2  4  6  8  10  12  0  5000  10000  15000  20000Solution Id LF of solutions for sentences with 5 tokens (a) Sentence length = 5 -1  0  1  2  3  4  5  6  7  8  0  50000  100000  150000  200000  250000Solution Id LF of solutions for sentences with 10 tokens (b) Sentence length = 10 -1  0  1  2  3  4  5  6  7  8  0  50000  100000  150000  200000  250000  300000  350000Solution Id LF of solutions for sentences with 15 tokens (c) Sentence length = 15 Figure 3: These plots show the log-frequencies of occurrences of part-of-speech sequences for sentences with five, ten and fifteen tokens.	LF	Lexical functions$long form$Logical Forms$logical form$late fusion$Log frequency$labelled F$Logical Form$left$	5
LF of  the head noun was not significantly correlated  with plausibility (r = .098), which suggests  that adjective-noun plausibility judgements are  not influenced by noun familiarity.	LF	Lexical functions$long form$Logical Forms$logical form$late fusion$Log frequency$labelled F$Logical Form$left$	5
0  2  4  6  8  10  12  0  5000  10000  15000  20000Solution Id LF of solutions for sentences with 5 tokens (a) Sentence length = 5 -1  0  1  2  3  4  5  6  7  8  0  50000  100000  150000  200000  250000Solution Id LF of solutions for sentences with 10 tokens (b) Sentence length = 10 -1  0  1  2  3  4  5  6  7  8  0  50000  100000  150000  200000  250000  300000  350000Solution Id LF of solutions for sentences with 15 tokens (c	LF	Lexical functions$long form$Logical Forms$logical form$late fusion$Log frequency$labelled F$Logical Form$left$	5
LF     This feature takes the logarithm of the co- occurrence frequency of the phrase pair.	LF	Lexical functions$long form$Logical Forms$logical form$late fusion$Log frequency$labelled F$Logical Form$left$	5
LF biased MD log P (xy) 2 P (x?)P (?	LF	Lexical functions$long form$Logical Forms$logical form$late fusion$Log frequency$labelled F$Logical Form$left$	5
4  6  8  10  12  0  5000  10000  15000  20000Solution Id LF of solutions for sentences with 5 tokens (a) Sentence length = 5 -1  0  1  2  3  4  5  6  7  8  0  50000  100000  150000  200000  250000Solution Id LF of solutions for sentences with 10 tokens (b) Sentence length = 10 -1  0  1  2  3  4  5  6  7  8  0  50000  100000  150000  200000  250000  300000  350000Solution Id LF of solutions for sentences with 15 tokens (c) Sentence length = 15 Figure 3: These plots show the log-frequencies of occurrences of part-of-speech sequences for sentences with five, ten and fifteen tokens.	LF	Lexical functions$long form$Logical Forms$logical form$late fusion$Log frequency$labelled F$Logical Form$left$	5
Middle column: post submission LF1- score.	LF	Lexical functions$long form$Logical Forms$logical form$late fusion$Log frequency$labelled F$Logical Form$left$	6
Us- ing a stochastic variant of Constraint Dependency Grammar (Wang and Harper, 2004) reached a 92.4% LF-score on the Penn Treebank, which slightly outperforms (Collins, 1999) who reports 92.0% on dependency structures automati- cally derived from phrase structure results.	LF	Lexical functions$long form$Logical Forms$logical form$late fusion$Log frequency$labelled F$Logical Form$left$	6
Right column: post submission unLF1- score. ?	LF	Lexical functions$long form$Logical Forms$logical form$late fusion$Log frequency$labelled F$Logical Form$left$	6
60.61 65.18 84.29 Table 1: Semantic labelled and unLF1-scores for each language and domain.	LF	Lexical functions$long form$Logical Forms$logical form$late fusion$Log frequency$labelled F$Logical Form$left$	6
18 According to the results, the unLF 1 (UF 1 ) is a closer estimation than the labelled one.	LF	Lexical functions$long form$Logical Forms$logical form$late fusion$Log frequency$labelled F$Logical Form$left$	6
My official submission scores are given in table 1, together with post submission labelled and un- LF1-scores.	LF	Lexical functions$long form$Logical Forms$logical form$late fusion$Log frequency$labelled F$Logical Form$left$	6
Initial experiments on the de- velopment data indicates that these simple heuristics slightly improves semantic parsing quality measured with LF1-score.	LF	Lexical functions$long form$Logical Forms$logical form$late fusion$Log frequency$labelled F$Logical Form$left$	6
Combining treebank transfor- mation techniques with a suffix analysis, (Dubey, 2005) trained a probabilistic parser and reached a LF-score of 76.3% on phrase structure an- notations for a subset of the sentences used here (with a maximum length of 40).	LF	Lexical functions$long form$Logical Forms$logical form$late fusion$Log frequency$labelled F$Logical Form$left$	6
Learn- ing to Map Sentences to LF: Structured Classification with Probabilistic Categorial Grammars.	LF	Lexical functions$long form$Logical Forms$logical form$late fusion$Log frequency$labelled F$Logical Form$left$	7
Working with LFs Skolemization: Skolemization of an existential formula of type (some x R S), where x is a variable, R is a restrictor formula and S is the nuclear scope, is performed via the transduction (/ (some ! !	LF	Lexical functions$long form$Logical Forms$logical form$late fusion$Log frequency$labelled F$Logical Form$left$	7
Since the introduction of Quasi LF (Al- shawi and Crouch, 1992), there has been a lot of work on designing constraint-based underspecifica- tion formalisms where the readings of a UR are not defined in a constructive fashion as shown above, but rather by a set of constraints.	LF	Lexical functions$long form$Logical Forms$logical form$late fusion$Log frequency$labelled F$Logical Form$left$	7
Proceedings of the AAAI Spring Symposium on LFalization of Commonsense Rea-soning.	LF	Lexical functions$long form$Logical Forms$logical form$late fusion$Log frequency$labelled F$Logical Form$left$	7
Pragmatics: lmplicature, Pre-  supposition, and LF.	LF	Lexical functions$long form$Logical Forms$logical form$late fusion$Log frequency$labelled F$Logical Form$left$	7
After this operation, the module produces Predicate-Argument Structures or PAS on the basis of a previously produced LF.	LF	Lexical functions$long form$Logical Forms$logical form$late fusion$Log frequency$labelled F$Logical Form$left$	7
He says that the red ball is the one on the LF.	LF	Lexical functions$long form$Logical Forms$logical form$late fusion$Log frequency$labelled F$Logical Form$left$	8
This algorithm searches the parse tree in a LF-  to-right, breadth-first fashion that obeys the  major reflexive pronoun constraints while giv-  ing a preference to antecedents hat are closer  to the pronoun.	LF	Lexical functions$long form$Logical Forms$logical form$late fusion$Log frequency$labelled F$Logical Form$left$	8
The item 4) is situated at the edge E of domain D.  This delinition covers a family of constraints depending on  the instantiations of the arguments: E is either LF (L) or  right (R), domain \]nay be syllable, foot or word, and ~b can  be any phonological object, such as stress or an affix.	LF	Lexical functions$long form$Logical Forms$logical form$late fusion$Log frequency$labelled F$Logical Form$left$	8
The representation for sentence 1 states that the first element of the 5-gram (-3; third word to the LF of the adjective) is empty (because the second element is a phrase boundary marker), that the sec- ond element is a	LF	Lexical functions$long form$Logical Forms$logical form$late fusion$Log frequency$labelled F$Logical Form$left$	8
The  cases where "it" is merely a dummy subject in  a cLF sentence (example 1) or has conventional  unspecified referents (example 2) are excluded  from computing the precision:  ?	LF	Lexical functions$long form$Logical Forms$logical form$late fusion$Log frequency$labelled F$Logical Form$left$	8
In resolving inter-sentential  pronouns, the algorithm searches the previous  sentence, again in LF-to-right, breadth-first or-  der.	LF	Lexical functions$long form$Logical Forms$logical form$late fusion$Log frequency$labelled F$Logical Form$left$	8
tences (target adjective in bold face, word window in italics; negative numbers indicate positions to the LF, positive ones positions to the right): 1.	LF	Lexical functions$long form$Logical Forms$logical form$late fusion$Log frequency$labelled F$Logical Form$left$	8
The lemmata were modelled using pairs of bi- grams: in a 4-word window (three to the LF and one to the right of the adjective), the first two tags formed a feature and the second two tags another feature.	LF	Lexical functions$long form$Logical Forms$logical form$late fusion$Log frequency$labelled F$Logical Form$left$	8
For this, we propose a lan- guage model for generating reviews that incorporates a description of objects and a generic RLM.	RLM	review language model$recurrent language model$	0
On the other hand, our pro- posed bidirectional models include the full source sentence relying on recurrency, yielding improve- ments over competitive baselines already includ- ing a RLM.	RLM	review language model$recurrent language model$	1
The continu- ous representations are obtained by applying a se- quence of convolutions, and the result is fed into the hidden layer of a RLM.	RLM	review language model$recurrent language model$	1
All results include a RLM.	RLM	review language model$recurrent language model$	1
Kim et al (2015) propose a RLM that re- places the word-indexed projection matrix with a convolution layer fed with the character sequence that constitutes each word to find morphological pat- terns.	RLM	review language model$recurrent language model$	1
Our own models are also implemented in Torch7 for easier comparison.1 Fi- nally, we selected the best performing convolutional and RLMs on Europarl-NC and the Baseline FFLM to be evaluated on the ukWaC corpus.	RLM	review language model$recurrent language model$	1
However, while previous work on translation modeling with recurrent neural net- works shows its effectiveness on standard base- lines, so far no notable gains have been presented on top of RLMs (Auli et al.,	RLM	review language model$recurrent language model$	1
Recognition  Sentence Classification  Background Classification  SVM Comorbidity Classification  SVM Diagnosis Classification  SVM Family History Classification  SVM ISF Classification  SVM Lifestyle Classification  SVM Symptom Classification  SVM Test Classification  Candidate Generation  SVM Candidate Filter  Question Recognition  SVM Sentence Classification  Question  Candidate Generation  SVM CRing  Exemplification Recognition  Candidate Filter  Candidate Generation  SVM CRing  Coordination Recognition  SVM Candidate Filter Coordination  Exemplification  Stanford  Parser  WordNet  SVM Treatment Classification  Figure 1: Question Decomposition Architecture.	CR	Candidate Rank$canonical representative$compression ratios$creation$causal relations$Coreference resolution$conference r gistration$critical$controlled requirements$corequirement$	0
This is a positive result, especially given 97 05 10 15 20 1 - 2 0 2 1 - 4 0 4 1 - 6 0 6 1 - 8 0 8 1 - 1 0 0 1 0 1 - 1 2 0 1 2 1 - 1 4 0 1 4 1 - 1 6 0 1 6 1 - 1 8 0 1 8 1 - 2 0 0 2 0 1 - 2 2 0 2 2 1 - 2 4 0 2 4 1 - 2 6 0 2 6 1 - 2 8 0 2 8 1 - 3 0 0 CR %   T i m e   C h o s e n   f o r   C o m p a r i s o n Figure 4: Histogram of the rank of the lower-ranked can- didate chosen in pair comparisons.	CR	Candidate Rank$canonical representative$compression ratios$creation$causal relations$Coreference resolution$conference r gistration$critical$controlled requirements$corequirement$	0
4.3 CRing Model Given an anaphor ai and a set of candidate antecedents C = {C1,C2, ...,Ck}, the problem of anaphora resolution is to choose the best candidate antecedent for ai.	CR	Candidate Rank$canonical representative$compression ratios$creation$causal relations$Coreference resolution$conference r gistration$critical$controlled requirements$corequirement$	0
ion  SVM Diagnosis Classification  SVM Family History Classification  SVM ISF Classification  SVM Lifestyle Classification  SVM Symptom Classification  SVM Test Classification  Candidate Generation  SVM Candidate Filter  Question Recognition  SVM Sentence Classification  Question  Candidate Generation  SVM CRing  Exemplification Recognition  Candidate Filter  Candidate Generation  SVM CRing  Coordination Recognition  SVM Candidate Filter Coordination  Exemplification  Stanford  Parser  WordNet  SVM Treatment Classification  Figure 1: Question Decomposition Architecture.	CR	Candidate Rank$canonical representative$compression ratios$creation$causal relations$Coreference resolution$conference r gistration$critical$controlled requirements$corequirement$	0
1 http://lhncbc.nlm.nih.gov/project/consumer-health- question-answering 30 Sentence Splitting  Request  Question  Sentence  Ignore  Sentence  Background  Sentence  Candidate Generation  UMLS  SVM CRing  Boundary Fixing  Focus  Focus Recognition  Sentence Classification  Background Classification  SVM Comorbidity Classification  SVM Diagnosis Classification  SVM Family History Classification  SVM ISF Classification  SVM Lifestyle Classification  SVM Symptom Classification  SVM Test Classification  Candidate Generation  SVM Candidate Filter  Question Recognition  SVM Sentence Cla	CR	Candidate Rank$canonical representative$compression ratios$creation$causal relations$Coreference resolution$conference r gistration$critical$controlled requirements$corequirement$	0
0 5 10 15 20 25 30 1 2 3 4 5 6 7 8 9 10 CR %   T i m e   C h o s e n   f o r   C o m p a r i s o n Figure 2: Histogram of the rank of the higher-ranked candidate chosen in pair comparisons.	CR	Candidate Rank$canonical representative$compression ratios$creation$causal relations$Coreference resolution$conference r gistration$critical$controlled requirements$corequirement$	0
Among the TAG el- ementary trees that correspond to a given lexical entry, there is the CR, and all the other representatives are represented by adding features to the CR.	CR	Candidate Rank$canonical representative$compression ratios$creation$causal relations$Coreference resolution$conference r gistration$critical$controlled requirements$corequirement$	1
If a match is found, the GO node name (deemed the CR for its set of synonyms) is associated with the abstract.	CR	Candidate Rank$canonical representative$compression ratios$creation$causal relations$Coreference resolution$conference r gistration$critical$controlled requirements$corequirement$	1
In practice, we still use paths and spans, and hash to a CR if desired.	CR	Candidate Rank$canonical representative$compression ratios$creation$causal relations$Coreference resolution$conference r gistration$critical$controlled requirements$corequirement$	1
Note that for idempotent f , image(f) consists of CRs f(s) of ker(f)?s equiva- lence classes {s ?	CR	Candidate Rank$canonical representative$compression ratios$creation$causal relations$Coreference resolution$conference r gistration$critical$controlled requirements$corequirement$	1
Derivation trees can then be  seen as CRs of classes of derivations  producing the same string, differing only in the order in  which the same productions are applied.	CR	Candidate Rank$canonical representative$compression ratios$creation$causal relations$Coreference resolution$conference r gistration$critical$controlled requirements$corequirement$	1
In addition,  we plan to automatically adjust cross-document  event aggregation operations according to specific  CR provided by the users.	CR	Candidate Rank$canonical representative$compression ratios$creation$causal relations$Coreference resolution$conference r gistration$critical$controlled requirements$corequirement$	2
Comparison of CR with other  techniques is sheik) in Table 2.	CR	Candidate Rank$canonical representative$compression ratios$creation$causal relations$Coreference resolution$conference r gistration$critical$controlled requirements$corequirement$	2
In the future, we would like to apply a similar ex- haustive search strategy, but this time with differ- ent CR, in order to see the impact of CR on the pdf of each domain.	CR	Candidate Rank$canonical representative$compression ratios$creation$causal relations$Coreference resolution$conference r gistration$critical$controlled requirements$corequirement$	2
4.1.1  Recall, Coverage, Retention and  Weighted Retention   Recall at different CR has been  used in summarization research (Mani 2001) to  measure how well an automatic system retains  important content of original documents.	CR	Candidate Rank$canonical representative$compression ratios$creation$causal relations$Coreference resolution$conference r gistration$critical$controlled requirements$corequirement$	2
i  sentence compression ratio of 0.2% and a character com-  pression of 0.3%, approximately two orders of magni-  tude different with CR used in single doc-  ument summarization.	CR	Candidate Rank$canonical representative$compression ratios$creation$causal relations$Coreference resolution$conference r gistration$critical$controlled requirements$corequirement$	2
These POS-based patterns are quite generic, al- lowing for the CR of large sets of characters.	CR	Candidate Rank$canonical representative$compression ratios$creation$causal relations$Coreference resolution$conference r gistration$critical$controlled requirements$corequirement$	3
Despite large typological differ- ences between Wambaya and the languages on which the development of the resource was based, the Grammar Matrix is found to pro- vide a significant jump-start in the CR of the grammar for Wambaya: With less than 5.5 person-weeks of development, the Wambaya grammar was able to assign correct seman- tic representations to 76% of the sentences in a naturally occurring text.	CR	Candidate Rank$canonical representative$compression ratios$creation$causal relations$Coreference resolution$conference r gistration$critical$controlled requirements$corequirement$	3
In (Celli, 2012), a list of linguis- tic features were used for the CR of character models in terms of the the Big Five personality di- mensions (Norman, 1963).	CR	Candidate Rank$canonical representative$compression ratios$creation$causal relations$Coreference resolution$conference r gistration$critical$controlled requirements$corequirement$	3
Riddles and metaphors: The CR of meaning.	CR	Candidate Rank$canonical representative$compression ratios$creation$causal relations$Coreference resolution$conference r gistration$critical$controlled requirements$corequirement$	3
A clear candidate is the use of named entities, but the CR of templates has also been tried in open domains (Srihari and Li 2000) and restricted domains (Weischedel, Xu, and Licuanan 2004).	CR	Candidate Rank$canonical representative$compression ratios$creation$causal relations$Coreference resolution$conference r gistration$critical$controlled requirements$corequirement$	3
It is not difficult to see that replacing any set of two or three nonterminals in p?s right-hand side forces the CR of a fresh nonterminal of fan-out larger than two.	CR	Candidate Rank$canonical representative$compression ratios$creation$causal relations$Coreference resolution$conference r gistration$critical$controlled requirements$corequirement$	3
Constraint Equations (11) and (12) help to re- cover CR.	CR	Candidate Rank$canonical representative$compression ratios$creation$causal relations$Coreference resolution$conference r gistration$critical$controlled requirements$corequirement$	4
To aid in extracting CR, we leverage on the identification of discourse relations to provide addi- tional contextual information.	CR	Candidate Rank$canonical representative$compression ratios$creation$causal relations$Coreference resolution$conference r gistration$critical$controlled requirements$corequirement$	4
(Beamer and Girju, 2009) tried to detect CR be- tween verbs in a corpus of screen plays, but limited themselves to consecutive, or adjacent verb pairs.	CR	Candidate Rank$canonical representative$compression ratios$creation$causal relations$Coreference resolution$conference r gistration$critical$controlled requirements$corequirement$	4
In this work, we auto- matically detect and extract CR between events in text.	CR	Candidate Rank$canonical representative$compression ratios$creation$causal relations$Coreference resolution$conference r gistration$critical$controlled requirements$corequirement$	4
Beamer and Girju, 2009) tried to detect CR be- tween verbs in a corpus of screen plays, but limited themselves to consecutive, or adjacent verb pairs.	CR	Candidate Rank$canonical representative$compression ratios$creation$causal relations$Coreference resolution$conference r gistration$critical$controlled requirements$corequirement$	4
Thus, their focus was not on identifying CR between events in a given text document.	CR	Candidate Rank$canonical representative$compression ratios$creation$causal relations$Coreference resolution$conference r gistration$critical$controlled requirements$corequirement$	4
Thus, their focus was not on identifying CR between events in a given	CR	Candidate Rank$canonical representative$compression ratios$creation$causal relations$Coreference resolution$conference r gistration$critical$controlled requirements$corequirement$	4
Why-question answering using intra- and inter-sentential CR.	CR	Candidate Rank$canonical representative$compression ratios$creation$causal relations$Coreference resolution$conference r gistration$critical$controlled requirements$corequirement$	4
This means that with the aid of discourse relations, we are able to recover more CR, as well as reduce false-positive predictions.	CR	Candidate Rank$canonical representative$compression ratios$creation$causal relations$Coreference resolution$conference r gistration$critical$controlled requirements$corequirement$	4
1 Introduction  CR aims to identify which  noun phrases (NPs, or mentions) refer to the  same real-world entity in a text.	CR	Candidate Rank$canonical representative$compression ratios$creation$causal relations$Coreference resolution$conference r gistration$critical$controlled requirements$corequirement$	5
c?2013 Association for Computational Linguistics Dynamic Knowledge-Base Alignment for Coreference Resolution Jiaping Zheng Luke Vilnis Sameer Singh Jinho D. Choi Andrew McCallum School of Computer Science University of Massachusetts Amherst MA 01003 {jzheng,luke,sameer,jdchoi,mccallum}@cs.umass.edu Abstract CR systems can benefit greatly from inclusion of global context, and a number of recent approaches have demonstrated improvements when precom- puting an alignment to external knowledge sources.	CR	Candidate Rank$canonical representative$compression ratios$creation$causal relations$Coreference resolution$conference r gistration$critical$controlled requirements$corequirement$	5
CR forms an important component for natural language process- ing and information extraction pipelines due to its utility in relation extraction, cross-document coref- erence, text summarization, and question answer- ing.	CR	Candidate Rank$canonical representative$compression ratios$creation$causal relations$Coreference resolution$conference r gistration$critical$controlled requirements$corequirement$	5
CR using competi- tion learning approach.	CR	Candidate Rank$canonical representative$compression ratios$creation$causal relations$Coreference resolution$conference r gistration$critical$controlled requirements$corequirement$	5
CR is first recast as a classifica- tion task, in which a pair of NPs is classified as co- referring or not based on constraints that are learned from an annotated corpus.	CR	Candidate Rank$canonical representative$compression ratios$creation$causal relations$Coreference resolution$conference r gistration$critical$controlled requirements$corequirement$	5
1 Introduction CR is the task of identifying sets of noun phrase mentions from a document that refer to the same real-world entities.	CR	Candidate Rank$canonical representative$compression ratios$creation$causal relations$Coreference resolution$conference r gistration$critical$controlled requirements$corequirement$	5
Two  large text databases about CR (1.4 ?	CR	Candidate Rank$canonical representative$compression ratios$creation$causal relations$Coreference resolution$conference r gistration$critical$controlled requirements$corequirement$	6
Although this shared stack mechanism ac-  counts for highly task-oriented and cooperative dia-  logues where one can assume that both speakers share  3Dialogue 1is extracted from a corpus of Japanese ATR  (Advanced Telecommunication Research) recorded simu-  lated CR telephone conversations.	CR	Candidate Rank$canonical representative$compression ratios$creation$causal relations$Coreference resolution$conference r gistration$critical$controlled requirements$corequirement$	6
Ti~e  dictionaries and the rules are made by extracting  the entries from the ATR corporaU0l concerning  CR (Table 1).	CR	Candidate Rank$canonical representative$compression ratios$creation$causal relations$Coreference resolution$conference r gistration$critical$controlled requirements$corequirement$	6
V: vocabulary size (= Vc + Vf )  Vc: number of content words  Vf : number of function words  Table 1: Number of parameters of each model  90  Figure 4: Network representation f the proposed language model  Task  Vocabulary Size  Speaker  Test Data  International CR  1,500 words  1 male speaker  261 sentences  (7.0 words/sentence, on average)  Table 2: Experimental conditions for speech recognition  The proposed model was compared with the word bigram and tri-  gram models in their perplexities for test sentences and in sentence  recognition rates.	CR	Candidate Rank$canonical representative$compression ratios$creation$causal relations$Coreference resolution$conference r gistration$critical$controlled requirements$corequirement$	6
We are implementing this model using the Span-  ish travel agency domain corpus and the Japanese  ATR CR corpus.	CR	Candidate Rank$canonical representative$compression ratios$creation$causal relations$Coreference resolution$conference r gistration$critical$controlled requirements$corequirement$	6
The corpus contains conversations about  international CR (Ogura et al  1989).	CR	Candidate Rank$canonical representative$compression ratios$creation$causal relations$Coreference resolution$conference r gistration$critical$controlled requirements$corequirement$	6
Anaphoric resolu- tion can sometimes be CR for these inferences.	CR	Candidate Rank$canonical representative$compression ratios$creation$causal relations$Coreference resolution$conference r gistration$critical$controlled requirements$corequirement$	7
In more general terms, we have identified the following features of currently used tagsets for Slavic in general and Polish in particular which seem problematic from the point of view of their reusability and cross-linguistic applicability:   unCR adoption of traditional and some- times ill-defined POS classes, such as ?	CR	Candidate Rank$canonical representative$compression ratios$creation$causal relations$Coreference resolution$conference r gistration$critical$controlled requirements$corequirement$	7
Introduction  Portability and domain independence are CR  challenges for Natural Language Processing (NLP)  systems.	CR	Candidate Rank$canonical representative$compression ratios$creation$causal relations$Coreference resolution$conference r gistration$critical$controlled requirements$corequirement$	7
For such domains, the balance between reacting to the domain events as they occur and maintaining the overall, high- level consistency is CR.	CR	Candidate Rank$canonical representative$compression ratios$creation$causal relations$Coreference resolution$conference r gistration$critical$controlled requirements$corequirement$	7
11 2 Background and related work Hawley et al (2007) described an experiment in which 8 dysarthric individuals (with either cere- bral palsy or multiple sclerosis) controlled non- CR devices in their home (e.g., TV) with auto- matic speech recognition.	CR	Candidate Rank$canonical representative$compression ratios$creation$causal relations$Coreference resolution$conference r gistration$critical$controlled requirements$corequirement$	7
University of Pennsylvania Word-level alignment of bilingual text is a CR resource for a growing variety of tasks.	CR	Candidate Rank$canonical representative$compression ratios$creation$causal relations$Coreference resolution$conference r gistration$critical$controlled requirements$corequirement$	7
Another type of CR are user stories.	CR	Candidate Rank$canonical representative$compression ratios$creation$causal relations$Coreference resolution$conference r gistration$critical$controlled requirements$corequirement$	8
For instance, the NFR corpus4 covers the sys- tem?s perspective of CR spec- ifications.	CR	Candidate Rank$canonical representative$compression ratios$creation$causal relations$Coreference resolution$conference r gistration$critical$controlled requirements$corequirement$	8
For the acquisi- tion of unCR, we adapted the idea of Vlas and Robinson (2011) that is based on feature requests gathered from the open-source soft- ware platform SourceForge5.	CR	Candidate Rank$canonical representative$compression ratios$creation$causal relations$Coreference resolution$conference r gistration$critical$controlled requirements$corequirement$	8
Special attention is paid to CRs.	CR	Candidate Rank$canonical representative$compression ratios$creation$causal relations$Coreference resolution$conference r gistration$critical$controlled requirements$corequirement$	9
Let us look now more carefully at several linguistic issues we consider to be important to characterize the notion of linguistic requirement: extensionality/ intensionality, soft/hard requirements, the scope of a condition, syntactic/semantic requirements, and CRs.	CR	Candidate Rank$canonical representative$compression ratios$creation$causal relations$Coreference resolution$conference r gistration$critical$controlled requirements$corequirement$	9
This phenomenon is called here CR.	CR	Candidate Rank$canonical representative$compression ratios$creation$causal relations$Coreference resolution$conference r gistration$critical$controlled requirements$corequirement$	9
Special attention will be paid to both the relativized view on word sense (i.e., contextual sense) and CRs.	CR	Candidate Rank$canonical representative$compression ratios$creation$causal relations$Coreference resolution$conference r gistration$critical$controlled requirements$corequirement$	9
The final linguistic issue to be introduced is the phenomenon referred to as CRs.	CR	Candidate Rank$canonical representative$compression ratios$creation$causal relations$Coreference resolution$conference r gistration$critical$controlled requirements$corequirement$	9
This notion is modeled by means of what we call CRs.	CR	Candidate Rank$canonical representative$compression ratios$creation$causal relations$Coreference resolution$conference r gistration$critical$controlled requirements$corequirement$	9
1 In t roduct ion   We present a statistical method for deTEin-  ing pronoun anaphora.	TE	term$template element$translation equivalents$textual entailment$true equivalents$temporal expression$Template Element$Textual Entailment$Term Extraction$Textual entailment$	0
In equation (4), the TE  P(h.	TE	term$template element$translation equivalents$textual entailment$true equivalents$temporal expression$Template Element$Textual Entailment$Term Extraction$Textual entailment$	0
Finally we attempted a fully automatic di-  rect test of the accuracy of both pronoun meth-  ods for gender deTEination.	TE	term$template element$translation equivalents$textual entailment$true equivalents$temporal expression$Template Element$Textual Entailment$Term Extraction$Textual entailment$	0
One can judge the pro-  gram informally by simply examining the re-  sults and deTEining if the program's gender  decisions are correct (occasionally ooking at the  text for difficult cases).	TE	term$template element$translation equivalents$textual entailment$true equivalents$temporal expression$Template Element$Textual Entailment$Term Extraction$Textual entailment$	0
This decision is made by ranking the refer-  ents by log-likelihood ratio, TEed salience, for  each referent.	TE	term$template element$translation equivalents$textual entailment$true equivalents$temporal expression$Template Element$Textual Entailment$Term Extraction$Textual entailment$	0
First, as one might expect given the al-  ready noted superior performance of the Hobbs  scheme over last-noun, Hobbs also performs bet-  ter at deTEining ender.	TE	term$template element$translation equivalents$textual entailment$true equivalents$temporal expression$Template Element$Textual Entailment$Term Extraction$Textual entailment$	0
This demonstrates that given a correct underlying analysis the development of the high level TE application is relatively trivial.	TE	term$template element$translation equivalents$textual entailment$true equivalents$temporal expression$Template Element$Textual Entailment$Term Extraction$Textual entailment$	1
2 Core ference  in  the  LaS IE  sys tem  The LaSIE system (Gaizauskas et al, 1995) has  been designed as a general purpose IE system  which can conform to the MUC task specific-  ations for named entity identification, corefer-  ence resolution, IE TE and re-  lation identification, and the construction of  scenario-specific IE templates.	TE	term$template element$translation equivalents$textual entailment$true equivalents$temporal expression$Template Element$Textual Entailment$Term Extraction$Textual entailment$	1
Therefore an extra TE was generated for both TCI and News Corp. We also did not recognize ING Barings as a company, and so did not print a template for it.	TE	term$template element$translation equivalents$textual entailment$true equivalents$temporal expression$Template Element$Textual Entailment$Term Extraction$Textual entailment$	1
Information Extraction Template Element For the TE task our ocial MUC-6 scores were: R: 66; P: 74; P & R: 69.8 and, with the addition of the two missed articles: R: 68; P: 74; P & R: 70.8.	TE	term$template element$translation equivalents$textual entailment$true equivalents$temporal expression$Template Element$Textual Entailment$Term Extraction$Textual entailment$	1
In the context of MUC, the coreference layer provides input to the TE task, where each named  entity is represented as a single template which collects information about that element from the multiple  mentions in the text.	TE	term$template element$translation equivalents$textual entailment$true equivalents$temporal expression$Template Element$Textual Entailment$Term Extraction$Textual entailment$	1
solely restricted to, carrying out the tasks specified  in MUC-6: named entity recognition, coreference  resolution, TE filling, and scenario  template filling tasks (see DARPA (1995) for further  details of the task descriptions).	TE	term$template element$translation equivalents$textual entailment$true equivalents$temporal expression$Template Element$Textual Entailment$Term Extraction$Textual entailment$	1
horacio@Isi.upc.es  Abst ract   This paper describes the enhancements made, within a  unification framework, based on typed feature  structures, in order to support linking of lexical  entries to their TE.	TE	term$template element$translation equivalents$textual entailment$true equivalents$temporal expression$Template Element$Textual Entailment$Term Extraction$Textual entailment$	2
This contrastive analysis serves as a basis for the work of a group of experts (the Harmonising Group) who will determine TE in French, Italian, German and Slovene (one-to- one correspondence) in the fields of spatial plan- ning and sustainable development for use within the Convention, thus optimising the understanding between the Alpine states at supranational level.	TE	term$template element$translation equivalents$textual entailment$true equivalents$temporal expression$Template Element$Textual Entailment$Term Extraction$Textual entailment$	2
c) A German noun list with English TE, gen-  der, and inflectional information.	TE	term$template element$translation equivalents$textual entailment$true equivalents$temporal expression$Template Element$Textual Entailment$Term Extraction$Textual entailment$	2
These  are based on a contrastive approach; they are included only for those  sets of TE for which these relations are not identical.	TE	term$template element$translation equivalents$textual entailment$true equivalents$temporal expression$Template Element$Textual Entailment$Term Extraction$Textual entailment$	2
In this way, we compiled the following lists:  a) A German verb list with English TE.	TE	term$template element$translation equivalents$textual entailment$true equivalents$temporal expression$Template Element$Textual Entailment$Term Extraction$Textual entailment$	2
This paper describes the enhancements made, to the LKB  system \[6\], in order to support linking of lexical entries to  their TE.	TE	term$template element$translation equivalents$textual entailment$true equivalents$temporal expression$Template Element$Textual Entailment$Term Extraction$Textual entailment$	2
Measuring comparability of documents in non-parallel corpora for efficient extraction of (semi-) parallel TE.	TE	term$template element$translation equivalents$textual entailment$true equivalents$temporal expression$Template Element$Textual Entailment$Term Extraction$Textual entailment$	2
Snow et al (2008) were among the first to use MTurk to obtain data for several NLP tasks, such as TE and word sense dis- ambiguation.	TE	term$template element$translation equivalents$textual entailment$true equivalents$temporal expression$Template Element$Textual Entailment$Term Extraction$Textual entailment$	3
Mirkin et al (2009), inter alia, frame paraphrasing as a special, symmetrical case of (WordNet-based) TE.	TE	term$template element$translation equivalents$textual entailment$true equivalents$temporal expression$Template Element$Textual Entailment$Term Extraction$Textual entailment$	3
STS is treated as computing the prob- ability of two TEs T |= H and H |= T , where T and H are the two sentences whose similarity is being judged.	TE	term$template element$translation equivalents$textual entailment$true equivalents$temporal expression$Template Element$Textual Entailment$Term Extraction$Textual entailment$	3
Contradictions and jus- tifications: Extensions to the TE task.	TE	term$template element$translation equivalents$textual entailment$true equivalents$temporal expression$Template Element$Textual Entailment$Term Extraction$Textual entailment$	3
The setting is also appropriate for cases that may require making  global decisions that involve multiple components, possibly pre-designed or pre- learned, as in summarization, paraphrasing, TE and question answering.	TE	term$template element$translation equivalents$textual entailment$true equivalents$temporal expression$Template Element$Textual Entailment$Term Extraction$Textual entailment$	3
There is recent interest on the use of graph methods for Natural Language Processing, such as document summarisation (Mihalcea, 2004) doc- ument retrieval (Montes-y-Go?mez et al, 2000; Mishne, 2004), and recognition of TE (Pazienza et al, 2005).	TE	term$template element$translation equivalents$textual entailment$true equivalents$temporal expression$Template Element$Textual Entailment$Term Extraction$Textual entailment$	3
The false friend candidates were categorized in the three categories defined in Section 2: false friends, partial false friends and TE.	TE	term$template element$translation equivalents$textual entailment$true equivalents$temporal expression$Template Element$Textual Entailment$Term Extraction$Textual entailment$	4
The second, more important factor, was that the translated documents were not TE of the original Korean documents.	TE	term$template element$translation equivalents$textual entailment$true equivalents$temporal expression$Template Element$Textual Entailment$Term Extraction$Textual entailment$	4
The function of this module is to recog- nize numerical, TE and others like product number, telephone number, credit number or alphabets.	TE	term$template element$translation equivalents$textual entailment$true equivalents$temporal expression$Template Element$Textual Entailment$Term Extraction$Textual entailment$	5
Terms which are not related to the central themes of a text, such as TEs, will be given a lower weight.	TE	term$template element$translation equivalents$textual entailment$true equivalents$temporal expression$Template Element$Textual Entailment$Term Extraction$Textual entailment$	5
GUTime (Mani and Wilson, 2000) annotates TEs according to the TimeML schema and normalizes their values.	TE	term$template element$translation equivalents$textual entailment$true equivalents$temporal expression$Template Element$Textual Entailment$Term Extraction$Textual entailment$	5
We pre- pare a phrase lookup table consisting of 350 frozen  phrases and TEs which are identi- fied before the input text is parsed.	TE	term$template element$translation equivalents$textual entailment$true equivalents$temporal expression$Template Element$Textual Entailment$Term Extraction$Textual entailment$	5
The system achieves F-scores of 85% and 82% for identification and normalization of TEs, respectively.	TE	term$template element$translation equivalents$textual entailment$true equivalents$temporal expression$Template Element$Textual Entailment$Term Extraction$Textual entailment$	5
However, before parsing, the input text is passed to  the preprocessing unit, where we try to identify the  frozen phrases2 and TEs3 which  the syntactic parser is unable to identify.	TE	term$template element$translation equivalents$textual entailment$true equivalents$temporal expression$Template Element$Textual Entailment$Term Extraction$Textual entailment$	5
However, the flexible numerical and TE cannot be solved by the above two methods.	TE	term$template element$translation equivalents$textual entailment$true equivalents$temporal expression$Template Element$Textual Entailment$Term Extraction$Textual entailment$	5
The scores for TE and Template  Relations are also high enough to make the technology  reliable for use by analysts.	TE	term$template element$translation equivalents$textual entailment$true equivalents$temporal expression$Template Element$Textual Entailment$Term Extraction$Textual entailment$	6
Although the corefercnce scores are lower than  the TE scores, enough coreference is  being processed to achieve reliable results in Template  Element.	TE	term$template element$translation equivalents$textual entailment$true equivalents$temporal expression$Template Element$Textual Entailment$Term Extraction$Textual entailment$	6
TEs Task The overall score on this task was: P&R  66.75 2P&R  69.74 P&2R  64.01 This result is probably the most satisfying of the three MUC tasks that the system entered.	TE	term$template element$translation equivalents$textual entailment$true equivalents$temporal expression$Template Element$Textual Entailment$Term Extraction$Textual entailment$	6
Although the corefercnce scores are lower than  the TE scores, enough coreference is  being processed to achieve reliable results in Template  Eleme	TE	term$template element$translation equivalents$textual entailment$true equivalents$temporal expression$Template Element$Textual Entailment$Term Extraction$Textual entailment$	6
The TEs  extracted from newswire articles are indicative of the  content of the article for most purposes.	TE	term$template element$translation equivalents$textual entailment$true equivalents$temporal expression$Template Element$Textual Entailment$Term Extraction$Textual entailment$	6
As part of this research, the TE development keys were analyzed to determine how  often the descriptors of an organization and person are directly associated by syntax.	TE	term$template element$translation equivalents$textual entailment$true equivalents$temporal expression$Template Element$Textual Entailment$Term Extraction$Textual entailment$	6
Task TE Scenario Template Recall 71 31 Precision 83 68 F-Measure 76.50 42.73 Figure 5: NYU scores on MUC-7 tasks candidate.	TE	term$template element$translation equivalents$textual entailment$true equivalents$temporal expression$Template Element$Textual Entailment$Term Extraction$Textual entailment$	6
In ACL-PASCAL Workshop on TE and Paraphrasing, Prague, Czech Republic.	TE	term$template element$translation equivalents$textual entailment$true equivalents$temporal expression$Template Element$Textual Entailment$Term Extraction$Textual entailment$	7
In Proceedings of the ACL-PASCAL Workshop on TE and Paraphrasing, pages 1?9, Prague, June.	TE	term$template element$translation equivalents$textual entailment$true equivalents$temporal expression$Template Element$Textual Entailment$Term Extraction$Textual entailment$	7
With the introduction of the MSR alignment cor- pus (Brockett, 2007) developed from the second Recognizing TE challenge data (Bar- Haim et al.,	TE	term$template element$translation equivalents$textual entailment$true equivalents$temporal expression$Template Element$Textual Entailment$Term Extraction$Textual entailment$	7
The PASCAL Recognising TE Chal- lenge.,	TE	term$template element$translation equivalents$textual entailment$true equivalents$temporal expression$Template Element$Textual Entailment$Term Extraction$Textual entailment$	7
Meth- ods for Using TE in Open-Domain Question Answering.	TE	term$template element$translation equivalents$textual entailment$true equivalents$temporal expression$Template Element$Textual Entailment$Term Extraction$Textual entailment$	7
Ask not what TE can do for You...?	TE	term$template element$translation equivalents$textual entailment$true equivalents$temporal expression$Template Element$Textual Entailment$Term Extraction$Textual entailment$	7
Automatic  TE and Document Similarity in Special  Text Corpora.	TE	term$template element$translation equivalents$textual entailment$true equivalents$temporal expression$Template Element$Textual Entailment$Term Extraction$Textual entailment$	8
A Simple but  Powerful Automatic TE Method.	TE	term$template element$translation equivalents$textual entailment$true equivalents$temporal expression$Template Element$Textual Entailment$Term Extraction$Textual entailment$	8
Bilingual TE The last phase consists in finding the translation of the domain terminol- ogy.	TE	term$template element$translation equivalents$textual entailment$true equivalents$temporal expression$Template Element$Textual Entailment$Term Extraction$Textual entailment$	8
approach 2778 2688 2549 Multiterm Extract 1337 N/A N/A Table 4: Figures after Log-Likelihood and Mutual Expectation reduction Anchor chunk approach Correct Not correct Maybe correct Multiwords 78.5% 19% 2.5% Single words 89.5% 9.5% 1% All terms 83% 15% 2% Multiterm Extract Correct Not correct Maybe correct Multiwords 51% 48.5% 0.5% Single words 83% 16% 1% All terms 66% 33.5% 0.5% Table 5: Results TE Module ?	TE	term$template element$translation equivalents$textual entailment$true equivalents$temporal expression$Template Element$Textual Entailment$Term Extraction$Textual entailment$	8
1436 4 Chinese Key TE  In Web mining of English-Chinese OOV term  translation, an important problem is to extract  the target translation candidates from the re- turned Chinese Web documents, which can be  considered as a key term extraction task.	TE	term$template element$translation equivalents$textual entailment$true equivalents$temporal expression$Template Element$Textual Entailment$Term Extraction$Textual entailment$	8
It is  based on the complementary use of two  tools: (1) a TE tool that  acquires term candidates from tagged  corpora through a shallow grammar of  noun phrases, and (2) a Term Cluster-  ing tool that groups syntactic variants  (insertions).	TE	term$template element$translation equivalents$textual entailment$true equivalents$temporal expression$Template Element$Textual Entailment$Term Extraction$Textual entailment$	8
TE systems are often based on en- tailment rules which specify a directional inference relation between two fragments.	TE	term$template element$translation equivalents$textual entailment$true equivalents$temporal expression$Template Element$Textual Entailment$Term Extraction$Textual entailment$	9
TE resolution via atomic propositions.	TE	term$template element$translation equivalents$textual entailment$true equivalents$temporal expression$Template Element$Textual Entailment$Term Extraction$Textual entailment$	9
5.2 TE The Recognizing Textual Entailment Challenge (Dagan et al, 2005) is a task in which systems as- sess whether a sentence is entailed by a short pas- sage or sentence.	TE	term$template element$translation equivalents$textual entailment$true equivalents$temporal expression$Template Element$Textual Entailment$Term Extraction$Textual entailment$	9
TE at EVALITA 2009.	TE	term$template element$translation equivalents$textual entailment$true equivalents$temporal expression$Template Element$Textual Entailment$Term Extraction$Textual entailment$	9
TE through extended lexical overlap and lexico-semantic match- ing.	TE	term$template element$translation equivalents$textual entailment$true equivalents$temporal expression$Template Element$Textual Entailment$Term Extraction$Textual entailment$	9
TE recognition based on dependency analysis and WordNet.	TE	term$template element$translation equivalents$textual entailment$true equivalents$temporal expression$Template Element$Textual Entailment$Term Extraction$Textual entailment$	9
QAg from struc- tured knowledge sources.	QA	Question answerin$Question answering$question answering$Question Answering$	0
QAg in role-playing games.	QA	Question answerin$Question answering$question answering$Question Answering$	0
QAg using enhanced lexical se- mantic models.	QA	Question answerin$Question answering$question answering$Question Answering$	0
QAg in terminology-rich technical domains.	QA	Question answerin$Question answering$question answering$Question Answering$	0
Information extraction over structured data: QAg with freebase.	QA	Question answerin$Question answering$question answering$Question Answering$	0
QAg with subgraph embed- dings.	QA	Question answerin$Question answering$question answering$Question Answering$	0
QA from struc- tured knowledge sources.	QA	Question answerin$Question answering$question answering$Question Answering$	1
QA in role-playing games.	QA	Question answerin$Question answering$question answering$Question Answering$	1
QA using enhanced lexical se- mantic models.	QA	Question answerin$Question answering$question answering$Question Answering$	1
QA in terminology-rich technical domains.	QA	Question answerin$Question answering$question answering$Question Answering$	1
Information extraction over structured data: QA with freebase.	QA	Question answerin$Question answering$question answering$Question Answering$	1
QA with subgraph embed- dings.	QA	Question answerin$Question answering$question answering$Question Answering$	1
The tasks set in these conferences have molded a specific kind of QA that is easy to evaluate and that focuses on the use of fast and shallow methods that are generally independent of the application domain.	QA	Question answerin$Question answering$question answering$Question Answering$	2
A main characteristic of QA in restricted domains is the integration of domain-specific information that is either developed for QA or that has been developed for other purposes.	QA	Question answerin$Question answering$question answering$Question Answering$	2
A main characteristic of QA in restricted domains is the integration of domain-specific information that is either developed for QA or that has been developed for other pur	QA	Question answerin$Question answering$question answering$Question Answering$	2
article is on the use of restricted domains for automated QA.	QA	Question answerin$Question answering$question answering$Question Answering$	2
From this perspective, QA focuses on finding text excerpts that contain the answer within large collections of documents.	QA	Question answerin$Question answering$question answering$Question Answering$	2
The focus of this article is on the use of restricted domains for automated QA.	QA	Question answerin$Question answering$question answering$Question Answering$	2
University of Alicante, Spain Automated QA has been a topic of research and development since the earliest AI applications.	QA	Question answerin$Question answering$question answering$Question Answering$	2
The setting is also appropriate for cases that may require making  global decisions that involve multiple components, possibly pre-designed or pre- learned, as in summarization, paraphrasing, textual entailment and QA.	QA	Question answerin$Question answering$question answering$Question Answering$	2
and Vicedo QA in Restricted Domains: An Overview AQUA (Vargas-Vera, Motta, and Domingue 2003) combines knowledge encoded in a database with domain-related documents through an ontology that describes aca- demic life.	QA	Question answerin$Question answering$question answering$Question Answering$	3
96   Special Section on Restricted-Domain QA QA in Restricted Domains: An Overview Diego Molla??	QA	Question answerin$Question answering$question answering$Question Answering$	3
A good characterisation of their semantics can help identify referents in a given (con)text in dialog- based tasks, QA systems, or even advanced Information Extraction tasks.	QA	Question answerin$Question answering$question answering$Question Answering$	3
and Vicedo QA in Restricted Domains: An Overview tion prints words onto the screen?	QA	Question answerin$Question answering$question answering$Question Answering$	3
Currently we are witnessing a surge of activity in the area from the perspective of IR, initiated by the QA track of TREC1 in 1999 (Voorhees 2001).	QA	Question answerin$Question answering$question answering$Question Answering$	3
la- bels by the ITSPOKE semantic understanding com- ponent: Correct, Incorrect, PC.	PC	Partially Correct$pathway curation$prepositional complement$	0
Scoring involved comparing the summaries against the answer key (annotated passages from the source documents) while judging whether the summary provided a Correct, PC, or Missing answer.	PC	Partially Correct$pathway curation$prepositional complement$	0
Two accuracy metrics were defined, ARL (An-  swer Recall Lenient) and ARS (Answer Recall  Strict):  ARL = (nl + (.5 * n2))/n3 (4)  ARS = nl/n3 (5)  where nl is the number of Correct answers in the  summary, n2 is the number of PC  answers in the summary, and n3 is the number of  questions answered in the key.	PC	Partially Correct$pathway curation$prepositional complement$	0
Correct 903 463 164 309 78 PC 219 261 93 333 80 Contradictory 61 126 91 103 36 Irrelevant 209 229 119 476 189 Non-Domain 0 0 0 2 18 Table 2: Confusion matrix of Run 1 in the 5-way Unseen Domains scenario.	PC	Partially Correct$pathway curation$prepositional complement$	0
We also see that the system ability to distinguish Correct and PC answers need to be improved.	PC	Partially Correct$pathway curation$prepositional complement$	0
For system  responses, the evaluators categorized each response as  follows:  Answer: further evaluated as Correct, Incorrect  PC or Can't Decide;  System Initiated Directive: further evaluated as  Appropriate, Inappropriate, or Can't Decide;  Diagnostic Message: further evaluated as Appropriate,  Inappropriate, or Can't Decide;  Failure-to-Understand Message: no further evaluation.	PC	Partially Correct$pathway curation$prepositional complement$	0
Participants in these shared tasks have introduced dozens of sys- tems for event extraction, and the resulting methods have been applied to automatically analyse the entire available domain literature (Bjo?rne et al, 2010) and applied in support of applications such as semantic literature search (Ohta et al, 2010; Van Landeghem et al, 2011b) and PC support (Kemper et al, 2010).	PC	Partially Correct$pathway curation$prepositional complement$	1
While preserving the classic event extraction tasks such as the GE task, the BioNLP-ST 2013 broad- ens the scope of application domains by introducing many new issues in biology such as cancer genetics and PC.	PC	Partially Correct$pathway curation$prepositional complement$	1
In BioNLP-ST 2013 series, additional training data for PC including chemical entities is available.	PC	Partially Correct$pathway curation$prepositional complement$	1
Rather, we intend the term to refer to a set of tasks where information extraction/text mining methods are applied in some role to con- tribute directly to PC, including, for example, the identification of specific texts in the literature relevant to anno- tated reactions, the automatic suggestion of further entities or reactions to add to a pathway, or even the fully automatic gen- eration of entire pathways from scratch.	PC	Partially Correct$pathway curation$prepositional complement$	1
Overview of the PC (pc) task of bionlp shared task 2013.	PC	Partially Correct$pathway curation$prepositional complement$	1
Analogously, the French noun compounds (NCs) were extracted from Europarl using the following pattern: a noun followed by either an adjective or a PC14.	PC	Partially Correct$pathway curation$prepositional complement$	2
AI/.G 1 (loe.atio~t) AI{GI (locat~tm) AI{G2 ( locat~m) AI{G2 (location)  We %rthermore assume that the semantics of  the predicates include a pointer to the semantics of  the PCs hey license.	PC	Partially Correct$pathway curation$prepositional complement$	2
First, the type system is augmented to al-  low for declaring the property of being an optional  or an obligatory PC, as in  figure 1.	PC	Partially Correct$pathway curation$prepositional complement$	2
l'he contextual factor  that resolves the ambiguity is the semantics of the  head of the PC which here  is tt~ken to specii~y whether the direct ob.ieet of the  verb is understood as the location and the oblique  complement as the locatum or v\[ee versa.	PC	Partially Correct$pathway curation$prepositional complement$	2
whenever a verb is encoun- tered we generate templates that are paths be- tween v and the verb?s modifiers, either ob- jects, PCs or infinite or gerund verb forms (paths ending at stop words, e.g. pronouns, are not generated).	PC	Partially Correct$pathway curation$prepositional complement$	2
One of the properties of the verb  "to be" is that a PC qual-  ifying this verb really qualifies the subject of the  verb.	PC	Partially Correct$pathway curation$prepositional complement$	2
HMM word and phrase alignment for SMT.	SMT	statistical machine translation$Statistical Machine Translation$Statistical Machine Translatio$statistical MT$statistical MT systems$Structure Mapping Theory$Statistical machine translation$Statistical MT$	0
3 Conclusion In this paper, we described a phrase-based unigram model for SMT.	SMT	statistical machine translation$Statistical Machine Translation$Statistical Machine Translatio$statistical MT$statistical MT systems$Structure Mapping Theory$Statistical machine translation$Statistical MT$	0
Introduction The seminal work of Brown et al (1993b) introduced a series of probabilistic models (IBM Models 1?5) for SMT and the concept of ?	SMT	statistical machine translation$Statistical Machine Translation$Statistical Machine Translatio$statistical MT$statistical MT systems$Structure Mapping Theory$Statistical machine translation$Statistical MT$	0
Europarl: A parallel corpus for SMT.	SMT	statistical machine translation$Statistical Machine Translation$Statistical Machine Translatio$statistical MT$statistical MT systems$Structure Mapping Theory$Statistical machine translation$Statistical MT$	0
A hierarchical phrase-based model for SMT.	SMT	statistical machine translation$Statistical Machine Translation$Statistical Machine Translatio$statistical MT$statistical MT systems$Structure Mapping Theory$Statistical machine translation$Statistical MT$	0
The mathematics of SMT: Parameter estimation.	SMT	statistical machine translation$Statistical Machine Translation$Statistical Machine Translatio$statistical MT$statistical MT systems$Structure Mapping Theory$Statistical machine translation$Statistical MT$	0
The Mathe- matic of SMT: Parameter Estimation.	SMT	statistical machine translation$Statistical Machine Translation$Statistical Machine Translatio$statistical MT$statistical MT systems$Structure Mapping Theory$Statistical machine translation$Statistical MT$	1
We use a state-of-the-art machine translation system,5 and follow the experimental setup used for the 2008 shared task on machine translation (ACL 2008 Third Workshop on SMT).	SMT	statistical machine translation$Statistical Machine Translation$Statistical Machine Translatio$statistical MT$statistical MT systems$Structure Mapping Theory$Statistical machine translation$Statistical MT$	1
In Proceedings of the Third Workshop on SMT, pages 151?154, Columbus, Ohio, June.	SMT	statistical machine translation$Statistical Machine Translation$Statistical Machine Translatio$statistical MT$statistical MT systems$Structure Mapping Theory$Statistical machine translation$Statistical MT$	1
94   A Phrase-Based Unigram Model for SMT Christoph Tillmann and Fei Xia IBM T.J. Watson Research Center Yorktown Heights, NY 10598 {ctill,feixia}@us.ibm.com Abstract In this paper, we describe a phrase-based un- igram model for statistical machine transla- tion that uses a much simpler set of model parameters than similar phrase-based models.	SMT	statistical machine translation$Statistical Machine Translation$Statistical Machine Translatio$statistical MT$statistical MT systems$Structure Mapping Theory$Statistical machine translation$Statistical MT$	1
The Mathematics of SMT: Parameter Estima- tion.	SMT	statistical machine translation$Statistical Machine Translation$Statistical Machine Translatio$statistical MT$statistical MT systems$Structure Mapping Theory$Statistical machine translation$Statistical MT$	1
Distor- tion Models for SMT.	SMT	statistical machine translation$Statistical Machine Translation$Statistical Machine Translatio$statistical MT$statistical MT systems$Structure Mapping Theory$Statistical machine translation$Statistical MT$	1
The Mathe- matic of SMTn: Parameter Estimation.	SMT	statistical machine translation$Statistical Machine Translation$Statistical Machine Translatio$statistical MT$statistical MT systems$Structure Mapping Theory$Statistical machine translation$Statistical MT$	2
We use a state-of-the-art machine translation system,5 and follow the experimental setup used for the 2008 shared task on machine translation (ACL 2008 Third Workshop on SMTn).	SMT	statistical machine translation$Statistical Machine Translation$Statistical Machine Translatio$statistical MT$statistical MT systems$Structure Mapping Theory$Statistical machine translation$Statistical MT$	2
In Proceedings of the Third Workshop on SMTn, pages 151?154, Columbus, Ohio, June.	SMT	statistical machine translation$Statistical Machine Translation$Statistical Machine Translatio$statistical MT$statistical MT systems$Structure Mapping Theory$Statistical machine translation$Statistical MT$	2
94   A Phrase-Based Unigram Model for SMTn Christoph Tillmann and Fei Xia IBM T.J. Watson Research Center Yorktown Heights, NY 10598 {ctill,feixia}@us.ibm.com Abstract In this paper, we describe a phrase-based un- igram model for statistical machine transla- tion that uses a much simpler set of model parameters than similar phrase-based models.	SMT	statistical machine translation$Statistical Machine Translation$Statistical Machine Translatio$statistical MT$statistical MT systems$Structure Mapping Theory$Statistical machine translation$Statistical MT$	2
The Mathematics of SMTn: Parameter Estima- tion.	SMT	statistical machine translation$Statistical Machine Translation$Statistical Machine Translatio$statistical MT$statistical MT systems$Structure Mapping Theory$Statistical machine translation$Statistical MT$	2
Distor- tion Models for SMTn.	SMT	statistical machine translation$Statistical Machine Translation$Statistical Machine Translatio$statistical MT$statistical MT systems$Structure Mapping Theory$Statistical machine translation$Statistical MT$	2
Hence, SMT methods are not fea- sible under such a condition.	SMT	statistical machine translation$Statistical Machine Translation$Statistical Machine Translatio$statistical MT$statistical MT systems$Structure Mapping Theory$Statistical machine translation$Statistical MT$	3
Due to the difficulty in obtaining  parallel corpora of ISL, the SMT ap- proaches may not be a feasible solu	SMT	statistical machine translation$Statistical Machine Translation$Statistical Machine Translatio$statistical MT$statistical MT systems$Structure Mapping Theory$Statistical machine translation$Statistical MT$	3
00) have proposed an ap- proach to collect corpus for SMT research,  in his approach first, annotation standard for the  various hand shape movements was developed,  then the Sign Language performances were re- corded, and finally the recorded videos were  manually transcribed.	SMT	statistical machine translation$Statistical Machine Translation$Statistical Machine Translatio$statistical MT$statistical MT systems$Structure Mapping Theory$Statistical machine translation$Statistical MT$	3
For example, in SMT the translation model and the language model are treated separately, characterised as faithfulness and fluency respectively (as in the treatment in Jurafsky and Martin (2000)).	SMT	statistical machine translation$Statistical Machine Translation$Statistical Machine Translatio$statistical MT$statistical MT systems$Structure Mapping Theory$Statistical machine translation$Statistical MT$	3
Niedle et al (2000) have proposed an ap- proach to collect corpus for SMT research,  in his approach first, annotation standard for the  various hand shape movements was developed,  then the Sign Language performances were re- corded, and finally the recorded videos were  manually transcribed.	SMT	statistical machine translation$Statistical Machine Translation$Statistical Machine Translatio$statistical MT$statistical MT systems$Structure Mapping Theory$Statistical machine translation$Statistical MT$	3
Due to the difficulty in obtaining  parallel corpora of ISL, the SMT ap- proaches may not be a feasible solution to our  problem.	SMT	statistical machine translation$Statistical Machine Translation$Statistical Machine Translatio$statistical MT$statistical MT systems$Structure Mapping Theory$Statistical machine translation$Statistical MT$	3
2006) has  proposed a SMT system which uses Hid- den Markov Model and IBM models for training  the data.	SMT	statistical machine translation$Statistical Machine Translation$Statistical Machine Translatio$statistical MT$statistical MT systems$Structure Mapping Theory$Statistical machine translation$Statistical MT$	3
Leveraging multiple languages to improve SMT word alignments.	SMT	statistical machine translation$Statistical Machine Translation$Statistical Machine Translatio$statistical MT$statistical MT systems$Structure Mapping Theory$Statistical machine translation$Statistical MT$	3
We assume here that the MT system is capable of providing word alignment (or equiva- lent) information during decoding, which is gener- ally true for current SMT.	SMT	statistical machine translation$Statistical Machine Translation$Statistical Machine Translatio$statistical MT$statistical MT systems$Structure Mapping Theory$Statistical machine translation$Statistical MT$	4
In SMT, au- tomatic evaluation of translations is essential for parameter optimization and system development.	SMT	statistical machine translation$Statistical Machine Translation$Statistical Machine Translatio$statistical MT$statistical MT systems$Structure Mapping Theory$Statistical machine translation$Statistical MT$	4
We also included two commercial off-the-shelf MT systems, two online SMT, and five online rule-based MT systems. (	SMT	statistical machine translation$Statistical Machine Translation$Statistical Machine Translatio$statistical MT$statistical MT systems$Structure Mapping Theory$Statistical machine translation$Statistical MT$	4
Compared to the general re-ordering models used in SMT, this type of feature is capable of modeling skeleton-level re-ordering, which is crucial to the fluency of MT output.	SMT	statistical machine translation$Statistical Machine Translation$Statistical Machine Translatio$statistical MT$statistical MT systems$Structure Mapping Theory$Statistical machine translation$Statistical MT$	4
We propose and evaluate three computationally efficient online methods for updating SMT in a scenario where post-edited MT output is constantly being returned to the system: (1) adding new rules to the translation model from the post-edited content, (2) updating a Bayesian language model of the target language that is used by the MT system, and (3) updating the MT system?s discriminative parameters with a MIRA step.	SMT	statistical machine translation$Statistical Machine Translation$Statistical Machine Translatio$statistical MT$statistical MT systems$Structure Mapping Theory$Statistical machine translation$Statistical MT$	4
We want to balance the disadvantage of rule- based systems with respect to lexical coverage when compared to SMT trained on large scale corpora.	SMT	statistical machine translation$Statistical Machine Translation$Statistical Machine Translatio$statistical MT$statistical MT systems$Structure Mapping Theory$Statistical machine translation$Statistical MT$	4
Task-Specific Alignment Evaluation In this section we evaluate the alignments resulting from using the proposed constraints in two different tasks: SMT where alignments are used to restrict the number of possible minimal translation units; and syntax transfer, where alignments are used to decide how to transfer dependency links.	SMT	statistical machine translation$Statistical Machine Translation$Statistical Machine Translatio$statistical MT$statistical MT systems$Structure Mapping Theory$Statistical machine translation$Statistical MT$	6
SMT with word- and sentence-aligned parallel corpora.	SMT	statistical machine translation$Statistical Machine Translation$Statistical Machine Translatio$statistical MT$statistical MT systems$Structure Mapping Theory$Statistical machine translation$Statistical MT$	6
SMT in its origi- nal formulation disregarded the actual forms of words, focusing instead exclusively on their co- occurrence patterns.	SMT	statistical machine translation$Statistical Machine Translation$Statistical Machine Translatio$statistical MT$statistical MT systems$Structure Mapping Theory$Statistical machine translation$Statistical MT$	6
2 Background SMT is a decision prob- lem where we need decide on the best of target sentence matching a source sentence.	SMT	statistical machine translation$Statistical Machine Translation$Statistical Machine Translatio$statistical MT$statistical MT systems$Structure Mapping Theory$Statistical machine translation$Statistical MT$	6
SMT and example- based machine translation require numerous high-quality bilingual corpora.	SMT	statistical machine translation$Statistical Machine Translation$Statistical Machine Translatio$statistical MT$statistical MT systems$Structure Mapping Theory$Statistical machine translation$Statistical MT$	6
SMT ex- perimental results corroborate that the result- ing high-n approximate small language model is as effective as models obtained from other count pruning methods.	SMT	statistical machine translation$Statistical Machine Translation$Statistical Machine Translatio$statistical MT$statistical MT systems$Structure Mapping Theory$Statistical machine translation$Statistical MT$	6
The underlying  MT architecture can be classified into i) Direct  translation system, ii) Transfer based architecture  and iii) SMT.	SMT	statistical machine translation$Statistical Machine Translation$Statistical Machine Translatio$statistical MT$statistical MT systems$Structure Mapping Theory$Statistical machine translation$Statistical MT$	7
of the 3rd Workshop on SMT, Columbus, Ohio, 115?118.	SMT	statistical machine translation$Statistical Machine Translation$Statistical Machine Translatio$statistical MT$statistical MT systems$Structure Mapping Theory$Statistical machine translation$Statistical MT$	7
SMT: It requires large parallel cor- pora    which is very difficult to collect.	SMT	statistical machine translation$Statistical Machine Translation$Statistical Machine Translatio$statistical MT$statistical MT systems$Structure Mapping Theory$Statistical machine translation$Statistical MT$	7
so like to thank the anonymous re-viewers for their comments, the providers of the NIST MT evaluation tool, and the organizers of the Third Workshop on SMT for making available the News Corpus.	SMT	statistical machine translation$Statistical Machine Translation$Statistical Machine Translatio$statistical MT$statistical MT systems$Structure Mapping Theory$Statistical machine translation$Statistical MT$	7
A Decoder for Syntax-based SMT.	SMT	statistical machine translation$Statistical Machine Translation$Statistical Machine Translatio$statistical MT$statistical MT systems$Structure Mapping Theory$Statistical machine translation$Statistical MT$	7
of the 3rd Workshop on SMT, Columbus, 70?106.	SMT	statistical machine translation$Statistical Machine Translation$Statistical Machine Translatio$statistical MT$statistical MT systems$Structure Mapping Theory$Statistical machine translation$Statistical MT$	7
We would also like to thank the anonymous re-viewers for their comments, the providers of the NIST MT evaluation tool, and the organizers of the Third Workshop on SMT for making available the News Corpus.	SMT	statistical machine translation$Statistical Machine Translation$Statistical Machine Translatio$statistical MT$statistical MT systems$Structure Mapping Theory$Statistical machine translation$Statistical MT$	7
4.1 Acoustic Confirmation Threshold When ASR produces a hypothesis of what has been said, it also returns an acoustic confidence score which the application can utilize to decide whether to reject the utterance, confirm it, or accept it right away.	ASR	a speech recognizer$automatic speech recognition$Automatic Speech Recognition$	0
Furthermore, although we have not yet exper-  imented with the pruned grammar on spoken input, we  expect that the pruned grammar will improve the abil-  ity of PUNDIT to reject ungrammatical candidates from  ASR.	ASR	a speech recognizer$automatic speech recognition$Automatic Speech Recognition$	0
In particular,  we use a dynamic programming tabular algorithm  to find the minimal cost transduction of a word  string or word-lattice from ASR.	ASR	a speech recognizer$automatic speech recognition$Automatic Speech Recognition$	0
Finally, it should also be pos-  sible to embed our phonetic shift model P(jle) in-  side ASR, to help adjust for a heavy  Japanese accent, although we have not experimented  in this area.	ASR	a speech recognizer$automatic speech recognition$Automatic Speech Recognition$	0
Estimation of probabilities from  sparse data for the language model compo-  nent of ASR.	ASR	a speech recognizer$automatic speech recognition$Automatic Speech Recognition$	0
Estimation of probabilities from  sparse data for the language model component  of ASR.	ASR	a speech recognizer$automatic speech recognition$Automatic Speech Recognition$	0
For data col-  lected at SRI, this was true; all other sites used some  ASR and/or natural language  understanding, with varying amounts of human tran-  scription and error correction.	ASR	a speech recognizer$automatic speech recognition$Automatic Speech Recognition$	1
For ASR experiments, we used as test set the 1997 Hub4 evaluation set consisting of 32,689 words.	ASR	a speech recognizer$automatic speech recognition$Automatic Speech Recognition$	1
Minimum Bayes risk ASR.	ASR	a speech recognizer$automatic speech recognition$Automatic Speech Recognition$	1
The most salient difference, however, is that they are motivated by the goal of improving ASR, and have an acoustic signal parallel to the undiacritized text.	ASR	a speech recognizer$automatic speech recognition$Automatic Speech Recognition$	1
1 Introduction CRIM?s ASR system has been applied to live closed-captioning of french- canadian television programs (Boulianne et al, 2006).	ASR	a speech recognizer$automatic speech recognition$Automatic Speech Recognition$	1
THE SPEECH RECOGNITION PROBLEM  Natural language ASR typically proceeds as follows.	ASR	a speech recognizer$automatic speech recognition$Automatic Speech Recognition$	1
We use a state of the art ASR system to transcribe the calls between agents and customers, which still results in high word error rates (40%) and show that even from these noisy transcrip- tions of calls we can automatically build a domain model.	ASR	a speech recognizer$automatic speech recognition$Automatic Speech Recognition$	2
Wald, M. 2006 Creating Accessible Educational Multi- media through Editing ASR Captioning in Real Time.	ASR	a speech recognizer$automatic speech recognition$Automatic Speech Recognition$	2
In ASR and Understanding.	ASR	a speech recognizer$automatic speech recognition$Automatic Speech Recognition$	2
Difficulties in ASR of Dysarthric Speakers and Implications for Speech-Based Applications Used by the Elderly: A Literature Review.	ASR	a speech recognizer$automatic speech recognition$Automatic Speech Recognition$	2
Active learning for rule-based and corpus-based  Spoken Language Understanding models, In Proceedings of IEEE Conference on ASR  and Understanding, pp.	ASR	a speech recognizer$automatic speech recognition$Automatic Speech Recognition$	2
1.2 Motivation Syntactic parsing is essential for many natural lan- guage applications such as Machine Translation, Question Answering, Information Extraction, Infor- mation Retrieval, ASR.	ASR	a speech recognizer$automatic speech recognition$Automatic Speech Recognition$	2
SR grammar com- pilation in Grammatical Framework.	SR	Speech recognition$summarization ratio$synkam$search$Separation Ratio$SEMANTIC ROLE$semantic relations$Serbian$semantic roles$semantic relatedness$	0
The parser is coded in C. SR is performed  by a Verbex 6000 user-dependent connected-speech recognizer unning on an IBM  PC, and the vocabulary is currently restricted to 125 words.	SR	Speech recognition$summarization ratio$synkam$search$Separation Ratio$SEMANTIC ROLE$semantic relations$Serbian$semantic roles$semantic relatedness$	0
SR and dysarthria: a single subject study of two individuals with profound impairment of speech and motor control.	SR	Speech recognition$summarization ratio$synkam$search$Separation Ratio$SEMANTIC ROLE$semantic relations$Serbian$semantic roles$semantic relatedness$	0
INTRODUCTION SR has advanced considerably, but has been lim- ited almost entirely either to situations in which close speaking mi- crophones are natural and acceptable (telephone, dictation, com- mand&control, etc.)	SR	Speech recognition$summarization ratio$synkam$search$Separation Ratio$SEMANTIC ROLE$semantic relations$Serbian$semantic roles$semantic relatedness$	0
SR  by composition of weighted finite automata.	SR	Speech recognition$summarization ratio$synkam$search$Separation Ratio$SEMANTIC ROLE$semantic relations$Serbian$semantic roles$semantic relatedness$	0
Error(%)  62.5 98.3  51.0 93.9  55.4 97.8  51.0 98.1  24.1 72.7  22.0 66.9  20.6 67.7  19.3 61.6  18.8 58.6  Table 1: SR results on the October '91 test set for the various experiments described in this paper.	SR	Speech recognition$summarization ratio$synkam$search$Separation Ratio$SEMANTIC ROLE$semantic relations$Serbian$semantic roles$semantic relatedness$	0
More concretely, we assume that the summary  81 sentences of a given document can be iteratively  chosen (i.e., one at each iteration) from the doc- ument until the aggregated summary reaches a  predefined target SR.	SR	Speech recognition$summarization ratio$synkam$search$Separation Ratio$SEMANTIC ROLE$semantic relations$Serbian$semantic roles$semantic relatedness$	1
The SR, defined as the ratio of  the number of words in the automatic (or manual)  summary to that in the reference transcript of a  spoken document, was set to 10% in this re- search.	SR	Speech recognition$summarization ratio$synkam$search$Separation Ratio$SEMANTIC ROLE$semantic relations$Serbian$semantic roles$semantic relatedness$	1
We have implemented the approach and evaluated in SR and their correctness.	SR	Speech recognition$summarization ratio$synkam$search$Separation Ratio$SEMANTIC ROLE$semantic relations$Serbian$semantic roles$semantic relatedness$	1
We computed the SR about these sentences.	SR	Speech recognition$summarization ratio$synkam$search$Separation Ratio$SEMANTIC ROLE$semantic relations$Serbian$semantic roles$semantic relatedness$	1
2 Evaluation Metrics for Extraction In summarization through sentence or word extrac- tion under a specific SR, the order of the sentences or words and the length of the sum- maries are restricted by the original documents or sentences.	SR	Speech recognition$summarization ratio$synkam$search$Separation Ratio$SEMANTIC ROLE$semantic relations$Serbian$semantic roles$semantic relatedness$	1
The SR is 94%.	SR	Speech recognition$summarization ratio$synkam$search$Separation Ratio$SEMANTIC ROLE$semantic relations$Serbian$semantic roles$semantic relatedness$	1
This algorithm SRes the parse tree in a left-  to-right, breadth-first fashion that obeys the  major reflexive pronoun constraints while giv-  ing a preference to antecedents hat are closer  to the pronoun.	SR	Speech recognition$summarization ratio$synkam$search$Separation Ratio$SEMANTIC ROLE$semantic relations$Serbian$semantic roles$semantic relatedness$	3
Since the reSR described  herein we have thought of other influences on  anaphora resolution and their statistical corre-  lates.	SR	Speech recognition$summarization ratio$synkam$search$Separation Ratio$SEMANTIC ROLE$semantic relations$Serbian$semantic roles$semantic relatedness$	3
7 Prev ious  Work   The literature on pronoun anaphora is too ex-  tensive to summarize, so we concentrate here on  corpus-based anaphora reSR.	SR	Speech recognition$summarization ratio$synkam$search$Separation Ratio$SEMANTIC ROLE$semantic relations$Serbian$semantic roles$semantic relatedness$	3
8 Conc lus t ion  and  Future  ReSR   We have presented a statistical method for  pronominal anaphora that achieves an accuracy  of 84.2%.	SR	Speech recognition$summarization ratio$synkam$search$Separation Ratio$SEMANTIC ROLE$semantic relations$Serbian$semantic roles$semantic relatedness$	3
In resolving inter-sentential  pronouns, the algorithm SRes the previous  sentence, again in left-to-right, breadth-first or-  der.	SR	Speech recognition$summarization ratio$synkam$search$Separation Ratio$SEMANTIC ROLE$semantic relations$Serbian$semantic roles$semantic relatedness$	3
This too is a topic for  future reSR.	SR	Speech recognition$summarization ratio$synkam$search$Separation Ratio$SEMANTIC ROLE$semantic relations$Serbian$semantic roles$semantic relatedness$	3
This pa-per has presented a novel representation of such dialogue with a tutoring domain, and has pre-sented and evaluated a feature selection method based on a new SR metric, which can inform the development of turn-taking poli-cies in dialogue systems.	SR	Speech recognition$summarization ratio$synkam$search$Separation Ratio$SEMANTIC ROLE$semantic relations$Serbian$semantic roles$semantic relatedness$	4
2.1 GROUPING SRS  Both on implementational nd on  theoretical grounds, we have grouped  certain semantic roles into superclasses.	SR	Speech recognition$summarization ratio$synkam$search$Separation Ratio$SEMANTIC ROLE$semantic relations$Serbian$semantic roles$semantic relatedness$	5
The semantic interpretation of syntactic  subjects and objects, of prepositions and  subordinate conjunctions has been treated in  numerous books and papers with titles  including words like DEEP CASES, CASE ROLES,  SRS and SEMANTIC RELATIONS.	SR	Speech recognition$summarization ratio$synkam$search$Separation Ratio$SEMANTIC ROLE$semantic relations$Serbian$semantic roles$semantic relatedness$	5
The arguments to the  predicates constitute the SRS of the  verb, which are similar to cases.	SR	Speech recognition$summarization ratio$synkam$search$Separation Ratio$SEMANTIC ROLE$semantic relations$Serbian$semantic roles$semantic relatedness$	5
C lause AJtalysls AlKorlChm  DECOMPOSE NOMINALIZATION  FOR EACH SEMANTIC  ROLE:  IF THERE ARE SYNTACTIC CONSTITUENTS -  PROPOSE SYNTACTIC CONSTITUENT FILLER  & CALL REFERENCE RESOLUTION  & TEST SELECTIONAL RESTRICTIONS  CALL TEMPORAL ANALYSIS ON DECOMPOSITION  CALL REFERENCE RESOLUTION FOR NOMINALIZATION NOUN PHRASE  FOR EACH SR:  IF ESSENTIAL ROLE AND UNFILLED  CALL REFERENCE RESOLUTION TO HYPOTHESIZE A FILLER  TEST SELECTIONAL RESTRICTIONS  ELSE LEAVE UNFILLED  FJKure 2.	SR	Speech recognition$summarization ratio$synkam$search$Separation Ratio$SEMANTIC ROLE$semantic relations$Serbian$semantic roles$semantic relatedness$	5
The correlation between the valence slots of AdvD and the main  verb can be formulated IN TERMS OF SRS as follows: if a valence slot of AdvD which  corresponds to semantic role R (Agent, Theme, Recipient) is instantiated, it is either filled by an AdvD  dependent (as in v podarok do?eri ?	SR	Speech recognition$summarization ratio$synkam$search$Separation Ratio$SEMANTIC ROLE$semantic relations$Serbian$semantic roles$semantic relatedness$	5
The arguments of the predicates  constitute the SRS of the verb, which  are slml\]ar to cases 4For example, fall decomposes  into become inoperat lve ,  with pat ient  as its  only semantic role.	SR	Speech recognition$summarization ratio$synkam$search$Separation Ratio$SEMANTIC ROLE$semantic relations$Serbian$semantic roles$semantic relatedness$	5
Research on this field is not new and varied methods have been proposed to achieve dif- ferent steps of this task including the extraction of SR (e.g. (Hearst, 1992) (Girju et al.,	SR	Speech recognition$summarization ratio$synkam$search$Separation Ratio$SEMANTIC ROLE$semantic relations$Serbian$semantic roles$semantic relatedness$	6
2009) is a lexical network consisting of triples denoting SR between words found in a dictionary.	SR	Speech recognition$summarization ratio$synkam$search$Separation Ratio$SEMANTIC ROLE$semantic relations$Serbian$semantic roles$semantic relatedness$	6
Most previous ap- proaches to this problem have focused on the interpre- tation of two word compounds whose nouns are related via a basic set of SR (e.g., CAUSE relates onion tears, FOR relates pet spray).	SR	Speech recognition$summarization ratio$synkam$search$Separation Ratio$SEMANTIC ROLE$semantic relations$Serbian$semantic roles$semantic relatedness$	6
We have already made a first approach on the extraction of relational triples from text, where, likewise Hearst (1992), we take advantage of textual patterns indicating SR.	SR	Speech recognition$summarization ratio$synkam$search$Separation Ratio$SEMANTIC ROLE$semantic relations$Serbian$semantic roles$semantic relatedness$	6
By recasting the interpretation problem in terms of paraphrasing, Lauer assumes that the SR of compound heads and modifiers can be expressed via prepositions that (in contrast to abstract SR) can be found in a corpus.	SR	Speech recognition$summarization ratio$synkam$search$Separation Ratio$SEMANTIC ROLE$semantic relations$Serbian$semantic roles$semantic relatedness$	6
Computer Science  University of Salford, UK  I.Spasic@salford.ac.uk Sophia Ananiadou  Computer Science   University of Salford, UK  S.Ananiadou@salford.ac.uk     Abstract  In this paper we discuss morpho-syntactic  clues that can be used to facilitate termi- nological processing in SR.	SR	Speech recognition$summarization ratio$synkam$search$Separation Ratio$SEMANTIC ROLE$semantic relations$Serbian$semantic roles$semantic relatedness$	7
985  Morpho-syntactic Clues for Terminological Processing in SR  Goran Nenadi?	SR	Speech recognition$summarization ratio$synkam$search$Separation Ratio$SEMANTIC ROLE$semantic relations$Serbian$semantic roles$semantic relatedness$	7
3.1 Term formation patterns  As a rule, the vast majority of multiword terms in  SR match the following general formation  pattern:4    (1)           (Adj | ProAdj | Num | Noun )+ Noun    which has been used for recognition of NPs in  SR (Nenadi?	SR	Speech recognition$summarization ratio$synkam$search$Separation Ratio$SEMANTIC ROLE$semantic relations$Serbian$semantic roles$semantic relatedness$	7
In Sec- tion 3 we discuss morpho-syntactic clues, the  normalisation approach and the foreign word  recognition that are used for singling out terms in  SR.	SR	Speech recognition$summarization ratio$synkam$search$Separation Ratio$SEMANTIC ROLE$semantic relations$Serbian$semantic roles$semantic relatedness$	7
3.1 Term formation patterns  As a rule, the vast majority of multiword terms in  SR match the following general formation  pattern:4    (1)           (Adj | ProAdj | Num | Noun )+ Noun    which has	SR	Speech recognition$summarization ratio$synkam$search$Separation Ratio$SEMANTIC ROLE$semantic relations$Serbian$semantic roles$semantic relatedness$	7
3.1 Term formation patterns  As a rule, the vast majority of multiword terms in  SR match the following general formation  pattern:4    (1)	SR	Speech recognition$summarization ratio$synkam$search$Separation Ratio$SEMANTIC ROLE$semantic relations$Serbian$semantic roles$semantic relatedness$	7
3 Morpho-syntactic clues for extraction  of terms in SR  In order to adjust the core C-value method for  SR, we have defined an appropriate set of  morpho-syntactic filters and rules for inflectional  normalisation of term candidates, and, addition- ally, a module for foreign word recognition.	SR	Speech recognition$summarization ratio$synkam$search$Separation Ratio$SEMANTIC ROLE$semantic relations$Serbian$semantic roles$semantic relatedness$	7
means a chunk in NomBank or PropBank, which demonstrates how SR occur around a specified predicate.	SR	Speech recognition$summarization ratio$synkam$search$Separation Ratio$SEMANTIC ROLE$semantic relations$Serbian$semantic roles$semantic relatedness$	8
We decom- pose our task into three phases: identify- ing an opinion-bearing word, labeling  SR related to the word in the  sentence, and then finding the holder and  the topic of the opinion word among the  labeled SR.	SR	Speech recognition$summarization ratio$synkam$search$Separation Ratio$SEMANTIC ROLE$semantic relations$Serbian$semantic roles$semantic relatedness$	8
Frey B.J. 1998, Graphical Models for Machine  Learning and Digital Communication,  Cambridge, MA, MIT Press  Gildea D., Jurafsky D. 2002, Automatic labeling of  SR, Computational Linguistics,  28(3):245-288.	SR	Speech recognition$summarization ratio$synkam$search$Separation Ratio$SEMANTIC ROLE$semantic relations$Serbian$semantic roles$semantic relatedness$	8
Their work  looked at the problem of assigning SR to  text based on a statistical model of the FrameNet1  data.	SR	Speech recognition$summarization ratio$synkam$search$Separation Ratio$SEMANTIC ROLE$semantic relations$Serbian$semantic roles$semantic relatedness$	8
The proposition bank: An annotated cor- pus of SR.	SR	Speech recognition$summarization ratio$synkam$search$Separation Ratio$SEMANTIC ROLE$semantic relations$Serbian$semantic roles$semantic relatedness$	8
We propose an alternative, parameter-free method in which we model the co- hesive structure of a discourse as a graph structure (called cohesion graph), where the vertices of the graph correspond to the content words of the text and the edges encode the SR be- tween pairs of words.	SR	Speech recognition$summarization ratio$synkam$search$Separation Ratio$SEMANTIC ROLE$semantic relations$Serbian$semantic roles$semantic relatedness$	9
The value of e ij rep- resents the SR of the two tokens t i , t j that e ij connects: e ij = h(t i , t j ) (5) where h is a SR assignment function.	SR	Speech recognition$summarization ratio$synkam$search$Separation Ratio$SEMANTIC ROLE$semantic relations$Serbian$semantic roles$semantic relatedness$	9
id i , id j ) (8) As the SR among the MWE component words does not contain any informa- tion of how these component words are seman- tically involved in the context, we do not count the edges between the MWE component words 77 (as e 45 in Figure 1).	SR	Speech recognition$summarization ratio$synkam$search$Separation Ratio$SEMANTIC ROLE$semantic relations$Serbian$semantic roles$semantic relatedness$	9
e i is the average SR of the to- ken t i in the discourse.	SR	Speech recognition$summarization ratio$synkam$search$Separation Ratio$SEMANTIC ROLE$semantic relations$Serbian$semantic roles$semantic relatedness$	9
Figure 1: Cohesion Graph for identifying literal or non-literal usage of MWEs 78 4 Modeling Semantic Relatedness In Section 3.1, we did not define how we model the SR between two tokens (h(t i , t j )).	SR	Speech recognition$summarization ratio$synkam$search$Separation Ratio$SEMANTIC ROLE$semantic relations$Serbian$semantic roles$semantic relatedness$	9
Modeling SR be- tween two terms is currently an area of active re- search.	SR	Speech recognition$summarization ratio$synkam$search$Separation Ratio$SEMANTIC ROLE$semantic relations$Serbian$semantic roles$semantic relatedness$	9
2 The DALE System 2.1 Automatically Labeling Examples DALE assigns Concept Unique Identifiers (CUIs) from the UMLS MT.	MT	Metathesaurus$machine translation test$machine translat$Madnani et$machine translation$Machine translation$Machine Trans lat ion$Machine Translation$	0
The difference be- tween the two approaches is that they make use of different information from the MT.	MT	Metathesaurus$machine translation test$machine translat$Madnani et$machine translation$Machine translation$Machine Trans lat ion$Machine Translation$	0
DALE is 1 able to identify a meaning for any term that is am- biguous in the MT and therefore has far greater coverage of ambiguous terms than other su- pervised WSD systems.	MT	Metathesaurus$machine translation test$machine translat$Madnani et$machine translation$Machine translation$Machine Trans lat ion$Machine Translation$	0
DALE uses the UMLS MT as both a sense inventory and as a source of infor- mation for automatically generating labeled training examples.	MT	Metathesaurus$machine translation test$machine translat$Madnani et$machine translation$Machine translation$Machine Trans lat ion$Machine Translation$	0
Both approaches take a single CUI, c, as input and use information from the UMLS MT to search Medline and identi	MT	Metathesaurus$machine translation test$machine translat$Madnani et$machine translation$Machine translation$Machine Trans lat ion$Machine Translation$	0
For biomedical documents the UMLS MT (Humphreys et al 1998b) is a more suitable lexical resource than WordNet and tech- niques have been developed to create automatically labeled examples for this resource (Stevenson and Guo, 2010).	MT	Metathesaurus$machine translation test$machine translat$Madnani et$machine translation$Machine translation$Machine Trans lat ion$Machine Translation$	0
Both approaches take a single CUI, c, as input and use information from the UMLS MT to search Medline and identify instances of c that can be used as labeled examples.	MT	Metathesaurus$machine translation test$machine translat$Madnani et$machine translation$Machine translation$Machine Trans lat ion$Machine Translation$	0
DALE (Disambiguation using Automatically La- beled Examples) is an online WSD system for biomedical documents that was developed by creat- ing automatically labeled examples for all ambigu- ous terms in the UMLS MT.	MT	Metathesaurus$machine translation test$machine translat$Madnani et$machine translation$Machine translation$Machine Trans lat ion$Machine Translation$	0
t Unique Identifiers (CUIs) from the UMLS MT.	MT	Metathesaurus$machine translation test$machine translat$Madnani et$machine translation$Machine translation$Machine Trans lat ion$Machine Translation$	0
Both approaches are provided with a set of ambiguous CUIs from the UMLS MT, which represent the possible meanings of an am- biguous term, and a target number of training ex- am	MT	Metathesaurus$machine translation test$machine translat$Madnani et$machine translation$Machine translation$Machine Trans lat ion$Machine Translation$	0
Performance on MT sets was the lowest that can be explained by very poor quality of the training data5: models for these subsets should have been trained separately.	MT	Metathesaurus$machine translation test$machine translat$Madnani et$machine translation$Machine translation$Machine Trans lat ion$Machine Translation$	1
On NIST MT sets, our reordering model  achieved a 0.6-1.2 BLEU point improvements for Chinese-English translation over a strong baseline  hierarchical phrase-based system.	MT	Metathesaurus$machine translation test$machine translat$Madnani et$machine translation$Machine translation$Machine Trans lat ion$Machine Translation$	1
We integrate our method into a state-of-the-art phrase-based baseline translation system, i.e., Moses (Koehn et al, 2007), and show that the integrated system consistently im- proves the performance of the baseline system on various NIST MT sets.	MT	Metathesaurus$machine translation test$machine translat$Madnani et$machine translation$Machine translation$Machine Trans lat ion$Machine Translation$	1
We performed experiments to eliminate the pos- sibility of data overlap between the training data and the MT data as cause for the large improvements.	MT	Metathesaurus$machine translation test$machine translat$Madnani et$machine translation$Machine translation$Machine Trans lat ion$Machine Translation$	1
4 Resu l ts  and  D iscuss ion   The algorithm was tested on a 3700 sentence  MT set of Japanese sen-  tences with English translations, produced by  a professional human translator.	MT	Metathesaurus$machine translation test$machine translat$Madnani et$machine translation$Machine translation$Machine Trans lat ion$Machine Translation$	1
On NIST MT sets,  our reordering model achieved a 0.6-1.2 BLEU point improvements for Chinese-English translation  over a strong baseline hierarchical phrase-based system.	MT	Metathesaurus$machine translation test$machine translat$Madnani et$machine translation$Machine translation$Machine Trans lat ion$Machine Translation$	1
It is compati- ble with the broader range of DELPH-IN tools, e.g., for MTion (L?nning and Oepen, 2006), treebanking (Oepen et al, 2004) and parse selection (Toutanova et al, 2005).	MT	Metathesaurus$machine translation test$machine translat$Madnani et$machine translation$Machine translation$Machine Trans lat ion$Machine Translation$	2
Introduction The seminal work of Brown et al (1993b) introduced a series of probabilistic models (IBM Models 1?5) for statistical MTion and the concept of ?	MT	Metathesaurus$machine translation test$machine translat$Madnani et$machine translation$Machine translation$Machine Trans lat ion$Machine Translation$	2
They used this information to re- attach PPs in a MTion system, report- ing an improvement in translation quality when translating into Japanese (where PP attachment is not ambiguous and therefore matters) and a de- crease when translating into Spanish (where at- tachment ambiguities are close to the original ones and therefore need not be resolved).	MT	Metathesaurus$machine translation test$machine translat$Madnani et$machine translation$Machine translation$Machine Trans lat ion$Machine Translation$	2
Re-usable tools for precision MTion.	MT	Metathesaurus$machine translation test$machine translat$Madnani et$machine translation$Machine translation$Machine Trans lat ion$Machine Translation$	2
Confidence es- timation for MTion.	MT	Metathesaurus$machine translation test$machine translat$Madnani et$machine translation$Machine translation$Machine Trans lat ion$Machine Translation$	2
We also report experiments on two different tasks where word alignments are required: phrase-based MTion and syntax transfer, and show promising improvements over standard methods.	MT	Metathesaurus$machine translation test$machine translat$Madnani et$machine translation$Machine translation$Machine Trans lat ion$Machine Translation$	2
Nevertheless, they are key to various NLP applications, including those benefiting from deep natural language understanding (e.g., textual inference (Bobrow et al, 2007)), generation of well- formed output (e.g., natural language weather alert systems (Lareau and Wanner, 2007)) or both (as in MTion (Oepen et al, 2007)).	MT	Metathesaurus$machine translation test$machine translat$Madnani et$machine translation$Machine translation$Machine Trans lat ion$Machine Translation$	2
Annotation Lastly, we believe that it is important to provide alternative corrections, as the agreement on what constitutes a mistake even among native English speakers can be quite low (MT al 2011).	MT	Metathesaurus$machine translation test$machine translat$Madnani et$machine translation$Machine translation$Machine Trans lat ion$Machine Translation$	3
Although there has been some recent work on paraphrasing that provided detailed error analysis of system outputs (Socher et al, 2011; MT al, 2012), more often than not such investigations are seen as above-and-beyond when assessing metrics.	MT	Metathesaurus$machine translation test$machine translat$Madnani et$machine translation$Machine translation$Machine Trans lat ion$Machine Translation$	3
Sentence compression is valuable in many applica- tions, for example when displaying texts on small screens (Corston-Oliver, 2001), in subtitle genera- tion (Vandeghinste and Pan, 2004), and in text sum- marization (MT al, 2007).	MT	Metathesaurus$machine translation test$machine translat$Madnani et$machine translation$Machine translation$Machine Trans lat ion$Machine Translation$	3
distance: This metric has been widely used in evaluation of sentence ordering (Lapata, 2003; Lapata, 2006; Bollegala et al 2006; MT al 2007)1.	MT	Metathesaurus$machine translation test$machine translat$Madnani et$machine translation$Machine translation$Machine Trans lat ion$Machine Translation$	3
Again, this is an oversimplified application of the aligner, even more so than in STS, since a small change in linguistic properties of two sentences (e.g. polarity or modality) can turn them into non- 5https://code.google.com/p/jacana/ 6http://research.microsoft.com/en-us/downloads/607d14d9- 20cd-47e3-85bc-a2f65cd28042/ 227 System Acc.% P% R% F1% MT al. (	MT	Metathesaurus$machine translation test$machine translat$Madnani et$machine translation$Machine translation$Machine Trans lat ion$Machine Translation$	3
On the other hand, training a classifier for ranking can- didate answers allows the exploitation of various features extracted from the question, candidate an- swer, and surrounding context (MT al, 2007; Zhang et al, 2007).	MT	Metathesaurus$machine translation test$machine translat$Madnani et$machine translation$Machine translation$Machine Trans lat ion$Machine Translation$	3
It is compati- ble with the broader range of DELPH-IN tools, e.g., for MT (L?nning and Oepen, 2006), treebanking (Oepen et al, 2004) and parse selection (Toutanova et al, 2005).	MT	Metathesaurus$machine translation test$machine translat$Madnani et$machine translation$Machine translation$Machine Trans lat ion$Machine Translation$	4
Introduction The seminal work of Brown et al (1993b) introduced a series of probabilistic models (IBM Models 1?5) for statistical MT and the concept of ?	MT	Metathesaurus$machine translation test$machine translat$Madnani et$machine translation$Machine translation$Machine Trans lat ion$Machine Translation$	4
They used this information to re- attach PPs in a MT system, report- ing an improvement in translation quality when translating into Japanese (where PP attachment is not ambiguous and therefore matters) and a de- crease when translating into Spanish (where at- tachment ambiguities are close to the original ones and therefore need not be resolved).	MT	Metathesaurus$machine translation test$machine translat$Madnani et$machine translation$Machine translation$Machine Trans lat ion$Machine Translation$	4
Re-usable tools for precision MT.	MT	Metathesaurus$machine translation test$machine translat$Madnani et$machine translation$Machine translation$Machine Trans lat ion$Machine Translation$	4
Confidence es- timation for MT.	MT	Metathesaurus$machine translation test$machine translat$Madnani et$machine translation$Machine translation$Machine Trans lat ion$Machine Translation$	4
We also report experiments on two different tasks where word alignments are required: phrase-based MT and syntax transfer, and show promising improvements over standard methods.	MT	Metathesaurus$machine translation test$machine translat$Madnani et$machine translation$Machine translation$Machine Trans lat ion$Machine Translation$	4
Nevertheless, they are key to various NLP applications, including those benefiting from deep natural language understanding (e.g., textual inference (Bobrow et al, 2007)), generation of well- formed output (e.g., natural language weather alert systems (Lareau and Wanner, 2007)) or both (as in MT (Oepen et al, 2007)).	MT	Metathesaurus$machine translation test$machine translat$Madnani et$machine translation$Machine translation$Machine Trans lat ion$Machine Translation$	4
85 1 1.5 2 2.5 3 3.5 429 29.5 30 30.5 31 log10(tau) BL EU   %     devtest test   Figure 2: MT results on Europarl,  English to French track, devtest and test sets.	MT	Metathesaurus$machine translation test$machine translat$Madnani et$machine translation$Machine translation$Machine Trans lat ion$Machine Translation$	5
1 1.5 2 2.5 3 3.5 420 20.5 21 21.5 22 log10(tau) BL EU   %     nc-test   Figure 3: MT results on Europarl,  English to French track, out-of	MT	Metathesaurus$machine translation test$machine translat$Madnani et$machine translation$Machine translation$Machine Trans lat ion$Machine Translation$	5
5.1 MT We obtain packed-forest English outputs from 116 short Chinese sentences computed by a string-to- tree machine translation system based on (Galley, et.	MT	Metathesaurus$machine translation test$machine translat$Madnani et$machine translation$Machine translation$Machine Trans lat ion$Machine Translation$	5
.5 429 29.5 30 30.5 31 log10(tau) BL EU   %     devtest test   Figure 2: MT results on Europarl,  English to French track, devtest and test sets.	MT	Metathesaurus$machine translation test$machine translat$Madnani et$machine translation$Machine translation$Machine Trans lat ion$Machine Translation$	5
--91  MT \[12\]  As the first step to the machine  translation, we are implementing a program which  generates Japanese from the LE obtained by  analyzing English.	MT	Metathesaurus$machine translation test$machine translat$Madnani et$machine translation$Machine translation$Machine Trans lat ion$Machine Translation$	5
1 1.5 2 2.5 3 3.5 420 20.5 21 21.5 22 log10(tau) BL EU   %     nc-test   Figure 3: MT results on Europarl,  English to French track, out-of-domain test sets.	MT	Metathesaurus$machine translation test$machine translat$Madnani et$machine translation$Machine translation$Machine Trans lat ion$Machine Translation$	5
MT diver- gences: A formal description and proposed solution.	MT	Metathesaurus$machine translation test$machine translat$Madnani et$machine translation$Machine translation$Machine Trans lat ion$Machine Translation$	5
MT.	MT	Metathesaurus$machine translation test$machine translat$Madnani et$machine translation$Machine translation$Machine Trans lat ion$Machine Translation$	5
\[NAGAO 86\] Nagao, M., Tsuj i i ,  J., The Transfer  Phase  of the Mu MT  System, Proc.	MT	Metathesaurus$machine translation test$machine translat$Madnani et$machine translation$Machine translation$Machine Trans lat ion$Machine Translation$	6
For example, one of the tasks  for the Wayne State Univers i ty  MT group was  to program a routine to group each nominal in a Russ ian sen-  tence with its preceding (dependent) modif iers.	MT	Metathesaurus$machine translation test$machine translat$Madnani et$machine translation$Machine translation$Machine Trans lat ion$Machine Translation$	6
Sumita, E. and Iida, H.: "Transfer  Dr iven MT  Ut i l iz ing  Empirical Knowledge," Transactions of  Information Processing Society of Japan, Vol.	MT	Metathesaurus$machine translation test$machine translat$Madnani et$machine translation$Machine translation$Machine Trans lat ion$Machine Translation$	6
Theoret i ca l  and Methodo lo l i ca l   l i nes  in  MT,  Cambridge University Press, to appear 1986.	MT	Metathesaurus$machine translation test$machine translat$Madnani et$machine translation$Machine translation$Machine Trans lat ion$Machine Translation$	6
ln ter l ingua l  MT:  a Parame-   ter ized Approach .	MT	Metathesaurus$machine translation test$machine translat$Madnani et$machine translation$Machine translation$Machine Trans lat ion$Machine Translation$	6
Shake-and-Bake  MT .	MT	Metathesaurus$machine translation test$machine translat$Madnani et$machine translation$Machine translation$Machine Trans lat ion$Machine Translation$	6
In MT Summit IX, New Orleans, Louisiana, USA.	MT	Metathesaurus$machine translation test$machine translat$Madnani et$machine translation$Machine translation$Machine Trans lat ion$Machine Translation$	7
Fifth MT Summit.	MT	Metathesaurus$machine translation test$machine translat$Madnani et$machine translation$Machine translation$Machine Trans lat ion$Machine Translation$	7
BLEU: a Method for Automatic Evaluation of MT.	MT	Metathesaurus$machine translation test$machine translat$Madnani et$machine translation$Machine translation$Machine Trans lat ion$Machine Translation$	7
The Mathe- matic of Statistical MT: Parameter Estimation.	MT	Metathesaurus$machine translation test$machine translat$Madnani et$machine translation$Machine translation$Machine Trans lat ion$Machine Translation$	7
In Proceedings of the Third Workshop on Statistical MT, pages 151?154, Columbus, Ohio, June.	MT	Metathesaurus$machine translation test$machine translat$Madnani et$machine translation$Machine translation$Machine Trans lat ion$Machine Translation$	7
Distor- tion Models for Statistical MT.	MT	Metathesaurus$machine translation test$machine translat$Madnani et$machine translation$Machine translation$Machine Trans lat ion$Machine Translation$	7
We determined order based on  the following precedence ranking:   Subject  Direct Object  IOt Any remaining ties (e.g., an utterance with two  direct objects) were resolved according to a left- to-right breadth-first traversal of the parse tree.	IO	Indirect Objec$ICE ICE lO0$Indirect Object$	0
IOt relationship between a verb and an indirect object (a verb expressing a mental state; expressing a cause) 8.	IO	Indirect Objec$ICE ICE lO0$Indirect Object$	0
Direct Object NPs and IOt NPs are all untagged? (	IO	Indirect Objec$ICE ICE lO0$Indirect Object$	0
PRD Predicative Elements VP, PREDP SBJ Grammatical Subjects NP, SBAR OBJ Direct Objects NP COM IOts NP, PP FInite Complements SBAR IC Infinitival Complements VP CNJ A Conjunct within a Conjunction Structure All Table 2: Grammatical Functions in the MHTB SP-PCFG Expansion P(C l n , . . . ,	IO	Indirect Objec$ICE ICE lO0$Indirect Object$	0
for non- possessive pronouns  Non-immediate  Clause (neither the  current or immediate  clause)  50  Possessive NP 65  Existential NP 70  Subject 80  Direct Object 50  IOt 40  Compliment of PP 30     Table 1: Salience Factors and weights    Improving pronominal resolution Using Name  Entity (NE) and WordNet: Pronouns such as  ?	IO	Indirect Objec$ICE ICE lO0$Indirect Object$	0
IOt Constructions in English and the Ordering of Transformations.	IO	Indirect Objec$ICE ICE lO0$Indirect Object$	0
We determined order based on  the following precedence ranking:   Subject  Direct Object  IO Any remaining ties (e.g., an utterance with two  direct objects) were resolved according to a left- to-right breadth-first traversal of the parse tree.	IO	Indirect Objec$ICE ICE lO0$Indirect Object$	2
IO relationship between a verb and an indirect object (a verb expressing a mental state; expressing a cause) 8.	IO	Indirect Objec$ICE ICE lO0$Indirect Object$	2
Direct Object NPs and IO NPs are all untagged? (	IO	Indirect Objec$ICE ICE lO0$Indirect Object$	2
PRD Predicative Elements VP, PREDP SBJ Grammatical Subjects NP, SBAR OBJ Direct Objects NP COM IOs NP, PP FInite Complements SBAR IC Infinitival Complements VP CNJ A Conjunct within a Conjunction Structure All Table 2: Grammatical Functions in the MHTB SP-PCFG Expansion P(C l n , . . . ,	IO	Indirect Objec$ICE ICE lO0$Indirect Object$	2
for non- possessive pronouns  Non-immediate  Clause (neither the  current or immediate  clause)  50  Possessive NP 65  Existential NP 70  Subject 80  Direct Object 50  IO 40  Compliment of PP 30     Table 1: Salience Factors and weights    Improving pronominal resolution Using Name  Entity (NE) and WordNet: Pronouns such as  ?	IO	Indirect Objec$ICE ICE lO0$Indirect Object$	2
IO Constructions in English and the Ordering of Transformations.	IO	Indirect Objec$ICE ICE lO0$Indirect Object$	2
For example, the ERG marker ne only occurs on subjects.	ERG	ergative$Ergative$	0
Finally, line 3 calls a template that assigns the volitionality features associated with ERG noun phrases.	ERG	ergative$Ergative$	0
Merlo and Stevenson (2001) report inter-judge  values of 0.53 to 0.66 for a task we consider to be comparable to ours, that of classifying verbs into unERG, unaccusative and object-drop, and argue that Car- letta?s ?	ERG	ergative$Ergative$	0
However, not all subjects are ERG.	ERG	ergative$Ergative$	0
To the contrary, subjects can occur in the ERG, nominative, dative, genitive, and instrumental cases.	ERG	ergative$Ergative$	0
Line 2 provides the inside-out functional uncertainty statement; it states that the f- structure of the ERG noun phrase, referred to as ?,	ERG	ergative$Ergative$	0
The lexical entry for the ERG case, for example, states that it applies to a subject.	ERG	ergative$Ergative$	0
n/ne ko/ko s/se m\/meN pr/par kA/kaa (ERG) (Dative) (Instrumental) (Locative) (Locative) (Genitive) k1(agent) 7222 575 21 11 3 612 k2(patient) 0 3448 451 8 24 39 k3(instrument) 0 0 347 0 0 1 k4(recipient) 0 1851 351 0 1 4 k4a(experiencer) 0 420 8 0 0 2 k5(source) 0 2 1176 12 1 0 k7(location) 0 1140 308 8707 3116 19 r6(possession) 0 3 1 0 0 2251 Table 4 : Distribution of case markers across case function.	ERG	ergative$Ergative$	1
2009) and verb classification (Merlo and Steven- 1ERG, Genitive, Instrumental, Dative, Accusative and Locative in the given order.	ERG	ergative$Ergative$	1
NPErg: ERG case with case marker  ?	ERG	ergative$Ergative$	1
Nominals marked with ERG case have highest control and the ones marked with Locative have lowest.	ERG	ergative$Ergative$	1
Following is an example profile: <Profile> <language code="WBP">Warlpiri</language> <ontologyNamespace prefix="gold"> http://linguistic-ontology.org/gold.owl# </ontologyNamespace> <feature="word_order"><value>SVO</value></feature> <feature="det_order"><value>DT-NN</value></feature> <feature="case"> <value>gold:DativeCase</value> <value>gold:ERGCase</value> <value>gold:NominativeCase</value> . . .	ERG	ergative$Ergative$	1
No- 796 tably the work of Merlo and Stevenson (2001) at- tempts to induce three main English verb classes on a large scale from parsed corpora, the class of ERG, Unaccusative, and Object-drop verbs.	ERG	ergative$Ergative$	1
Joshua 2.0: A toolkit for parsing-based machine translation with syn- tax, semirings, disCRF and other good- ies.	CRF	criminative training$Conditional random field$Conditional Random  Fields$Conditional Random Feild$	0
DisCRF meth- ods for Hidden Markov Models: theory and experiments with perceptron algorithms.	CRF	criminative training$Conditional random field$Conditional Random  Fields$Conditional Random Feild$	0
DisCRF meth- ods for hidden Markov models: Theory and exper- iments with perceptron algorithms.	CRF	criminative training$Conditional random field$Conditional Random  Fields$Conditional Random Feild$	0
Instead of using disCRF meth- ods to tune the weights of generative models, in this paper we propose to use a discrimina- tive word aligner to produce reliable constraints for the EM algorithm.	CRF	criminative training$Conditional random field$Conditional Random  Fields$Conditional Random Feild$	0
However, the disCRF in these methods is re- stricted in using the model components of gener- ative models, in other words, incorporating new features is difficult.	CRF	criminative training$Conditional random field$Conditional Random  Fields$Conditional Random Feild$	0
CRFs: Probabilis- tic models for segmenting and labeling sequence data.	CRF	criminative training$Conditional random field$Conditional Random  Fields$Conditional Random Feild$	1
CRFs: Prob- abilistic models for segmenting and labeling se- quence data.	CRF	criminative training$Conditional random field$Conditional Random  Fields$Conditional Random Feild$	1
CRFs: probabilistic models for segmenting and labeling sequence data.	CRF	criminative training$Conditional random field$Conditional Random  Fields$Conditional Random Feild$	1
CRFs: Probabilistic mod- els for segmenting and labeling sequence data.	CRF	criminative training$Conditional random field$Conditional Random  Fields$Conditional Random Feild$	1
Identifying Sources of Opinions with CRF and Extraction Patterns.	CRF	criminative training$Conditional random field$Conditional Random  Fields$Conditional Random Feild$	2
We have developed a domain  independent system for ATR  from unrestricted text.	ATR	automatic term recognition$Automatic Term Recognition$atrans$	0
The result of ATR for ???	ATR	automatic term recognition$Automatic Term Recognition$atrans$	0
A proba- bilistic framework for ATR.	ATR	automatic term recognition$Automatic Term Recognition$atrans$	0
We use C-value, a measurement of ATR, to score source phrases.	ATR	automatic term recognition$Automatic Term Recognition$atrans$	0
Method for ATR.	ATR	automatic term recognition$Automatic Term Recognition$atrans$	0
Statistical and  linguistic approaches to ATR  NTCIR experiments at Matsushita.	ATR	automatic term recognition$Automatic Term Recognition$atrans$	0
Methods of  ATR: A Review.	ATR	automatic term recognition$Automatic Term Recognition$atrans$	1
An Application  and Evaluation of the C/NC-Value Approach for the  ATR of Multi-Word Units in  Japanese.	ATR	automatic term recognition$Automatic Term Recognition$atrans$	1
Lauriston, A. (1996) ATR :  performance of Linguistic and Statistical  Techniques.	ATR	automatic term recognition$Automatic Term Recognition$atrans$	1
Nakagawa, H. and Mori, T.: ATR based on Statistics of Compound Nouns and their Components.	ATR	automatic term recognition$Automatic Term Recognition$atrans$	1
ATR using Contextual Cues.	ATR	automatic term recognition$Automatic Term Recognition$atrans$	1
Jong-Hoon Oh, Jae-Ho Kim, Key-Sun Choi, ATR Through EM  Algorithm, http://nlplab.kaist.ac.kr/, 2003  15.	ATR	automatic term recognition$Automatic Term Recognition$atrans$	1
Noisy-parallel texts are characterised  by heavy reformatting at the translation stage, in-  cluding large sections of uATRlated text and tex-  tual reordering.	ATR	automatic term recognition$Automatic Term Recognition$atrans$	2
Data Translator/Verifier -- DTV  The data translation can be seen as ATRlat ion  from linearized character strings to certain  organized structures.	ATR	automatic term recognition$Automatic Term Recognition$atrans$	2
If  we had wanted to take prefixes into consideration then  the as  attribute, containing ATRcription of the whole  word, could have been used instead.	ATR	automatic term recognition$Automatic Term Recognition$atrans$	2
Our present effort is to describe ATRformation ISD  such that ISD _C L ?	ATR	automatic term recognition$Automatic Term Recognition$atrans$	2
Since it does not exis~ i~:  this first parse, DM~COMMAND creates that concept 7, This  newly created concept is mi instance of 'rATRqi'amC~ m~d  its object slot is now filled not by genetic ~*CONCt?!~r  N~t in-  stead by '*HAVE.-A--PATN'~ specific to Ollr input so~t_e~i:eo '\['h~',  final concept.-mfined concept is the result of the pacse?'o  5One firing to note here is that the concept 'aIIAVE-.AopAIN' at i~ acti-  vate, d by input "*IIAVE-A-IPAIN" is not part of the memo W Petwock tb~ Ne  I)M-COMMAND~S MT system eonnna	ATR	automatic term recognition$Automatic Term Recognition$atrans$	2
It is still  arbitrary  to be seen whether the proposed system can be used for an ATRforma-  tional grammar.	ATR	automatic term recognition$Automatic Term Recognition$atrans$	2
stitute University of Amsterdam, Kruislaan 403 NL-1098 SJ Amsterdam, The Netherlands erikt@science.uva.nl Sander Canisius, Antal van den Bosch, Toine Bogers ILK / Computational Linguistics and AI Tilburg University, P.O. Box 90153, NL-5000 LE Tilburg, The Netherlands {S.V.M.Canisius,Antal.vdnBosch, A.M.Bogers}@uvt.nl 1 Introduction This paper describes our approach to the CoNLL- 2005 shared task: SRL.	SRL	system for the Semeval-2007$Salient Referent List$semantic role labelling$semantic role labeler$Semantic role labeling$semantic role labeling$Semantic Role Labeling$	2
Collective SRL with markov logic.	SRL	system for the Semeval-2007$Salient Referent List$semantic role labelling$semantic role labeler$Semantic role labeling$semantic role labeling$Semantic Role Labeling$	2
We adapted the method to our needs and applied it for improving SRL output.	SRL	system for the Semeval-2007$Salient Referent List$semantic role labelling$semantic role labeler$Semantic role labeling$semantic role labeling$Semantic Role Labeling$	2
1998) to provide associations between verbs and semantic roles, that are then mapped onto the cur- rent instance, as shown by the systems competing in SRL competitions (Carreras and Marquez, 2004; Carreras and Marquez, 2005) and also (Gildea and Jurafsky, 2002; Pradhan et al, 2005; Shi and Mihalcea, 2005).	SRL	system for the Semeval-2007$Salient Referent List$semantic role labelling$semantic role labeler$Semantic role labeling$semantic role labeling$Semantic Role Labeling$	2
c?2005 Association for Computational Linguistics Applying spelling error correction techniques for improving SRL Erik Tjong Kim Sang Informatics Institute University of Amsterdam, Kruislaan 403 NL-1098 SJ Amsterdam, The Netherlands erikt@science.uva.nl Sander Canisius, Antal van den Bosch, Toine Bogers ILK / Computational Linguistics and AI Tilburg University, P.O. Box 90153, NL-5000 LE Tilburg, The Netherlands {S.V.M.Canisius,Antal.vdnBosch, A.M.Bogers}@uvt.nl 1 Introduction This pap	SRL	system for the Semeval-2007$Salient Referent List$semantic role labelling$semantic role labeler$Semantic role labeling$semantic role labeling$Semantic Role Labeling$	2
2013), and she and her colleagues have shown that an abduction engine using a knowledge base de- rived from these sources is competitive with the best of the statistical systems in recognizing tex- tual entailment and in SRL.	SRL	system for the Semeval-2007$Salient Referent List$semantic role labelling$semantic role labeler$Semantic role labeling$semantic role labeling$Semantic Role Labeling$	2
The proposed system consists of five mod- ules: syntactic dependency parser, predi- cate identifier, local SRL, global role sequence candidate generator, and role sequence selector.	SRL	system for the Semeval-2007$Salient Referent List$semantic role labelling$semantic role labeler$Semantic role labeling$semantic role labeling$Semantic Role Labeling$	3
2005) and Toutanova et al (2005), our local SRL needs to enhance the perfor- mance.	SRL	system for the Semeval-2007$Salient Referent List$semantic role labelling$semantic role labeler$Semantic role labeling$semantic role labeling$Semantic Role Labeling$	3
In this paper, we test this hypothesis by combining an incremental TAG parser with an incremental SRL in a discriminative framework.	SRL	system for the Semeval-2007$Salient Referent List$semantic role labelling$semantic role labeler$Semantic role labeling$semantic role labeling$Semantic Role Labeling$	3
Currently, our graph takes non-auxiliary verbs and a few eventive nouns as predicates provided by a SRL.	SRL	system for the Semeval-2007$Salient Referent List$semantic role labelling$semantic role labeler$Semantic role labeling$semantic role labeling$Semantic Role Labeling$	3
The Chi- nese propositional structure was predicted with the Chinese SRL described in (Xue, 2008), retrained on the OntoNotes v5.0 data.	SRL	system for the Semeval-2007$Salient Referent List$semantic role labelling$semantic role labeler$Semantic role labeling$semantic role labeling$Semantic Role Labeling$	3
We parsed all data using the dependency parser, the SRL, the named entity tagger, and the coreference resolution in ClearNLP (Choi and McCallum, 2013; Choi, 2012).	SRL	system for the Semeval-2007$Salient Referent List$semantic role labelling$semantic role labeler$Semantic role labeling$semantic role labeling$Semantic Role Labeling$	3
2.2 Semantic Role Labeling  SRL is the task of identifying  semantic roles such as Agent, Patient, Speaker,  or Topic, in a sentence.	SRL	system for the Semeval-2007$Salient Referent List$semantic role labelling$semantic role labeler$Semantic role labeling$semantic role labeling$Semantic Role Labeling$	4
SRL contains two problems:  identification and labeling.	SRL	system for the Semeval-2007$Salient Referent List$semantic role labelling$semantic role labeler$Semantic role labeling$semantic role labeling$Semantic Role Labeling$	4
SRL using maximum entropy model.	SRL	system for the Semeval-2007$Salient Referent List$semantic role labelling$semantic role labeler$Semantic role labeling$semantic role labeling$Semantic Role Labeling$	4
SRL us- ing dependency trees.	SRL	system for the Semeval-2007$Salient Referent List$semantic role labelling$semantic role labeler$Semantic role labeling$semantic role labeling$Semantic Role Labeling$	4
SRL using dependency trees.	SRL	system for the Semeval-2007$Salient Referent List$semantic role labelling$semantic role labeler$Semantic role labeling$semantic role labeling$Semantic Role Labeling$	4
3.1.1 Semantic Role Constraints SRL generally includes the three subtasks: predicate identification; argument role la- beling; sense disambiguation.	SRL	system for the Semeval-2007$Salient Referent List$semantic role labelling$semantic role labeler$Semantic role labeling$semantic role labeling$Semantic Role Labeling$	4
This setting includes a  broad range of structured prediction problems such as SRL, named  entity and relation recognition, co-reference resolution, dependency parsing and  semantic parsing.	SRL	system for the Semeval-2007$Salient Referent List$semantic role labelling$semantic role labeler$Semantic role labeling$semantic role labeling$Semantic Role Labeling$	5
2 Related Work  This section reviews previous works in both  sentiment detection and SRL.	SRL	system for the Semeval-2007$Salient Referent List$semantic role labelling$semantic role labeler$Semantic role labeling$semantic role labeling$Semantic Role Labeling$	5
1 Introduction The joint parsing of syntactic and semantic depen- dencies introduced by the shared task of CoNLL- 08 is more complicated than syntactic dependency parsing or SRL alone (Surdeanu et al, 2008).	SRL	system for the Semeval-2007$Salient Referent List$semantic role labelling$semantic role labeler$Semantic role labeling$semantic role labeling$Semantic Role Labeling$	5
Cal- ibrating features for SRL.	SRL	system for the Semeval-2007$Salient Referent List$semantic role labelling$semantic role labeler$Semantic role labeling$semantic role labeling$Semantic Role Labeling$	5
introduces related work both in sentiment  analysis and SRL.	SRL	system for the Semeval-2007$Salient Referent List$semantic role labelling$semantic role labeler$Semantic role labeling$semantic role labeling$Semantic Role Labeling$	5
These advantages and the availability of off-the-shelf solvers have led to a  large variety of NLP tasks being formulated within it, including SRL,  syntactic parsing, co-reference resolution, summarization, transliteration and joint  information extraction.	SRL	system for the Semeval-2007$Salient Referent List$semantic role labelling$semantic role labeler$Semantic role labeling$semantic role labeling$Semantic Role Labeling$	5
This paper is organized as follows: Section 2  briefly introduces related work both in sentiment  analysis and SRL.	SRL	system for the Semeval-2007$Salient Referent List$semantic role labelling$semantic role labeler$Semantic role labeling$semantic role labeling$Semantic Role Labeling$	5
2.3 Local SRL Prediate identification is followed by argument la- beling.	SRL	system for the Semeval-2007$Salient Referent List$semantic role labelling$semantic role labeler$Semantic role labeling$semantic role labeling$Semantic Role Labeling$	6
2.2 SRL  Semantic role labeling is the task of identifying  semantic roles such as Agent, Patient, Speaker,  or Topic, in a sentence.	SRL	system for the Semeval-2007$Salient Referent List$semantic role labelling$semantic role labeler$Semantic role labeling$semantic role labeling$Semantic Role Labeling$	6
3.2 SRL  To find a potential holder and topic of an opinion  word in a sentence, we first label semantic roles  in a sentence.	SRL	system for the Semeval-2007$Salient Referent List$semantic role labelling$semantic role labeler$Semantic role labeling$semantic role labeling$Semantic Role Labeling$	6
SRL: an Introduc- tion to the Special Issue.	SRL	system for the Semeval-2007$Salient Referent List$semantic role labelling$semantic role labeler$Semantic role labeling$semantic role labeling$Semantic Role Labeling$	6
In this paper, we propose a novel method that  employs SRL, a task of iden- tifying semantic roles given a sentence.	SRL	system for the Semeval-2007$Salient Referent List$semantic role labelling$semantic role labeler$Semantic role labeling$semantic role labeling$Semantic Role Labeling$	6
Graph Aligment for Semi-Supervised SRL.	SRL	system for the Semeval-2007$Salient Referent List$semantic role labelling$semantic role labeler$Semantic role labeling$semantic role labeling$Semantic Role Labeling$	6
c?2007 Association for Computational Linguistics Support SVM for Query-focused Summarization trained and evaluated on Pyramid data Maria Fuentes TALP Research Center Universitat Polite`cnica de Catalunya mfuentes@lsi.upc.edu Enrique Alfonseca Computer Science Departament Universidad Auto?noma de Madrid Enrique.Alfonseca@gmail.com Horacio Rodr??guez TALP Research Center Universitat Polite`cnica de Catalunya horacio@lsi.upc.edu Abstract This p	SVM	Vector Machines$Support vector machine$Support Vector Machin$support vector machine$	0
Learning to Classify Text  Using Support SVM: Methods, Theory,  and Algorithms.	SVM	Vector Machines$Support vector machine$Support Vector Machin$support vector machine$	0
Learning with Kernels: Support SVM,  Regularization, Optimization, and Beyond.	SVM	Vector Machines$Support vector machine$Support Vector Machin$support vector machine$	0
Shallow Se- mantic Parsing Using Support SVM.	SVM	Vector Machines$Support vector machine$Support Vector Machin$support vector machine$	0
In this  paper, we describe the use of annotated  datasets and Support SVM  to induce larger monolingual para- phrase corpora from a comparable cor- pus of news clusters found on the  World Wide Web.	SVM	Vector Machines$Support vector machine$Support Vector Machin$support vector machine$	0
931  Support SVM for Paraphrase Identification   and Corpus Construction  Chris Brockett and William B. Dolan  Natural Language Processing Group  Microsoft Research  One Microsoft Way, Redmond, WA 98502, U.S.A.  {chrisbkt, billdol}@microsoft.com  Abstract  The lack of readily-available large cor- pora of aligned monolingual sentence  pairs is a major obstacle to the devel- opment of Statistical Mac	SVM	Vector Machines$Support vector machine$Support Vector Machin$support vector machine$	0
SVM learning for interdependent and structured output spaces.	SVM	Vector Machines$Support vector machine$Support Vector Machin$support vector machine$	1
For this, for each training question, our approach is to hire 7SVMs yielded worse results.	SVM	Vector Machines$Support vector machine$Support Vector Machin$support vector machine$	1
SVM learning for interdepen- dent and structured output spaces.	SVM	Vector Machines$Support vector machine$Support Vector Machin$support vector machine$	1
SVM active learning with applications to text classification.	SVM	Vector Machines$Support vector machine$Support Vector Machin$support vector machine$	1
Svmlight: SVM.	SVM	Vector Machines$Support vector machine$Support Vector Machin$support vector machine$	1
SVMs for mapping histories to parser actions (Kudo and Matsumoto, 2002).	SVM	Vector Machines$Support vector machine$Support Vector Machin$support vector machine$	1
Learning to Classify Text  Using SVMes: Methods, Theory,  and Algorithms.	SVM	Vector Machines$Support vector machine$Support Vector Machin$support vector machine$	2
931  SVMes for Paraphrase Identification   and Corpus Construction  Chris Brockett and William B. Dolan  Natural Language Processing Group  Microsoft Research  One Microsoft Way, Redmond, WA 98502, U.S.A.  {chrisbkt, billdol}@microsoft.com  Abstract  The lack of readily-available large cor- pora of aligned monolingual sentence  pairs is a major obstacle to the devel- opment of Statist	SVM	Vector Machines$Support vector machine$Support Vector Machin$support vector machine$	2
Learning with Kernels: SVMes,  Regularization, Optimization, and Beyond.	SVM	Vector Machines$Support vector machine$Support Vector Machin$support vector machine$	2
Shallow Se- mantic Parsing Using SVMes.	SVM	Vector Machines$Support vector machine$Support Vector Machin$support vector machine$	2
c?2007 Association for Computational Linguistics SVMes for Query-focused Summarization trained and evaluated on Pyramid data Maria Fuentes TALP Research Center Universitat Polite`cnica de Catalunya mfuentes@lsi.upc.edu Enrique Alfonseca Computer Science Departament Universidad Auto?noma de Madrid Enrique.Alfonseca@gmail.com Horacio Rodr??guez TALP Research Center Universitat Polite`cnica de Catalunya horacio@lsi.upc.edu Abstrac	SVM	Vector Machines$Support vector machine$Support Vector Machin$support vector machine$	2
In this  paper, we describe the use of annotated  datasets and SVMes  to induce larger monolingual para- phrase corpora from a comparable cor- pus of news clusters found on the  World Wide Web.	SVM	Vector Machines$Support vector machine$Support Vector Machin$support vector machine$	2
A com- parative study of SVMs applied  to the supervised word sense disambiguation prob- lem in the medical domain.	SVM	Vector Machines$Support vector machine$Support Vector Machin$support vector machine$	3
While memory-based and margin-based learn- ing approaches such as SVMs are popularly applied to shift-reduce parsing, our work provides evidence that the maximum en- tropy model can achieve a comparative perfor- mance with the aid of a suitable feature set.	SVM	Vector Machines$Support vector machine$Support Vector Machin$support vector machine$	3
SVMs (Cortes and Vapnik, 1995) and voted perceptrons (Freund 3A spurious solution is a string that does not belong to the language under consideration.	SVM	Vector Machines$Support vector machine$Support Vector Machin$support vector machine$	3
Since our  final  goal  was to  build  a  comma checker, we would have to have chosen  the classifier that gave us the best precision, that  is, the SVM based one.	SVM	Vector Machines$Support vector machine$Support Vector Machin$support vector machine$	3
Chunking with SVM.	SVM	Vector Machines$Support vector machine$Support Vector Machin$support vector machine$	3
But the  recall of the SVM based classi?	SVM	Vector Machines$Support vector machine$Support Vector Machin$support vector machine$	3
The first two rows are devoted to the probabilities of particular kind of antecedent (pronouns, proper nouns, and CN generating a pronoun, holding ev- erything constant except the type of antecedent.	CN	common nouns)$common oun$Chinese person name$common noun$confusion network$common nouns$CoTrain$Chinese Language$Concept node$	0
His implementation is different from ours: he annotates the case of pro- nouns and common nouns, whereas we focus on ar- ticles and pronouns (articles are pronouns are more strongly marked for case than CN.	CN	common nouns)$common oun$Chinese person name$common noun$confusion network$common nouns$CoTrain$Chinese Language$Concept node$	0
Starting with it set of basic categoric.v, which for  tile purposes of this paper will be {txt, s, n, cn}  (for texts, sentences, names and CN,  we define it category to be either a basic category  or anything of one of the forms a / b or b \ a,  where a and b are categories.	CN	common nouns)$common oun$Chinese person name$common noun$confusion network$common nouns$CoTrain$Chinese Language$Concept node$	0
A dependency path is the shortest path of lexico-syntactic elements, i.e. shortest lexico-syntactic pattern, connecting enti- ties (proper and CN in their parse- trees.	CN	common nouns)$common oun$Chinese person name$common noun$confusion network$common nouns$CoTrain$Chinese Language$Concept node$	0
1 Introduction Coreference resolution is used to determine which noun phrases (including pronouns, proper names, and CN refer to the same entities in documents.	CN	common nouns)$common oun$Chinese person name$common noun$confusion network$common nouns$CoTrain$Chinese Language$Concept node$	0
2.3 Reference Detection between Entities   We assume that the syntactic relationships between  entities (proper or CN in a text give us  information on their semantic reference status.	CN	common nouns)$common oun$Chinese person name$common noun$confusion network$common nouns$CoTrain$Chinese Language$Concept node$	0
The experiment has been carried out on 60 sentences  with 1201 different lectures, and formed by using seven  verbs (wr~te, eat, smell, corrode, buy, receive, assocza~e)  coupled with fifty CNs and two proper nouns.	CN	common nouns)$common oun$Chinese person name$common noun$confusion network$common nouns$CoTrain$Chinese Language$Concept node$	1
Translation  ( Selection of Head Noun )  ~t  n ( Dete =tion of Type of Noun )  I ~, Proper Noun  \[ Determination f Semantic \]  Attributes for Proper Nouns  \[ Determination f Semantic )  Attributes for Common Nouns  ~t  _\[ Translation by ALT-J/E )  Fig.2 Method of Auto .rm.atically Determining  Semantic Attributes  The procedures consist of determining the head  noun, noun type (proper and/or CN), proper  noun SAs (for proper nouns) and CN SAs (for  beth common and proper nouns).	CN	common nouns)$common oun$Chinese person name$common noun$confusion network$common nouns$CoTrain$Chinese Language$Concept node$	1
We have the following rules for  CNs:  (R2a) n(Vl,pizza(V1)) ~ \[pizza\]  (R2b) n(Vl,man(V1))--* \[man\]  (R2c) n(Vl,woman(V1))~ \[woman\].	CN	common nouns)$common oun$Chinese person name$common noun$confusion network$common nouns$CoTrain$Chinese Language$Concept node$	1
For  proper nouns, both CN SAs and proper noun  S/ks(beth more than one) are given.	CN	common nouns)$common oun$Chinese person name$common noun$confusion network$common nouns$CoTrain$Chinese Language$Concept node$	1
Common nouns in the semantic word dictionary are  given CN SAs (generally more than one).	CN	common nouns)$common oun$Chinese person name$common noun$confusion network$common nouns$CoTrain$Chinese Language$Concept node$	1
A potential subject is not a subject if it has no de-  terminer, unless it is a proper noun or it is a coordinated  CN.	CN	common nouns)$common oun$Chinese person name$common noun$confusion network$common nouns$CoTrain$Chinese Language$Concept node$	1
In Chinese language Processing, Chert and  Lee (1996) present various strategies to identify  and classify three types of proper nouns, i.e.,  CNs, Chinese transliterated  person names and organization names.	CN	common nouns)$common oun$Chinese person name$common noun$confusion network$common nouns$CoTrain$Chinese Language$Concept node$	2
A CN is composed of  surname and name parts.	CN	common nouns)$common oun$Chinese person name$common noun$confusion network$common nouns$CoTrain$Chinese Language$Concept node$	2
(4) special verbs  The same set of speech-act verbs used in  CNs are also used for  transliterated person names.	CN	common nouns)$common oun$Chinese person name$common noun$confusion network$common nouns$CoTrain$Chinese Language$Concept node$	2
(2) titles  Titles used in CNs are  233  also applicable to transliterated person  names.	CN	common nouns)$common oun$Chinese person name$common noun$confusion network$common nouns$CoTrain$Chinese Language$Concept node$	2
Compared with CNs, the  length of transliterated names is not restricted to  2 to 6 characters.	CN	common nouns)$common oun$Chinese person name$common noun$confusion network$common nouns$CoTrain$Chinese Language$Concept node$	2
Thus the length of CNs range  from 2 to 6 characters.	CN	common nouns)$common oun$Chinese person name$common noun$confusion network$common nouns$CoTrain$Chinese Language$Concept node$	2
cn CN *pe preposition co coordinating elem.	CN	common nouns)$common oun$Chinese person name$common noun$confusion network$common nouns$CoTrain$Chinese Language$Concept node$	3
Verbs whose subjects are CNs account for 57.8% of all verbs that have subjects (verbs with different types of subjects, most of which are personal pronouns, are not considered here, since these subjects are not part of the noun classifier).	CN	common nouns)$common oun$Chinese person name$common noun$confusion network$common nouns$CoTrain$Chinese Language$Concept node$	3
The last column of Table 5 shows that valid subject-verb structures account for 67.5% of all verbs whose subjects are CNs (51.7% are cases where the words are adjacent).	CN	common nouns)$common oun$Chinese person name$common noun$confusion network$common nouns$CoTrain$Chinese Language$Concept node$	3
If all CNs (e.g.  'person', 'share price', etc.)	CN	common nouns)$common oun$Chinese person name$common noun$confusion network$common nouns$CoTrain$Chinese Language$Concept node$	3
CN?	CN	common nouns)$common oun$Chinese person name$common noun$confusion network$common nouns$CoTrain$Chinese Language$Concept node$	3
For words which failed to be guessed by  tile guessing rules we applied the standard method  of classifying them as CNs (NN) if they  are not capitalised inside a sentence and proper  nouns (NP) otherwise.	CN	common nouns)$common oun$Chinese person name$common noun$confusion network$common nouns$CoTrain$Chinese Language$Concept node$	3
The representation for sentence 1 states that the first element of the 5-gram (-3; third word to the left of the adjective) is empty (because the second element is a phrase boundary marker), that the sec- ond element is a clause delimiter (conjunction that), the third one (-1; word preceding the adjective) is a definite determiner, and the fourth one (+1; word following the adjective) is a CN.	CN	common nouns)$common oun$Chinese person name$common noun$confusion network$common nouns$CoTrain$Chinese Language$Concept node$	3
It should be noted that the live SLU used the word CN, not made available in the challenge.	CN	common nouns)$common oun$Chinese person name$common noun$confusion network$common nouns$CoTrain$Chinese Language$Concept node$	4
Nevertheless, despite this hand- icap, the best results were obtained from word- based tracking directly on the ASR output, rather than using the CN generated SLU output.	CN	common nouns)$common oun$Chinese person name$common noun$confusion network$common nouns$CoTrain$Chinese Language$Concept node$	4
Moreover, this log-linear model includes a word penalty, a language model trained on the in- put hypotheses, a binary feature which penalizes word deletions in the CN and a pri- mary feature which marks the system which pro- vides the word order.	CN	common nouns)$common oun$Chinese person name$common noun$confusion network$common nouns$CoTrain$Chinese Language$Concept node$	4
Recent several years have witnessed the rapid development of system combination methods based on CNs (e.g., (Rosti et al, 2007; He et al, 2008)), which show state-of-the- art performance in MT benchmarks.	CN	common nouns)$common oun$Chinese person name$common noun$confusion network$common nouns$CoTrain$Chinese Language$Concept node$	4
automatic translations of the same sentence, we show how to com- bine them into a CN, whose various paths represent composite translations that could be considered in a subsequent rescoring step.	CN	common nouns)$common oun$Chinese person name$common noun$confusion network$common nouns$CoTrain$Chinese Language$Concept node$	4
The word CN is known to provide stronger features than theN -best list for language understanding (Henderson et al.,	CN	common nouns)$common oun$Chinese person name$common noun$confusion network$common nouns$CoTrain$Chinese Language$Concept node$	4
Note that a subject in subject-verb structures is always third person, since all subjects in subject-verb structures are CN; other subjects, including pronouns, are ex- cluded.	CN	common nouns)$common oun$Chinese person name$common noun$confusion network$common nouns$CoTrain$Chinese Language$Concept node$	5
Verbs whose subjects are CN account for 57.8% of all verbs that have subjects (verbs with different types of subjects, most of which are personal pronouns, are not considered here, since these subjects are not part of the noun classifier).	CN	common nouns)$common oun$Chinese person name$common noun$confusion network$common nouns$CoTrain$Chinese Language$Concept node$	5
The last column of Table 5 shows that valid subject-verb structures account for 67.5% of all verbs whose subjects are CN (51.7% are cases where the words are adjacent).	CN	common nouns)$common oun$Chinese person name$common noun$confusion network$common nouns$CoTrain$Chinese Language$Concept node$	5
For words which failed to be guessed by  tile guessing rules we applied the standard method  of classifying them as CN (NN) if they  are not capitalised inside a sentence and proper  nouns (NP) otherwise.	CN	common nouns)$common oun$Chinese person name$common noun$confusion network$common nouns$CoTrain$Chinese Language$Concept node$	5
If all CN (e.g.  'person', 'share price', etc.)	CN	common nouns)$common oun$Chinese person name$common noun$confusion network$common nouns$CoTrain$Chinese Language$Concept node$	5
SIM2 Whether there are CN modifying the common nucleus noun or not in the problem report and aid message.	CN	common nouns)$common oun$Chinese person name$common noun$confusion network$common nouns$CoTrain$Chinese Language$Concept node$	5
In particular, on the IT168 test set, the best accuracy is achieved by CN with GoogleTranslate, and on the 360BUY test set, the best result is achieved by CN with YahooTranslate.	CN	common nouns)$common oun$Chinese person name$common noun$confusion network$common nouns$CoTrain$Chinese Language$Concept node$	6
Tables 9 and 10 show the results of significance tests between CN and the baseline methods on the two test sets, respectively.	CN	common nouns)$common oun$Chinese person name$common noun$confusion network$common nouns$CoTrain$Chinese Language$Concept node$	6
As can be seen in Tables 1 through 8, no matter which machine translation service is used, the proposed co-training approach (CN) outperforms all baseline methods on the overall accuracy metric and most other metrics on the two test sets.	CN	common nouns)$common oun$Chinese person name$common noun$confusion network$common nouns$CoTrain$Chinese Language$Concept node$	6
Even the two component classifiers in CN can perform as well as or better than the baseline methods.	CN	common nouns)$common oun$Chinese person name$common noun$confusion network$common nouns$CoTrain$Chinese Language$Concept node$	6
ts of significance tests between CN and the baseline methods on the two test sets, respectively.	CN	common nouns)$common oun$Chinese person name$common noun$confusion network$common nouns$CoTrain$Chinese Language$Concept node$	6
Even the two component classifiers in CN can perform as well as or better than the basel	CN	common nouns)$common oun$Chinese person name$common noun$confusion network$common nouns$CoTrain$Chinese Language$Concept node$	6
The p-values for sign tests are presented; the performance difference between CN and a baseline method is	CN	common nouns)$common oun$Chinese person name$common noun$confusion network$common nouns$CoTrain$Chinese Language$Concept node$	6
Our model is based on the DLCN algo- rithm proposed by (Collins and Singer, 1999), which applies a co-training procedure to decision list classifiers for two independent sets of fea- tures.	CN	common nouns)$common oun$Chinese person name$common noun$confusion network$common nouns$CoTrain$Chinese Language$Concept node$	6
The parameter values for CN and SelfTrain are set as I = 80 and p = n = 5.	CN	common nouns)$common oun$Chinese person name$common noun$confusion network$common nouns$CoTrain$Chinese Language$Concept node$	6
The p-values for sign tests are presented; the performance difference between CN and a baseline method is statistically significant at a 95% level if the p-value is smaller than 0.05.	CN	common nouns)$common oun$Chinese person name$common noun$confusion network$common nouns$CoTrain$Chinese Language$Concept node$	6
154 Sixth SIGHAN Workshop on CN Processing  Lexicon Design Using a Paradigmatic Approach  Cristian Dumitrescu  Research Institute for Informatics  Alex.	CN	common nouns)$common oun$Chinese person name$common noun$confusion network$common nouns$CoTrain$Chinese Language$Concept node$	7
In Pro- ceedings of the Second CIPS-SIGHAN Joint Conr- erence on CN Processing, pages 35?	CN	common nouns)$common oun$Chinese person name$common noun$confusion network$common nouns$CoTrain$Chinese Language$Concept node$	7
The Third International Chi- nese Language Processing Bakeoff: Word Seg- mentation and Named Entity Recognition, In  Proceedings of SIGHAN5 the 3rd International  CN Processing Bakeoff at Col- ing/ACL 2006, July, Sydney, Australia, 108-117.	CN	common nouns)$common oun$Chinese person name$common noun$confusion network$common nouns$CoTrain$Chinese Language$Concept node$	7
Top 5 left, right and bi-direction WB to- kens derived from the AS corpus  151 Sixth SIGHAN Workshop on CN Processing 2.2 WBT Frequency and WBT Probability  We first give the computation of WBT frequency,  then, the computation of WBT probability.	CN	common nouns)$common oun$Chinese person name$common noun$confusion network$common nouns$CoTrain$Chinese Language$Concept node$	7
152 Sixth SIGHAN Workshop on CN Processing Table 2 is an example of applying  WBTM(2, ???,	CN	common nouns)$common oun$Chinese person name$common noun$confusion network$common nouns$CoTrain$Chinese Language$Concept node$	7
153 Sixth SIGHAN Workshop on CN Processing                Coverage (%)     2-char 3-char 4-char > 4-char  CKIP  68% 24% 4% 4%  CityU  78% 19% 2% 1%   Total  75% 21% 3% 1%  Table 7.	CN	common nouns)$common oun$Chinese person name$common noun$confusion network$common nouns$CoTrain$Chinese Language$Concept node$	7
Some of the patterns gen- erated were revised manually and inconsisten- 1CNs are essentially case frames which are triggered through a lexical item and its corresponding linguistic context (Riloff, 1993) cies were removed.	CN	common nouns)$common oun$Chinese person name$common noun$confusion network$common nouns$CoTrain$Chinese Language$Concept node$	8
CNs take the following form: [concept_label] or [concept_label: role_indicator].	CN	common nouns)$common oun$Chinese person name$common noun$confusion network$common nouns$CoTrain$Chinese Language$Concept node$	8
CNs are frame-like structures that are triggered by relevant words or phrases an d filled by local syntactic constituents using semantic constraints and preferences .	CN	common nouns)$common oun$Chinese person name$common noun$confusion network$common nouns$CoTrain$Chinese Language$Concept node$	8
CNs refer  to words and features in Hownet.	CN	common nouns)$common oun$Chinese person name$common noun$confusion network$common nouns$CoTrain$Chinese Language$Concept node$	8
CNs included in the summary graph are shaded.	CN	common nouns)$common oun$Chinese person name$common noun$confusion network$common nouns$CoTrain$Chinese Language$Concept node$	8
CNs and Textref nodes are linked by an event with the internal action words_used.	CN	common nouns)$common oun$Chinese person name$common noun$confusion network$common nouns$CoTrain$Chinese Language$Concept node$	8
(v, svo) (subj,nil) know air pilots bring (co mp l,n il) AGENT OB JE CT According to the metric, the networks match and the pairs (watch, wWEAman) and (know, air pi- lots) match, so the semantic relation for the pair (know, air pilots) is proposed as a possible relation for pair (watch, wWEAman) .	WEA	eather$weapon$	0
WWEA forecasts  System by Korea University and Sangmyung University (Chung et al 2004) 8.	WEA	eather$weapon$	0
For example, for the sentence: WWEAmen watch the clouds day and night.	WEA	eather$weapon$	0
A German Sign Language corpus of the domain wWEA report.	WEA	eather$weapon$	0
the system builds the following network centered on the predicate watch2: cloudwWEAman watch (v, svo) (subj,nil) (c om pl ,n il) day and night (co mp l,n il) The system locates among previously stored net- works those centered around verbs3.	WEA	eather$weapon$	0
Nevertheless, they are key to various NLP applications, including those benefiting from deep natural language understanding (e.g., textual inference (Bobrow et al, 2007)), generation of well- formed output (e.g., natural language wWEA alert systems (Lareau and Wanner, 2007)) or both (as in machine translation (Oepen et al, 2007)).	WEA	eather$weapon$	0
In general,/Generally speaking,/Generally,  <clause>  Term.example  ... matters of process, such as exposing the WEAs  industry's influence ...  The EXAMPLE discourse relation discussed above links  entire relations: I like dogs.	WEA	eather$weapon$	1
\[Rather,\] We should  limit our involvement in defense and WEAry to  matters of process ...  The writer negates a proposition and, furnishing a  correction, asserts a sharply contrasting one.	WEA	eather$weapon$	1
articles required in one language but optional in the other (e.g., English Cars use gas and Portuguese Os carros usam gasolina), cases where the content is expressed using multiple words in one language and a single word in the other language (e.g., agglutination such as English WEAs of mass destruction and German Massenvernichtungswaffen), and expres- sions translated indirectly.	WEA	eather$weapon$	1
We  should limit our involvement in defense and WEAry  to matters of process, such as exposing the WEAs  industry's influence of the political process.	WEA	eather$weapon$	1
Rather, we should limit out involvement in defense and  WEAry to matters of process, for instance exposing the  WEAs industry's influence of the political process.	WEA	eather$weapon$	1
As shown in Figure 2, the MO may oc- cur in different grammatical relations with respect to the verb (subject, direct object, object of a prepo- sition), but they each have an ARG1 semantic role label in PropBank.3 Furthermore, only one of the MO needs to be specified, as in Exam- ple 3 where the second matched object (presumably the company?s prices) is unstated.	MO	matched objects$Modifier$matched object$	0
The tem-  plate pattern search recognizes relationships  between MO in the defined pat-  tern a.s well a.s recognizing the.	MO	matched objects$Modifier$matched object$	0
The MO constitute here a FL expres- sion instead of a video sequence track.	MO	matched objects$Modifier$matched object$	0
ions of how the arguments and  modifiers of the light verbs and their true  predicates are annotated are mentioned in Table  1, but notably, none of the examples in it  currently include the annotation of arguments   Pass 1: Pass 2: Pass 3:   Light Verb Annotation True Predicate Annotation Merge of Pass1&2 Annotation  Relation Light verb True predicate Light verb + true predicate  Arguments  and  MOs  - Predicating expression is  annotated with ARG-PRX  - Arguments and modifiers of  the light verb are annotated  - Arguments and modifiers of  the true predicate are annotated  - Arguments and modifiers  found in the two passes are  merged, preferably  automatically.	MO	matched objects$Modifier$matched object$	1
MO head and modifier 6The tags outside of a given XP are approximated using the marginally most likely tags given the parse.	MO	matched objects$Modifier$matched object$	1
Pass 1: Pass 2:  Pass 3:   Light Verb Identification LVC Annotation Deterministic relation merge  Relation Light verb True predicate Light verb + true predicate  Arguments  & MOs  - Predicating expression is  annotated with ARG-PRX  - Arguments and modifiers of  the LVCs are annotated  - Arguments and modifiers  are taken from Pass 2  Frame File <no Frame File needed> LVC?s Frame File LVC?s Frame File    Example  ?	MO	matched objects$Modifier$matched object$	1
on (one) roof-of  house    3.2 Post MOs    Nouns in prepositional phrases can expand  with post modifiers while nouns in our structure  cannot.	MO	matched objects$Modifier$matched object$	1
Subsequent work (McDonald & Pereira, 2006; Carreras, 2007; Koo & Collins, 2010) looked at allowing features to access more complex, higher-order relationships, including tri- gram and 4-gram relationships, e.g., all features apart from MO, below.	MO	matched objects$Modifier$matched object$	1
wing features to access more complex, higher-order relationships, including tri- gram and 4-gram relationships, e.g., all features apart from MO, below.	MO	matched objects$Modifier$matched object$	1
MOs such as species designators were excluded from annota- tions whenever possible.	MO	matched objects$Modifier$matched object$	1
As shown in Figure 2, the MOs may oc- cur in different grammatical relations with respect to the verb (subject, direct object, object of a prepo- sition), but they each have an ARG1 semantic role label in PropBank.3 Furthermore, only one of the MOs needs to be specified, as in Exam- ple 3 where the second MO (presumably the company?s prices) is unstated.	MO	matched objects$Modifier$matched object$	2
The tem-  plate pattern search recognizes relationships  between MOs in the defined pat-  tern a.s well a.s recognizing the.	MO	matched objects$Modifier$matched object$	2
Max- imum likelihood models can be estimated from mil- lions of sentences of bitext, but optimize a mis- MOive, predicting events observed in word aligned bitext instead of optimizing translation quality.	MO	matched objects$Modifier$matched object$	2
The MOs constitute here a FL expres- sion instead of a video sequence track.	MO	matched objects$Modifier$matched object$	2
FFin~l OBJ?	OBJ	Object$object$Objective$	0
i :  "~)   Fin de la  f igure  ~5  Macro  : ~rn   Initial OBJ?	OBJ	Object$object$Objective$	0
OBJ recognition from local scale-invariant features.	OBJ	Object$object$Objective$	0
Theory and Prac-  tice of OBJ Systems, 2(1):31-41.	OBJ	Object$object$Objective$	0
OBJs in cluster 1, corresponding to binary ad- jectives, have high values for most of the features containing a preposition after the adjective (observe +1pe, ?	OBJ	Object$object$Objective$	0
OBJs in cluster 0 (unary adjectives), symmetrically, have low values for these features, and high values for the default adjective position in Catalan (directly postnominal: -1cn).	OBJ	Object$object$Objective$	0
Here we would compare the degree to which  each possible candidate antecedent (A Japanese  company, television picture tubes, Japan, TV   sets, and Malaysia in this example) could serve  as the direct OBJ of "export".	OBJ	Object$object$Objective$	1
We specifically extract the character reference CH either from the dependency relation nsubj, which links a speech verb SV with a CH that is the syntactic subject of a clause, or from the dependency relation dobj, which links a SV with a CH that is the direct OBJ of the speech verb, across a conjunct (e.g., and).	OBJ	Object$object$Objective$	1
A canonical example of selectional  restriction is that of the verb "eat", which se-  lects food as its direct OBJ.	OBJ	Object$object$Objective$	1
Characters in chil- dren?s stories can either be human or non-human entities, i.e., animals and non-living OBJs, ex- hibiting anthropomorphic traits.	OBJ	Object$object$Objective$	1
The passive char- acters were identified via the following relations extracted by dependency parsing: nsubjpass (passive nominal subject) and pobj (OBJ of a preposition).	OBJ	Object$object$Objective$	1
To that end, we  devised a more OBJive test, useful only for  scoring the subset of referents that are names  of people.	OBJ	Object$object$Objective$	1
would compare the degree to which  each possible candidate antecedent (A Japanese  company, television picture tubes, Japan, TV   sets, and Malaysia in this example) could serve  as the direct OBJ of "export".	OBJ	Object$object$Objective$	1
4 OBJ Functions for Parsing A split PCFG is a grammar G over symbols of the form X-k where X is an evaluation symbol (such as NP) and k is some indicator of a subcategory, such as a parent annotation.	OBJ	Object$object$Objective$	2
SEMI-AUTOMATED FORCES (SAF) PROJECT 8  C/1 TB is to the east and its mission is to attack  OBJ GAMMA from ES646905 to ES758911  at 141423 Apr. All TB is to the south.	OBJ	Object$object$Objective$	2
The content of Figure 1 can be reconstructed  straightforwardly asa category structure subject o a set  of L c constraints (for a closely related analysis of this  10  {~ Animate  Question _ _  -- Subjective  Case OBJ  Reflex ve  Possessive Possessive-Determ ner  _ I First  Personal ~_ .P__~ Second _ _ I Feminine  Ingu la r - -  J Neuter  f \[ | Plural  Demonstrative - - l~  Near  / Far  Figure 1: Systemic Network for English Pronouns  example, developed independently, see Mellish (1986).	OBJ	Object$object$Objective$	2
These include, on the one hand, inner participants or arguments,  such as Actor, Addressee, OBJ, and, on the other hand, free modifications, uch  as Locative, Means, Manner, Cause, several temporal and directional modifications,  those of Condition, Regard, Accompaniment, etc.	OBJ	Object$object$Objective$	2
I. OBJs of the Parser  We designed a new syntactic analy-  sis system (a parser) with the follow-  ing objectives.	OBJ	Object$object$Objective$	2
While these three methods yield 409 OBJ P R F1 EX BEST DERIVATION Viterbi Derivation 89.6 89.4 89.5 37.4 RERANKING Random 87.6 87.7 87.7 16.4 Precision (sampled) 91.1 88.1 89.6 21.4 Recall (sampled) 88.2 91.3 89.7 21.5 F1 (sampled) 90.2 89.3 89.8 27.2 Exact (sampled) 89.5 89.5 89.5 25.8 Exact (non-sampled) 90.8 90.8 90.8 41.7 Exact/F1 (oracle) 95.3 94.4 95.0 63.9 DYNAMIC PROGRAMMING VARIATIONAL 90.7 90.9 90.8 41.4 MAX-RULE-SUM	OBJ	Object$object$Objective$	2
4.2 Collaborative Filtering CF techniques make prediction using information from similar users.	CF	Collaborative filtering$context factor$confidence$Crowdflower$Control Features$corpus frequency$communicative function$Chain frequency$certainty factor$	0
CF is the technique of using  peer opinions to predict the interests of others.	CF	Collaborative filtering$context factor$confidence$Crowdflower$Control Features$corpus frequency$communicative function$Chain frequency$certainty factor$	0
CF via gaussian probabilistic latent semantic analysis.	CF	Collaborative filtering$context factor$confidence$Crowdflower$Control Features$corpus frequency$communicative function$Chain frequency$certainty factor$	0
0.14  0.16  0.18  0.2  0.22  1  10  100 MA P  10 Number of Groups Ubuntu Forum Latent Group SVM  0.15  0.2  0.25  0.3  0.35  1  10  100 MA P  10 Number of Groups Wow Gaming Latent Group SVM  0.2  0.3  0.4  0.5  0.6  1  10  100 MA P  10 Number of Groups Fitness Forum Latent Group SVM Figure 4: CF results: MAP@10 vs. user group number.	CF	Collaborative filtering$context factor$confidence$Crowdflower$Control Features$corpus frequency$communicative function$Chain frequency$certainty factor$	0
CF, on the other hand, uses co-occurrence information from a collection of users for recommendation.	CF	Collaborative filtering$context factor$confidence$Crowdflower$Control Features$corpus frequency$communicative function$Chain frequency$certainty factor$	0
CF systems can be ei- ther model based or memory based (Breese et al 1998).	CF	Collaborative filtering$context factor$confidence$Crowdflower$Control Features$corpus frequency$communicative function$Chain frequency$certainty factor$	0
when estimating a higher-order gram, instead of using the raw occurrence count, only a portion is used and the remainder is computed using a lower- order model in which one of the CFs 2 Note that factors at indices 0,?1, . . . ,?(	CF	Collaborative filtering$context factor$confidence$Crowdflower$Control Features$corpus frequency$communicative function$Chain frequency$certainty factor$	1
The proposed  model for reference resolution elaborates on Alshawi's (1987) notions of CFs and  salience and integrates both linguistic and perceptual context effects.	CF	Collaborative filtering$context factor$confidence$Crowdflower$Control Features$corpus frequency$communicative function$Chain frequency$certainty factor$	1
The CF is also recognized by Dybkj?r and Bernsen (2000).	CF	Collaborative filtering$context factor$confidence$Crowdflower$Control Features$corpus frequency$communicative function$Chain frequency$certainty factor$	1
Finally, new termhood esti- mations (NC-values) are calculated as a linear  combination of the C-values and CFs.	CF	Collaborative filtering$context factor$confidence$Crowdflower$Control Features$corpus frequency$communicative function$Chain frequency$certainty factor$	1
Subse- quently, CFs are assigned to candidate  terms according to their co-occurrence with top- ranked context words.	CF	Collaborative filtering$context factor$confidence$Crowdflower$Control Features$corpus frequency$communicative function$Chain frequency$certainty factor$	1
Additionally, we remove low CF alignment links from the word alignment of a bilingual training corpus, which increases the alignment F-score, improves Chinese-English and Arabic-English translation quality and sig- nificantly reduces the phrase translation table size.	CF	Collaborative filtering$context factor$confidence$Crowdflower$Control Features$corpus frequency$communicative function$Chain frequency$certainty factor$	2
c?2009 ACL and AFNLP Confidence Measure for Word Alignment Fei Huang IBM T.J.Watson Research Center Yorktown Heights, NY 10598, USA huangfe@us.ibm.com Abstract In this paper we present a CF mea- sure for word alignment based on the posterior probability of alignment links.	CF	Collaborative filtering$context factor$confidence$Crowdflower$Control Features$corpus frequency$communicative function$Chain frequency$certainty factor$	2
ter-judge (J1, J2, J3), and with GS J1 J2 J3 %agr  %agr  %agr  J2 0.83 0.74 J3 0.88 0.80 0.80 0.68 GS 0.93 0.89 0.83 0.74 0.92 0.87 Table 3: Agreement for the basic/event/object pa- rameter: inter-judge (J1, J2, J3), and with GS As can be seen, the agreement among judges is remarkably high for a lexical semantics task: All but one values of the kappa statistics are above 0.6 (+/-0.13 for a 95% CF interval).	CF	Collaborative filtering$context factor$confidence$Crowdflower$Control Features$corpus frequency$communicative function$Chain frequency$certainty factor$	2
nt Fei Huang IBM T.J.Watson Research Center Yorktown Heights, NY 10598, USA huangfe@us.ibm.com Abstract In this paper we present a CF mea- sure for word alignment based on the posterior probability of alignment links.	CF	Collaborative filtering$context factor$confidence$Crowdflower$Control Features$corpus frequency$communicative function$Chain frequency$certainty factor$	2
We illustrate the correlation between the align- ment CF measure and the alignment qual- ity on the sentence level, and present several ap- proaches to improve alignment accuracy based on the proposed CF measure: sentence align- ment selection, alignment link	CF	Collaborative filtering$context factor$confidence$Crowdflower$Control Features$corpus frequency$communicative function$Chain frequency$certainty factor$	2
In this paper we introduce a CF mea- sure for word alignment, which is robust to extra or missing words in the bilingual sentence pairs, as well as word alignment errors.	CF	Collaborative filtering$context factor$confidence$Crowdflower$Control Features$corpus frequency$communicative function$Chain frequency$certainty factor$	2
Based on these mea- sures, we improve the alignment qual- ity by selecting high CF sentence alignments and alignment links from mul- tiple word alignments of the same sen- tence pair.	CF	Collaborative filtering$context factor$confidence$Crowdflower$Control Features$corpus frequency$communicative function$Chain frequency$certainty factor$	2
We propose a sentence alignment CF measure based on the alignment?s posterior probability, and ex- tend it to the alignment link CF measure.	CF	Collaborative filtering$context factor$confidence$Crowdflower$Control Features$corpus frequency$communicative function$Chain frequency$certainty factor$	2
We illustrate the correlation between the align- ment CF measure and the alignment qu	CF	Collaborative filtering$context factor$confidence$Crowdflower$Control Features$corpus frequency$communicative function$Chain frequency$certainty factor$	2
244   Figure 1: A NE type task in the CF interface   After these initial tasks, inter-annotator agree- ment was estimated at 91%, which can be taken  to be a reasonable upper bound for our automat- ed system.	CF	Collaborative filtering$context factor$confidence$Crowdflower$Control Features$corpus frequency$communicative function$Chain frequency$certainty factor$	3
Merging the type labels produced by Turkers  (with the help of CF) is an interesting  problem in itself.	CF	Collaborative filtering$context factor$confidence$Crowdflower$Control Features$corpus frequency$communicative function$Chain frequency$certainty factor$	3
CF is a  crowdsourcing service built on top of AMT that  associates a trust level with workers based on  their performance on gold data and uses these  trust levels to determine the correctness of work- er responses.	CF	Collaborative filtering$context factor$confidence$Crowdflower$Control Features$corpus frequency$communicative function$Chain frequency$certainty factor$	3
Instead of putting these tasks directly onto  AMT, we chose to leverage CF for its  added quality control.	CF	Collaborative filtering$context factor$confidence$Crowdflower$Control Features$corpus frequency$communicative function$Chain frequency$certainty factor$	3
0.4 0.5 0.6 0.7  0  2  4  6 Number of human annotations x 1,000 Ac cu ra cy algorithm MomResp LogResp Majority CF Annotations Figure 5: Inferred label accuracy on annotations gathered from CrowdFlower over a subset of 1000 instances of the 20 Newsgroups dataset.	CF	Collaborative filtering$context factor$confidence$Crowdflower$Control Features$corpus frequency$communicative function$Chain frequency$certainty factor$	3
4.2 Human Judgements of Image Relevance Human judgements of the suitability of each im- age were obtained using an online crowdsourcing platform, CF7.	CF	Collaborative filtering$context factor$confidence$Crowdflower$Control Features$corpus frequency$communicative function$Chain frequency$certainty factor$	3
CF: avoid rare words where possible; ?	CF	Collaborative filtering$context factor$confidence$Crowdflower$Control Features$corpus frequency$communicative function$Chain frequency$certainty factor$	5
As we expected, the CF baseline does very well.	CF	Collaborative filtering$context factor$confidence$Crowdflower$Control Features$corpus frequency$communicative function$Chain frequency$certainty factor$	5
Note that the \[S NP VP\]  fragment with the unspecified NUM value is produced  for both sentences and thus its CF is 2.	CF	Collaborative filtering$context factor$confidence$Crowdflower$Control Features$corpus frequency$communicative function$Chain frequency$certainty factor$	5
Then we multiply these measures by the  CF of this particular word and av-  erage them.	CF	Collaborative filtering$context factor$confidence$Crowdflower$Control Features$corpus frequency$communicative function$Chain frequency$certainty factor$	5
and the scaled CF are interpolated to get a smoothed model tpos.	CF	Collaborative filtering$context factor$confidence$Crowdflower$Control Features$corpus frequency$communicative function$Chain frequency$certainty factor$	5
Its designers relied mainly on linguistic evidence, such as CF, rather than psycholinguistic motivations.	CF	Collaborative filtering$context factor$confidence$Crowdflower$Control Features$corpus frequency$communicative function$Chain frequency$certainty factor$	5
into single, nonhierarchical argumentative moves (i.e., rhetorically coherent pieces of text, which perform the same CF).	CF	Collaborative filtering$context factor$confidence$Crowdflower$Control Features$corpus frequency$communicative function$Chain frequency$certainty factor$	6
Some subordinate it entirely to structure, some  attempt to combine structure and function felici-  tously, others place CF clearly  in the foreground.	CF	Collaborative filtering$context factor$confidence$Crowdflower$Control Features$corpus frequency$communicative function$Chain frequency$certainty factor$	6
June 21-24, 1994  produce texts whose CF has to be interpreted in terms of the concrete  situation in which they were produced.	CF	Collaborative filtering$context factor$confidence$Crowdflower$Control Features$corpus frequency$communicative function$Chain frequency$certainty factor$	6
All studies discussed so far are only concerned with sequences of CFs, and disregard the semantic content of dialogue acts.	CF	Collaborative filtering$context factor$confidence$Crowdflower$Control Features$corpus frequency$communicative function$Chain frequency$certainty factor$	6
For the most part, however, it is assumed that  there is a stereotyped set of functions involved  in performing a global CF  in a restricted omain.	CF	Collaborative filtering$context factor$confidence$Crowdflower$Control Features$corpus frequency$communicative function$Chain frequency$certainty factor$	6
Acces- sed online on 16 March 2012 from: http://www- bcf.usc.edu/~billmann/diversity/DDivers-site.htm   Misu T, Ohtake K, Hori C, Kashioka H, Nakamura S  (2009) Annotating CF and se- mantic content in dialogue act for construction of  consulting dialogue systems.	CF	Collaborative filtering$context factor$confidence$Crowdflower$Control Features$corpus frequency$communicative function$Chain frequency$certainty factor$	6
Heuristic answers are expressed in terms of  CFs which, as in the MYCIN system  (Shortliffe 1976), take their values in the range  (-1,+ 15: "-I" expresses absolute disbelief; "0"  expresses complete uncertainty; "1" expresses ab-  solute befief.	CF	Collaborative filtering$context factor$confidence$Crowdflower$Control Features$corpus frequency$communicative function$Chain frequency$certainty factor$	8
checking for a PARTOF relation be-  tween a head and a "with" complement:  I. if the head is not a noun, the relation  doesn't hold (CF = - 15;  2.	CF	Collaborative filtering$context factor$confidence$Crowdflower$Control Features$corpus frequency$communicative function$Chain frequency$certainty factor$	8
follows:  HI- for checking for an INSTRUMENT relation  between a head and a "with" complement:  I. if the head is not a verb, the relation  doesn't hold (CF = -15;  2.	CF	Collaborative filtering$context factor$confidence$Crowdflower$Control Features$corpus frequency$communicative function$Chain frequency$certainty factor$	8
if some "part-of pattern" (see below)  exists in the dictionary definition of the  complement, and if this pattern points  to a defining term that can be linked  with the head, then the PARTOF re-  lation probably holds (CF  = 0.7);  3.	CF	Collaborative filtering$context factor$confidence$Crowdflower$Control Features$corpus frequency$communicative function$Chain frequency$certainty factor$	8
The two main heuristics that are used to  evaluate the plausability of (la) against (Ib) can  be described in English as follows:  HI- for checking for an INSTRUMENT relation  between a head and a "with" complement:  I. if the head is not a verb, the relation  doesn't hold (CF = -15;  2.	CF	Collaborative filtering$context factor$confidence$Crowdflower$Control Features$corpus frequency$communicative function$Chain frequency$certainty factor$	8
n  of the complement, and if this pattern  points to a defining term that can be  linked with the head, then the relation  probably holds (CF = 0.7);  3.	CF	Collaborative filtering$context factor$confidence$Crowdflower$Control Features$corpus frequency$communicative function$Chain frequency$certainty factor$	8
H2- for  Each CF refers to the specific  proposition (or goal) to which the heuristic is  applied.	CF	Collaborative filtering$context factor$confidence$Crowdflower$Control Features$corpus frequency$communicative function$Chain frequency$certainty factor$	8
hold (CF = - 15;  2.	CF	Collaborative filtering$context factor$confidence$Crowdflower$Control Features$corpus frequency$communicative function$Chain frequency$certainty factor$	8
Thus, if clause 3 of heuristic 112 is used  when applied to the proposition (lb), the result-  ing CF -0.3 will indicate a relatively  moderate disbelief in this proposition, stemming  from the fact that the system has not been able  to find any positive evidence in the dictionary to  s	CF	Collaborative filtering$context factor$confidence$Crowdflower$Control Features$corpus frequency$communicative function$Chain frequency$certainty factor$	8
Thus, if clause 3 of heuristic 112 is used  when applied to the proposition (lb), the result-  ing CF -0.3 will indicate a relatively  moderate	CF	Collaborative filtering$context factor$confidence$Crowdflower$Control Features$corpus frequency$communicative function$Chain frequency$certainty factor$	8
if some "instrument pattern" (see be-  low) exists in the dictionary def'mition  of the complement, and if this pattern  points to a defining term that can be  linked with the head, then the relation  probably holds (CF = 0.7);  3.	CF	Collaborative filtering$context factor$confidence$Crowdflower$Control Features$corpus frequency$communicative function$Chain frequency$certainty factor$	8
CDC research has re- cently become more popular due to the increasing interests in the web person search task (Artiles et al, 2007).	CDC	Cross document coreference$Context Dependent Claim$	0
3.6 Cross Document Coreference Resolution CDC resolution is performed via the phylogenetic entity clustering model of Andrews et al (2014).	CDC	Cross document coreference$Context Dependent Claim$	0
CDC, on the other hand, is a more challenging task because these linguistics cues and sentence structures no longer apply, given the wide variety of context and styles in different documents.	CDC	Cross document coreference$Context Dependent Claim$	0
Purity improves because of the removal of noise entities, though at the sacrifice of inverse purity and the Table 2: CDC performance on subsets (I. Purity denotes inverse purity).	CDC	Cross document coreference$Context Dependent Claim$	0
CDC Detection Ran Levy Yonatan Bilu Daniel Hershcovich Ehud Aharoni Noam Slonim IBM Haifa Research Lab / Mount Carmel, Haifa, 31905, Israel {ranl,yonatanb,danielh,aehud,noams}@il.ibm.com Abstract While discussing a concrete controversial topic, most humans will find it challenging to swiftly raise a diverse set of convincing and relevant claims that should set the basis of thei	CDC	Cross document coreference$Context Dependent Claim$	1
Besides the LMl trained in riddle style, we also train a general lan- guage model with the web documents.	LM	language mode$Landmark$Language Modeling$Long Match$Language Model$linear$local minimum$monosemous links$language model$Bigram Model$	0
For this technique, we represent the  snippets in the form of a probability distribution of  words, creating a so-called entity LMl (Allan  and Raghavan, 2002).	LM	language mode$Landmark$Language Modeling$Long Match$Language Model$linear$local minimum$monosemous links$language model$Bigram Model$	0
These techniques were first discussed by Brill et al (2001), who observed that, as the size of the text corpus increases, it becomes more likely that the answer to a specific question can be found with data-intensive methods that do not require a complex LMl.	LM	language mode$Landmark$Language Modeling$Long Match$Language Model$linear$local minimum$monosemous links$language model$Bigram Model$	0
Im- proving word alignment with LMl based confidence scores.	LM	language mode$Landmark$Language Modeling$Long Match$Language Model$linear$local minimum$monosemous links$language model$Bigram Model$	0
The data for training LMl in riddle style include two parts: One is the corpus of rid- dles mentioned above, and the other is a corpus of Chinese poem and Chinese couplets because of the similar language style.	LM	language mode$Landmark$Language Modeling$Long Match$Language Model$linear$local minimum$monosemous links$language model$Bigram Model$	0
In providing the fundamental organization of the gram- 983 mar, to the extent that that organization is consistent with the LMled, these types significantly ease the path to creating a working grammar.	LM	language mode$Landmark$Language Modeling$Long Match$Language Model$linear$local minimum$monosemous links$language model$Bigram Model$	0
Some of the factors effecting this decision are object 4See (Gorniak and Roy, 2004) for further discussion on the use of spatial extrema of the scene and groups of objects in the scene as landmarks Figure 5: LM salience salience and the functional relationships between objects.	LM	language mode$Landmark$Language Modeling$Long Match$Language Model$linear$local minimum$monosemous links$language model$Bigram Model$	1
spatialRelation (UsedLM) ?	LM	language mode$Landmark$Language Modeling$Long Match$Language Model$linear$local minimum$monosemous links$language model$Bigram Model$	1
name (LM) ?	LM	language mode$Landmark$Language Modeling$Long Match$Language Model$linear$local minimum$monosemous links$language model$Bigram Model$	1
LMs in OpenLS: A data structure for cognitive ergonomic route directions.	LM	language mode$Landmark$Language Modeling$Long Match$Language Model$linear$local minimum$monosemous links$language model$Bigram Model$	1
LM Locality: Binary feature indicat- ing if l?	LM	language mode$Landmark$Language Modeling$Long Match$Language Model$linear$local minimum$monosemous links$language model$Bigram Model$	1
3.2 LMs and Descriptions If we want to use a locative expression, we must choose another object in the scene to function as landmark.	LM	language mode$Landmark$Language Modeling$Long Match$Language Model$linear$local minimum$monosemous links$language model$Bigram Model$	1
Statistical LM Using Leaving-One-Out", in S. Young & G. Bloothooft (eds.),	LM	language mode$Landmark$Language Modeling$Long Match$Language Model$linear$local minimum$monosemous links$language model$Bigram Model$	2
Combining Semantic and Syntactic Structure for LM", Proceed- ings ICSLP-2000, Beijing, China.	LM	language mode$Landmark$Language Modeling$Long Match$Language Model$linear$local minimum$monosemous links$language model$Bigram Model$	2
Toward a Unified Approach to Statistical LM for Chinese.	LM	language mode$Landmark$Language Modeling$Long Match$Language Model$linear$local minimum$monosemous links$language model$Bigram Model$	2
Structured  LM.	LM	language mode$Landmark$Language Modeling$Long Match$Language Model$linear$local minimum$monosemous links$language model$Bigram Model$	2
"Principles of Lexical LM for  Speech Recognition".	LM	language mode$Landmark$Language Modeling$Long Match$Language Model$linear$local minimum$monosemous links$language model$Bigram Model$	2
A LM Approach for Georeferenc- ing Textual Documents.	LM	language mode$Landmark$Language Modeling$Long Match$Language Model$linear$local minimum$monosemous links$language model$Bigram Model$	2
Statistical LMing Using Leaving-One-Out", in S. Young & G. Bloothooft (eds.),	LM	language mode$Landmark$Language Modeling$Long Match$Language Model$linear$local minimum$monosemous links$language model$Bigram Model$	4
Combining Semantic and Syntactic Structure for LMing", Proceed- ings ICSLP-2000, Beijing, China.	LM	language mode$Landmark$Language Modeling$Long Match$Language Model$linear$local minimum$monosemous links$language model$Bigram Model$	4
Estima.tion of Probabilities fi:om Sparse Data for the LM Com-  ponent of a. Speech Recogniser.	LM	language mode$Landmark$Language Modeling$Long Match$Language Model$linear$local minimum$monosemous links$language model$Bigram Model$	4
Structured  LMing.	LM	language mode$Landmark$Language Modeling$Long Match$Language Model$linear$local minimum$monosemous links$language model$Bigram Model$	4
A LMing Approach for Georeferenc- ing Textual Documents.	LM	language mode$Landmark$Language Modeling$Long Match$Language Model$linear$local minimum$monosemous links$language model$Bigram Model$	4
3.2.2 LM For our language model, we use a Kneser-Ney smoothed trigram model learned from a version of the British National Corpus modified to use Americanized spellings (Chen and Goodman, 1996; Burnard, 1995).	LM	language mode$Landmark$Language Modeling$Long Match$Language Model$linear$local minimum$monosemous links$language model$Bigram Model$	4
In Section 2, we detail how the task of sequential labeling is formalized in terms of LM classifi- cation, and explain the Viterbi algorithm required for prediction.	LM	language mode$Landmark$Language Modeling$Long Match$Language Model$linear$local minimum$monosemous links$language model$Bigram Model$	5
LMity  The reader may be concerned that tim regularity consmtint  in\]poses tmdue restrictions (51' LMity (m the candidate  forms, and, in doing so, vitiates the phonological advan-  tages of non-LM epresentations.	LM	language mode$Landmark$Language Modeling$Long Match$Language Model$linear$local minimum$monosemous links$language model$Bigram Model$	5
We will first explain LM classification, and then apply a Markov assumption to the classification formal- ism.	LM	language mode$Landmark$Language Modeling$Long Match$Language Model$linear$local minimum$monosemous links$language model$Bigram Model$	5
We next present several algorithms for optimizing the weight vector in a LM classi- fier in Section 3.	LM	language mode$Landmark$Language Modeling$Long Match$Language Model$linear$local minimum$monosemous links$language model$Bigram Model$	5
For a cleaner presentation, we detail the gradient derivation in Appendix A. Given that the optimization problem is not convex, initializing the model from a good projection matrix often helps re- duce training time and may lead to convergence to a better LM.	LM	language mode$Landmark$Language Modeling$Long Match$Language Model$linear$local minimum$monosemous links$language model$Bigram Model$	6
Thus, we could land in a non-optimal LM using online learning.	LM	language mode$Landmark$Language Modeling$Long Match$Language Model$linear$local minimum$monosemous links$language model$Bigram Model$	6
Unlike correlative models (Amari S. and Maginu  K., 1988), neither distortion of pattern nor pseudo  LM solutions arise from memorizing  other patterns.	LM	language mode$Landmark$Language Modeling$Long Match$Language Model$linear$local minimum$monosemous links$language model$Bigram Model$	6
Then, in a manner quite similar to (Hearst, 1994), the algorithm determines for every LM mi how sharp of a change there is in the lexical cohesion function.	LM	language mode$Landmark$Language Modeling$Long Match$Language Model$linear$local minimum$monosemous links$language model$Bigram Model$	6
However, these for- mulations are often non-convex and thus suffer from LM.	LM	language mode$Landmark$Language Modeling$Long Match$Language Model$linear$local minimum$monosemous links$language model$Bigram Model$	6
The phrase acquisition method is  a greedy algorithm that performs local optimization  based on an iterative process which converges to a  LM of PP(T) .	LM	language mode$Landmark$Language Modeling$Long Match$Language Model$linear$local minimum$monosemous links$language model$Bigram Model$	6
Besides the LM trained in riddle style, we also train a general lan- guage model with the web documents.	LM	language mode$Landmark$Language Modeling$Long Match$Language Model$linear$local minimum$monosemous links$language model$Bigram Model$	8
For this technique, we represent the  snippets in the form of a probability distribution of  words, creating a so-called entity LM (Allan  and Raghavan, 2002).	LM	language mode$Landmark$Language Modeling$Long Match$Language Model$linear$local minimum$monosemous links$language model$Bigram Model$	8
These techniques were first discussed by Brill et al (2001), who observed that, as the size of the text corpus increases, it becomes more likely that the answer to a specific question can be found with data-intensive methods that do not require a complex LM.	LM	language mode$Landmark$Language Modeling$Long Match$Language Model$linear$local minimum$monosemous links$language model$Bigram Model$	8
Im- proving word alignment with LM based confidence scores.	LM	language mode$Landmark$Language Modeling$Long Match$Language Model$linear$local minimum$monosemous links$language model$Bigram Model$	8
The data for training LM in riddle style include two parts: One is the corpus of rid- dles mentioned above, and the other is a corpus of Chinese poem and Chinese couplets because of the similar language style.	LM	language mode$Landmark$Language Modeling$Long Match$Language Model$linear$local minimum$monosemous links$language model$Bigram Model$	8
In providing the fundamental organization of the gram- 983 mar, to the extent that that organization is consistent with the LMed, these types significantly ease the path to creating a working grammar.	LM	language mode$Landmark$Language Modeling$Long Match$Language Model$linear$local minimum$monosemous links$language model$Bigram Model$	8
4 LM 4.1 The Hierarchical Dirichlet Process Model The results of our unigram experiments suggested that word segmentation could be improved by taking into account dependencies between words.	LM	language mode$Landmark$Language Modeling$Long Match$Language Model$linear$local minimum$monosemous links$language model$Bigram Model$	9
LM This model, inspired by the ap- proach of Collins et al (1999) for parsing the Prague Dependency Treebank, builds on Collins?	LM	language mode$Landmark$Language Modeling$Long Match$Language Model$linear$local minimum$monosemous links$language model$Bigram Model$	9
4.1 LM The simplest model we define over dialogs is the bi- gram model of Eckert et al1997): p (ui|m) = p (ui|mi?1) (1) p (u|m) = ?	LM	language mode$Landmark$Language Modeling$Long Match$Language Model$linear$local minimum$monosemous links$language model$Bigram Model$	9
4.1 LMs {DPI, Feedback} Model Indeed the first signifi- cant models that include a DA bigram include the {DPI, Feedback} DA sequence.	LM	language mode$Landmark$Language Modeling$Long Match$Language Model$linear$local minimum$monosemous links$language model$Bigram Model$	9
4.2 LM The problem of the unigram model can be alleviated by the bigram model based on a hierarchical Dirich- let process (Goldwater et al, 2009).	LM	language mode$Landmark$Language Modeling$Long Match$Language Model$linear$local minimum$monosemous links$language model$Bigram Model$	9
5.2 Statistical LM Similar to Ringger et al (2004), we find the order with the highest probability conditioned on syntac- tic and semantic categories.	LM	language mode$Landmark$Language Modeling$Long Match$Language Model$linear$local minimum$monosemous links$language model$Bigram Model$	9
The contribution of this paper is to show how to model a troubleshooting SDS as a partially observable Markov decision process (POMDP).	SDS	spoken dialog system$Spoken dialogue systems$Spoken dialogue system$	0
(4) 3 Timestep n Timestep n+1 x' am au    y' au      r' x am au   y au     r d'd c,au ~com com com'ts ts' ' c,au ~com' Figure 3: Influence diagram of a troubleshooting SDS.	SDS	spoken dialog system$Spoken dialogue systems$Spoken dialogue system$	0
The conditional probability tables composing the model were handcrafted based on conversations with troubleshooting experts and past experience with SDSs.	SDS	spoken dialog system$Spoken dialogue systems$Spoken dialogue system$	0
Section 2 re- views POMDPs, the general troubleshooting prob- lem, and POMDP-based SDSs; section 3 explains how these two POMDPs can be combined to model a troubleshooting SDS; sections 4-5 present results from simulation; and section 6 concludes.	SDS	spoken dialog system$Spoken dialogue systems$Spoken dialogue system$	0
on: DSL-1 To illustrate the general framework, we first created a very simple troubleshooting SDS called DSL-1.	SDS	spoken dialog system$Spoken dialogue systems$Spoken dialogue system$	0
4 Illustration: DSL-1 To illustrate the general framework, we first created a very simple troubleshooting SDS called DSL-1.	SDS	spoken dialog system$Spoken dialogue systems$Spoken dialogue system$	0
6 Conclusions This paper has shown how a SDS for troubleshooting can be cast as a POMDP.	SDS	spoken dialog system$Spoken dialogue systems$Spoken dialogue system$	0
1 Introduction SDS are complex frameworks, involving the integration of speech recognition, speech synthesis, natural language understanding and generation, dialogue management, and interac- tion with domain-specific applications.	SDS	spoken dialog system$Spoken dialogue systems$Spoken dialogue system$	1
1 Introduction 1.1 Spoken Dialogue Systems and Non-Native Speakers SDS rely on models of human lan- guage to understand users?	SDS	spoken dialog system$Spoken dialogue systems$Spoken dialogue system$	1
1 Introduction SDS that can detect and adapt to user affect1 are fast becoming reality (Schuller et al, 2009b; Batliner et al, 2008; Prendinger and Ishizuka, 2005; Vidrascu and Devillers, 2005; Lee ?	SDS	spoken dialog system$Spoken dialogue systems$Spoken dialogue system$	1
What?s the Trouble: Automatically Identifying Problematic Dialogues in DARPA Communicator Dialogue Systems Helen Wright Hastie, Rashmi Prasad, Marilyn Walker AT& T Labs - Research 180 Park Ave, Florham Park, N.J. 07932, U.S.A. hhastie,rjprasad,walker@research.att.com Abstract SDS promise effi- cient and natural access to information services from any phone.	SDS	spoken dialog system$Spoken dialogue systems$Spoken dialogue system$	1
2 Introduction SDS for cars emerged in the late 1990s with the appearance of advanced information and communication systems.	SDS	spoken dialog system$Spoken dialogue systems$Spoken dialogue system$	1
1 Introduction SDS have traditionally fo- cused on task-oriented dialogues, such as mak- ing flight bookings or providing public transport timetables.	SDS	spoken dialog system$Spoken dialogue systems$Spoken dialogue system$	1
1 Introduction SDSs are complex frameworks, involving the integration of speech recognition, speech synthesis, natural language understanding and generation, dialogue management, and interac- tion with domain-specific applications.	SDS	spoken dialog system$Spoken dialogue systems$Spoken dialogue system$	2
1 Introduction 1.1 Spoken Dialogue Systems and Non-Native Speakers SDSs rely on models of human lan- guage to understand users?	SDS	spoken dialog system$Spoken dialogue systems$Spoken dialogue system$	2
1 Introduction SDSs that can detect and adapt to user affect1 are fast becoming reality (Schuller et al, 2009b; Batliner et al, 2008; Prendinger and Ishizuka, 2005; Vidrascu and Devillers, 2005; Lee ?	SDS	spoken dialog system$Spoken dialogue systems$Spoken dialogue system$	2
What?s the Trouble: Automatically Identifying Problematic Dialogues in DARPA Communicator Dialogue Systems Helen Wright Hastie, Rashmi Prasad, Marilyn Walker AT& T Labs - Research 180 Park Ave, Florham Park, N.J. 07932, U.S.A. hhastie,rjprasad,walker@research.att.com Abstract SDSs promise effi- cient and natural access to information services from any phone.	SDS	spoken dialog system$Spoken dialogue systems$Spoken dialogue system$	2
2 Introduction SDSs for cars emerged in the late 1990s with the appearance of advanced information and communication systems.	SDS	spoken dialog system$Spoken dialogue systems$Spoken dialogue system$	2
1 Introduction SDSs have traditionally fo- cused on task-oriented dialogues, such as mak- ing flight bookings or providing public transport timetables.	SDS	spoken dialog system$Spoken dialogue systems$Spoken dialogue system$	2
This  eliminates the potential BS that clustering the entities  becomes easier if the entities are clearly from different  genres.	BS	BI abstracts$bias$Bacillus subtilis$beam search$ScrollBw$	1
However, this would introduce systematic BSes into the counts, because nouns do in fact very often occur adjacently to prepositions that modify them, but many verbs do not.	BS	BI abstracts$bias$Bacillus subtilis$beam search$ScrollBw$	1
(Volk, 2002) already suggested that this count- ing method introduced a general BS toward verb attachment, and when comparing the results for very frequent words (for which more reliable evi- dence is available from the treebank) we find that verb attachments are in fact systematically over- estimated.	BS	BI abstracts$bias$Bacillus subtilis$beam search$ScrollBw$	1
Nor  do we use the gender/animiticity information  gathered from the much smaller hand-marked  text, both because we were interested in seeing  what unsupervised learning could accomplish,  and because we were concerned with inherit-  ing strong BSes from the limited hand-marked  data.	BS	BI abstracts$bias$Bacillus subtilis$beam search$ScrollBw$	1
7.2 Domain-specific sub-corpora         The person-x corpus may appear to be BSed due to  the manner of its construction.	BS	BI abstracts$bias$Bacillus subtilis$beam search$ScrollBw$	1
Both methods face obvi- ous problems: The data-driven approach is at the mercy of its training set and cannot easily avoid mistakes that result from BSed or scarce data.	BS	BI abstracts$bias$Bacillus subtilis$beam search$ScrollBw$	1
A first set of 23,000 doc- uments was retrieved, identifying the presence of the bacterium BS in the text and/or in the MeSH terms.	BS	BI abstracts$bias$Bacillus subtilis$beam search$ScrollBw$	2
In this study we aimed at extracting a gene regulatory network of the popular model organism the BS.	BS	BI abstracts$bias$Bacillus subtilis$beam search$ScrollBw$	2
Con- necting parts with processes: SubtiWiki and Subti- Pathways integrate gene and pathway annotation for BS.	BS	BI abstracts$bias$Bacillus subtilis$beam search$ScrollBw$	2
A community-curated con- sensual annotation that is continuously updated: The BS centred wiki SubtiWiki.	BS	BI abstracts$bias$Bacillus subtilis$beam search$ScrollBw$	2
3.1 Bacteria Gene Interactions corpus The source of the Bacteria GI Task corpus is a set of PubMed abstracts mainly dealing with the tran- 70 scription of genes in BS.	BS	BI abstracts$bias$Bacillus subtilis$beam search$ScrollBw$	2
Operon | Regulon | Family PMID 3127379 : Three promoters direct transcription of the sigA (rpoD) operon in BS.	BS	BI abstracts$bias$Bacillus subtilis$beam search$ScrollBw$	2
2012) use a simple two-level genera- tive hierarchical approach using Naive Bayes, but to our knowledge no previous work implements a multi-level discriminative hierarchical model with BS for geolocation.	BS	BI abstracts$bias$Bacillus subtilis$beam search$ScrollBw$	3
We use a DP-based BS procedure similar to the one presented in (Tillmann, 2001).	BS	BI abstracts$bias$Bacillus subtilis$beam search$ScrollBw$	3
Rather than computing the 339 probability of every leaf cell using equation 3, we use a stratified BS: starting at the root cell, keep the b highest-probability cells at each level until reaching the leaf node level.	BS	BI abstracts$bias$Bacillus subtilis$beam search$ScrollBw$	3
In addition to allowing one to use many classi- fiers that each have a manageable number of out- comes, the hierarchical approach naturally lends itself to BS.	BS	BI abstracts$bias$Bacillus subtilis$beam search$ScrollBw$	3
For reference, we show the most accurate models from Alberti et al (2015) and Weiss et al (2015), which use a deeper model and BS for inference.	BS	BI abstracts$bias$Bacillus subtilis$beam search$ScrollBw$	3
2004; Kaji and Kitsuregawa, 2013) or BS (Jiang et al.,	BS	BI abstracts$bias$Bacillus subtilis$beam search$ScrollBw$	3
Clear Concept: PaSbPlSSb, SSbSbPaPl, PaPlSSbSb, PlSSbSbPa (a combination of SeekBw and BS clicks, indicating high tussle with the video lecture content) ?	BS	BI abstracts$bias$Bacillus subtilis$beam search$ScrollBw$	4
clicks in the follow- ing 8 categories: Play (Pl), Pause (Pa), SeekFw (Sf), SeekBw (Sb), ScrollFw (SSf), BS (SSb), RatechangeFast (Rf), RatechangeSlow (Rs).	BS	BI abstracts$bias$Bacillus subtilis$beam search$ScrollBw$	4
Building applied NLG systems.	NLG	natural language generation$Natural Language Generation$Natural language Generation$	0
In these approaches to NLG  there is a gap between the plan, which is usually  represented in the terms of the application program, and  the resources used by the realization component tocarry  out that plan, which are the concrete words, syntax,  morphemes, etc.	NLG	natural language generation$Natural Language Generation$Natural language Generation$	0
The order of prenominal adjectives in NLG.	NLG	natural language generation$Natural Language Generation$Natural language Generation$	0
5 Ordering of Prenominal Adjectives The ordering of prenominal modifiers is important for NLG systems where the text must be both fluent and grammatical.	NLG	natural language generation$Natural Language Generation$Natural language Generation$	0
Building NLG systems.	NLG	natural language generation$Natural Language Generation$Natural language Generation$	0
The role of the University of Edinburgh in this project  is the development of a NLG com-  ponent which can automatically derive various kinds of  specification documents from the common underlying  database.	NLG	natural language generation$Natural Language Generation$Natural language Generation$	0
59  References  \[Barnett, 90\] J. Barnett and I. Mani, "Using Bidi-  rectional Semantic Rules for Generation",  Proceedings of the Fifth International Work-  shop on NLG, pp.47-  53, Dawson, Pa., 3-6 June, 1990.	NLG	natural language generation$Natural Language Generation$Natural language Generation$	1
In International Workshop on NLG, pages 178?187, Niagara-on-the-Lake, Canada, August.	NLG	natural language generation$Natural Language Generation$Natural language Generation$	1
framenet.icsi.berkeley.edu  9  CORECT: Combining CSCW with NLG  for Collaborative Requirements Capture  John Levine" and Chris Mellish t Department of Artificial Intelligence,  University of Edinburgh,  80 South Bridge,  Edinburgh EH1 1HN, Scotland, UK.	NLG	natural language generation$Natural Language Generation$Natural language Generation$	1
In Proceedings of the Eleventh Eu- ropean Workshop on NLG, pages 139?142.	NLG	natural language generation$Natural Language Generation$Natural language Generation$	1
I  I  I  I  I  i  I  I  i  i  I  I  I  I   Towards Automatic Generation of NLG Systems John Chen?,	NLG	natural language generation$Natural Language Generation$Natural language Generation$	1
While at first glance, one might think that prob-  lems of NLG only occur  in the last phase of the system processing, we  believe that a "generation" perspective on the  entire process is extremely beneficial.	NLG	natural language generation$Natural Language Generation$Natural language Generation$	1
I would like a higher optical zoom, the W200 does a great digital zoom translation... Table 3: Opinion Organization Result for Sony Cybershot DSC-W200 Camera listed in Freebase, but we can find it in people?s online discussion using MI.	MI	mutual information$Mutual Infromation$multiple imputation$mation$Mutual Information$misclassification index$Mutual information$	0
Note that this binary reference matrix can  be converted to a strength-of-association matrix by multiplying it with a diagonal matrix that con- tains the strength-of-association scores, e.g. log  likelihood ratio, Mutual Information, Pointwise MI, Chi-squared to name a few.	MI	mutual information$Mutual Infromation$multiple imputation$mation$Mutual Information$misclassification index$Mutual information$	0
2 statistic and the log- likelihood ratio perform best, the dice coefficient the second best, and the MI the worst.	MI	mutual information$Mutual Infromation$multiple imputation$mation$Mutual Information$misclassification index$Mutual information$	0
Ld Then, based on the contingency table of co- occurrence document frequencies of tE and tJ below, we estimate bilingual term correspon- dences according to the statistical measures such as the MI, the ?	MI	mutual information$Mutual Infromation$multiple imputation$mation$Mutual Information$misclassification index$Mutual information$	0
i.e., we label the largest clusters by se- lecting phrases with top MI.	MI	mutual information$Mutual Infromation$multiple imputation$mation$Mutual Information$misclassification index$Mutual information$	0
Word association norms, MI, and  lexicography.	MI	mutual information$Mutual Infromation$multiple imputation$mation$Mutual Information$misclassification index$Mutual information$	0
Since our focus 225 is not on finding the best extraction method, but on judging the benefit of statistical components to parsing, we employ a collocation measure related to the idea of MI: a collocation between a word w and a preposition p is judged more likely the more often it appears, and the less often its component words appear.	MI	mutual information$Mutual Infromation$multiple imputation$mation$Mutual Information$misclassification index$Mutual information$	0
Schafer and Graham (2002) review techniques to deal with missing data, and recommend two approaches: maximum likelihood estimation and Bayesian MI.	MI	mutual information$Mutual Infromation$multiple imputation$mation$Mutual Information$misclassification index$Mutual information$	2
This method is known in the liter- ature as MI (Rubin, 1987).	MI	mutual information$Mutual Infromation$multiple imputation$mation$Mutual Information$misclassification index$Mutual information$	2
Tree-bank  text (Marcus et al, 1993) that has been marked  with co-reference inforMI.	MI	mutual information$Mutual Infromation$multiple imputation$mation$Mutual Information$misclassification index$Mutual information$	3
We present some experiments il-  lustrating the accuracy of the method and note  that with this inforMI added, our pronoun  resolution method achieves 84.2% accuracy.	MI	mutual information$Mutual Infromation$multiple imputation$mation$Mutual Information$misclassification index$Mutual information$	3
The first piece of useful inforMI we con-  sider is the distance between the pronoun  and the candidate antecedent.	MI	mutual information$Mutual Infromation$multiple imputation$mation$Mutual Information$misclassification index$Mutual information$	3
forMI added, our pronoun  resolution method achieves 84.2% accuracy.	MI	mutual information$Mutual Infromation$multiple imputation$mation$Mutual Information$misclassification index$Mutual information$	3
The second half of the paper describes a  method for using (portions of) t~e aforemen-  tioned program to learn automatically the typi-  cal gender of English words, inforMI that is  itself used in the pronoun resolution program.	MI	mutual information$Mutual Infromation$multiple imputation$mation$Mutual Information$misclassification index$Mutual information$	3
This program differs  from earlier work in its almost complete lack of  hand-crafting, relying instead on a very small  corpus of Penn Wall Street Journal Tree-bank  text (Marcus et al, 1993) that has been marked  with co-reference inforMI.	MI	mutual information$Mutual Infromation$multiple imputation$mation$Mutual Information$misclassification index$Mutual information$	3
The second  experiment investigates a method for unsuper-  vised learning of gender/number/animaticity  inforMI.	MI	mutual information$Mutual Infromation$multiple imputation$mation$Mutual Information$misclassification index$Mutual information$	3
Our first experiment  shows the relative contribution of each source  Of inforMI and demonstrates a uccess rate  of 82.9% for all sources combined.	MI	mutual information$Mutual Infromation$multiple imputation$mation$Mutual Information$misclassification index$Mutual information$	3
We also study the behavior of approxi- mate Pointwise MI and Log Likeli- hood Ratio for the sketches.	MI	mutual information$Mutual Infromation$multiple imputation$mation$Mutual Information$misclassification index$Mutual information$	4
Note that this binary reference matrix can  be converted to a strength-of-association matrix by multiplying it with a diagonal matrix that con- tains the strength-of-association scores, e.g. log  likelihood ratio, MI, Pointwise mutual information, Chi-squared to name a few.	MI	mutual information$Mutual Infromation$multiple imputation$mation$Mutual Information$misclassification index$Mutual information$	4
However, the raw frequency is certainly not  the only way to rank the verbs; we plan on explor- ing other metrics such as MI.	MI	mutual information$Mutual Infromation$multiple imputation$mation$Mutual Information$misclassification index$Mutual information$	4
(Baroni and Vegnaduzzo, 2004) and (Grefenstette et al, 2004) gathered subjective adjectives from the web calculating the MI score.	MI	mutual information$Mutual Infromation$multiple imputation$mation$Mutual Information$misclassification index$Mutual information$	4
III, 2011a), computing approximate as- sociation scores like Pointwise MI (Li et al 2008; Van Durme and Lall, 2009b; Goyal and Daume?	MI	mutual information$Mutual Infromation$multiple imputation$mation$Mutual Information$misclassification index$Mutual information$	4
In ad- dition to those clustering algorithms, we are also exam- ining the use of various lexical association measures such as MI, Dice coefficient, ?	MI	mutual information$Mutual Infromation$multiple imputation$mation$Mutual Information$misclassification index$Mutual information$	4
2MI, though potentially of interest as a  measure of collocational status, was not tested due to its  well-known property of overemphasising the significance of  rare events (Church and Hanks, 1990).	MI	mutual information$Mutual Infromation$multiple imputation$mation$Mutual Information$misclassification index$Mutual information$	6
These pairs were manually tagged as  arguments, therefore MI makes  the right prediction.	MI	mutual information$Mutual Infromation$multiple imputation$mation$Mutual Information$misclassification index$Mutual information$	6
t t I t t t t  150 200 250 300 350 400 450 500  Window size (sentences)  Figure 1: MI gain varying window  size  5 Discussion  The main result of this paper is to show that  analogous to the case of words in language mod-  elling, a significant amount of extrasentential infor-  mation can be extracted from the long-range his-  tory of a document, using trigger pairs for tags and  rules.	MI	mutual information$Mutual Infromation$multiple imputation$mation$Mutual Information$misclassification index$Mutual information$	6
MI values in table 2 go  along with the manual tagging for these last pairs  as well, because the MI values are  low as should correspond to adjuncts.	MI	mutual information$Mutual Infromation$multiple imputation$mation$Mutual Information$misclassification index$Mutual information$	6
In contrast, the precisions of MI and Dice formula  were not so ideal.	MI	mutual information$Mutual Infromation$multiple imputation$mation$Mutual Information$misclassification index$Mutual information$	6
Mutual Information Bits MI clustering, as  described in \[10\], creates a a class "tree" for a given vocab-  ulary.	MI	mutual information$Mutual Infromation$multiple imputation$mation$Mutual Information$misclassification index$Mutual information$	6
Chinese NLP resources     Part 2: Text Processing  2.1 Lexical processing   a. Segmentation   b. Disambiguation   c. Unknown word detection   d. NERn  2.2 Syntactic processing   a. Issues in PoS tagging   b. Hidden Markov Models  2.3 NLP Applications  References   Academia Sinica Balance Corpus of Mandarin Chi- nese.	NER	Named Entity Recognitio$named entity recognition$Named Entity Recognition$named entity taggers$nition$named entity recogniser$Named Entity Recognizer$Name Entity Recognizer$Named entity recognition$	0
Workshop on NERn.	NER	Named Entity Recognitio$named entity recognition$Named Entity Recognition$named entity taggers$nition$named entity recogniser$Named Entity Recognizer$Name Entity Recognizer$Named entity recognition$	0
The Third International Chi- nese Language Processing Bakeoff: Word Seg- mentation and NERn, In  Proceedings of SIGHAN5 the 3rd International  Chinese Language Processing Bakeoff at Col- ing/ACL 2006, July, Sydney, Australia, 108-117.	NER	Named Entity Recognitio$named entity recognition$Named Entity Recognition$named entity taggers$nition$named entity recogniser$Named Entity Recognizer$Name Entity Recognizer$Named entity recognition$	0
3) We employed IE methods (including pattern sets  and NERn) as initial extraction  steps.	NER	Named Entity Recognitio$named entity recognition$Named Entity Recognition$named entity taggers$nition$named entity recogniser$Named Entity Recognizer$Name Entity Recognizer$Named entity recognition$	0
A Maximum Entropy Ap- proach to NERn.	NER	Named Entity Recognitio$named entity recognition$Named Entity Recognition$named entity taggers$nition$named entity recogniser$Named Entity Recognizer$Name Entity Recognizer$Named entity recognition$	0
Ex- tracting Personal Names from Email: Applying  NERn to Informal Text, Proc.	NER	Named Entity Recognitio$named entity recognition$Named Entity Recognition$named entity taggers$nition$named entity recogniser$Named Entity Recognizer$Name Entity Recognizer$Named entity recognition$	0
Annotation guide- lines for machine learning-based NER in microbiology.	NER	Named Entity Recognitio$named entity recognition$Named Entity Recognition$named entity taggers$nition$named entity recogniser$Named Entity Recognizer$Name Entity Recognizer$Named entity recognition$	1
Early results for NER with conditional random fields, feature induction and web-enhanced lexicons.	NER	Named Entity Recognitio$named entity recognition$Named Entity Recognition$named entity taggers$nition$named entity recogniser$Named Entity Recognizer$Name Entity Recognizer$Named entity recognition$	1
We used two approaches for identifying story characters motivated by (Elson and McKeown, 2010): 1) NER was used for identifying proper names, e.g., ?	NER	Named Entity Recognitio$named entity recognition$Named Entity Recognition$named entity taggers$nition$named entity recogniser$Named Entity Recognizer$Name Entity Recognizer$Named entity recognition$	1
Joint inference of NER and normal- ization for tweets.	NER	Named Entity Recognitio$named entity recognition$Named Entity Recognition$named entity taggers$nition$named entity recogniser$Named Entity Recognizer$Name Entity Recognizer$Named entity recognition$	1
This includes (i) tokenization, (ii) sen- tence splitting and identification of paragraph boundaries, (iii) part-of-speech (POS) tagging, (iv) lemmatization, (v) NER, (vi) dependency parsing, and (vii) co-reference analysis.	NER	Named Entity Recognitio$named entity recognition$Named Entity Recognition$named entity taggers$nition$named entity recogniser$Named Entity Recognizer$Name Entity Recognizer$Named entity recognition$	1
Two broad approaches for the iden- tification of story characters were followed: (i) NER, and (ii) identification of character nominals, e.g., ?	NER	Named Entity Recognitio$named entity recognition$Named Entity Recognition$named entity taggers$nition$named entity recogniser$Named Entity Recognizer$Name Entity Recognizer$Named entity recognition$	1
Chinese NLP resources     Part 2: Text Processing  2.1 Lexical processing   a. Segmentation   b. Disambiguation   c. Unknown word detection   d. NER  2.2 Syntactic processing   a. Issues in PoS tagging   b. Hidden Markov Models  2.3 NLP Applications  References   Academia Sinica Balance Corpus of Mandarin Chi- nese.	NER	Named Entity Recognitio$named entity recognition$Named Entity Recognition$named entity taggers$nition$named entity recogniser$Named Entity Recognizer$Name Entity Recognizer$Named entity recognition$	2
Workshop on NER.	NER	Named Entity Recognitio$named entity recognition$Named Entity Recognition$named entity taggers$nition$named entity recogniser$Named Entity Recognizer$Name Entity Recognizer$Named entity recognition$	2
The Third International Chi- nese Language Processing Bakeoff: Word Seg- mentation and NER, In  Proceedings of SIGHAN5 the 3rd International  Chinese Language Processing Bakeoff at Col- ing/ACL 2006, July, Sydney, Australia, 108-117.	NER	Named Entity Recognitio$named entity recognition$Named Entity Recognition$named entity taggers$nition$named entity recogniser$Named Entity Recognizer$Name Entity Recognizer$Named entity recognition$	2
3) We employed IE methods (including pattern sets  and NER) as initial extraction  steps.	NER	Named Entity Recognitio$named entity recognition$Named Entity Recognition$named entity taggers$nition$named entity recogniser$Named Entity Recognizer$Name Entity Recognizer$Named entity recognition$	2
A Maximum Entropy Ap- proach to NER.	NER	Named Entity Recognitio$named entity recognition$Named Entity Recognition$named entity taggers$nition$named entity recogniser$Named Entity Recognizer$Name Entity Recognizer$Named entity recognition$	2
Ex- tracting Personal Names from Email: Applying  NER to Informal Text, Proc.	NER	Named Entity Recognitio$named entity recognition$Named Entity Recognition$named entity taggers$nition$named entity recogniser$Named Entity Recognizer$Name Entity Recognizer$Named entity recognition$	2
However, the main disadvantage of relying on semantic analyzers, NER and the like, is that for some languages these tools are not yet well developed.	NER	Named Entity Recognitio$named entity recognition$Named Entity Recognition$named entity taggers$nition$named entity recogniser$Named Entity Recognizer$Name Entity Recognizer$Named entity recognition$	3
4 Recommended Uses We have found the component corpora of MedTag to be useful for the following functions: 1) Training and evaluating part-of-speech taggers 2) Training and evaluating gene/protein NER 1http://cancerweb.ncl.ac.uk/omd/copyleft.html http://www.onelook.com/ 3) Developing and evaluating a noun phrase bracketer for PubMed phrase indexing 4) Statistical analysis of grammatical usage in medical text 5) Feature generation for machine learn- ing The MedPost tagger was recently ported to Java and is currently being employed in MetaMap, a pro- gram that maps natural la	NER	Named Entity Recognitio$named entity recognition$Named Entity Recognition$named entity taggers$nition$named entity recogniser$Named Entity Recognizer$Name Entity Recognizer$Named entity recognition$	3
The CALBC Silver Standard Corpus for biomedical named entities: A study in harmonizing the contributions from four indepen- dent NER.	NER	Named Entity Recognitio$named entity recognition$Named Entity Recognition$named entity taggers$nition$named entity recogniser$Named Entity Recognizer$Name Entity Recognizer$Named entity recognition$	3
We plan to utilise existing NER de- veloped in our group as a pre-annotation step in the creation of our reference standard.	NER	Named Entity Recognitio$named entity recognition$Named Entity Recognition$named entity taggers$nition$named entity recogniser$Named Entity Recognizer$Name Entity Recognizer$Named entity recognition$	3
This research was partially funded by the EC?s 7th Framework Pro- gramme within the CALBC project (FP7-231727) and the GERONTOSYS research initiative from the 13We used a gold standard in which some unusual entities (e.g., protein families) had been annotated for which most NER have not been trained.	NER	Named Entity Recognitio$named entity recognition$Named Entity Recognition$named entity taggers$nition$named entity recogniser$Named Entity Recognizer$Name Entity Recognizer$Named entity recognition$	3
NER.2 The various contributions covered, among others, an- notations for genes and proteins, chemicals, dis- eases, etc (Rebholz-Schuhmann et al, 2010b).	NER	Named Entity Recognitio$named entity recognition$Named Entity Recognition$named entity taggers$nition$named entity recogniser$Named Entity Recognizer$Name Entity Recognizer$Named entity recognition$	3
The item 4) is situated at the edge E of domain D.  This deliNER covers a family of constraints depending on  the instantiations of the arguments: E is either left (L) or  right (R), domain \]nay be syllable, foot or word, and ~b can  be any phonological object, such as stress or an affix.	NER	Named Entity Recognitio$named entity recognition$Named Entity Recognition$named entity taggers$nition$named entity recogniser$Named Entity Recognizer$Name Entity Recognizer$Named entity recognition$	4
Therefore, the first task in future work will be to review the defiNER and characterisation of this class.	NER	Named Entity Recognitio$named entity recognition$Named Entity Recognition$named entity taggers$nition$named entity recogniser$Named Entity Recognizer$Name Entity Recognizer$Named entity recognition$	4
We used two approaches for identifying story characters motivated by (Elson and McKeown, 2010): 1) named entity recogNER was used for identifying proper names, e.g., ?	NER	Named Entity Recognitio$named entity recognition$Named Entity Recognition$named entity taggers$nition$named entity recogniser$Named Entity Recognizer$Name Entity Recognizer$Named entity recognition$	4
Unsupervised personality recogNER for social network sites.	NER	Named Entity Recognitio$named entity recognition$Named Entity Recognition$named entity taggers$nition$named entity recogniser$Named Entity Recognizer$Name Entity Recognizer$Named entity recognition$	4
This includes (i) tokenization, (ii) sen- tence splitting and identification of paragraph boundaries, (iii) part-of-speech (POS) tagging, (iv) lemmatization, (v) named entity recogNER, (vi) dependency parsing, and (vii) co-reference analysis.	NER	Named Entity Recognitio$named entity recognition$Named Entity Recognition$named entity taggers$nition$named entity recogniser$Named Entity Recognizer$Name Entity Recognizer$Named entity recognition$	4
Two broad approaches for the iden- tification of story characters were followed: (i) named entity recogNER, and (ii) identification of character nominals, e.g., ?	NER	Named Entity Recognitio$named entity recognition$Named Entity Recognition$named entity taggers$nition$named entity recogniser$Named Entity Recognizer$Name Entity Recognizer$Named entity recognition$	4
A high precision rate means a low false 2 Base named entities such as PERSON and LOCATION were found using Stanford?s NER (Finkel et al, 2005).	NER	Named Entity Recognitio$named entity recognition$Named Entity Recognition$named entity taggers$nition$named entity recogniser$Named Entity Recognizer$Name Entity Recognizer$Named entity recognition$	5
2.1 Protein and small molecule recognition Two dictionary-based NERs were used to detect the names of proteins and small molecules in the full collection of MEDLINE ab- stracts, with the two source dictionaries constructed using the resources UniProt (Apweiler et al, 2004) and ChEBI (De Matos et al, 2006) respectively.	NER	Named Entity Recognitio$named entity recognition$Named Entity Recognition$named entity taggers$nition$named entity recogniser$Named Entity Recognizer$Name Entity Recognizer$Named entity recognition$	5
corpus annotation and evaluation: use the default NER to boot- strap the manual annotation of the test data for the dialogue application; evaluate the performance of the default NE gram- mars on the dialogue texts; suggest possi- ble improvements on the basis of the infor- mation about missed and incorrect anno- tations provided by the corpus benchmark tool.	NER	Named Entity Recognitio$named entity recognition$Named Entity Recognition$named entity taggers$nition$named entity recogniser$Named Entity Recognizer$Name Entity Recognizer$Named entity recognition$	5
The whole collection of MED- LINE was filtered using a co-occurrence approach and a NER.	NER	Named Entity Recognitio$named entity recognition$Named Entity Recognition$named entity taggers$nition$named entity recogniser$Named Entity Recognizer$Name Entity Recognizer$Named entity recognition$	5
A conventional NER is then trained on the data containing the joined labels.	NER	Named Entity Recognitio$named entity recognition$Named Entity Recognition$named entity taggers$nition$named entity recogniser$Named Entity Recognizer$Name Entity Recognizer$Named entity recognition$	5
We used a state-of-the-art  NER that was part of a larger  toolbox for named entity recognition.	NER	Named Entity Recognitio$named entity recognition$Named Entity Recognition$named entity taggers$nition$named entity recogniser$Named Entity Recognizer$Name Entity Recognizer$Named entity recognition$	5
Training and Evalu- ating a German NER with Se- mantic Generalization.	NER	Named Entity Recognitio$named entity recognition$Named Entity Recognition$named entity taggers$nition$named entity recogniser$Named Entity Recognizer$Name Entity Recognizer$Named entity recognition$	6
We first ex- tract named entities from scattered opinions DT using Stanford NER (Finkel et al, 2005).	NER	Named Entity Recognitio$named entity recognition$Named Entity Recognition$named entity taggers$nition$named entity recogniser$Named Entity Recognizer$Name Entity Recognizer$Named entity recognition$	6
For the Named Entity classifier, we added the feature Named-Entity-type as obtained by the NER.	NER	Named Entity Recognitio$named entity recognition$Named Entity Recognition$named entity taggers$nition$named entity recogniser$Named Entity Recognizer$Name Entity Recognizer$Named entity recognition$	6
We run the Stanford NER (Finkel et al, 2005) and record the number of PERSONs, ORGANIZATIONs, and LOCATIONs.	NER	Named Entity Recognitio$named entity recognition$Named Entity Recognition$named entity taggers$nition$named entity recogniser$Named Entity Recognizer$Name Entity Recognizer$Named entity recognition$	6
We use a NER which has an accuracy above 90% for proper names.	NER	Named Entity Recognitio$named entity recognition$Named Entity Recognition$named entity taggers$nition$named entity recogniser$Named Entity Recognizer$Name Entity Recognizer$Named entity recognition$	6
M. Faruqui, S. Pado, Training and Evaluating a German NER with Semantic Generaliza- tion, Proceedings of Konvens 2010, Saarbrucken, Ger- many.	NER	Named Entity Recognitio$named entity recognition$Named Entity Recognition$named entity taggers$nition$named entity recogniser$Named Entity Recognizer$Name Entity Recognizer$Named entity recognition$	6
Using a  normal html parser, such as lynx, the text may  lack its usual grammatical structure which may  drastically decrease the performances of sen- tence splitters, NERs and  parsers.	NER	Named Entity Recognitio$named entity recognition$Named Entity Recognition$named entity taggers$nition$named entity recogniser$Named Entity Recognizer$Name Entity Recognizer$Named entity recognition$	7
Accord- ingly, we extract all the named entities in a sentence using Stanford?s NER (Finkel et al.,	NER	Named Entity Recognitio$named entity recognition$Named Entity Recognition$named entity taggers$nition$named entity recogniser$Named Entity Recognizer$Name Entity Recognizer$Named entity recognition$	7
5.1 Modules The pipeline for processing archaeological articles integrates three main modules: a module for recov- ering the logical structure of the documents, a mod- ule for Italian and English POS tagging and a gen- eral NER and finally, a Gazetteer Based NER.	NER	Named Entity Recognitio$named entity recognition$Named Entity Recognition$named entity taggers$nition$named entity recogniser$Named Entity Recognizer$Name Entity Recognizer$Named entity recognition$	7
We resort to Stanford NER 5 to extract seven types of named entities including time, location, organization, person, money, percent and date.	NER	Named Entity Recognitio$named entity recognition$Named Entity Recognition$named entity taggers$nition$named entity recogniser$Named Entity Recognizer$Name Entity Recognizer$Named entity recognition$	7
To identify words that belong to such semantic classes, NERs are used, since most of these words represent names.	NER	Named Entity Recognitio$named entity recognition$Named Entity Recognition$named entity taggers$nition$named entity recogniser$Named Entity Recognizer$Name Entity Recognizer$Named entity recognition$	7
NER through classifier combination.	NER	Named Entity Recognitio$named entity recognition$Named Entity Recognition$named entity taggers$nition$named entity recogniser$Named Entity Recognizer$Name Entity Recognizer$Named entity recognition$	8
NER would help in cases like the following, LHIP provides a processing method which allows selected portions of the input to be ignored or handled differently. (	NER	Named Entity Recognitio$named entity recognition$Named Entity Recognition$named entity taggers$nition$named entity recogniser$Named Entity Recognizer$Name Entity Recognizer$Named entity recognition$	8
NER in tweets: An exper- imental study.	NER	Named Entity Recognitio$named entity recognition$Named Entity Recognition$named entity taggers$nition$named entity recogniser$Named Entity Recognizer$Name Entity Recognizer$Named entity recognition$	8
NER in tweets: an experimental study.	NER	Named Entity Recognitio$named entity recognition$Named Entity Recognition$named entity taggers$nition$named entity recogniser$Named Entity Recognizer$Name Entity Recognizer$Named entity recognition$	8
NER for bacterial type IV secretion systems.	NER	Named Entity Recognitio$named entity recognition$Named Entity Recognition$named entity taggers$nition$named entity recogniser$Named Entity Recognizer$Name Entity Recognizer$Named entity recognition$	8
NER through classifier combina- tion.	NER	Named Entity Recognitio$named entity recognition$Named Entity Recognition$named entity taggers$nition$named entity recogniser$Named Entity Recognizer$Name Entity Recognizer$Named entity recognition$	8
The used patterns are: 1) (DT|CD) (NN|NNS), 2) DT JJ (NN|NNS), 3) NNPOS (NN|NNS), and 4) PRP$ JJ (NN|NNS).	NP	N P$Nuclearity Principle$np$no(:~n phrase$N ~ Cati(P$Noun Phras$Noun Phrase$named page$najprostsze$noun phrase$noun  phrase$nominal parent$	0
The evaluation metrics were precision P (the number of true pos- K TP FP FNP (%) R (%) F1 (%) 10 641 80 149 88.90 81.14 84.84 20 644 79 146 89.07 81.52 85.13 30 644 80 146 88.95 81.52 85.07 40 645 81 145 88.84 81.65 85.09 50 645 80 145 88.97 81.65 85.15 Table 2: Effects of K in Bayes Point Machines itives divided by the total number of elements la- beled as belonging to the positive class) recall R (the number of true positives divided by the to- tal number of elements	NP	N P$Nuclearity Principle$np$no(:~n phrase$N ~ Cati(P$Noun Phras$Noun Phrase$named page$najprostsze$noun phrase$noun  phrase$nominal parent$	0
TP FP FNP (%) R (%) F1 (%) 1 647 79 143 89.12 81.90 85.36 2 647 80 143 89.00 81.90 85.30 1,2 647 81 143 88.87 81.90 85.24 Table 3: Effects of removing features (1) or (2), or both Table 3 shows the effect of removing (1), (2), or both (1) and (2), showing that they overfit the training data.	NP	N P$Nuclearity Principle$np$no(:~n phrase$N ~ Cati(P$Noun Phras$Noun Phrase$named page$najprostsze$noun phrase$noun  phrase$nominal parent$	0
Perceptron TP FP FNP (%) R (%) F1 (%) 671 128 119 83.98 84.94 84.46 Conditional Random Fields TP FP FNP (%) R (%) F1 (%) 643 78 147 89.18 81.39 85.11 Bayes Point Machines TP FP FNP (%) R (%) F1 (%) 647 79 143 89.12 81.90 85.36 Table 4: Performance of different optimization strategies 6 Conclusion To tackle the hedge cue detection problem posed by the CoNLL-2010 shared task, we utilized a classifier for sequentia	NP	N P$Nuclearity Principle$np$no(:~n phrase$N ~ Cati(P$Noun Phras$Noun Phrase$named page$najprostsze$noun phrase$noun  phrase$nominal parent$	0
Perceptron TP FP FNP (%) R (%) F1 (%) 671 128 119 83.98 84.94 84.46 Conditional Random Fields TP FP FNP (%) R (%) F1 (%) 643 78 147 89.18 81.39 85.11 Bayes Point Machines TP FP FNP (%) R (%) F1 (%) 647 79 143 89.12 81.90 85.36 Table 4: Performance of different optimization strategies 6 Conclusion To tackle the hedge cue detection problem posed by the CoNLL-2010 shared task, we utilized a classifier for sequential labeling following previ- ous work (Morante and Daelemans, 2009).	NP	N P$Nuclearity Principle$np$no(:~n phrase$N ~ Cati(P$Noun Phras$Noun Phrase$named page$najprostsze$noun phrase$noun  phrase$nominal parent$	0
2013) use a su- pervised machine learning approach to address the same problem, though many of their preliminary steps and iNPut features are similar to those used in (Elson and McKeown, 2010).	NP	N P$Nuclearity Principle$np$no(:~n phrase$N ~ Cati(P$Noun Phras$Noun Phrase$named page$najprostsze$noun phrase$noun  phrase$nominal parent$	2
48 P. J. Stone, D. C. DuNPhy, M. S. Smith, and D. M. Ogilvie.	NP	N P$Nuclearity Principle$np$no(:~n phrase$N ~ Cati(P$Noun Phras$Noun Phrase$named page$najprostsze$noun phrase$noun  phrase$nominal parent$	2
For binary constrainls, the ewl-  luation of harmony is a sitNPle affair.	NP	N P$Nuclearity Principle$np$no(:~n phrase$N ~ Cati(P$Noun Phras$Noun Phrase$named page$najprostsze$noun phrase$noun  phrase$nominal parent$	2
The iNPut to the system is simply the text of a story with no additional annotation.	NP	N P$Nuclearity Principle$np$no(:~n phrase$N ~ Cati(P$Noun Phras$Noun Phrase$named page$najprostsze$noun phrase$noun  phrase$nominal parent$	2
Consider, lbr exatNPle, tile hinary constraint  ONS(P&S:25).	NP	N P$Nuclearity Principle$np$no(:~n phrase$N ~ Cati(P$Noun Phras$Noun Phrase$named page$najprostsze$noun phrase$noun  phrase$nominal parent$	2
INTRODUCTION  Recent years have sect) two major trends in phonology:  I heor ies  \[ lave begOlllC \[11o1{3 or iented  arOl.llld const la i \ [ l t s   Ihan transformations, while itNPlenmntations have come to  rely increasingly on finite state attlomata nd transducers.	NP	N P$Nuclearity Principle$np$no(:~n phrase$N ~ Cati(P$Noun Phras$Noun Phrase$named page$najprostsze$noun phrase$noun  phrase$nominal parent$	2
A Stochastic Parts Program and NPe Parser for Unrestricted Text.	NP	N P$Nuclearity Principle$np$no(:~n phrase$N ~ Cati(P$Noun Phras$Noun Phrase$named page$najprostsze$noun phrase$noun  phrase$nominal parent$	5
Towards an Annotation  Scheme for NPe Generation.	NP	N P$Nuclearity Principle$np$no(:~n phrase$N ~ Cati(P$Noun Phras$Noun Phrase$named page$najprostsze$noun phrase$noun  phrase$nominal parent$	5
Corpus-based Identifi- cation of Non-Anaphoric NPes.	NP	N P$Nuclearity Principle$np$no(:~n phrase$N ~ Cati(P$Noun Phras$Noun Phrase$named page$najprostsze$noun phrase$noun  phrase$nominal parent$	5
Formal Model of  NPes in Serbo-Croatian.	NP	N P$Nuclearity Principle$np$no(:~n phrase$N ~ Cati(P$Noun Phras$Noun Phrase$named page$najprostsze$noun phrase$noun  phrase$nominal parent$	5
In particular, only NPes were anno-  tated (thereby circumventing problems of null  anaphora, summation, abstraction, etc.,	NP	N P$Nuclearity Principle$np$no(:~n phrase$N ~ Cati(P$Noun Phras$Noun Phrase$named page$najprostsze$noun phrase$noun  phrase$nominal parent$	5
The Semantics of Definite and  Indefinite NPes PhD Thesis, University  of Massachussetts, 1982.	NP	N P$Nuclearity Principle$np$no(:~n phrase$N ~ Cati(P$Noun Phras$Noun Phrase$named page$najprostsze$noun phrase$noun  phrase$nominal parent$	5
A Stochastic Parts Program and NP Parser for Unrestricted Text.	NP	N P$Nuclearity Principle$np$no(:~n phrase$N ~ Cati(P$Noun Phras$Noun Phrase$named page$najprostsze$noun phrase$noun  phrase$nominal parent$	6
Towards an Annotation  Scheme for NP Generation.	NP	N P$Nuclearity Principle$np$no(:~n phrase$N ~ Cati(P$Noun Phras$Noun Phrase$named page$najprostsze$noun phrase$noun  phrase$nominal parent$	6
Corpus-based Identifi- cation of Non-Anaphoric NPs.	NP	N P$Nuclearity Principle$np$no(:~n phrase$N ~ Cati(P$Noun Phras$Noun Phrase$named page$najprostsze$noun phrase$noun  phrase$nominal parent$	6
Formal Model of  NPs in Serbo-Croatian.	NP	N P$Nuclearity Principle$np$no(:~n phrase$N ~ Cati(P$Noun Phras$Noun Phrase$named page$najprostsze$noun phrase$noun  phrase$nominal parent$	6
In particular, only NPs were anno-  tated (thereby circumventing problems of null  anaphora, summation, abstraction, etc.,	NP	N P$Nuclearity Principle$np$no(:~n phrase$N ~ Cati(P$Noun Phras$Noun Phrase$named page$najprostsze$noun phrase$noun  phrase$nominal parent$	6
The Semantics of Definite and  Indefinite NPs PhD Thesis, University  of Massachussetts, 1982.	NP	N P$Nuclearity Principle$np$no(:~n phrase$N ~ Cati(P$Noun Phras$Noun Phrase$named page$najprostsze$noun phrase$noun  phrase$nominal parent$	6
6.2 Representation and Structured Knowledge Transfer Then for each expected English entity e h , if there is a cross-lingual link to link it to an LL (Chinese) entry e l in the KB, we added the title of the LL entry or its redirected/reNP c l as its LL translation.	NP	N P$Nuclearity Principle$np$no(:~n phrase$N ~ Cati(P$Noun Phras$Noun Phrase$named page$najprostsze$noun phrase$noun  phrase$nominal parent$	7
NP zasady i dlaczego trzeba je stosowa? ?	NP	N P$Nuclearity Principle$np$no(:~n phrase$N ~ Cati(P$Noun Phras$Noun Phrase$named page$najprostsze$noun phrase$noun  phrase$nominal parent$	8
We incorpo-  rate multiple anaphora resolution factors into  a statistical framework - -  specifically the dis-  tance between the pronoun and the proposed  antecedent, gender/number/animaticity of the  proposed antecedent, governing head informa-  tion and NP repetition.	NP	N P$Nuclearity Principle$np$no(:~n phrase$N ~ Cati(P$Noun Phras$Noun Phrase$named page$najprostsze$noun phrase$noun  phrase$nominal parent$	9
Figure 1 shows the 43  NPs with the highest salience figures  (run using the Hobbs algorithm).	NP	N P$Nuclearity Principle$np$no(:~n phrase$N ~ Cati(P$Noun Phras$Noun Phrase$named page$najprostsze$noun phrase$noun  phrase$nominal parent$	9
However a singular NP can be the ref-  erent of a plural pronoun, as illustrated by the  following example:  "I think if I tell Viacom I need more  time, they will take 'Cosby' across the  street," says the general manager ol a  network a~liate.	NP	N P$Nuclearity Principle$np$no(:~n phrase$N ~ Cati(P$Noun Phras$Noun Phrase$named page$najprostsze$noun phrase$noun  phrase$nominal parent$	9
GREENSPAN 120(68.7807) 0.9083 0 0.0916  AT&T 198(67.9668) 0.0252 0.0050 0.9696  MINISTER 125(67.7475) 0.864 0.064 0.072  JUDGE - 239(67.5899) 0.7154 0.0836 0.2008  Figure 1: Top 43 NPs according to salience  168  o  o~  o  l .O -   0.8-   0 .5 -   U  ? ?	NP	N P$Nuclearity Principle$np$no(:~n phrase$N ~ Cati(P$Noun Phras$Noun Phrase$named page$najprostsze$noun phrase$noun  phrase$nominal parent$	9
The method described  here is based on simply counting co-occurrences  of pronouns and NPs, and thus can  employ any method of analysis of the text  stream that results in referent/pronoun pairs  (cf. (	NP	N P$Nuclearity Principle$np$no(:~n phrase$N ~ Cati(P$Noun Phras$Noun Phrase$named page$najprostsze$noun phrase$noun  phrase$nominal parent$	9
Gener-  ally, a singular pronoun cannot refer to a plural  NP, so that in resolving such a pro-  noun any plural candidates should be ruled out.	NP	N P$Nuclearity Principle$np$no(:~n phrase$N ~ Cati(P$Noun Phras$Noun Phrase$named page$najprostsze$noun phrase$noun  phrase$nominal parent$	9
In a very  large corpus, it is possible to find many reasonable  paraphrases  of  NPs.	NP	N P$Nuclearity Principle$np$no(:~n phrase$N ~ Cati(P$Noun Phras$Noun Phrase$named page$najprostsze$noun phrase$noun  phrase$nominal parent$	10
the use of a  delctlc In a definite NP, such as "thls X" or  "the last X", hints that the object was either mentioned  prev ious ly  or that  it p robab ly  was evoked by some  prev ious  re ference ,  and  that  it is searchab le )  but  we  will not  examine them here .	NP	N P$Nuclearity Principle$np$no(:~n phrase$N ~ Cati(P$Noun Phras$Noun Phrase$named page$najprostsze$noun phrase$noun  phrase$nominal parent$	10
In our dictionary, however, the verbal  entry also contains organizing elements: in  the first instance, the prepositions that the  verb requires or allows to be placed before  the NPs.	NP	N P$Nuclearity Principle$np$no(:~n phrase$N ~ Cati(P$Noun Phras$Noun Phrase$named page$najprostsze$noun phrase$noun  phrase$nominal parent$	10
In  case  o f  NPs   there  are  two  kerne ls :V  and  one  VG,  and   these  are  mapped to  oneParea .	NP	N P$Nuclearity Principle$np$no(:~n phrase$N ~ Cati(P$Noun Phras$Noun Phrase$named page$najprostsze$noun phrase$noun  phrase$nominal parent$	10
For  example,  Thai  NP  '???(	NP	N P$Nuclearity Principle$np$no(:~n phrase$N ~ Cati(P$Noun Phras$Noun Phrase$named page$najprostsze$noun phrase$noun  phrase$nominal parent$	10
2 Uses  o f  a NP  parser   The recognition and analysis of subclausal structural  units, e.g. noun phrases, is useful for several pur-  poses.	NP	N P$Nuclearity Principle$np$no(:~n phrase$N ~ Cati(P$Noun Phras$Noun Phrase$named page$najprostsze$noun phrase$noun  phrase$nominal parent$	10
In terms of the NPs m, the time complexity in Step 3.b is in O(n m): for every markable, the corresponding term element is to be found, tak- ing at most n repositioning operations on the term layer.	NPs	number of markables$NPB nodes$	0
We present the absolute NPs se- lected as relevant by separate annotators and in two Gold Standards.	NPs	number of markables$NPB nodes$	0
In the rows below the line, N shows  the total NPs, while Z gives the num-  ber of agreements between the annotations.	NPs	number of markables$NPB nodes$	0
Also, we indicate the percentage, given the total NPs 3275.	NPs	number of markables$NPB nodes$	0
N is the total NPs, Z is total num-  ber of agreements between annotators, PE is the expected  agreement by chance.	NPs	number of markables$NPB nodes$	0
Our results show that it is hard to build a justifiable hy- pothesis on the NPs which is larger than the number of actually annotated entities while keeping ?	NPs	number of markables$NPB nodes$	0
Presumably, this was done as a consideration for the NPs?	NPs	number of markables$NPB nodes$	1
For consistency, an extra NP bracket is inserted around NPs not already dominated by an NP.	NPs	number of markables$NPB nodes$	1
These NPs are removed before evaluation.	NPs	number of markables$NPB nodes$	1
NNs for Pattern Recogni- tion, chapter 6.4: Modelling conditional distributions.	NN	Neural Network$Noun$Nearest Neighbor$nouns$native$Neural Networks$NP ( NR$singular or mass noun$	0
Part of Speech Tagging with  NNs.	NN	Neural Network$Noun$Nearest Neighbor$nouns$native$Neural Networks$NP ( NR$singular or mass noun$	0
c?2014 Association for Computational Linguistics Learning Image Embeddings using Convolutional NNs for Improved Multi-Modal Semantics Douwe Kiela ?	NN	Neural Network$Noun$Nearest Neighbor$nouns$native$Neural Networks$NP ( NR$singular or mass noun$	0
In NNs: Tricks of the Trade, pages 599?619.	NN	Neural Network$Noun$Nearest Neighbor$nouns$native$Neural Networks$NP ( NR$singular or mass noun$	0
Second Turkish Symposium on  Artificial Intelligence and NNs?,	NN	Neural Network$Noun$Nearest Neighbor$nouns$native$Neural Networks$NP ( NR$singular or mass noun$	0
NN based Embedding.	NN	Neural Network$Noun$Nearest Neighbor$nouns$native$Neural Networks$NP ( NR$singular or mass noun$	0
NN phrases are proper nouns and personal pronouns.	NN	Neural Network$Noun$Nearest Neighbor$nouns$native$Neural Networks$NP ( NR$singular or mass noun$	1
NN phrases that are mentioned  repeatedly are preferred.	NN	Neural Network$Noun$Nearest Neighbor$nouns$native$Neural Networks$NP ( NR$singular or mass noun$	1
For example, a set  of general filters for English may include the fol- lowing patterns:3    NN+ NN   (Adj | NN)+ NN   (Adj | NN)+| ((Adj | NN)* Prep?) (	NN	Neural Network$Noun$Nearest Neighbor$nouns$native$Neural Networks$NP ( NR$singular or mass noun$	1
Adj | NN)* NN     Although these patterns are regular expressions,  the filters are implemented as unification-like  LR(1) rules (Mima et al, 1995) in order to facili- tate processing of grammatical agreements (if  any) within term candidates.	NN	Neural Network$Noun$Nearest Neighbor$nouns$native$Neural Networks$NP ( NR$singular or mass noun$	1
2.2.3 NN Cosine The nearest neighbor cosine classifier required the creation of a term-document matrix, which contains a row for each training instance of an ambiguous word, and a column for each feature that can occur in the context of an ambiguous word.	NN	Neural Network$Noun$Nearest Neighbor$nouns$native$Neural Networks$NP ( NR$singular or mass noun$	2
1)  PEBLS, a K-NN algorithm (Cost  and Salzberg 1993); (2) C4.5, a decision tree al-  gorithm (Quinlan 1994); (3) Ripper, an induc-  tive rule based classifier (Cohen 1996); (4) the  Naive Bayes classifier; and (5), a probabilistic  model search procedure (Bruce & Wiebe 1994)  using the public domain software CoCo (Bads-  berg 1995).	NN	Neural Network$Noun$Nearest Neighbor$nouns$native$Neural Networks$NP ( NR$singular or mass noun$	2
2007) combine a naive Bayes classifier that uses a vocabulary-based language model with a k-NNs classifier us- ing grammatical features and interpolate the two to predict reading grade level.	NN	Neural Network$Noun$Nearest Neighbor$nouns$native$Neural Networks$NP ( NR$singular or mass noun$	2
The classifiers that use PMI features are Decision Trees, Decision Rules, Na??ve Bayes, K-NN, Kernel Density, and Boosting a weak classifier (De- cision Stumps ?	NN	Neural Network$Noun$Nearest Neighbor$nouns$native$Neural Networks$NP ( NR$singular or mass noun$	2
ML method (Weka) Features Accuracy Decision Trees PMI scores 65.4% Decision Rules PMI scores 65.5% Na??ve Bayes PMI scores 52.5% K-NN PMI scores 64.5% Kernel Density PMI scores 60.5% Boosting (Dec. Stumps) PMI scores 67.7% Na??ve Bayes 500 words 68.0% Decision Trees 500 words 67.0% Na??ve Bayes PMI + 500 words 66.5% Boosting (Dec. Stumps) PMI + 500 words 69.2% Table 6: Comparative results for the supervised learning method using various ML learning algo- rithms (Weka), averaged over the seven groups of near-syno	NN	Neural Network$Noun$Nearest Neighbor$nouns$native$Neural Networks$NP ( NR$singular or mass noun$	2
We also implemented a k-NNs clas- sifier, which treats each individual training instance 1http://tedlab.mit.edu/?dr/SVDLIBC/ 309 as a separate vector (instead of treating each set of training instances that makes up a given sense as a single vector), and finds the k-nearest training in- stances to the test instance.	NN	Neural Network$Noun$Nearest Neighbor$nouns$native$Neural Networks$NP ( NR$singular or mass noun$	2
In particular, the scheme infers the gender of a  referent from the gender of the proNN that  161  refer to it and selects referents using the pro-  noun anaphora program.	NN	Neural Network$Noun$Nearest Neighbor$nouns$native$Neural Networks$NP ( NR$singular or mass noun$	3
In resolving inter-sentential  proNN, the algorithm searches the previous  sentence, again	NN	Neural Network$Noun$Nearest Neighbor$nouns$native$Neural Networks$NP ( NR$singular or mass noun$	3
In resolving inter-sentential  proNN, the a	NN	Neural Network$Noun$Nearest Neighbor$nouns$native$Neural Networks$NP ( NR$singular or mass noun$	3
3.1 The  gender /an imat ic i ty  stat ist ics   After we have identified the correct antecedents  it is a simple counting procedure to compute  P(p\[wa) where wa is in the correct antecedent  for the pronoun p (Note the proNN are  grouped by their gender):  \[ wain the antecedent for p \[  P(pl o) =  When there are multiple relevant words in the  antecedent we apply the likelihood test designed  by Dunning (1993) on all the words in the candi-  date NP.	NN	Neural Network$Noun$Nearest Neighbor$nouns$native$Neural Networks$NP ( NR$singular or mass noun$	3
Given the above possible sources of informar  tion, we arrive at the following equation, where  F(p) denotes a function from proNN to their  antecedents:  F(p) = argmaxP( A(p) = alp, h, l~', t, l, so, d~ A~')  where A(p) is a random variable denoting the  referent of the pronoun p and a is a proposed  antecedent.	NN	Neural Network$Noun$Nearest Neighbor$nouns$native$Neural Networks$NP ( NR$singular or mass noun$	3
flexive  proNN.	NN	Neural Network$Noun$Nearest Neighbor$nouns$native$Neural Networks$NP ( NR$singular or mass noun$	3
In resolving inter-sentential  proNN, the algorithm searches the previous  sentence, again in left-to-right, breadth-first or-  der.	NN	Neural Network$Noun$Nearest Neighbor$nouns$native$Neural Networks$NP ( NR$singular or mass noun$	3
One classical approach to resolving  proNN in text that takes some syntactic fac-  tors into consideration is that of Hobbs (1976).	NN	Neural Network$Noun$Nearest Neighbor$nouns$native$Neural Networks$NP ( NR$singular or mass noun$	3
The most well  studied constraints are those involving reflexive  proNN.	NN	Neural Network$Noun$Nearest Neighbor$nouns$native$Neural Networks$NP ( NR$singular or mass noun$	3
An alterNN, more objec- tive selection method would be to perform ANOVA, which we plan to test in the near future.	NN	Neural Network$Noun$Nearest Neighbor$nouns$native$Neural Networks$NP ( NR$singular or mass noun$	4
Our alterNN mind ergonomic  approach addresses the following aspects: the  constitution of vast knowledge spaces, the best  example being the Web; the evolution of  communication models (one-to-one, one-to-many,  many-to-many); the variety of forms of access,  from the traditional question-answer model to  hyperlinks and timescaling.	NN	Neural Network$Noun$Nearest Neighbor$nouns$native$Neural Networks$NP ( NR$singular or mass noun$	4
DiscrimiNN training meth- ods for hidden Markov models: Theory and exper- iments with perceptron algorithms.	NN	Neural Network$Noun$Nearest Neighbor$nouns$native$Neural Networks$NP ( NR$singular or mass noun$	4
2 Sequential Labeling We discrimiNNly train a Markov model us- ing Bayes Point Machines (BPM).	NN	Neural Network$Noun$Nearest Neighbor$nouns$native$Neural Networks$NP ( NR$singular or mass noun$	4
We then prepared features, and fed the training data to a sequential labeling system, a discrimiNN Markov model much like Conditional Random Fields (CRF), with the difference being that the model parameters are tuned using Bayes Point Machines (BPM), and then compared our model against an equivalent CRF model.	NN	Neural Network$Noun$Nearest Neighbor$nouns$native$Neural Networks$NP ( NR$singular or mass noun$	4
The regression testing facilities of [incr tsdb()] allowed for rapid experimentation with alterNN analyses as new phenomena were brought into the grammar (cf.	NN	Neural Network$Noun$Nearest Neighbor$nouns$native$Neural Networks$NP ( NR$singular or mass noun$	4
NN for Pattern Recogni- tion, chapter 6.4: Modelling conditional distributions.	NN	Neural Network$Noun$Nearest Neighbor$nouns$native$Neural Networks$NP ( NR$singular or mass noun$	5
Part of Speech Tagging with  NN.	NN	Neural Network$Noun$Nearest Neighbor$nouns$native$Neural Networks$NP ( NR$singular or mass noun$	5
Second Turkish Symposium on  Artificial Intelligence and NN?,	NN	Neural Network$Noun$Nearest Neighbor$nouns$native$Neural Networks$NP ( NR$singular or mass noun$	5
c?2014 Association for Computational Linguistics Learning Image Embeddings using Convolutional NN for Improved Multi-Modal Semantics Douwe Kiela ?	NN	Neural Network$Noun$Nearest Neighbor$nouns$native$Neural Networks$NP ( NR$singular or mass noun$	5
In NN: Tricks of the Trade, pages 599?619.	NN	Neural Network$Noun$Nearest Neighbor$nouns$native$Neural Networks$NP ( NR$singular or mass noun$	5
2016 Association for Computational Linguistics Globally Normalized Transition-Based NN Daniel Andor, Chris Alberti, David Weiss, Aliaksei Severyn, Alessandro Presta, Kuzman Ganchev, Slav Petrov and Michael Collins ?	NN	Neural Network$Noun$Nearest Neighbor$nouns$native$Neural Networks$NP ( NR$singular or mass noun$	5
This could be explained 7a + plural noun; to + past tense format; the more + the + base form of adjective 8NN + the 3rd person singular present format; have + past participle format + since by two reasons: (1) A sentence may contain sev- eral kinds of errors.	NN	Neural Network$Noun$Nearest Neighbor$nouns$native$Neural Networks$NP ( NR$singular or mass noun$	7
condition EMPTY are NNs  6b *  The expansion of branch 4d is one of the more interesting aspects of  the context-sensitive analysis since it involves a relative clause.	NN	Neural Network$Noun$Nearest Neighbor$nouns$native$Neural Networks$NP ( NR$singular or mass noun$	7
In fact, the only difference  from orthography alone is the way succeeding orthography can signal a discourse use  for a NN, and a sentential use for adverbs.	NN	Neural Network$Noun$Nearest Neighbor$nouns$native$Neural Networks$NP ( NR$singular or mass noun$	7
However, the interaction plot reveals that manhat- tan distance with RI is the best com- bination, outperforming the second best (cosine without dimensionality reduction) by a consider- able margin.	RI	random indexing$Random Indexing$Rand Index$	0
Singu- lar value decomposition (parameter dimensional- ity reduction, figure 15) weakens the correlation values achieved by the models, but no significant difference is found between RI and the unreduced data.	RI	random indexing$Random Indexing$Rand Index$	0
Examples of the first category are HiDeX (Shaoul and Westbury, 2010), a modern reimplementation of the HAL model, and Semantic Vectors (Widdows and Cohen, 2010), which enforces a RI representation in order to improve scalability.	RI	random indexing$Random Indexing$Rand Index$	0
Interest- ingly, dimensionality reduction is found to neg- atively affect model performance: as shown in figure 7, both RI (ri) and singular 11Backward rank is equivalent to distance in this task.	RI	random indexing$Random Indexing$Rand Index$	0
2009) or RI, for computing a distance matrix between a set of row vectors, and for the identification of the nearest neighbours of a given target term.	RI	random indexing$Random Indexing$Rand Index$	0
Based on the partial effects of the indi- vidual parameters, any combination of manhattan or cosine distance with RI or no di- mensionality reduction should be close to optimal.	RI	random indexing$Random Indexing$Rand Index$	0
To achieve that goal, we devel- oped a strategy based on RI and vec- tor permutations.	RI	random indexing$Random Indexing$Rand Index$	1
D. Then, the method adopted to construct a semantic space that takes into account both syntactic dependencies and RI can be defined as follows: 1.	RI	random indexing$Random Indexing$Rand Index$	1
Section 2 describes RI, the strategy for build- ing our WordSpace, while details about the method used to encode syntactic dependencies are reported in Section 3.	RI	random indexing$Random Indexing$Rand Index$	1
Section 2 describes RI, the strategy for build- ing our WordSpace, while details about the method used to encode syntactic dependencies are report	RI	random indexing$Random Indexing$Rand Index$	1
6 Conclusions In this work, we propose an approach to encode syn- tactic dependencies in WordSpace using vector per- mutations and RI.	RI	random indexing$Random Indexing$Rand Index$	1
a context vector is assigned to each term, as de- scribed in Section 2 (RI); 2.	RI	random indexing$Random Indexing$Rand Index$	1
We propose an approach based on vector permutation and RI to encode several syntactic contexts in a single WordSpace.	RI	random indexing$Random Indexing$Rand Index$	1
BLANC: Implementing the RI for Coreference Evaluation.	RI	random indexing$Random Indexing$Rand Index$	2
For completeness, in Table 2 we report the results of evaluation against Gold PoS tags us- ing two metrics, Variation of Information (Meila, 2003) and Adjusted RI (Hubert & Arabie, 1985).	RI	random indexing$Random Indexing$Rand Index$	2
4.3 Qualitative Analysis Higher values measured with automated metrics such as log-likelihood on a held-out set and the cross-validation classification metric discussed here 246 0 10 20 30 40 50Word Error Rate0.0 0.1 0.2 0.3 0.4 0.5 ari 20_newsgroupsreuters21578enron (a) Adjusted RI results 0 10 20 30 40 50Word Error Rate2.0 2.5 3.0 3.5 4.0 4.5 vi 20_newsgroupsreuters21578enron (b) Variation of Information results (lower is better) Figure 5: Results for the clustering experiments on the three synthetic OCR datasets with TNPD feature selection.	RI	random indexing$Random Indexing$Rand Index$	2
However, this may be an artifact of the relatively poor starting performance for this dataset, a result of the fact that the gold standard labels do not align 245 0 5 10 15 20 25 30 35 40 45Word Error Rate0.0 0.1 0.2 0.3 0.4 0.5 ari 20_newsgroupsreuters21578enron (a) Adjusted RI results 0 5 10 15 20 25 30 35 40 45Word Error Rate2.0 2.5 3.0 3.5 4.0 4.5 vi 20_newsgroupsreuters21578enron (b) Variation of Information results (lower is better) Figure 4: Results for the clustering experiments on the three synthetic datasets without feature selection.	RI	random indexing$Random Indexing$Rand Index$	2
BLANC: Implementing the RI for coreference evalua- tion.	RI	random indexing$Random Indexing$Rand Index$	2
Clustering per- formance is measured on the Indo-European task according to the RI, F-score, Normalized Edit Score (Pantel, 2003) and Normalized Variation of Information (Meila, 2003).	RI	random indexing$Random Indexing$Rand Index$	2
For example, if we want to compare named entity  (NE) processing for a broadcast news source,  created via automatic speech recognition and NE  tagging, we need to compare it to data created by  careful HT and manual NE  tagging.. But the underlying texts--the  recogmzer output and the gold standard  transcription--differ, and the MUC algorithm  cannot be used.	HT	human transcription$hashtag$	0
Fifth, we do not require HT - a labor-intensive step that hinders broader use in a clinical setting.	HT	human transcription$hashtag$	0
The correlations decrease somewhat due to recog- nition errors when evaluated on the output of an automatic speech recognition system; how- ever, the additional use of word confidence scores can achieve correlations at a similar level as for HTs.	HT	human transcription$hashtag$	0
We  tested on the HT (0% WER)  and the ASR (15% WER) versions of the 1998  evaluation transcripts.	HT	human transcription$hashtag$	0
The tool also serves as a fast and systematic method of checking HT accuracy and thereby facilitates better methods of phonetic transcription (Cucchiarini, 1996; Shriberg, Hinke, & Trost-Steffen, 1987).	HT	human transcription$hashtag$	0
training date Figure 1: Dependency between amount of training data and transcription automation rate less errors than a human being as it is trained on HTs?	HT	human transcription$hashtag$	0
They are: retweet; HT; reply; link, if the tweet con- tains a link; punctuation (exclamation and ques- tions marks); emoticons (textual expression rep- resenting facial expressions); and upper cases (the number of words that starts with upper case in the tweet).	HT	human transcription$hashtag$	1
Another measured how high-level tweet features (i.e., link, mention, and HT fre- quencies) vary across languages (Weerkamp et al 2011).	HT	human transcription$hashtag$	1
e.g., k-top HTs) refers to the k most discriminating items of that type for each label (i.e., Male/Female).	HT	human transcription$hashtag$	1
The features we employed were: k-top words, k-top digrams and trigrams, k-top HTs, k-top mentions, tweet/retweet/HT/link/mention frequencies, and out/in-neighborhood size.	HT	human transcription$hashtag$	1
However, few of these studies have considered measures beyond simple HT fre- quencies, relative mention counts among politicians, and retweet counts.	HT	human transcription$hashtag$	1
1 Introduction Since its early years, the CLs field has devoted much effort to the development of formal systems for modeling the syntax of nat- ural language.	CL	computational linguistic$clause$Class$classifier$chain length$chosen algorithms$clitics$closed$computational linguistics$	0
He has developed several machine learning based natural language  processing systems that are widely used in the CLs community and  in industry and has presented invited talks and tutorials in several major conferences.	CL	computational linguistic$clause$Class$classifier$chain length$chosen algorithms$clitics$closed$computational linguistics$	0
Generalizing the core hierarchy and libraries of the Matrix to sup- port languages like Wambaya can extend its typo- logical reach and further its development as an in- vestigation in CL typology.	CL	computational linguistic$clause$Class$classifier$chain length$chosen algorithms$clitics$closed$computational linguistics$	0
Most of the early work attempted to implement QA systems from the early per- spective of AI or CLs.	CL	computational linguistic$clause$Class$classifier$chain length$chosen algorithms$clitics$closed$computational linguistics$	0
During the 1970s and 1980s there was intensive research on the development of the- oretical bases for CLs.	CL	computational linguistic$clause$Class$classifier$chain length$chosen algorithms$clitics$closed$computational linguistics$	0
1 In t roduct ion   Attempts to the automatic identification of a struc-  ture in discourse have so far met with a limited  success in the CLs literature.	CL	computational linguistic$clause$Class$classifier$chain length$chosen algorithms$clitics$closed$computational linguistics$	0
tag gloss tag gloss *cd CL delimiter aj adjective *dd def.	CL	computational linguistic$clause$Class$classifier$chain length$chosen algorithms$clitics$closed$computational linguistics$	1
Lexical adverbs, including manner, time, and loca- tion, and adverbs of negation, which vary by CL type (declarative, imperative, or interrogative) ?	CL	computational linguistic$clause$Class$classifier$chain length$chosen algorithms$clitics$closed$computational linguistics$	1
Verbless CLs: nouns, adjectives, and adverbs, lexical or derived, functioning as predicates ?	CL	computational linguistic$clause$Class$classifier$chain length$chosen algorithms$clitics$closed$computational linguistics$	1
We specifically extract the character reference CH either from the dependency relation nsubj, which links a speech verb SV with a CH that is the syntactic subject of a CL, or from the dependency relation dobj, which links a SV with a CH that is the direct object of the speech verb, across a conjunct (e.g., and).	CL	computational linguistic$clause$Class$classifier$chain length$chosen algorithms$clitics$closed$computational linguistics$	1
and CLs expressing prior or simultaneous events ?	CL	computational linguistic$clause$Class$classifier$chain length$chosen algorithms$clitics$closed$computational linguistics$	1
Subordinate CLs: clausal complements of verbs like ?	CL	computational linguistic$clause$Class$classifier$chain length$chosen algorithms$clitics$closed$computational linguistics$	1
That is, aside from the constraint that verbal CLs require a clitic cluster (marking subject and object agreement and tense, aspect and mood) in second position, the word order is otherwise free, to the point that noun phrases can be non-contiguous, with head nouns and their modifiers separated by un- related words.	CL	computational linguistic$clause$Class$classifier$chain length$chosen algorithms$clitics$closed$computational linguistics$	1
non-finite subor- dinate CLs such as purposives (?	CL	computational linguistic$clause$Class$classifier$chain length$chosen algorithms$clitics$closed$computational linguistics$	1
The representation for sentence 1 states that the first element of the 5-gram (-3; third word to the left of the adjective) is empty (because the second element is a phrase boundary marker), that the sec- ond element is a CL delimiter (conjunction that), the third one (-1; word preceding the adjective) is a definite determiner, and the fourth one (+1; word following the adjective) is a common noun.	CL	computational linguistic$clause$Class$classifier$chain length$chosen algorithms$clitics$closed$computational linguistics$	1
Validating the Cover- age of Lexical Resources for Affect Analysis and Au- tomatically CLifying New Words along Semantic Axes.	CL	computational linguistic$clause$Class$classifier$chain length$chosen algorithms$clitics$closed$computational linguistics$	2
2 CLification and Hypothesis As mentioned above, the semantic classification of adjectives is not settled in theoretical linguistics.	CL	computational linguistic$clause$Class$classifier$chain length$chosen algorithms$clitics$closed$computational linguistics$	2
3.2 Graph-based CLifier The cohesion graph based classifier compares the cohesion graph connectivity of the discourse in- cluding the MWE component words with the con- nectivity of the discourse excluding the MWE component words to check how well the MWE component words are semantically connected to the context.	CL	computational linguistic$clause$Class$classifier$chain length$chosen algorithms$clitics$closed$computational linguistics$	2
CLification is to choose y such that y = argmaxy?(w>?(x,y?)).	CL	computational linguistic$clause$Class$classifier$chain length$chosen algorithms$clitics$closed$computational linguistics$	2
CLifying latent user attributes in twitter.	CL	computational linguistic$clause$Class$classifier$chain length$chosen algorithms$clitics$closed$computational linguistics$	2
701,3   Acquisition of Semantic CLes for Adjectives from Distributional Evidence Gemma Boleda GLiCom Universitat Pompeu Fabra La Rambla 30-32 08002 Barcelona gemma.boleda@upf.edu Toni Badia GLiCom Universitat Pompeu Fabra La Rambla 30-32 08002 Barcelona toni.badia@upf.edu Eloi Batlle Audiovisual Institute Universitat Pompeu Fabra Pg.	CL	computational linguistic$clause$Class$classifier$chain length$chosen algorithms$clitics$closed$computational linguistics$	2
Finally, we introduced our graph- based CL for distinguishing literal and non- literal use of MWEs.	CL	computational linguistic$clause$Class$classifier$chain length$chosen algorithms$clitics$closed$computational linguistics$	3
While there are other large combination features such as ones involving F 4, F 9, F 12, F 15 and F 19, we find that they do help improving the performance of the CL.	CL	computational linguistic$clause$Class$classifier$chain length$chosen algorithms$clitics$closed$computational linguistics$	3
As increasing the number of perceptrons re- sults in more thorough exploration of the version space V (D), we expect that the performance of the CL would improve as K increases.	CL	computational linguistic$clause$Class$classifier$chain length$chosen algorithms$clitics$closed$computational linguistics$	3
They report an average accuracy of 72% for their canonical form (CForm) CL.	CL	computational linguistic$clause$Class$classifier$chain length$chosen algorithms$clitics$closed$computational linguistics$	3
We also evaluated thispenalized version, varying the trade-off parameter C. Bayes Point Machines (BPM) for structured prediction (Corston-Oliver et al, 2006) is an en- semble learning algorithm that attempts to set the weight w to be the Bayes Point which approxi- mates to Bayesian inference for linear CLs.	CL	computational linguistic$clause$Class$classifier$chain length$chosen algorithms$clitics$closed$computational linguistics$	3
We therefore integrate a data-driven CL for the special task of PP attachment into an existing rule- based parser and measure the effect that the addi- tional information has on the overall accuracy.	CL	computational linguistic$clause$Class$classifier$chain length$chosen algorithms$clitics$closed$computational linguistics$	3
Perceptron TP FP FN P (%) R (%) F1 (%) 671 128 119 83.98 84.94 84.46 Conditional Random Fields TP FP FN P (%) R (%) F1 (%) 643 78 147 89.18 81.39 85.11 Bayes Point Machines TP FP FN P (%) R (%) F1 (%) 647 79 143 89.12 81.90 85.36 Table 4: Performance of different optimization strategies 6 Conclusion To tackle the hedge cue detection problem posed by the CoNLL-2010 shared task, we utilized a CL for sequential labeling following previ- ous work (Morante and Daelemans, 2009).	CL	computational linguistic$clause$Class$classifier$chain length$chosen algorithms$clitics$closed$computational linguistics$	3
Therefore we can- not calculate CL by simply taking the dis- tance between the first and last points in the chain.	CL	computational linguistic$clause$Class$classifier$chain length$chosen algorithms$clitics$closed$computational linguistics$	4
The coreferential CL of a candidate, or its variants such as occurrence frequency and TFIDF, has been used as a salience factor in some learning-based reference resolution sys- tems (Iida et al, 2003; Mitkov, 1998; Paul et al.,	CL	computational linguistic$clause$Class$classifier$chain length$chosen algorithms$clitics$closed$computational linguistics$	4
E.g., the non-parallel bitext map in Figure 1, which was created without the CL param- eter, has on average 630	CL	computational linguistic$clause$Class$classifier$chain length$chosen algorithms$clitics$closed$computational linguistics$	4
In contrast, running SIMR on the same pair of non- parallel documents with a maximum CL of 700 yielded only 22 points of correspondence, or 3032 characters between points on average.	CL	computational linguistic$clause$Class$classifier$chain length$chosen algorithms$clitics$closed$computational linguistics$	4
E.g., the non-parallel bitext map in Figure 1, which was created without the CL param- eter, has on average 630 characters between points.	CL	computational linguistic$clause$Class$classifier$chain length$chosen algorithms$clitics$closed$computational linguistics$	4
Therefore, in parallel texts SIMR will find many chains and limiting the CL will have a minimal effect on the number of chains SIMR will find.	CL	computational linguistic$clause$Class$classifier$chain length$chosen algorithms$clitics$closed$computational linguistics$	4
SIMR-cl uses the standard SIMR parameters plus the additional CL parameter discussed above.	CL	computational linguistic$clause$Class$classifier$chain length$chosen algorithms$clitics$closed$computational linguistics$	4
SIMR will find many chains and limiting the CL will have a minimal effect on the number of chains SIMR will find.	CL	computational linguistic$clause$Class$classifier$chain length$chosen algorithms$clitics$closed$computational linguistics$	4
More detailed documentation includes lower-level design decisions, such as the reasons for the CL or data structures.	CL	computational linguistic$clause$Class$classifier$chain length$chosen algorithms$clitics$closed$computational linguistics$	5
Habash et al(2009)  proposed methods to tackle the Arabic enCL.	CL	computational linguistic$clause$Class$classifier$chain length$chosen algorithms$clitics$closed$computational linguistics$	6
In addition to these  simple sentences, difficult problems are also han-  dled: CL, complex determiners, completives, var-  ious forms of questions, extraction and non limited  dependancies, coordinations, comparatives.	CL	computational linguistic$clause$Class$classifier$chain length$chosen algorithms$clitics$closed$computational linguistics$	6
There are also grammatical rules, such as those  concerning the positions of the verb (e.g., in the "second position" in German), of the  adjective or another modifier before or after the head noun in a noun group, and of  CL.	CL	computational linguistic$clause$Class$classifier$chain length$chosen algorithms$clitics$closed$computational linguistics$	6
the syntactic structure that hosts the verb, the  patterns of verbal inflections for every instance  of verb use (e.g. subject number, person, and  gender, as well as other morphosyntactic aspects  of the Arabic verb), the semantic properties of  other components of the construction (e.g. se- mantic properties of the subject), as well as the  inclusion or exclusion of phrases, lexical items,  or CL denoting a starting point of the event  (SOURCE), a terminal point of the event (GOAL),  etc.	CL	computational linguistic$clause$Class$classifier$chain length$chosen algorithms$clitics$closed$computational linguistics$	6
The arc list can  be conceptually divided in two parts: one contains  the stems of the verbs, nouns and adjectives; the  other contains a number of sub-lexicons that provide  the endings for these lexical categories as well as the  CL?	CL	computational linguistic$clause$Class$classifier$chain length$chosen algorithms$clitics$closed$computational linguistics$	6
E.g., the French pronominal CL can be treated in this way.	CL	computational linguistic$clause$Class$classifier$chain length$chosen algorithms$clitics$closed$computational linguistics$	6
3.1 Gloss~me - -  A Closed Subsystem of  English  A dictionary is a CL paraphrasing system of nat-  ural language.	CL	computational linguistic$clause$Class$classifier$chain length$chosen algorithms$clitics$closed$computational linguistics$	7
Gloss~me is a CL subsystem of English.	CL	computational linguistic$clause$Class$classifier$chain length$chosen algorithms$clitics$closed$computational linguistics$	7
This is also evi- dent by examination of average lexeme sentiment of top loaded terms of each vector, not disCL in the paper.	CL	computational linguistic$clause$Class$classifier$chain length$chosen algorithms$clitics$closed$computational linguistics$	7
Appendix A. Structure of Paradigme  w Mapping Gloss~me onto Paradigme  The semantic network Paradigme is systematically  constructed from the small and CL English dictio-  nary Glossdme.	CL	computational linguistic$clause$Class$classifier$chain length$chosen algorithms$clitics$closed$computational linguistics$	7
In case of a single constraint, this pro- gram has a CL-form solution.	CL	computational linguistic$clause$Class$classifier$chain length$chosen algorithms$clitics$closed$computational linguistics$	7
We adopted Longman Dictionary of Contemporary  English (LDOCE) \[1987\] assuch a CL system of  English.	CL	computational linguistic$clause$Class$classifier$chain length$chosen algorithms$clitics$closed$computational linguistics$	7
1 Introduction Since its early years, the CL field has devoted much effort to the development of formal systems for modeling the syntax of nat- ural language.	CL	computational linguistic$clause$Class$classifier$chain length$chosen algorithms$clitics$closed$computational linguistics$	8
He has developed several machine learning based natural language  processing systems that are widely used in the CL community and  in industry and has presented invited talks and tutorials in several major conferences.	CL	computational linguistic$clause$Class$classifier$chain length$chosen algorithms$clitics$closed$computational linguistics$	8
This account contributes to the  discipline of CL in labeling  prepositions in Farsi, as this area of preposition  labeling has been very challenging.	CL	computational linguistic$clause$Class$classifier$chain length$chosen algorithms$clitics$closed$computational linguistics$	8
Most of the early work attempted to implement QA systems from the early per- spective of AI or CL.	CL	computational linguistic$clause$Class$classifier$chain length$chosen algorithms$clitics$closed$computational linguistics$	8
During the 1970s and 1980s there was intensive research on the development of the- oretical bases for CL.	CL	computational linguistic$clause$Class$classifier$chain length$chosen algorithms$clitics$closed$computational linguistics$	8
1 In t roduct ion   Attempts to the automatic identification of a struc-  ture in discourse have so far met with a limited  success in the CL literature.	CL	computational linguistic$clause$Class$classifier$chain length$chosen algorithms$clitics$closed$computational linguistics$	8
mplates  * Here and elsewhere, I use the following ARPABET-Iike substitutions for the  standard phonetic symbols:  \[I\] = high front lax vowel  \[E\] = mid front lax vowel  \[ae\] = low front vowel  \[U\] = high back lax vowel  \[0\] = low-mld back lax vowel  \[uh\] = mid-central or reduced vowel ("carrot" or schwa)  \[R\] = rhotaclzed mid-central vowel (i.e., syllabic \[r\])  \[sh\] = voiceless AP stop  \[zh\] = voiced AP stop  \[dh\] = voiced interdental fricative  \[th\] = voiceless interdental fricative  \[D\] = flap  14  for each variant pronunciation or by listing alternate paths in an allophonlc-  segment-based HHM model (Kopec and Bush 1985).	AP	alveopalatal$article+preposition$averaged perceptron$Averaged Perceptron$appropriate$Base+Appositives$Agreement Principle$Associ-  ated Press$Absolute Position$adverbial phrase$adjective or adverbial phrase$adJective phrase$answer processing$Averaged Prec is ion$	0
The only difference between our symbols  and the ones used by IPA are in voiceless and  voiced AP fricatives [S] and [Z], the  voiceless and voiced affricates [C] and [J], and the  palatal glide [y].	AP	alveopalatal$article+preposition$averaged perceptron$Averaged Perceptron$appropriate$Base+Appositives$Agreement Principle$Associ-  ated Press$Absolute Position$adverbial phrase$adjective or adverbial phrase$adJective phrase$answer processing$Averaged Prec is ion$	0
e the following ARPABET-Iike substitutions for the  standard phonetic symbols:  \[I\] = high front lax vowel  \[E\] = mid front lax vowel  \[ae\] = low front vowel  \[U\] = high back lax vowel  \[0\] = low-mld back lax vowel  \[uh\] = mid-central or reduced vowel ("carrot" or schwa)  \[R\] = rhotaclzed mid-central vowel (i.e., syllabic \[r\])  \[sh\] = voiceless AP stop  \[zh\] = voiced AP stop  \[dh\] = voiced interdental fricative  \[th\] = voiceless interdental fricative  \[D\] = flap  14  for each variant pronunciation or by listing alternate paths in an allophonlc-  segment-based HHM model (Kopec and Bush 1985).	AP	alveopalatal$article+preposition$averaged perceptron$Averaged Perceptron$appropriate$Base+Appositives$Agreement Principle$Associ-  ated Press$Absolute Position$adverbial phrase$adjective or adverbial phrase$adJective phrase$answer processing$Averaged Prec is ion$	0
high tone,  W long syllable, ~ rhythm break, ~ voiced retro-  flex AP fricative, ~ retroflex flap, cuV  labiovelar stop.	AP	alveopalatal$article+preposition$averaged perceptron$Averaged Perceptron$appropriate$Base+Appositives$Agreement Principle$Associ-  ated Press$Absolute Position$adverbial phrase$adjective or adverbial phrase$adJective phrase$answer processing$Averaged Prec is ion$	0
verb+pAP combinations such as break in on;  ?	AP	alveopalatal$article+preposition$averaged perceptron$Averaged Perceptron$appropriate$Base+Appositives$Agreement Principle$Associ-  ated Press$Absolute Position$adverbial phrase$adjective or adverbial phrase$adJective phrase$answer processing$Averaged Prec is ion$	1
In the AP, the val- ues of every coefficient are added up at each up- date, which happens (possibly) at each training sentence, and their arithmetic average is used in- stead.9 This trick makes the algorithm more re- sistant to weight oscillations during training (or, more precisely, at the end of it) and as a result, it substantially improves its performance.10 7And lemmas, which are th	AP	alveopalatal$article+preposition$averaged perceptron$Averaged Perceptron$appropriate$Base+Appositives$Agreement Principle$Associ-  ated Press$Absolute Position$adverbial phrase$adjective or adverbial phrase$adJective phrase$answer processing$Averaged Prec is ion$	2
It is a reimplementation of the AP de- scribed in (Collins, 2002), which uses such fea- tures that it behaves like an HMM tagger and thus the standard Viterbi decoding is possible.	AP	alveopalatal$article+preposition$averaged perceptron$Averaged Perceptron$appropriate$Base+Appositives$Agreement Principle$Associ-  ated Press$Absolute Position$adverbial phrase$adjective or adverbial phrase$adJective phrase$answer processing$Averaged Prec is ion$	2
4 The perceptron feature sets The AP?s accuracy is determined (to a large extent) by the set of features used.	AP	alveopalatal$article+preposition$averaged perceptron$Averaged Perceptron$appropriate$Base+Appositives$Agreement Principle$Associ-  ated Press$Absolute Position$adverbial phrase$adjective or adverbial phrase$adJective phrase$answer processing$Averaged Prec is ion$	2
Morphological tagging based on AP.	AP	alveopalatal$article+preposition$averaged perceptron$Averaged Perceptron$appropriate$Base+Appositives$Agreement Principle$Associ-  ated Press$Absolute Position$adverbial phrase$adjective or adverbial phrase$adJective phrase$answer processing$Averaged Prec is ion$	2
Because we find that the AP significantly outperforms L 1 SVM.	AP	alveopalatal$article+preposition$averaged perceptron$Averaged Perceptron$appropriate$Base+Appositives$Agreement Principle$Associ-  ated Press$Absolute Position$adverbial phrase$adjective or adverbial phrase$adJective phrase$answer processing$Averaged Prec is ion$	2
The transition and out- put scores for the candidate tags are based on a large number of binary-valued features and their weights, which are determined during iterative training by the AP algorithm.	AP	alveopalatal$article+preposition$averaged perceptron$Averaged Perceptron$appropriate$Base+Appositives$Agreement Principle$Associ-  ated Press$Absolute Position$adverbial phrase$adjective or adverbial phrase$adJective phrase$answer processing$Averaged Prec is ion$	2
The model makes use of the AP algorithm (Freund and Schapire, 1996) and is trained on the training data of the shared task with rich features.	AP	alveopalatal$article+preposition$averaged perceptron$Averaged Perceptron$appropriate$Base+Appositives$Agreement Principle$Associ-  ated Press$Absolute Position$adverbial phrase$adjective or adverbial phrase$adJective phrase$answer processing$Averaged Prec is ion$	3
There has been a lot of work on correcting preposition and determiner errors, where discriminative models such as Maximum Entropy and AP (De Felice and Pulman, 2008; Rozovskaya and Roth, 2011) and/or probablistic language models (Gamon, 2010) are generally used.	AP	alveopalatal$article+preposition$averaged perceptron$Averaged Perceptron$appropriate$Base+Appositives$Agreement Principle$Associ-  ated Press$Absolute Position$adverbial phrase$adjective or adverbial phrase$adJective phrase$answer processing$Averaged Prec is ion$	3
Morphological Tagging Based on AP.	AP	alveopalatal$article+preposition$averaged perceptron$Averaged Perceptron$appropriate$Base+Appositives$Agreement Principle$Associ-  ated Press$Absolute Position$adverbial phrase$adjective or adverbial phrase$adJective phrase$answer processing$Averaged Prec is ion$	3
AP Algorithm  5 Experiments  We evaluate our method on both Chinese and  English syntactic parsing task with the standard  division on Chinese Penn Treebank Version 5.0  and WSJ English Treebank 3.0 (Marcus et al  1993) as shown in Table 1.	AP	alveopalatal$article+preposition$averaged perceptron$Averaged Perceptron$appropriate$Base+Appositives$Agreement Principle$Associ-  ated Press$Absolute Position$adverbial phrase$adjective or adverbial phrase$adJective phrase$answer processing$Averaged Prec is ion$	3
Simulated Annealing Algorithm  4.2 AP  Another algorithm we apply is the averaged per- ceptron algorithm.	AP	alveopalatal$article+preposition$averaged perceptron$Averaged Perceptron$appropriate$Base+Appositives$Agreement Principle$Associ-  ated Press$Absolute Position$adverbial phrase$adjective or adverbial phrase$adJective phrase$answer processing$Averaged Prec is ion$	3
c?2009 Association for Computational Linguistics Semi-supervised Training for the AP POS Tagger Drahom??ra ?	AP	alveopalatal$article+preposition$averaged perceptron$Averaged Perceptron$appropriate$Base+Appositives$Agreement Principle$Associ-  ated Press$Absolute Position$adverbial phrase$adjective or adverbial phrase$adJective phrase$answer processing$Averaged Prec is ion$	3
Predicting character-AP voices for a TTS- based storyteller system.	AP	alveopalatal$article+preposition$averaged perceptron$Averaged Perceptron$appropriate$Base+Appositives$Agreement Principle$Associ-  ated Press$Absolute Position$adverbial phrase$adjective or adverbial phrase$adJective phrase$answer processing$Averaged Prec is ion$	4
It is, therefore, saliz and AP  to delete it.	AP	alveopalatal$article+preposition$averaged perceptron$Averaged Perceptron$appropriate$Base+Appositives$Agreement Principle$Associ-  ated Press$Absolute Position$adverbial phrase$adjective or adverbial phrase$adJective phrase$answer processing$Averaged Prec is ion$	4
2006) have shown that assign- ing an AP voice for a character in a digi- tal storyteller system is significant for understand- ing a story, perceiving affective content, perceiv- ing the voice as credible, and overall listener sat- isfaction.	AP	alveopalatal$article+preposition$averaged perceptron$Averaged Perceptron$appropriate$Base+Appositives$Agreement Principle$Associ-  ated Press$Absolute Position$adverbial phrase$adjective or adverbial phrase$adJective phrase$answer processing$Averaged Prec is ion$	4
If the arc points to a state with a  delined harmony value, the hamlony value of the better  palh is retained by that state, and its position ill tile sorted  list adjusted APly.	AP	alveopalatal$article+preposition$averaged perceptron$Averaged Perceptron$appropriate$Base+Appositives$Agreement Principle$Associ-  ated Press$Absolute Position$adverbial phrase$adjective or adverbial phrase$adJective phrase$answer processing$Averaged Prec is ion$	4
2012) have shown that the APness of the voice assigned to a syn- thetic character is strongly related to knowing the gender, age and other salient personality attributes of the character.	AP	alveopalatal$article+preposition$averaged perceptron$Averaged Perceptron$appropriate$Base+Appositives$Agreement Principle$Associ-  ated Press$Absolute Position$adverbial phrase$adjective or adverbial phrase$adJective phrase$answer processing$Averaged Prec is ion$	4
2006), gen- erate dialogs uttered by different characters using synthetic voices AP for each character?s gender, age and personality (Greene et al.,	AP	alveopalatal$article+preposition$averaged perceptron$Averaged Perceptron$appropriate$Base+Appositives$Agreement Principle$Associ-  ated Press$Absolute Position$adverbial phrase$adjective or adverbial phrase$adJective phrase$answer processing$Averaged Prec is ion$	4
2012) have shown that the APness of the voice assigned to a syn- thetic character is strongly related to knowing the gender, age and other salient personality at	AP	alveopalatal$article+preposition$averaged perceptron$Averaged Perceptron$appropriate$Base+Appositives$Agreement Principle$Associ-  ated Press$Absolute Position$adverbial phrase$adjective or adverbial phrase$adJective phrase$answer processing$Averaged Prec is ion$	4
It contains analogs of our Codomain  AP, mappings and base orders.	AP	alveopalatal$article+preposition$averaged perceptron$Averaged Perceptron$appropriate$Base+Appositives$Agreement Principle$Associ-  ated Press$Absolute Position$adverbial phrase$adjective or adverbial phrase$adJective phrase$answer processing$Averaged Prec is ion$	6
Compat ib le  mapp ings  are those  that allow the Codomain AP to  be satisfied, and they are easily identified by ex-  amining the base order component of the MTrans  rules being used.	AP	alveopalatal$article+preposition$averaged perceptron$Averaged Perceptron$appropriate$Base+Appositives$Agreement Principle$Associ-  ated Press$Absolute Position$adverbial phrase$adjective or adverbial phrase$adJective phrase$answer processing$Averaged Prec is ion$	6
One of the language-universal gram-  matical principles (the Control AP)  requires that the semantic controller of a controlled  complement always be the next grammatical relation  (in the order specified by the value of the SUBCAT  feature of the head) after the controlled complement  to combine with the head.	AP	alveopalatal$article+preposition$averaged perceptron$Averaged Perceptron$appropriate$Base+Appositives$Agreement Principle$Associ-  ated Press$Absolute Position$adverbial phrase$adjective or adverbial phrase$adJective phrase$answer processing$Averaged Prec is ion$	6
We call this  the Codomain AP, and it is  through this principle that pairability constraints  are enforced.	AP	alveopalatal$article+preposition$averaged perceptron$Averaged Perceptron$appropriate$Base+Appositives$Agreement Principle$Associ-  ated Press$Absolute Position$adverbial phrase$adjective or adverbial phrase$adJective phrase$answer processing$Averaged Prec is ion$	6
167  \]Feature Pass ing   The theory of HPSG embodies a number of sub-  stantive hypotheses about universal granunatical prin-  ciples, Such principles as the Head Feature Princi-  ple, the Binding Inheritance Principle, and the Con-  trol AP, require that certain syntac-  tic features specified on daughters in syntactic trees are  inherited by the mothers.	AP	alveopalatal$article+preposition$averaged perceptron$Averaged Perceptron$appropriate$Base+Appositives$Agreement Principle$Associ-  ated Press$Absolute Position$adverbial phrase$adjective or adverbial phrase$adJective phrase$answer processing$Averaged Prec is ion$	6
This additional  information is used to enforce the Codomain  AP and to help in the user inter-  action described in Section 3.5.	AP	alveopalatal$article+preposition$averaged perceptron$Averaged Perceptron$appropriate$Base+Appositives$Agreement Principle$Associ-  ated Press$Absolute Position$adverbial phrase$adjective or adverbial phrase$adJective phrase$answer processing$Averaged Prec is ion$	6
AP: Given a post P i in a thread, the absolute position of post P i is i Relative Position: Given a post P i in a thread with n posts, the relative position of P i is i/n We construct the CSN corpus by randomly sampling 3,000 threads from CSN between 2000 and 2010.	AP	alveopalatal$article+preposition$averaged perceptron$Averaged Perceptron$appropriate$Base+Appositives$Agreement Principle$Associ-  ated Press$Absolute Position$adverbial phrase$adjective or adverbial phrase$adJective phrase$answer processing$Averaged Prec is ion$	8
So the operat ion  of a compounding rule is enabled only i f  a  c h a r a c t e r i s t i c  aspect  is associa ted  with t he  verb; i n  English,  this is usua l ly  indicated  b y  a n  adverb o r  an AP.	AP	alveopalatal$article+preposition$averaged perceptron$Averaged Perceptron$appropriate$Base+Appositives$Agreement Principle$Associ-  ated Press$Absolute Position$adverbial phrase$adjective or adverbial phrase$adJective phrase$answer processing$Averaged Prec is ion$	9
Some annotators (but not all) seemed to decide that any AP (ADVP) headed by an ?	AP	alveopalatal$article+preposition$averaged perceptron$Averaged Perceptron$appropriate$Base+Appositives$Agreement Principle$Associ-  ated Press$Absolute Position$adverbial phrase$adjective or adverbial phrase$adJective phrase$answer processing$Averaged Prec is ion$	9
Systems that  ignore this and begin with units that are inevitably  realized as kernel clauses under-utilize the expressive  power of natural language, which can use complex noun  phrases, nominalizations, APs, and other  adjuncts to pack information from multiple units into  one clause.	AP	alveopalatal$article+preposition$averaged perceptron$Averaged Perceptron$appropriate$Base+Appositives$Agreement Principle$Associ-  ated Press$Absolute Position$adverbial phrase$adjective or adverbial phrase$adJective phrase$answer processing$Averaged Prec is ion$	9
There is  normal ly  no more than two  APs before or after  the nominal.	AP	alveopalatal$article+preposition$averaged perceptron$Averaged Perceptron$appropriate$Base+Appositives$Agreement Principle$Associ-  ated Press$Absolute Position$adverbial phrase$adjective or adverbial phrase$adJective phrase$answer processing$Averaged Prec is ion$	9
a red large ball" and the typical ordering  of temporal before spatial APs in German.	AP	alveopalatal$article+preposition$averaged perceptron$Averaged Perceptron$appropriate$Base+Appositives$Agreement Principle$Associ-  ated Press$Absolute Position$adverbial phrase$adjective or adverbial phrase$adJective phrase$answer processing$Averaged Prec is ion$	9
The matrix phrase and the AP have IP-MAT and IP-ADV tags respectively.	AP	alveopalatal$article+preposition$averaged perceptron$Averaged Perceptron$appropriate$Base+Appositives$Agreement Principle$Associ-  ated Press$Absolute Position$adverbial phrase$adjective or adverbial phrase$adJective phrase$answer processing$Averaged Prec is ion$	9
Sec-  tion 2 presents the flmctionality of several knowledge  processing modules and describes tile NLP tech-  niques for question and AP.	AP	alveopalatal$article+preposition$averaged perceptron$Averaged Perceptron$appropriate$Base+Appositives$Agreement Principle$Associ-  ated Press$Absolute Position$adverbial phrase$adjective or adverbial phrase$adJective phrase$answer processing$Averaged Prec is ion$	12
We have sepm'ately measured the effect  of tile integration of the knowledge-based methods  at question processing and AP level.	AP	alveopalatal$article+preposition$averaged perceptron$Averaged Perceptron$appropriate$Base+Appositives$Agreement Principle$Associ-  ated Press$Absolute Position$adverbial phrase$adjective or adverbial phrase$adJective phrase$answer processing$Averaged Prec is ion$	12
An im-  pressive improvenmnt of 14% is achieved when more  knowledge-intensive NLP techniques are ai)plied a.t  both question and AP level.	AP	alveopalatal$article+preposition$averaged perceptron$Averaged Perceptron$appropriate$Base+Appositives$Agreement Principle$Associ-  ated Press$Absolute Position$adverbial phrase$adjective or adverbial phrase$adJective phrase$answer processing$Averaged Prec is ion$	12
In the rest of the paper we will shortly over- view the components of such a framework and  will describe the relevant aspects of the solution  offered for each of them, aspects that should ac- count for a large variety of question types,  document collections and AP  techniques, as well as for several languages.	AP	alveopalatal$article+preposition$averaged perceptron$Averaged Perceptron$appropriate$Base+Appositives$Agreement Principle$Associ-  ated Press$Absolute Position$adverbial phrase$adjective or adverbial phrase$adJective phrase$answer processing$Averaged Prec is ion$	12
Our work suggests that considerable gains in performance can be obtained by incorporating TE during both AP and passage re- trieval.	AP	alveopalatal$article+preposition$averaged perceptron$Averaged Perceptron$appropriate$Base+Appositives$Agreement Principle$Associ-  ated Press$Absolute Position$adverbial phrase$adjective or adverbial phrase$adJective phrase$answer processing$Averaged Prec is ion$	12
1 Introduction  Different projects, different evaluation forums,  different tasks, different languages, different  document collections, different question types,  different AP strategies ?	AP	alveopalatal$article+preposition$averaged perceptron$Averaged Perceptron$appropriate$Base+Appositives$Agreement Principle$Associ-  ated Press$Absolute Position$adverbial phrase$adjective or adverbial phrase$adJective phrase$answer processing$Averaged Prec is ion$	12
We introduce a new inventory of preposition relations that covers the 34 prepositions that formed the basis of the SemEval 2007 task of PSD.	PSD	preposition sense disambiguation$phrase sense disambiguation$Preposition sense disambiguation$	0
In addition to prior work on prepositional phrase attachment, a highly re- lated problem is PSD (Hovy et al, 2011; Srikumar and Roth, 2013).	PSD	preposition sense disambiguation$phrase sense disambiguation$Preposition sense disambiguation$	0
3.1 Preposition Relation Inventory We build our relation inventory using the sense an- notation in the Preposition Project, focusing on the 34 prepositions3 annotated for the SemEval-2007 shared task of PSD.	PSD	preposition sense disambiguation$phrase sense disambiguation$Preposition sense disambiguation$	0
6 Conclusion We showed that using a number of simple linguis- tically motivated features can improve the accu- racy of PSD.	PSD	preposition sense disambiguation$phrase sense disambiguation$Preposition sense disambiguation$	0
2.3 Classifier Training We chose maximum entropy (Berger et al, 1996) as our primary classifier, since it had been successfully applied by the highest performing systems in both the SemEval-2007 PSD task (Ye and Baldwin, 2007) and the general word sense disambiguation task (Tratz et al, 2007).	PSD	preposition sense disambiguation$phrase sense disambiguation$Preposition sense disambiguation$	0
The SemEval data provides a natural method for comparing the per- formance of PSD sys- tems.	PSD	preposition sense disambiguation$phrase sense disambiguation$Preposition sense disambiguation$	0
For example, Carpuat and Wu (2007a) extend word sense dis- ambiguation to PSD and show improved performance due to the better fit with multiple possible segmentations in a phrase- based system.	PSD	preposition sense disambiguation$phrase sense disambiguation$Preposition sense disambiguation$	1
To summarize Experiment I, which is a vari- ant of a supervised PSD task, demonstrates that we can use LSA to distin- guish between literal and the idiomatic usage of an MWE by using local linguistic context.	PSD	preposition sense disambiguation$phrase sense disambiguation$Preposition sense disambiguation$	1
We operate un- der the framework of PSD (Carpuat and Wu, 2007), in which we take au- tomatically align parallel data in an old domain to generate an initial old-domain sense inventory.	PSD	preposition sense disambiguation$phrase sense disambiguation$Preposition sense disambiguation$	1
Approaches that have been tried for SMT model adaptation include mixture models, transductive learning, data selection, data weighting, and PSD.	PSD	preposition sense disambiguation$phrase sense disambiguation$Preposition sense disambiguation$	1
How PSD outperforms word sense disambiguation for statistical machine translation.	PSD	preposition sense disambiguation$phrase sense disambiguation$Preposition sense disambiguation$	1
Thus, the scripts are functioning as a rep- resentation language for lexical ambiguity like the  semantic PSD model for  SMT (Carpuat and Wu, 2007).	PSD	preposition sense disambiguation$phrase sense disambiguation$Preposition sense disambiguation$	1
Melb-yb: PSD using rich seman- tic features.	PSD	preposition sense disambiguation$phrase sense disambiguation$Preposition sense disambiguation$	2
PSD was one of the SemEval 2007 tasks (Litkowski and Hargraves, 2007), and was subsequently explored in a number of papers using supervised approaches: O?Hara and Wiebe (2009) present a supervised preposition sense disambiguation approach which explores different settings; Tratz and Hovy (2009), Hovy et al (2010) make explicit use of the arguments for preposition sense	PSD	preposition sense disambiguation$phrase sense disambiguation$Preposition sense disambiguation$	2
ECt Candidates set could be any  combination of the translation of words appearing  in Chinese context.	EC	English Contex$English consonant$Entailment Core$Economics$enhanced connectivity$Economies$	0
430   Back Transliteration from Japanese to English  Using Target ECt   Isao Goto?,	EC	English Contex$English consonant$Entailment Core$Economics$enhanced connectivity$Economies$	0
ECt Candidates set could be  any combination of translations and each  combination could be selected as the English context.	EC	English Contex$English consonant$Entailment Core$Economics$enhanced connectivity$Economies$	0
c?2013 Association for Computational Linguistics Gender Inference of Twitter Users in Non-ECts Morgane Ciot School of Computer Science McGill University Montreal, Quebec, Canada morgane.ciot@mail.mcgill.ca Morgan Sonderegger Department of Linguistics McGill University Montreal, Quebec, Canada morgan.sonderegger@mcgill.ca Derek Ruths School of Computer Science McGill University Montreal, Quebec, Canada derek.ruths@mcgill.ca Abstract While much work has considered the problem	EC	English Contex$English consonant$Entailment Core$Economics$enhanced connectivity$Economies$	0
ECt Candidates set is the  translations set of the Chinese context.	EC	English Contex$English consonant$Entailment Core$Economics$enhanced connectivity$Economies$	0
f   Figure 2: IITK-Roman to EC mapping    A (??)?	EC	English Contex$English consonant$Entailment Core$Economics$enhanced connectivity$Economies$	1
Klatt, D.H. (1968) 'Structure of confusions in short-term memory between  ECs' J. Acoust.	EC	English Contex$English consonant$Entailment Core$Economics$enhanced connectivity$Economies$	1
2, 401-7,  Miller, G.A. and Nicely, P.E. (1955) 'An analysis of perceptusl con-  fusions among same ECs' J. Acoumt.	EC	English Contex$English consonant$Entailment Core$Economics$enhanced connectivity$Economies$	1
We constructed a table relating He- brew and ECs, based on common knowledge patterns that relate sound to spelling in both languages.	EC	English Contex$English consonant$Entailment Core$Economics$enhanced connectivity$Economies$	1
Wickelgren, W.A. (1966) 'Distinctive features snd errors in short-term  memory for ECs' J. Acoust.	EC	English Contex$English consonant$Entailment Core$Economics$enhanced connectivity$Economies$	1
A Korean vowel - (eu) is most commonly inserted between two ECs in transliteration.	EC	English Contex$English consonant$Entailment Core$Economics$enhanced connectivity$Economies$	1
for Computational Linguistics Building bilingual lexicon to create Dialect Tunisian corpora and  adapt language model      Rahma Boujelbane  Miracl Laboratory, ANLP Research  Group, University of Sfax, Tunisia  Rahma.boujelbane@gmail.com  Mariem Ellouze khemekhem  Miracl Laboratory, ANLP Research  Group, University of Sfax, Tunisia  mariem.ellouze@planet.com             Siwar BenAyed  Faculty of EC and Management  of Sfax  siwar.ben.ayed@gmail.com      Lamia Hadrich Belguith  Miracl Laboratory, ANLP Research  Group, University of Sfax, Tunisia  l.belguith@fsegs.rnu.tn    Abstract  Since the Tunisian revolution, Tunisian Dialect (TD)  used in daily life, has became progressively used and  represented in interviews, news and debate programs  instead of Modern Standard Arabic (MSA).	EC	English Contex$English consonant$Entailment Core$Economics$enhanced connectivity$Economies$	3
Machinc-Lc~irs~abic, Non-Compositional Se-  maolic.~ fiJr Domain Independent Speech or Text  Recognition to appear in Proceedings of 2nd Hellenic-  European Conference on Mathematics and Informat-  ies (HERMIS), Athens University of EC and  Business.	EC	English Contex$English consonant$Entailment Core$Economics$enhanced connectivity$Economies$	3
EC Business administration, national economics, finance and accounting, trade, mar- keting and public relations, and insurance.	EC	English Contex$English consonant$Entailment Core$Economics$enhanced connectivity$Economies$	3
EC of Education Review, 30:682?690.	EC	English Contex$English consonant$Entailment Core$Economics$enhanced connectivity$Economies$	3
EC) ?	EC	English Contex$English consonant$Entailment Core$Economics$enhanced connectivity$Economies$	3
stling Department of Linguistics Stockholm University SE-106 91 Stockholm robert@ling.su.se Andre Smolentzov Department of Linguistics Stockholm University SE-106 91 Stockholm asmolentzov@gmail.com Bj?rn Tyrefors Hinnerich Department of EC Stockholm University SE-106 91 Stockholm bjorn.hinnerich@ne.su.se Erik H?glin National Institute of Economic Research Kungsgatan 12-14 103 62 Stockholm erik.hoglin@konj.se Abstract We present the first system developed for auto- mated grading of high school essays written in Swedish.	EC	English Contex$English consonant$Entailment Core$Economics$enhanced connectivity$Economies$	3
EC flourish in free markets?.	EC	English Contex$English consonant$Entailment Core$Economics$enhanced connectivity$Economies$	5
The Wisdom of Crowds: Why the Many Are Smarter Than the Few and How Col- lective Wisdom Shapes Business, EC, Societies and Nations.	EC	English Contex$English consonant$Entailment Core$Economics$enhanced connectivity$Economies$	5
To get the amount of user effort into context they should be measured against the corresponding error ratios of compara- ble non-interactive systems: Word Error Rate for WSR and CER for KSMR.	CER	Character Error Rate$Concept Error Rate$Constant Entropy Rate$character error rate$	0
People Daily(96) Stories modified Kneser-Ney 5.82% 14.48% Static PPM 6.00% 16.55% Adaptive PPM 4.98% 14.24% Table 3: CERs for Kneser-Ney, Static and Adaptive PPM 5 Conclusion We have introduced a method for Pinyin input based on an adaptive PPM model.	CER	Character Error Rate$Concept Error Rate$Constant Entropy Rate$character error rate$	0
Our research question is closely related to this indirect prediction of the CER hypothesis.	CER	Character Error Rate$Concept Error Rate$Constant Entropy Rate$character error rate$	2
Our experiments showed  substantial improvement on the translitera- tion accuracy over a state-of-the-art baseline  system, significantly reducing the  transliteration CER from  50.29% to 12.84%.	CER	Character Error Rate$Concept Error Rate$Constant Entropy Rate$character error rate$	3
The three ver-  sions of the documents were the original documents,  the documents that resulted after the originals were  subjected to an optical character recognition (OCR)  process with a CER of approximately  5%, and the documents produced through OCR with  a 20% error rate (caused by down-sampling the im-  age before doing the OCR).	CER	Character Error Rate$Concept Error Rate$Constant Entropy Rate$character error rate$	3
We report two types of error rates: word  error rate and CER.	CER	Character Error Rate$Concept Error Rate$Constant Entropy Rate$character error rate$	3
The following Table 3 shows the results in terms of CER.	CER	Character Error Rate$Concept Error Rate$Constant Entropy Rate$character error rate$	3
A detailed study of Table 3 tells us that the re- duction rate of CER ( recall) of Model  in the target domain (9.36%) is much larger than that in the general domain (3.37%).	CER	Character Error Rate$Concept Error Rate$Constant Entropy Rate$character error rate$	3
The CER is the sum of  deletion, insertion and substitution errors.	CER	Character Error Rate$Concept Error Rate$Constant Entropy Rate$character error rate$	3
In the last step, candidate terms selected this way are tested for their senti- ment strength and PP (in other words, how positive or negative are the conotations).	PP	polarity$prepositional phrase$Parent$PRODUCT-PRODUCER$Prepositional Phrase$perplexity$prep$Prediction precision$Pos i t ive  Prec is ion$proper part$Perplex$prepo-$Practical Parser$particle phrase$	0
sentiment PP: select words with the highest absolute human scores.	PP	polarity$prepositional phrase$Parent$PRODUCT-PRODUCER$Prepositional Phrase$perplexity$prep$Prediction precision$Pos i t ive  Prec is ion$proper part$Perplex$prepo-$Practical Parser$particle phrase$	0
For the task of as- signing evaluative PP, it is computed as num- ber of co-occurrences of candidate words with each of the paradigm positive and negative words, denoted as pw and nw.	PP	polarity$prepositional phrase$Parent$PRODUCT-PRODUCER$Prepositional Phrase$perplexity$prep$Prediction precision$Pos i t ive  Prec is ion$proper part$Perplex$prepo-$Practical Parser$particle phrase$	0
The method we followed could be substituted by any other technique which results in a set of highly sentimental lexemes, pos- sibly of varying unknown PP and strength.	PP	polarity$prepositional phrase$Parent$PRODUCT-PRODUCER$Prepositional Phrase$perplexity$prep$Prediction precision$Pos i t ive  Prec is ion$proper part$Perplex$prepo-$Practical Parser$particle phrase$	0
Ide- ally, such a scoring should not only inform about PP (indication whether a word is positive or negative), but also about association strength (the degree of positivity or negativity).	PP	polarity$prepositional phrase$Parent$PRODUCT-PRODUCER$Prepositional Phrase$perplexity$prep$Prediction precision$Pos i t ive  Prec is ion$proper part$Perplex$prepo-$Practical Parser$particle phrase$	0
1 Introduction This paper seeks to improve one of the main meth- ods of unsupervised lexeme sentiment PP as- signment.	PP	polarity$prepositional phrase$Parent$PRODUCT-PRODUCER$Prepositional Phrase$perplexity$prep$Prediction precision$Pos i t ive  Prec is ion$proper part$Perplex$prepo-$Practical Parser$particle phrase$	0
Since no further processing of text has  been performed, another source of problems is  the detection of boundaries of frozen parts in  PPs (e.g. na osnovu (Engl.	PP	polarity$prepositional phrase$Parent$PRODUCT-PRODUCER$Prepositional Phrase$perplexity$prep$Prediction precision$Pos i t ive  Prec is ion$proper part$Perplex$prepo-$Practical Parser$particle phrase$	1
Statistical models for unsu- pervised PP attachment.	PP	polarity$prepositional phrase$Parent$PRODUCT-PRODUCER$Prepositional Phrase$perplexity$prep$Prediction precision$Pos i t ive  Prec is ion$proper part$Perplex$prepo-$Practical Parser$particle phrase$	1
Some believe they are  compounds without analyzing them (Mashkur  (1346), Khatib Rahbar (1367), Gharib (1371),  Meshkatodini (1366)) and still some have  defined them as PPs in one way  or another (Gholam Alizade (1371), Samiian  (1983)).	PP	polarity$prepositional phrase$Parent$PRODUCT-PRODUCER$Prepositional Phrase$perplexity$prep$Prediction precision$Pos i t ive  Prec is ion$proper part$Perplex$prepo-$Practical Parser$particle phrase$	1
In addition, the first child following the head of a  PP is marked as a complement.	PP	polarity$prepositional phrase$Parent$PRODUCT-PRODUCER$Prepositional Phrase$perplexity$prep$Prediction precision$Pos i t ive  Prec is ion$proper part$Perplex$prepo-$Practical Parser$particle phrase$	1
Also, the PP itself is correctly predicted.	PP	polarity$prepositional phrase$Parent$PRODUCT-PRODUCER$Prepositional Phrase$perplexity$prep$Prediction precision$Pos i t ive  Prec is ion$proper part$Perplex$prepo-$Practical Parser$particle phrase$	1
An unsupervised approach to PP attachment using contextu- ally similar words.	PP	polarity$prepositional phrase$Parent$PRODUCT-PRODUCER$Prepositional Phrase$perplexity$prep$Prediction precision$Pos i t ive  Prec is ion$proper part$Perplex$prepo-$Practical Parser$particle phrase$	1
Our corpus consists of the 45 news articles from the Agence France Press used in the training and test sets described by (PP et al, 2008).	PP	polarity$prepositional phrase$Parent$PRODUCT-PRODUCER$Prepositional Phrase$perplexity$prep$Prediction precision$Pos i t ive  Prec is ion$proper part$Perplex$prepo-$Practical Parser$particle phrase$	2
This approach differs from that of (PP et al, 2008) in that it relies almost en- tirely on lexical processing.	PP	polarity$prepositional phrase$Parent$PRODUCT-PRODUCER$Prepositional Phrase$perplexity$prep$Prediction precision$Pos i t ive  Prec is ion$proper part$Perplex$prepo-$Practical Parser$particle phrase$	2
2 A System for TimeML Annotation in French (PP et al, 2008) provide the description and evaluation of a system for the TimeML annota- tion of events and temporal expressions in French texts.	PP	polarity$prepositional phrase$Parent$PRODUCT-PRODUCER$Prepositional Phrase$perplexity$prep$Prediction precision$Pos i t ive  Prec is ion$proper part$Perplex$prepo-$Practical Parser$particle phrase$	2
temporal noun) NNF Formal noun (general) NNFV Formal noun (adverbial) PX Prefix SX Suffix NUM Numeral CL Classifier VB Verb ADJ Adjective ADNOM Adnominal adjective ADV Adverb PCS Case particle PBD Binding particle PADN Adnominal particle PCO Parallel particle PCJ Conjunctive particle PEND Sentence-ending particle P Particle (others) AUX Auxiliary verb CONJ Conjunction PNC Punctuation PAR PPhesis SYM Symbol FIL Filler Table 1: Preterminal tags automatically converted from dependency structure to phrase structure by the previously described method (Uematsu et al 2013), and conversion er- rors of structures and tags were manually corrected.	PP	polarity$prepositional phrase$Parent$PRODUCT-PRODUCER$Prepositional Phrase$perplexity$prep$Prediction precision$Pos i t ive  Prec is ion$proper part$Perplex$prepo-$Practical Parser$particle phrase$	2
respectively, re- quiring 1) PP-Sibling to recognize that ?	PP	polarity$prepositional phrase$Parent$PRODUCT-PRODUCER$Prepositional Phrase$perplexity$prep$Prediction precision$Pos i t ive  Prec is ion$proper part$Perplex$prepo-$Practical Parser$particle phrase$	2
PPhesis.	PP	polarity$prepositional phrase$Parent$PRODUCT-PRODUCER$Prepositional Phrase$perplexity$prep$Prediction precision$Pos i t ive  Prec is ion$proper part$Perplex$prepo-$Practical Parser$particle phrase$	2
3 Annotation Modules In this section, we describe an annotation system, similar to that of (PP et al, 2008) described above, although based on a rich cascade of finite state transducers and a shallow syntactic analysis, as opposed to a full dependency parse.	PP	polarity$prepositional phrase$Parent$PRODUCT-PRODUCER$Prepositional Phrase$perplexity$prep$Prediction precision$Pos i t ive  Prec is ion$proper part$Perplex$prepo-$Practical Parser$particle phrase$	2
SemEval 2010 Task 8 (Hendrickx et al, 2008) provides a new standard benchmark for semantic relation classification to a wider community, where it defines 9 relations includ- ing CAUSE-EFFECT, COMPONENT-WHOLE, CONTENT-CONTAINER, ENTITY-DESTINATION, ENTITY-ORIGIN, INSTRUMENT-AGENCY, MEMBER-COLLECTION, MESSAGE-TOPIC, PP, and a tenth pseudo- relation OTHER (where relation is not one of the 9 annotated relations).	PP	polarity$prepositional phrase$Parent$PRODUCT-PRODUCER$Prepositional Phrase$perplexity$prep$Prediction precision$Pos i t ive  Prec is ion$proper part$Perplex$prepo-$Practical Parser$particle phrase$	3
PP The boy who made the threat was arrested, charged, and had items confiscated from his home.	PP	polarity$prepositional phrase$Parent$PRODUCT-PRODUCER$Prepositional Phrase$perplexity$prep$Prediction precision$Pos i t ive  Prec is ion$proper part$Perplex$prepo-$Practical Parser$particle phrase$	3
Seven relations were used in the task: CAUSE-EFFECT, INSTRUMENT-AGENCY, PP, ORIGIN-ENTITY, THEME- TOOL, PART-WHOLE and CONTENT-CONTAINER.	PP	polarity$prepositional phrase$Parent$PRODUCT-PRODUCER$Prepositional Phrase$perplexity$prep$Prediction precision$Pos i t ive  Prec is ion$proper part$Perplex$prepo-$Practical Parser$particle phrase$	3
From this table it can be observed that the PP, INSTRUMENT-AGENCY, and CAUSE-EFFECT rela- tions were detected with a relatively very high per- formance score, whereas the THEME-TOOL relation classification yielded a relatively small score.	PP	polarity$prepositional phrase$Parent$PRODUCT-PRODUCER$Prepositional Phrase$perplexity$prep$Prediction precision$Pos i t ive  Prec is ion$proper part$Perplex$prepo-$Practical Parser$particle phrase$	3
A total of 10 relations were used includ- ing CAUSE-EFFECT, COMPONENT-WHOLE, CONTENT-CONTAINER, ENTITY-ORIGIN, ENTITY-DESTINATION, INSTRUMENT-AGENCY, MEMBER-COLLECTION, MESSAGE-TOPIC, OTHER, and PP.	PP	polarity$prepositional phrase$Parent$PRODUCT-PRODUCER$Prepositional Phrase$perplexity$prep$Prediction precision$Pos i t ive  Prec is ion$proper part$Perplex$prepo-$Practical Parser$particle phrase$	3
R1 R2 R3 R4 R5 R6 R7 before 682 1200 913 898 861 849 677 after 13 19 10 15 15 8 16 Table 4: The number of features before and af- ter Weka selection, for each semantic relation dataset: R1 CAUSE-EFFECT, R2 INSTRUMENT- AGENCY, R3 PP, R4 ORIGIN- ENTITY, R5 THEME-TOOL, R6 PART-WHOLE, and R7 CONTENT-CONTAINER.	PP	polarity$prepositional phrase$Parent$PRODUCT-PRODUCER$Prepositional Phrase$perplexity$prep$Prediction precision$Pos i t ive  Prec is ion$proper part$Perplex$prepo-$Practical Parser$particle phrase$	3
\[Boggus et al 1992\] L. Boggess, R. Agarwal, R.  Davis, Disambiguation of PPs in  Automatically Labeled Technical Texts, Proc.	PP	polarity$prepositional phrase$Parent$PRODUCT-PRODUCER$Prepositional Phrase$perplexity$prep$Prediction precision$Pos i t ive  Prec is ion$proper part$Perplex$prepo-$Practical Parser$particle phrase$	4
At- taching Multiple PPs: General- ized Backed-off Estimation.	PP	polarity$prepositional phrase$Parent$PRODUCT-PRODUCER$Prepositional Phrase$perplexity$prep$Prediction precision$Pos i t ive  Prec is ion$proper part$Perplex$prepo-$Practical Parser$particle phrase$	4
Attachment  and Transfer of PPs with Con-  straint Propagation".	PP	polarity$prepositional phrase$Parent$PRODUCT-PRODUCER$Prepositional Phrase$perplexity$prep$Prediction precision$Pos i t ive  Prec is ion$proper part$Perplex$prepo-$Practical Parser$particle phrase$	4
A Rule-Based Approach to PP Attachment  Disambiguation.	PP	polarity$prepositional phrase$Parent$PRODUCT-PRODUCER$Prepositional Phrase$perplexity$prep$Prediction precision$Pos i t ive  Prec is ion$proper part$Perplex$prepo-$Practical Parser$particle phrase$	4
A Maximum Entropy Model for PP Attachment.	PP	polarity$prepositional phrase$Parent$PRODUCT-PRODUCER$Prepositional Phrase$perplexity$prep$Prediction precision$Pos i t ive  Prec is ion$proper part$Perplex$prepo-$Practical Parser$particle phrase$	4
combinations in Farsi that  apparently a PP almost  behaves as Compound Prepositions.	PP	polarity$prepositional phrase$Parent$PRODUCT-PRODUCER$Prepositional Phrase$perplexity$prep$Prediction precision$Pos i t ive  Prec is ion$proper part$Perplex$prepo-$Practical Parser$particle phrase$	4
We intend to perform experi-  ments to compare the PP of the various mod-  els, and a structurally similar 'pure' PCFG 1?.	PP	polarity$prepositional phrase$Parent$PRODUCT-PRODUCER$Prepositional Phrase$perplexity$prep$Prediction precision$Pos i t ive  Prec is ion$proper part$Perplex$prepo-$Practical Parser$particle phrase$	5
We propose a set of features that model both the translations and the translators, such as country of resi- dence, LM PP of the translation, edit rate from the other translations, and (option- ally) calibration against professional transla- tors.	PP	polarity$prepositional phrase$Parent$PRODUCT-PRODUCER$Prepositional Phrase$perplexity$prep$Prediction precision$Pos i t ive  Prec is ion$proper part$Perplex$prepo-$Practical Parser$particle phrase$	5
The lower PP means the better predictive ability of the LM.	PP	polarity$prepositional phrase$Parent$PRODUCT-PRODUCER$Prepositional Phrase$perplexity$prep$Prediction precision$Pos i t ive  Prec is ion$proper part$Perplex$prepo-$Practical Parser$particle phrase$	5
The PP was improved by approximately 46% on English and 49% on Czech compared with standalone MKN.	PP	polarity$prepositional phrase$Parent$PRODUCT-PRODUCER$Prepositional Phrase$perplexity$prep$Prediction precision$Pos i t ive  Prec is ion$proper part$Perplex$prepo-$Practical Parser$particle phrase$	5
Our experiments with English and Czech corpora show significant PP re- ductions (up to 46% for English and 49% for Czech) compared with standalone 4-gram Modified Kneser-Ney language model.	PP	polarity$prepositional phrase$Parent$PRODUCT-PRODUCER$Prepositional Phrase$perplexity$prep$Prediction precision$Pos i t ive  Prec is ion$proper part$Perplex$prepo-$Practical Parser$particle phrase$	5
We measure the quality of LTLM by PP that is the standard measure used for LMs.	PP	polarity$prepositional phrase$Parent$PRODUCT-PRODUCER$Prepositional Phrase$perplexity$prep$Prediction precision$Pos i t ive  Prec is ion$proper part$Perplex$prepo-$Practical Parser$particle phrase$	5
cn common noun *pe PPosition co coordinating elem.	PP	polarity$prepositional phrase$Parent$PRODUCT-PRODUCER$Prepositional Phrase$perplexity$prep$Prediction precision$Pos i t ive  Prec is ion$proper part$Perplex$prepo-$Practical Parser$particle phrase$	6
PPosition to the right?).	PP	polarity$prepositional phrase$Parent$PRODUCT-PRODUCER$Prepositional Phrase$perplexity$prep$Prediction precision$Pos i t ive  Prec is ion$proper part$Perplex$prepo-$Practical Parser$particle phrase$	6
We then PPared features, and fed the training data to a sequential labeling system, a discriminative Markov model much like Conditional Random Fields (CRF), with the difference being that the model parameters are tuned using Bayes Point Machines (BPM), and then compared our model against an equivalent CRF model.	PP	polarity$prepositional phrase$Parent$PRODUCT-PRODUCER$Prepositional Phrase$perplexity$prep$Prediction precision$Pos i t ive  Prec is ion$proper part$Perplex$prepo-$Practical Parser$particle phrase$	6
The passive char- acters were identified via the following relations extracted by dependency parsing: nsubjpass (passive nominal subject) and pobj (object of a PPosition).	PP	polarity$prepositional phrase$Parent$PRODUCT-PRODUCER$Prepositional Phrase$perplexity$prep$Prediction precision$Pos i t ive  Prec is ion$proper part$Perplex$prepo-$Practical Parser$particle phrase$	6
Objects in cluster 1, corresponding to binary ad- jectives, have high values for most of the features containing a PPosition after the adjective (observe +1pe, ?	PP	polarity$prepositional phrase$Parent$PRODUCT-PRODUCER$Prepositional Phrase$perplexity$prep$Prediction precision$Pos i t ive  Prec is ion$proper part$Perplex$prepo-$Practical Parser$particle phrase$	6
4 count of 1st person singular pronouns 5 count of negative particles 6 count of numbers 7 count of PPositions 8 count of pronouns 9 count of ?	PP	polarity$prepositional phrase$Parent$PRODUCT-PRODUCER$Prepositional Phrase$perplexity$prep$Prediction precision$Pos i t ive  Prec is ion$proper part$Perplex$prepo-$Practical Parser$particle phrase$	6
The remaining 60% translation pairs do not only reflect word alignment errors, but also cases where we find a PPiciple in the German sentence that has a correct adverbial translation for other reasons.	PP	polarity$prepositional phrase$Parent$PRODUCT-PRODUCER$Prepositional Phrase$perplexity$prep$Prediction precision$Pos i t ive  Prec is ion$proper part$Perplex$prepo-$Practical Parser$particle phrase$	9
2nd + 3rd = .\]  Note that the PP of lst+2nd excludes 3rd person.	PP	polarity$prepositional phrase$Parent$PRODUCT-PRODUCER$Prepositional Phrase$perplexity$prep$Prediction precision$Pos i t ive  Prec is ion$proper part$Perplex$prepo-$Practical Parser$particle phrase$	9
After each stage only 'maximal' fragments are  retained (a fragment is maximal if its segment is  not a PP of the segment of any other frag-  Inent).	PP	polarity$prepositional phrase$Parent$PRODUCT-PRODUCER$Prepositional Phrase$perplexity$prep$Prediction precision$Pos i t ive  Prec is ion$proper part$Perplex$prepo-$Practical Parser$particle phrase$	9
Constitutive Role: the relation between an object and its constituents, or  PPs.	PP	polarity$prepositional phrase$Parent$PRODUCT-PRODUCER$Prepositional Phrase$perplexity$prep$Prediction precision$Pos i t ive  Prec is ion$proper part$Perplex$prepo-$Practical Parser$particle phrase$	9
8WN has chosen a restrictive sense for the Great Divide, making it a PP of the Continental Divide.	PP	polarity$prepositional phrase$Parent$PRODUCT-PRODUCER$Prepositional Phrase$perplexity$prep$Prediction precision$Pos i t ive  Prec is ion$proper part$Perplex$prepo-$Practical Parser$particle phrase$	9
A fragment is called maxi-  mal if its segment is not a PP of the segment  of any other fragment belonging to A. The set C is the  final result of partial parsing.	PP	polarity$prepositional phrase$Parent$PRODUCT-PRODUCER$Prepositional Phrase$perplexity$prep$Prediction precision$Pos i t ive  Prec is ion$proper part$Perplex$prepo-$Practical Parser$particle phrase$	9
LTLM 39.9 36.4 32.8 30.3 28.1 26.0 24.9 64.4 56.1 51.5 47.3 43.4 39.9 37.2 Table 3: PPity results on the test data for LTLMs and STLMs with different number of roles.	PP	polarity$prepositional phrase$Parent$PRODUCT-PRODUCER$Prepositional Phrase$perplexity$prep$Prediction precision$Pos i t ive  Prec is ion$proper part$Perplex$prepo-$Practical Parser$particle phrase$	10
PP- ity is a measure of uncertainty.	PP	polarity$prepositional phrase$Parent$PRODUCT-PRODUCER$Prepositional Phrase$perplexity$prep$Prediction precision$Pos i t ive  Prec is ion$proper part$Perplex$prepo-$Practical Parser$particle phrase$	10
LTLM 24.9 (-43.1%) 37.2 (-49.4%) Table 2: PPity results on the test data.	PP	polarity$prepositional phrase$Parent$PRODUCT-PRODUCER$Prepositional Phrase$perplexity$prep$Prediction precision$Pos i t ive  Prec is ion$proper part$Perplex$prepo-$Practical Parser$particle phrase$	10
020406080100120140 '0 20 25 30 35 40 50 70 100 125 150 175 Top-r anked   gener al-dom ain se ntenc es (in k ) Devset PPity In-dom ain ba seline Cross - Entrop y Moore - Lewis bilingu al M-L Figure 1: Corpus Selection Results The perplexity of the dev set according to LMs trained on the top-ranked sentences varied from 77 to 120, depending on the size of the subset and the method used.	PP	polarity$prepositional phrase$Parent$PRODUCT-PRODUCER$Prepositional Phrase$perplexity$prep$Prediction precision$Pos i t ive  Prec is ion$proper part$Perplex$prepo-$Practical Parser$particle phrase$	10
PP- ity of n-gram and dependency language models.	PP	polarity$prepositional phrase$Parent$PRODUCT-PRODUCER$Prepositional Phrase$perplexity$prep$Prediction precision$Pos i t ive  Prec is ion$proper part$Perplex$prepo-$Practical Parser$particle phrase$	10
PPities on the test set are given in Table 1.	PP	polarity$prepositional phrase$Parent$PRODUCT-PRODUCER$Prepositional Phrase$perplexity$prep$Prediction precision$Pos i t ive  Prec is ion$proper part$Perplex$prepo-$Practical Parser$particle phrase$	10
But even the ostensibly disambiguating PP  sition by, is itself ambiguous, since it might introduce  a manner or locative phrase consistent with the main  clause analysis.	PP	polarity$prepositional phrase$Parent$PRODUCT-PRODUCER$Prepositional Phrase$perplexity$prep$Prediction precision$Pos i t ive  Prec is ion$proper part$Perplex$prepo-$Practical Parser$particle phrase$	11
The only manually encoded knowledge is a dictio- nary of markers (subordinators, coordinators, PP sitions).	PP	polarity$prepositional phrase$Parent$PRODUCT-PRODUCER$Prepositional Phrase$perplexity$prep$Prediction precision$Pos i t ive  Prec is ion$proper part$Perplex$prepo-$Practical Parser$particle phrase$	11
Also, instead of using the PP sition itself as the argument head, we used the ac- tual content word modifying the preposition.	PP	polarity$prepositional phrase$Parent$PRODUCT-PRODUCER$Prepositional Phrase$perplexity$prep$Prediction precision$Pos i t ive  Prec is ion$proper part$Perplex$prepo-$Practical Parser$particle phrase$	11
Therefore, we relax the adjacency con- dition for verb attachment and also count PP sitions that occur within a fixed distance of their suspected regent.	PP	polarity$prepositional phrase$Parent$PRODUCT-PRODUCER$Prepositional Phrase$perplexity$prep$Prediction precision$Pos i t ive  Prec is ion$proper part$Perplex$prepo-$Practical Parser$particle phrase$	11
Context features {Head word of first NP in PP sition phrase, left and right sibling head words and syntactic categories, first and last word in phrase yield and their PoS, parent syntactic category and head word}.	PP	polarity$prepositional phrase$Parent$PRODUCT-PRODUCER$Prepositional Phrase$perplexity$prep$Prediction precision$Pos i t ive  Prec is ion$proper part$Perplex$prepo-$Practical Parser$particle phrase$	11
Extra function words (determiners and PP sitions) in source or target language are linked together with their noun to the noun?s transla- tion.	PP	polarity$prepositional phrase$Parent$PRODUCT-PRODUCER$Prepositional Phrase$perplexity$prep$Prediction precision$Pos i t ive  Prec is ion$proper part$Perplex$prepo-$Practical Parser$particle phrase$	11
Using Decision Trees to Con- struct a PP.	PP	polarity$prepositional phrase$Parent$PRODUCT-PRODUCER$Prepositional Phrase$perplexity$prep$Prediction precision$Pos i t ive  Prec is ion$proper part$Perplex$prepo-$Practical Parser$particle phrase$	12
1998 : "Using Decision Trees  to Construct a PP".	PP	polarity$prepositional phrase$Parent$PRODUCT-PRODUCER$Prepositional Phrase$perplexity$prep$Prediction precision$Pos i t ive  Prec is ion$proper part$Perplex$prepo-$Practical Parser$particle phrase$	12
1998 : "Using Decision ~l~'ees to  Construct a PP", Procccding.s o\[  COLING-A CL-08 pp505-511  Sadao Kurohashi, Makoto Nagao.	PP	polarity$prepositional phrase$Parent$PRODUCT-PRODUCER$Prepositional Phrase$perplexity$prep$Prediction precision$Pos i t ive  Prec is ion$proper part$Perplex$prepo-$Practical Parser$particle phrase$	12
Using Decision Trees to Construct a PP.	PP	polarity$prepositional phrase$Parent$PRODUCT-PRODUCER$Prepositional Phrase$perplexity$prep$Prediction precision$Pos i t ive  Prec is ion$proper part$Perplex$prepo-$Practical Parser$particle phrase$	12
proof, since the sum of linear elements i linear:  2 The PP   From now on we assume that strings of  nodes  are natural language sentences and discuss a  fully implemented parser (DCParser) that parses  Finnish sentences.	PP	polarity$prepositional phrase$Parent$PRODUCT-PRODUCER$Prepositional Phrase$perplexity$prep$Prediction precision$Pos i t ive  Prec is ion$proper part$Perplex$prepo-$Practical Parser$particle phrase$	12
Table 2: Typical hand-written compound  noun indexing rule patterns for Korean  Noun without case makers / Noun  Noun with a genitive case maker / Noun  Noun with a nominal case maker or  an accusative case maker \[  Verbal common oun or adjectival common noun  Noun with an adnominal ending \] Noun  Noun within predicate PP / Noun  (The two nouns before and after a slash  in the pattern can form a single compound noun.)	PP	polarity$prepositional phrase$Parent$PRODUCT-PRODUCER$Prepositional Phrase$perplexity$prep$Prediction precision$Pos i t ive  Prec is ion$proper part$Perplex$prepo-$Practical Parser$particle phrase$	13
Currently this rescoring  is used to fine-tune attachments of PPs in  Japanese sentences.	PP	polarity$prepositional phrase$Parent$PRODUCT-PRODUCER$Prepositional Phrase$perplexity$prep$Prediction precision$Pos i t ive  Prec is ion$proper part$Perplex$prepo-$Practical Parser$particle phrase$	13
cor-  resl)onding to the grammar of (9)-(10) is ~s follows~:  (14) (Vi, j ,k, l ,e,p)pp(i, j ,e) A pp(j,k,e)  A v(< t, ^ > ,(i, l, e)  (15) (Vi, j,k,z,e,pari),,p(i,j,;,:)  A pa,'t ide(j, k, l,a,'t ) A pa,'t(x,e)  -o pp(i, k, e)  (16) (Vi,j,k,l,.~,V),u,(i,j,~j) A pa,'iicle(j,k,,,o)  a t, A ,,,o(v,  .p(i, l, z)  (17) (Vi, j ,w,z)n(i , j ,w) A w(.~) D np(i,j,z)  pp(i, j ,  e) mean.~ that there is a PP from i  to j with the missing a.rgumenl, e. part is a particle  and the predicate it encodes.	PP	polarity$prepositional phrase$Parent$PRODUCT-PRODUCER$Prepositional Phrase$perplexity$prep$Prediction precision$Pos i t ive  Prec is ion$proper part$Perplex$prepo-$Practical Parser$particle phrase$	13
COMLEX cites  the specific: prepositions and adverbs hi preposi-  tional anti PPs associated with partic-  ular verbs.	PP	polarity$prepositional phrase$Parent$PRODUCT-PRODUCER$Prepositional Phrase$perplexity$prep$Prediction precision$Pos i t ive  Prec is ion$proper part$Perplex$prepo-$Practical Parser$particle phrase$	13
An idiomatic transla-  tion of sentence (2) is  pp(i, j, e) means that there is a PP from i to  j with the missing argument e. part is a particle .and  the predicate it encodes.	PP	polarity$prepositional phrase$Parent$PRODUCT-PRODUCER$Prepositional Phrase$perplexity$prep$Prediction precision$Pos i t ive  Prec is ion$proper part$Perplex$prepo-$Practical Parser$particle phrase$	13
3 Methods Statistical PP attachment is based on the obser- vation that the identities of content words can be used to predict which PPs mod- ify which words, and achieve better-than-chance accuracy.	PPs	preposition  phrases$prepositional phrases$Prepositional phrases$	1
3 Extending the structure under  discussion    3.1 Premodifiers    The noun in PPs, can be  extended in different ways while as the examples  below show, the related structures cannot:      3.1.1 Demonstratives    6.	PPs	preposition  phrases$prepositional phrases$Prepositional phrases$	1
on (one) roof-of  house    3.2 Post Modifiers    Nouns in PPs can expand  with post modifiers while nouns in our structure  cannot.	PPs	preposition  phrases$prepositional phrases$Prepositional phrases$	1
If  the noun here expands as other nouns in other  PPs we can conclude that the  related structure is a phrase, otherwise it is better  to think about them as compound prepositions.	PPs	preposition  phrases$prepositional phrases$Prepositional phrases$	1
A linguistic analyser of spatial expressions (nominal and PPs) have been designed, which recognise them and produces a symbolic representation of their "content".	PPs	preposition  phrases$prepositional phrases$Prepositional phrases$	1
WITH and WITHOUT  PPs containing WITH and WITH-  OUT can also be related to OF-phrases (Lees  1960:93) and to HAVE (Poldauf I967:33f.) ,	PPs	preposition  phrases$prepositional phrases$Prepositional phrases$	2
PPs are free in their location be- cause the preposition is already a unique identi- fier.	PPs	preposition  phrases$prepositional phrases$Prepositional phrases$	2
PPs, however, although they  show a high degree of locality in the syntax, are involved in  complex, non-local interactions in the semantics, with a cor-  responding complication of the processing.	PPs	preposition  phrases$prepositional phrases$Prepositional phrases$	2
PP Interpretation  PPs, in both post-copular nd post-nominal  positions, are very common in the ATIS domain (and most  other domains as well).	PPs	preposition  phrases$prepositional phrases$Prepositional phrases$	2
PPs would seem to be  trouble, however, because the preposi-  119  -9 -   tion simultaneously interacts with both  its subject and its object.	PPs	preposition  phrases$prepositional phrases$Prepositional phrases$	2
PPs: Our original method also fails to correctly determine the negation scope when the negated event is followed by a prepositional phrase, as it may be seen in Figure 2, where the syntax tree for the sen- tence: [There was] no [attempt at robbery] is shown.	PPs	preposition  phrases$prepositional phrases$Prepositional phrases$	2
Using MSR for  Word Sense Disambiguation.	MSR	Measures of Semantic Relatedness$Microsoft Research$	0
Using MSR for Word Sense Disambiguation.	MSR	Measures of Semantic Relatedness$Microsoft Research$	0
Using MSR for Word  Sense Disambiguation, LNCS 2588 - CICLing  2003, pp.	MSR	Measures of Semantic Relatedness$Microsoft Research$	0
2 MSR Approaches to measuring semantic relatedness that use lexical resources transform these resources into a network or graph and compute relatedness using paths in it (see Budanitsky & Hirst (2006) for an ex- tensive review).	MSR	Measures of Semantic Relatedness$Microsoft Research$	0
Automatically Creating Datasets for MSR.	MSR	Measures of Semantic Relatedness$Microsoft Research$	0
MSR Asia +University of Edinburgh ?	MSR	Measures of Semantic Relatedness$Microsoft Research$	1
MSR Paraphrase Corpus.	MSR	Measures of Semantic Relatedness$Microsoft Research$	1
The work was done when the first author and the third author were interns at MSR Asia.	MSR	Measures of Semantic Relatedness$Microsoft Research$	1
931  Support Vector Machines for Paraphrase Identification   and Corpus Construction  Chris Brockett and William B. Dolan  Natural Language Processing Group  MSR  One Microsoft Way, Redmond, WA 98502, U.S.A.  {chrisbkt, billdol}@microsoft.com  Abstract  The lack of readily-available large cor- pora of aligned monolingual sentence  pairs is a major obstacle to the devel- opment of Statistical Machine Transla- tion-based paraphrase models.	MSR	Measures of Semantic Relatedness$Microsoft Research$	1
MSR, Redmond, WA 98052, USA {jianshuc, xiaohe, jfgao, lihongli, deng}@microsoft.com Abstract This paper introduces a novel architec- ture for reinforcement learning with deep neural networks designed to handle state and action spaces characterized by natural language, as found in text-based games.	MSR	Measures of Semantic Relatedness$Microsoft Research$	1
Role-playing games  System by MSR (Kacmarcik 2005) 61   Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 846?855, Austin, Texas, November 1-5, 2016.	MSR	Measures of Semantic Relatedness$Microsoft Research$	1
The quote-to-speaker attribution was evaluated in terms of PRE (AT p ), while the estimation of speakers?	PRE	precision$PO$	0
The  cases where "it" is merely a dummy subject in  a cleft sentence (example 1) or has conventional  unspecified referents (example 2) are excluded  from computing the PRE:  ?	PRE	precision$PO$	0
Thus we  compute PRE as follows:  PRE =  \ [ ra t t r ib .	PRE	precision$PO$	0
The PRE score computed over all phrases  containing any of the target honorifics are 66.0%  l In effect, this is the same as admi t t ing  that  a ref-  erent  can be in different gender  classes across different  observ	PRE	precision$PO$	0
The PRE score computed over all phrases  containing any of the target honorifics are 66.0%  l In effect, this is the same as admi t t ing  that  a ref-  erent  can be in different gender  classes a	PRE	precision$PO$	0
In order to exclude the second type of error, the PRE of gender estimation was also computed for only the true story speaker	PRE	precision$PO$	0
In order to exclude the second type of error, the PRE of gender estimation was also computed for only the true story speaker identified by the system (G?	PRE	precision$PO$	0
gender was evaluated in terms of PRE (G p ) and recall (G r ).	PRE	precision$PO$	0
The used patterns are: 1) (DT|CD) (NN|NNS), 2) DT JJ (NN|NNS), 3) NN PRES (NN|NNS), and 4) PRP$ JJ (NN|NNS).	PRE	precision$PO$	1
These PRES-based patterns are quite generic, al- lowing for the creation of large sets of characters.	PRE	precision$PO$	1
No.1-20 + PRES tags for No.21 3.4.3 Estimation of Personality Attributes A machine-learning based approach was also used for personality attribute estimation.	PRE	precision$PO$	1
MEESE 166(82.1007) 0.8734 0 0.1265  BRAZIL 285(79.7311) 0.0596 0 0.9403  SPREKESMAN 665(78	PRE	precision$PO$	1
GORBACHEV 205(108.776) 0.8926 0.0048 0.1024  JUDGE BORK 212(108.746) 0.8820 0 0.1179  HUSBAND 91(107.438) 0.3626 0.5714 0.0659  JAPAN 450(100.727) 0.0755 0.0111 0.9133  AGENCY 476(97.4016) 0.0840 0.0147 0.9012  WIFE 153(93.7485) 0.6143 0.2875 0.0980  DOLLAR 621(90.8963) 0.1304 0.0096 0.8599  STANDARD PREOR 200(90.1062) 0 0 1  FATHER 146(89.4178-) 0.8082 0.1438 0.0479  UTILITY 242(87.1821) 0.0247 0 0.9752  MR.	PRE	precision$PO$	1
MEESE 166(82.1007) 0.8734 0 0.1265  BRAZIL 285(79.7311) 0.0596 0 0.9403  SPREKESMAN 665(78.3441) 0.6075 0.0045 0.3879  MR.	PRE	precision$PO$	1
This includes (i) tokenization, (ii) sen- tence splitting and identification of paragraph boundaries, (iii) part-of-speech (PRES) tagging, (iv) lemmatization, (v) named entity recognition, (vi) dependency parsing, and (vii) co-reference analysis.	PRE	precision$PO$	1
8599  STANDARD PREOR 200(90.1062) 0 0 1  FATHER 146(89.4178-) 0.8082 0.1438 0.0479  UTILITY 242(87.1821) 0.0247 0 0.9752  MR.	PRE	precision$PO$	1
To determine conceptual categories that POBJ belong to, WordNet (Fellbaum, 1998) appears to be an ideal tool.	POBJ	objects of prepositions$object of preposition$	0
generated by Stanford Parser  There are various argument relations like sub- ject, object, POBJ and clausal  complements, modifier relations like adjectival,  adverbial, participial, infinitival modifiers and  other relations like coordination, conjunct, exple- tive and punctuation.	POBJ	objects of prepositions$object of preposition$	0
In addition, Stanford distinguishes POBJ from objects of verbs, while PARC and GR collapse the two into a single relation.	POBJ	objects of prepositions$object of preposition$	0
GEN is assigned to POBJ and to possessors in idafa (possessive) construction.	POBJ	objects of prepositions$object of preposition$	0
Therefore, another im- portant question arises: can we automatically learn rules or patterns to achieve the same ob- 2 They are 1) verbs and their arguments, 2) adjectives and their arguments and 3) propositionally modified tokens and POBJ.	POBJ	objects of prepositions$object of preposition$	0
CHILDES labelled unlabelled RASP 60.1 69.2 CCG parser 39.1 66.5 MSTParser 93.8 95.4 MEGRASP 90.7 93.5 CCC labelled unlabelled RASP 66.7 72.3 CCG parser 60.2 68.5 F1-scores, such as auxiliaries, determiners, sub- jects, and POBJ.5 3.3 CCC The Cambridge Cookie-theft Corpus (CCC, TO APPEAR, 2010) contains audio-recorded mono- logues of 196 subjects that were asked to fully de- scribe a scene in a picture.	POBJ	objects of prepositions$object of preposition$	0
We use POBJ, subject, direct object, tense as our features.	POBJ	objects of prepositions$object of preposition$	1
b3 if (a) NP2 is a pronoun ; and (b) both NP1 and NP2 are at the same type of grammatical rol e (subject, object of verb, POBJ, etc.) .	POBJ	objects of prepositions$object of preposition$	1
Table 2 shows the recall of the easy-first dependency parser of (Goldberg and Elhadad, 2010) on Section 23 of the Penn Treebank for identifying the governor and POBJs.	POBJ	objects of prepositions$object of preposition$	1
A second problem is that CFG's naturally abstract away from  syntactic function: for example, in a CFG, a noun phrase is  described by the same set of rules whether it occurs as sub-  ject, object, POBJ or whatever.	POBJ	objects of prepositions$object of preposition$	1
Our syn- tactic errors are either selection (e.g., wrong case as POBJ) or agreement errors (e.g., subject-verb disagreement in number).	POBJ	objects of prepositions$object of preposition$	1
Recall Governor Object Parser 88.88 92.37 Best(Parser, Heuristics) 92.50 93.06 Table 2: Identifying governor and POBJs in the Penn Treebank data.	POBJ	objects of prepositions$object of preposition$	1
The 0-1 loss also has a loss of 1 for FP and false negatives.	FP	false positives$false positive$Filled Pause$False Positives$False Positive$	0
The high-recall loss function penalizes FP with 0.1 and false negatives with 5.	FP	false positives$false positive$Filled Pause$False Positives$False Positive$	0
are considered FP.	FP	false positives$false positive$Filled Pause$False Positives$False Positive$	0
The baseline system returns nearly all cues but since it matches every string, it also returns many FP, resulting in low precision.	FP	false positives$false positive$Filled Pause$False Positives$False Positive$	0
This implies that although we get a rather satisfying score in terms of precision and recall, the number of FP that we get is rather high in relation to our universe.	FP	false positives$false positive$Filled Pause$False Positives$False Positive$	0
The high-precision loss function penalizes false negatives with 0.1 and FP with 5.	FP	false positives$false positive$Filled Pause$False Positives$False Positive$	0
but guessed the wrong type, we call this a cross- labeling; (2) a FP occurs when the learner guessed some relation while there should have been none; (3) the reverse is a false negative.	FP	false positives$false positive$Filled Pause$False Positives$False Positive$	1
The high-recall loss function penalizes FPs with 0.1 and false negatives with 5.	FP	false positives$false positive$Filled Pause$False Positives$False Positive$	1
The 0-1 loss also has a loss of 1 for FPs and false negatives.	FP	false positives$false positive$Filled Pause$False Positives$False Positive$	1
are considered FPs.	FP	false positives$false positive$Filled Pause$False Positives$False Positive$	1
The baseline system returns nearly all cues but since it matches every string, it also returns many FPs, resulting in low precision.	FP	false positives$false positive$Filled Pause$False Positives$False Positive$	1
The high-precision loss function penalizes false negatives with 0.1 and FPs with 5.	FP	false positives$false positive$Filled Pause$False Positives$False Positive$	1
c?2013 Association for Computational Linguistics Open-ended, Extensible System Utterances Are Preferred, Even If They Require FPs Timo Baumann Universit?t Hamburg Department of Informatics Germany baumann@informatik.uni-hamburg.de David Schlangen University of Bielefeld Faculty of Linguistics and Literary Studies Germany david.schlangen@uni-bielefeld.de Abstract In many environments (e. g. sports com- mentary), situations incrementally unfold over time and often the future appearance of a relevant event can be	FP	false positives$false positive$Filled Pause$False Positives$False Positive$	2
130  Category Read Speech Spontaneous Speech  TOTAL 103 269  Mouth Clicks 60 106  Breath Noise 37 117  FPs 0 30  Others 6 16  Table 2: Number of occurrences of non-speech vocalizations for read and spontaneous speech from 9 male  and 9 female speakers.	FP	false positives$false positive$Filled Pause$False Positives$False Positive$	2
702  Modeling FPs in Medical Dictations  Serge)' V.. Pakhomov  University of Minnesota  190 Klaeber Court  320-16 th Ave.	FP	false positives$false positive$Filled Pause$False Positives$False Positive$	2
Word Accuracy  \ [ \ ]  Sentence ACc, lr'~ev  66.6  86.9  0.0  Partial Words FP No Non-Speech  (1.5 %) (4.5 %) (94 %)  Condition  Figure 2: Breakdown of word and sentence accuracy for the spontaneous speech test sets, depending on  whether the sentences contain false starts or filled pauses.	FP	false positives$false positive$Filled Pause$False Positives$False Positive$	2
The Communicative Value of FPs in Spontaneous Speech.	FP	false positives$false positive$Filled Pause$False Positives$False Positive$	2
FPs and Gestures: It's not  coincidence," Journal of Psycholinguistic  Research, Vol.	FP	false positives$false positive$Filled Pause$False Positives$False Positive$	2
Syntax Only  Marked Marked  as  as   Repair False Positive  Repairs 68 (96%) 56 (30%)  FP 3 (4%) 131 (70%)  Syntax and Semantics  Marked Marked  as  as   Repair False Positive  Repairs 64 (85%) 23 (20%)  FP 11 (15%) 90 (80%)  Table 5: Syntax and Semantics Results  59  dataset of 335 sentences, of which 179 contained  repairs and 176 contained false positives.	FP	false positives$false positive$Filled Pause$False Positives$False Positive$	3
The number of True Positives, True Negatives, and  FP are listed.	FP	false positives$false positive$Filled Pause$False Positives$False Positive$	3
3.6 Querying the Model and FP The construction we have described above ensures that for any n-gram xi ?	FP	false positives$false positive$Filled Pause$False Positives$False Positive$	3
rise fall stress speech  Repairs .00 1.00 .00 .00  FP .87 .00 .87 .73  6\]  Table 7: Acoustic Characteristics of Cue Words  8000  !	FP	false positives$false positive$Filled Pause$False Positives$False Positive$	3
14  True Positives 191 246  FP 82 133  False Negatives 1587 1874  Correct Labels 189 237  Precision 0.700 0.649  Recall 0.107 0.116  F-Score 0.186 0.197  Label Accuracy 0.106 0.112    As can be seen, for entries with patterns (albeit  a low recall), a substantial number of frame ele- ments could be recognized with high precision  from a very small number of constituent match- ing functions.	FP	false positives$false positive$Filled Pause$False Positives$False Positive$	3
Syntax Only  Marked Marked  as  as   Repair FP  Repairs 68 (96%) 56 (30%)  FPs 3 (4%) 131 (70%)  Syntax and Semantics  Marked Marked  as  as   Repair FP  Repairs 64 (85%) 23 (20%)  FPs 11 (15%) 90 (80%)  Table 5: Syntax and Semantics Results  59  dataset of 335 sentences, of which 179 contained  repairs and 176 contained false positives.	FP	false positives$false positive$Filled Pause$False Positives$False Positive$	4
3.6 Querying the Model and FPs The construction we have described above ensures that for any n-gram xi ?	FP	false positives$false positive$Filled Pause$False Positives$False Positive$	4
5.2 FP Rates All n-grams explicitly inserted into our randomized language model are retrieved without error; how- ever, n-grams not stored may be incorrectly assigned a value resulting in a false positive.	FP	false positives$false positive$Filled Pause$False Positives$False Positive$	4
Links that erroneously start a new cluster when it is coreferent with other men- tions to the left is marked as FN.	FN	false new$false negatives$False Negative$FrameNet$following noun$false negative$For PER$	0
def cost_based_on_consistency(arc): ana, ante = arc consistent = \ ana.decision_is_consistent(ante) # FN if not consistent and \ ante.is_dummy(): return 2 # wrong link elif not consistent: return 1 # correct else: return 0 tent structure for one document into substructures, and the candidate arcs for each substructure.	FN	false new$false negatives$False Negative$FrameNet$following noun$false negative$For PER$	0
FN?	FN	false new$false negatives$False Negative$FrameNet$following noun$false negative$For PER$	0
Uturku used an SVM- based approach for extraction, and it is thus delicate to account for the FN in a simple and concise way.	FN	false new$false negatives$False Negative$FrameNet$following noun$false negative$For PER$	1
The 0-1 loss also has a loss of 1 for false positives and FN.	FN	false new$false negatives$False Negative$FrameNet$following noun$false negative$For PER$	1
The high-recall loss function penalizes false positives with 0.1 and FN with 5.	FN	false new$false negatives$False Negative$FrameNet$following noun$false negative$For PER$	1
For example, we con- sider every WordNet sense to be plausible, which produces FN.	FN	false new$false negatives$False Negative$FrameNet$following noun$false negative$For PER$	1
However, a concern was that the transformed query would cause numerous FN.	FN	false new$false negatives$False Negative$FrameNet$following noun$false negative$For PER$	1
Surprisingly, we also found FN in rather trivial examples (?	FN	false new$false negatives$False Negative$FrameNet$following noun$false negative$For PER$	1
The high-precision loss function penalizes FN with 0.1 and false positives with 5.	FN	false new$false negatives$False Negative$FrameNet$following noun$false negative$For PER$	1
The FN error  was high for only one user, while the majority  of - the users exhibited no FN  I  I  I  I  I  I  I  I  I  I  I  I  I  I  I  I  I  I  I  Table 2: Relevance Classes by User  User  5 0  4 0  I  2  3  4  5  1  7  8  9  10  11  12  13  14  True False True False  Positive Positive Negative Negative  5 0 0 0  4  7  1  5 0  4 0  0  4  5  0  2  0  0  0  0  0  0 0  0 1  0 0  0 1  0 2  ~0  0  0  0  0  1 0 2 2  0 1 6 0  I 1 4  errors, a worse error to commit han wasting	FN	false new$false negatives$False Negative$FrameNet$following noun$false negative$For PER$	2
The (1,0) point represents perfect performance with 100% True Positive Rate and 0% FN Rate.	FN	false new$false negatives$False Negative$FrameNet$following noun$false negative$For PER$	2
Baseline + Graph Accuracy/Kappa 0.624/0.298 0.646/0.173 FN Rate 0.095 0.482 Table 1: Performance metrics for machine learning experiments.	FN	false new$false negatives$False Negative$FrameNet$following noun$false negative$For PER$	2
After that we use the Word2Vec tool (Mikolov et al, 2013b) to generate FN (%) Time (s) Linear Search 0 342 LSH 14.29 69 RBV 9.08 19 Table 2: Performance of linear search, locality sensitive hashing, and redundant bit vectors, for k = 200.	FN	false new$false negatives$False Negative$FrameNet$following noun$false negative$For PER$	2
Graph Accuracy/Kappa 0.692/0.365 # 0.693/0.277 # FN Rate 0.157 0.397 3.	FN	false new$false negatives$False Negative$FrameNet$following noun$false negative$For PER$	2
14  True Positives 191 246  False Positives 82 133  FNs 1587 1874  Correct Labels 189 237  Precision 0.700 0.649  Recall 0.107 0.116  F-Score 0.186 0.197  Label Accuracy 0.106 0.112    As can be seen, for entries with patterns (albeit  a low recall), a substantial number of frame ele- ments could be recognized with high precision  from a very small number of constituent match- ing functions.	FN	false new$false negatives$False Negative$FrameNet$following noun$false negative$For PER$	2
Baseline Accuracy/Kappa 0.623/0.297 0.647/0.173 FN Rate 0.095 0.485 2.	FN	false new$false negatives$False Negative$FrameNet$following noun$false negative$For PER$	2
The FN error  was high for only one user, while the majority  of - the users exhibited no FN  I  I  I  I  I  I  I  I  I  I  I  I  I  I  I  I  I  I  I  Table 2: Relevance Classes by User  User  5 0  4 0  I  2  3  4  5  1  7  8  9  10  11  12  13  14  True False True False  Positive Positive Negative Negative  5 0 0 0  4  7  1  5 0  4 0  0  4  5  0  2  0  0  0  0  0  0 0  0 1  0	FN	false new$false negatives$False Negative$FrameNet$following noun$false negative$For PER$	2
The Berkeley FN project.	FN	false new$false negatives$False Negative$FrameNet$following noun$false negative$For PER$	3
Our approach demonstrates the  use of two human built knowledge bases  (WordNet and FN) for the task of  semantic extraction.	FN	false new$false negatives$False Negative$FrameNet$following noun$false negative$For PER$	3
For projects such as FN (Baker et al, 1998), 1Note that we do not state that adjectives denote objects or events, but that they imply an object or event in their denota- tion.	FN	false new$false negatives$False Negative$FrameNet$following noun$false negative$For PER$	3
Their work  looked at the problem of assigning semantic roles to  text based on a statistical model of the FN1  data.	FN	false new$false negatives$False Negative$FrameNet$following noun$false negative$For PER$	3
In this paper we report results obtained from  combining IE and graphical modeling techniques,  with semantic resources (WordNet and FN)  for automatic Semantic Extraction.	FN	false new$false negatives$False Negative$FrameNet$following noun$false negative$For PER$	3
The ongoing development of public  knowledge bases such as WordNet, FN, CYC,  etc.	FN	false new$false negatives$False Negative$FrameNet$following noun$false negative$For PER$	3
(37) If the CWS is an NP and the TOS is an S, then  construct the FN phrase and push it  to the STACK:  (NP (HEAD CWS)  (MOD (rep_emn TOS CWS)))  To illustrate how (37) works, we will trace the noun  phrase (38), which is included in all sentences cited in  (4b) through (4e).	FN	false new$false negatives$False Negative$FrameNet$following noun$false negative$For PER$	4
In this way the parser will be forced to attach of-PPs FNs as their complements.	FN	false new$false negatives$False Negative$FrameNet$following noun$false negative$For PER$	4
Lexical Token n-gram Token n-grams in a 2 word window around the preposition POS n-gram POS n-grams in a 2 word window around the preposition HEAD PREC VP The head verb in the preceding verb phrase HEAD PREC NP The head noun in the preceding noun phrase HEAD FOLLOW NP The head noun in the FN phrase Parsing HEAD Head of the preposition HEAD POS POS of the head COMP Complement of the preposition COMPLEMENT POS POS of the complement HEAD RELATION Prep-Head relation name COMPLEMENT RELATION Prep-Comp relation name Phrase Structure PARENT TAG TAG of the preposition?s parent GRANDPARENT TAG TAG of the preposition?s grandparent PARENT LEFT Left context of the preposition paren	FN	false new$false negatives$False Negative$FrameNet$following noun$false negative$For PER$	4
Premodifier relations specify the proper adjective or proper noun premodifier and the FN it modifies, e.g.: [the [Seattle] zoo] Possessive indicates that the first mention is in a possessive case, e.g.: [[California] ?	FN	false new$false negatives$False Negative$FrameNet$following noun$false negative$For PER$	4
2 (7) If the CWS is an NP and the TOS is an S, then  construct the FN phrase and push it to  the STACK:  (NP (HEAD CWS)  (MOD (rep_emn TOS CWS)))  - CWS is the word or phrase on which the PROCES-  SOR is currently working.	FN	false new$false negatives$False Negative$FrameNet$following noun$false negative$For PER$	4
For instance, note the structure of the FN phrase, which is shown in the middle tree in Figure 2: ?-?	FN	false new$false negatives$False Negative$FrameNet$following noun$false negative$For PER$	4
but guessed the wrong type, we call this a cross- labeling; (2) a false positive occurs when the learner guessed some relation while there should have been none; (3) the reverse is a FN.	FN	false new$false negatives$False Negative$FrameNet$following noun$false negative$For PER$	5
The high-recall loss function penalizes false positives with 0.1 and FNs with 5.	FN	false new$false negatives$False Negative$FrameNet$following noun$false negative$For PER$	5
itive occurs when the learner guessed some relation while there should have been none; (3) the reverse is a FN.	FN	false new$false negatives$False Negative$FrameNet$following noun$false negative$For PER$	5
The 0-1 loss also has a loss of 1 for false positives and FNs.	FN	false new$false negatives$False Negative$FrameNet$following noun$false negative$For PER$	5
However, a concern was that the transformed query would cause numerous FNs.	FN	false new$false negatives$False Negative$FrameNet$following noun$false negative$For PER$	5
In order to boost recall, we defined the loss function as the num- ber of FN trigger chunks.	FN	false new$false negatives$False Negative$FrameNet$following noun$false negative$For PER$	5
sed the wrong type, we call this a cross- labeling; (2) a false positive occurs when the learner guessed some relation while there should have been none; (3) the reverse is a FN.	FN	false new$false negatives$False Negative$FrameNet$following noun$false negative$For PER$	5
The high-precision loss function penalizes FNs with 0.1 and false positives with 5.	FN	false new$false negatives$False Negative$FrameNet$following noun$false negative$For PER$	5
FN-based confidence estimation, the direct phrase-based confidence measure and the count-based confidence measure calculated over N-best lists show the best performance.	FN	false new$false negatives$False Negative$FrameNet$following noun$false negative$For PER$	6
FN_OUT,   unigram EOD kernel was used.	FN	false new$false negatives$False Negative$FrameNet$following noun$false negative$For PER$	6
FNSONAGE and GRAG2, personality traits are mainly used to adapt linguistic styles.	FN	false new$false negatives$False Negative$FrameNet$following noun$false negative$For PER$	6
FNSON objects, this  challenge is small, since the only additional bit of  information required is the person's title ("Mr.,"  "Ms.," "Dr.," etc.),	FN	false new$false negatives$False Negative$FrameNet$following noun$false negative$For PER$	6
FN and ORG queries, we select named entities in DQ that contain NQ as a substring.	FN	false new$false negatives$False Negative$FrameNet$following noun$false negative$For PER$	6
FN, the classifier based on word counts dominates all other confidence measures.	FN	false new$false negatives$False Negative$FrameNet$following noun$false negative$For PER$	6
HMMs proposed by Miller et al (1999), and have shown to outperform tf, idf in TREC information retrieval tasks.	HMM	Hidden Markov Model$Hidden Maxkov Mode$hidden Markov model$Hidden Markov model$	0
We use HMM for this stochastic process, where the classes are assumed to be hid- den states.	HMM	Hidden Markov Model$Hidden Maxkov Mode$hidden Markov model$Hidden Markov model$	0
Fujie et al (2004) used HMMs to perform head nod recognition.	HMM	Hidden Markov Model$Hidden Maxkov Mode$hidden Markov model$Hidden Markov model$	0
A Second- Order HMM for Part-of-Speech Tagging.	HMM	Hidden Markov Model$Hidden Maxkov Mode$hidden Markov model$Hidden Markov model$	0
689  Combining Optimal Clustering and HMMs for Extractive Summarization Pascale Fung  Human Language Technology Center, Dept.	HMM	Hidden Markov Model$Hidden Maxkov Mode$hidden Markov model$Hidden Markov model$	0
Applying Many-to-Many Alignments and HMMs to Letter-to-Phoneme Conversion.	HMM	Hidden Markov Model$Hidden Maxkov Mode$hidden Markov model$Hidden Markov model$	0
Robust Part-of-Speech Tagging using a HMMl.	HMM	Hidden Markov Model$Hidden Maxkov Mode$hidden Markov model$Hidden Markov model$	1
Discriminative training meth- ods for HMMs: Theory and exper- iments with perceptron algorithms.	HMM	Hidden Markov Model$Hidden Maxkov Mode$hidden Markov model$Hidden Markov model$	2
The EM-like method for learning dependency  relations described in Section 3.3 has also been  applied to other tasks such as HMM  training (Rabiner, 1989), syntactic relation learning  (Yuret, 1998), and Chinese word segmentation  (Gao et al, 2002a).	HMM	Hidden Markov Model$Hidden Maxkov Mode$hidden Markov model$Hidden Markov model$	2
Type- supervised HMMs for part-of- speech tagging with incomplete tag dictionaries.	HMM	Hidden Markov Model$Hidden Maxkov Mode$hidden Markov model$Hidden Markov model$	2
We focus on the simple and tractable HMM, and present an efficient learning algorithm for incorporating approximate bijectivity and symmetry constraints.	HMM	Hidden Markov Model$Hidden Maxkov Mode$hidden Markov model$Hidden Markov model$	2
A spectral algorithm for learning HMMs.	HMM	Hidden Markov Model$Hidden Maxkov Mode$hidden Markov model$Hidden Markov model$	2
The speech recognition hypotheses are obtained by using the HMM Toolkit (HTK) speech rec- ognizer adapted to our application domain in which the word error rate (WER) is 21.03%.	HMM	Hidden Markov Model$Hidden Maxkov Mode$hidden Markov model$Hidden Markov model$	3
of Electrical & Electronic Engineering, University of Science & Technology (HKUST) Clear Water Bay, Hong Kong eepercy@ee.ust.hk Abstract We propose HMMs with unsupervised training for extractive sum- marization.	HMM	Hidden Markov Model$Hidden Maxkov Mode$hidden Markov model$Hidden Markov model$	3
2009) presented one of the initial efforts at spectral-based parameter estimation (us- ing SVD) of observed moments for latent-variable models, in the case of HMMs.	HMM	Hidden Markov Model$Hidden Maxkov Mode$hidden Markov model$Hidden Markov model$	3
3.1 Probabilistic Disambiguation  The HMM is the most widely  used method for statistical part of speech tag- ging.	HMM	Hidden Markov Model$Hidden Maxkov Mode$hidden Markov model$Hidden Markov model$	3
4); Gricean Maxims (Sripada et al, 2003); Integer Linear Programming (Lampouras and An- droutsopoulos, 2013); collective content selection (Barzilay and Lapata, 2004); interest scores as- signed to content (Androutsopoulos et al, 2013); a combination of statistical and template-based ap- proaches to NLG (Kondadadi et al, 2013); statis- tical acquisition of rules (Duboue and McKeown, 2003) and the HMM approach for Content Selection and ordering (Barzilay and Lee, 2004).	HMM	Hidden Markov Model$Hidden Maxkov Mode$hidden Markov model$Hidden Markov model$	3
Inspired on HMMs (Baum  and Petrie, 1966) and following the idea that  words combinations are finite in an evaluation  text, we decided to create a finite automata in  graph form to represent all these relations  extracted from a training corpus.	HMM	Hidden Markov Model$Hidden Maxkov Mode$hidden Markov model$Hidden Markov model$	3
Hidden HMMs proposed by Miller et al (1999), and have shown to outperform tf, idf in TREC information retrieval tasks.	HMMs	Markov Models$hidden Markov models$Hidden Markov models$	0
Fujie et al (2004) used Hidden HMMs to perform head nod recognition.	HMMs	Markov Models$hidden Markov models$Hidden Markov models$	0
Com- puting word similarity and identifying cognates with Pair Hidden HMMs.	HMMs	Markov Models$hidden Markov models$Hidden Markov models$	0
We are currently working on NLTK modules for Hidden HMMs, language modeling, and tree adjoining grammars.	HMMs	Markov Models$hidden Markov models$Hidden Markov models$	0
689  Combining Optimal Clustering and Hidden HMMs for Extractive Summarization Pascale Fung  Human Language Technology Center, Dept.	HMMs	Markov Models$hidden Markov models$Hidden Markov models$	0
Applying Many-to-Many Alignments and Hidden HMMs to Letter-to-Phoneme Conversion.	HMMs	Markov Models$hidden Markov models$Hidden Markov models$	0
For CRFs with general graphical structure, calcu- lation of Ep(s|o)[ fk] is intractable, but for the linear chain case Lafferty et al (2001) describe an efficient dynamic programming procedure for inference, sim- ilar in nature to the forward-backward algorithm in HMMs.	HMMs	Markov Models$hidden Markov models$Hidden Markov models$	1
Applying many-to-many alignments and HMMs to letter-to-phoneme con- version.	HMMs	Markov Models$hidden Markov models$Hidden Markov models$	1
These alignment models are similar to the con- cept of HMMs (HMM) in speech recognition.	HMMs	Markov Models$hidden Markov models$Hidden Markov models$	2
HMMs (HMM) have  to date been accepted as an effective classification method  for large vocabulary continuous peech recognition, e.g., the  ARPA-sponsored SPHINX and DECIPHER.	HMMs	Markov Models$hidden Markov models$Hidden Markov models$	2
These alignment models are sire-  ilar to the concept of HMMs  (HMM) in speech recognition.	HMMs	Markov Models$hidden Markov models$Hidden Markov models$	2
HDPes.?	HDP	Hierarchical Dirichlet Process$cal Dirichlet Process$Hierarchical Dirichlet process$	0
Information Retrieval Using HDPes.	HDP	Hierarchical Dirichlet Process$cal Dirichlet Process$Hierarchical Dirichlet process$	0
Their systems use a HDP (Teh et al 2006) to automatically infer the number of senses from contextual and positional features.	HDP	Hierarchical Dirichlet Process$cal Dirichlet Process$Hierarchical Dirichlet process$	0
Evolutionary HDP for Timeline Summarization.	HDP	Hierarchical Dirichlet Process$cal Dirichlet Process$Hierarchical Dirichlet process$	0
Notwithstanding that, ideally we would like to avoid having to pre-specify the number of classes for the word class induction module: we thus plan to investigate non-parametric models such as HDP for this purpose.	HDP	Hierarchical Dirichlet Process$cal Dirichlet Process$Hierarchical Dirichlet process$	0
These include the Latent Dirichlet Al- location (LDA) model of Blei et al (2003) and the HDP model of Teh et al (2006).	HDP	Hierarchical Dirichlet Process$cal Dirichlet Process$Hierarchical Dirichlet process$	0
HierarchiHDPes.?	HDP	Hierarchical Dirichlet Process$cal Dirichlet Process$Hierarchical Dirichlet process$	1
Information Retrieval Using HierarchiHDPes.	HDP	Hierarchical Dirichlet Process$cal Dirichlet Process$Hierarchical Dirichlet process$	1
Their systems use a HierarchiHDP (Teh et al 2006) to automatically infer the number of senses from contextual and positional features.	HDP	Hierarchical Dirichlet Process$cal Dirichlet Process$Hierarchical Dirichlet process$	1
A Hierar- chiHDP Model for Joint Part-of- Speech and Morphology Induction.	HDP	Hierarchical Dirichlet Process$cal Dirichlet Process$Hierarchical Dirichlet process$	1
These include the Latent Dirichlet Al- location (LDA) model of Blei et al (2003) and the HierarchiHDP model of Teh et al (2006).	HDP	Hierarchical Dirichlet Process$cal Dirichlet Process$Hierarchical Dirichlet process$	1
= { 1 2P E 0 (e) if |f | = 0 1 2P F 0 (f) if |e| = 0 The terminal translation phrase pair distribution is a hierarchiHDP in which each phrase are independently distributed according to DPs:4 PP1 (z ? ?	HDP	Hierarchical Dirichlet Process$cal Dirichlet Process$Hierarchical Dirichlet process$	1
HDPes.	HDP	Hierarchical Dirichlet Process$cal Dirichlet Process$Hierarchical Dirichlet process$	2
2 A corpus of action images and captions Image collection and sentence annotation We have constructed a corpus consisting of 8108 pho- tographs from Flickr.com, each paired with five one-sentence descriptive captions written by Ama- zon?s MTurk workers.	MTurk	Mechanical Turk1$Mechanical Turk$Mechanical Turk2$	0
Other such projects include Ama- zon.com?s MTurk4, LiTgloss15, The ESP Game16, and the Wiktionary17.	MTurk	Mechanical Turk1$Mechanical Turk$Mechanical Turk2$	0
We presented 100 ran- dom samples for each of the 3 distances as well as 100 unperturbed groups (original) to annotators at Amazon MTurk, asking which word fits the group the least.	MTurk	Mechanical Turk1$Mechanical Turk$Mechanical Turk2$	0
3 Annotation of Social Variables In order to study how entrainment in various dimen- sions correlated with perceived social behaviors of our subjects, we asked Amazon MTurk annotators to label the 168 Objects games in our cor- pus for an array of social behaviors perceived for each of the speakers, which we term here ?	MTurk	Mechanical Turk1$Mechanical Turk$Mechanical Turk2$	0
2 Mechanical Turk Amazon?s MTurk is an online market- place for work.	MTurk	Mechanical Turk1$Mechanical Turk$Mechanical Turk2$	0
The approach can be particularly useful for the modern crowd- sourcing approaches, such as those em- ploying the Amazon?s MTurk or CrowdFlower2.	MTurk	Mechanical Turk1$Mechanical Turk$Mechanical Turk2$	0
This evaluation is performed us- ing a set of human-corrected sentences gathered via Amazon MTurk, an online service where workers are paid to perform a short task, and further filtered for correctness by an undergraduate research assistant.	MTurk	Mechanical Turk1$Mechanical Turk$Mechanical Turk2$	1
We recreate the NIST 2009 Urdu-to- English evaluation set with MTurk, and quantitatively show that our models are able to select translations within the range of quality that we expect from professional trans- lators.	MTurk	Mechanical Turk1$Mechanical Turk$Mechanical Turk2$	1
Noting that ESL er- rors tend to occur in groups within sentences and Figure 3: Human judgments of corrected sentences gath- ered using MTurk.	MTurk	Mechanical Turk1$Mechanical Turk$Mechanical Turk2$	1
We then utilized the Amazon MTurk Service1.	MTurk	Mechanical Turk1$Mechanical Turk$Mechanical Turk2$	1
In (Gao et al, 2010) we experimented with using a dictionary to generate such constraints, and in (Gao and Vogel, 2010) we experimented with manual word align- ments from MTurk.	MTurk	Mechanical Turk1$Mechanical Turk$Mechanical Turk2$	1
This issue is exacerbated by the fact that MTurk work- ers were instructed to change each ESL sentence as little as possible, which helps their consistency but hurts these particular models?	MTurk	Mechanical Turk1$Mechanical Turk$Mechanical Turk2$	1
To look at more perceptual measures of dialogue quality, we used Amazon MTurk to an- notate each task (the sub-units of each game) in the Games Corpus for what we term social variables, the perceived social characteristics of an interaction and ints participants.	MTurk	Mechanical Turk1$Mechanical Turk$Mechanical Turk2$	2
Crowd-sourcing seemed particularly  appropriate, given the nature of the task, so we  opted to use Amazon MTurk (AMT).	MTurk	Mechanical Turk1$Mechanical Turk$Mechanical Turk2$	2
The NAACL 2010 Workshop on Creating Speech and Language Data with Amazon?s MTurk presents an interesting opportunity to ex-tend this collaboration in a novel data collection task.	MTurk	Mechanical Turk1$Mechanical Turk$Mechanical Turk2$	2
In this paper, we used mi- cro-task crowd-sourcing, i.e. a central platform  like for example Amazon MTurk  or  CrowdFlower3 assigns small tasks (called HITs,  human-intelligence tasks) to workers for monetary  compensation.	MTurk	Mechanical Turk1$Mechanical Turk$Mechanical Turk2$	2
Deceptive reviews are gathered using Amazon MTurk.	MTurk	Mechanical Turk1$Mechanical Turk$Mechanical Turk2$	2
Another, less time-consuming approach of crowdsourcing is us- ing platforms such as Amazon MTurk.	MTurk	Mechanical Turk1$Mechanical Turk$Mechanical Turk2$	2
The ATIS tests consist of tests  of (1) ATIS-domain spontaneous speech (lexicons typically  less than 2,000 words), (2) natural language understanding,  and (3) SLU.	SLU	spoken language understanding$Spoken Language Understanding$	0
D. A. Dahl, L. Hirschman, L. M. Norton, M. C.  Linebarger, D. Magerman, and C. N. Ball, "Training and  evaluation of a SLU system,"  in Proceedings of the DARPA Speech and Language Work-  shop, (Hidden Valley, PA), June 1990.	SLU	spoken language understanding$Spoken Language Understanding$	0
Comparing stochastic approaches to SLU in multiple lan- guages.	SLU	spoken language understanding$Spoken Language Understanding$	0
However,  in the field of SLU which is always dealing with noise, no complete  comparison between different active learning methods has been done.	SLU	spoken language understanding$Spoken Language Understanding$	0
Opt ica l  Character  Recogn i t ion   Although the nbest architecture was developed in the  context, of SLU, it is in fact  applicable to any kind of input where indeterminacies  in the input result in misrecognitions.	SLU	spoken language understanding$Spoken Language Understanding$	0
This paper compares the  best known active learning methods in noisy conditions for SLU.	SLU	spoken language understanding$Spoken Language Understanding$	0
Multi-Site Data Collection and Evalu-  ation in SLU".	SLU	spoken language understanding$Spoken Language Understanding$	1
Hirschman, L., et al, "Multi-Site Data Collection and  Evaluation in SLU", in Pro-  ceedings of the Human Language Technology Workshop,  March 1993 (M. Bates, ed.)	SLU	spoken language understanding$Spoken Language Understanding$	1
SLU: Systems for Extracting Semantic  Information from Speech, First Edition, John Wiley & Sons.	SLU	spoken language understanding$Spoken Language Understanding$	1
Active learning for rule-based and corpus-based  SLU models, In Proceedings of IEEE Conference on Automatic Speech Recognition  and Understanding, pp.	SLU	spoken language understanding$Spoken Language Understanding$	1
Improving SLU with information retrieval  and active learning methods, In Proceedings of IEEE International Conference on Acoustics, Speech, and Signal  Processing, pp.	SLU	spoken language understanding$Spoken Language Understanding$	1
Active Learning for SLU,  In Proceedings of IEEE International Conference on Acoustics, Speech, and Signal Processing, vol.	SLU	spoken language understanding$Spoken Language Understanding$	1
Each decision point ID for easy reference.	ID	is numbered$information density$identification$index$immediate dominance$immediate  dominance$id_schema$Immediate Dom-  inance$initial descr$Infectious Diseases$	0
The syntactic structure of the sentence can be depicted as a  tree where each node ID :  sentence, i  verb.2 sdir.3 sent ~f.4  transfer NH  Det.5 Head.6 nh_pf .	ID	is numbered$information density$identification$index$immediate dominance$immediate  dominance$id_schema$Immediate Dom-  inance$initial descr$Infectious Diseases$	0
The position that extends the kth  subsequence to the left of the head outwards from  the head ID -2k  + 1, while the position  that extends this same subsequence inwards to-  wards the head is labeled -2k .	ID	is numbered$information density$identification$index$immediate dominance$immediate  dominance$id_schema$Immediate Dom-  inance$initial descr$Infectious Diseases$	0
Finally, each PERS labeled entity ID by order of ap- parition and is associated with the sentences refer- ence number where it appears (consecutive PERS labeled words, not separated by punctuation mark, receive the same index number).	ID	is numbered$information density$identification$index$immediate dominance$immediate  dominance$id_schema$Immediate Dom-  inance$initial descr$Infectious Diseases$	0
Each sentence in the TEXT  region ID with paragraph and sentence  numbers, so, for instance, 2-1 is the first sentence  in the second paragraph.	ID	is numbered$information density$identification$index$immediate dominance$immediate  dominance$id_schema$Immediate Dom-  inance$initial descr$Infectious Diseases$	0
Each word sense whether irregular or regular  ID separate ly  with Arabic numerals.	ID	is numbered$information density$identification$index$immediate dominance$immediate  dominance$id_schema$Immediate Dom-  inance$initial descr$Infectious Diseases$	0
plementizer choice can improve upon the prediction accuracy of a state-of- the-art realization ranking model, arguably in ways that make a substantial difference to fluency and in- telligiblity.2 In particular, we report results on a bi- nary classification task for predicting the presence or absence of a that-complementizer using features adapted from Jaeger?s (2010) investigation of the uniform ID principle in the con- text of that-mentioning.	ID	is numbered$information density$identification$index$immediate dominance$immediate  dominance$id_schema$Immediate Dom-  inance$initial descr$Infectious Diseases$	1
Furthermore, Strict Local Density (SLD) method is proposed  based on a new concept of local density and a new technique of utilizing ID  measures.	ID	is numbered$information density$identification$index$immediate dominance$immediate  dominance$id_schema$Immediate Dom-  inance$initial descr$Infectious Diseases$	1
Effect of locality degree (in computation of ID) on performance of active  learning methods.	ID	is numbered$information density$identification$index$immediate dominance$immediate  dominance$id_schema$Immediate Dom-  inance$initial descr$Infectious Diseases$	1
In Jaeger?s study, uniform ID emerges as an impor- tant predictor of speakers?	ID	is numbered$information density$identification$index$immediate dominance$immediate  dominance$id_schema$Immediate Dom-  inance$initial descr$Infectious Diseases$	1
for predicting the presence or absence of a that-complementizer using features adapted from Jaeger?s (2010) investigation of the uniform ID principle in the con- text of that-mentioning.	ID	is numbered$information density$identification$index$immediate dominance$immediate  dominance$id_schema$Immediate Dom-  inance$initial descr$Infectious Diseases$	1
We report results on a binary classification task for predicting the presence/absence of a that-complementizer using features adapted from Jaeger?s (2010) investigation of the uniform ID principle in the context of that-mentioning.	ID	is numbered$information density$identification$index$immediate dominance$immediate  dominance$id_schema$Immediate Dom-  inance$initial descr$Infectious Diseases$	1
To address this problem and yet avoid outliers we choose to compute ID for each  instance locally, i.e. using k nearest instances and not all instances.	ID	is numbered$information density$identification$index$immediate dominance$immediate  dominance$id_schema$Immediate Dom-  inance$initial descr$Infectious Diseases$	1
In this work, we address both sub- problems, namely, anaphoric speaker and implicit speaker ID.	ID	is numbered$information density$identification$index$immediate dominance$immediate  dominance$id_schema$Immediate Dom-  inance$initial descr$Infectious Diseases$	2
2003), the quote-ID module detects whether a piece of quoted speech is a new quote (NEW), spoken by a speaker dif- ferent from the previous speaker, or a continuation quote (CONT) spoken by the same speaker as that of the previous quote.	ID	is numbered$information density$identification$index$immediate dominance$immediate  dominance$id_schema$Immediate Dom-  inance$initial descr$Infectious Diseases$	2
This system performs the following story analysis tasks: ID of charac- ters in each story; attribution of quotes to specific story characters; ID of character age, gender and other salient personality attributes; and finally, affective analysis of the quoted material.	ID	is numbered$information density$identification$index$immediate dominance$immediate  dominance$id_schema$Immediate Dom-  inance$initial descr$Infectious Diseases$	2
A long list of heuristics for character ID is proposed in (Mamede and Chaleira, 2004).	ID	is numbered$information density$identification$index$immediate dominance$immediate  dominance$id_schema$Immediate Dom-  inance$initial descr$Infectious Diseases$	2
Two broad approaches for the iden- tification of story characters were followed: (i) named entity recognition, and (ii) ID of character nominals, e.g., ?	ID	is numbered$information density$identification$index$immediate dominance$immediate  dominance$id_schema$Immediate Dom-  inance$initial descr$Infectious Diseases$	2
so that they can check for case and number/gender compatibility and connect the semantic ID of the argument they modify to a role in their own semantic contribution (in this case, the ARG1 of the ?	ID	is numbered$information density$identification$index$immediate dominance$immediate  dominance$id_schema$Immediate Dom-  inance$initial descr$Infectious Diseases$	3
The underlying information is provided  with a gnoseological map of known interpretations,  as well as with an IDing utility in which the  learner or researcher advances new hypotheses or new interpretations a these arise through the  continuous progress of knowledge acquisition and  discovery.	ID	is numbered$information density$identification$index$immediate dominance$immediate  dominance$id_schema$Immediate Dom-  inance$initial descr$Infectious Diseases$	3
The underlying information is provided  with a gnoseological map of known interpretations,  as well as with an IDing utility in which the  learner or researcher advances new hypothe	ID	is numbered$information density$identification$index$immediate dominance$immediate  dominance$id_schema$Immediate Dom-  inance$initial descr$Infectious Diseases$	3
If we treat a as an ID into the vector 1~,  then (a, I.V') is simply the ath candidate in  the list ffz.	ID	is numbered$information density$identification$index$immediate dominance$immediate  dominance$id_schema$Immediate Dom-  inance$initial descr$Infectious Diseases$	3
to accomplish this goal, we suggest a  multimedia database with powerful IDing and  classification functions. (	ID	is numbered$information density$identification$index$immediate dominance$immediate  dominance$id_schema$Immediate Dom-  inance$initial descr$Infectious Diseases$	3
In order to accomplish this goal, we suggest a  multimedia database with powerful IDing and  classification functions. (	ID	is numbered$information density$identification$index$immediate dominance$immediate  dominance$id_schema$Immediate Dom-  inance$initial descr$Infectious Diseases$	3
The grammar is able to connect x7 (the ID of ?	ID	is numbered$information density$identification$index$immediate dominance$immediate  dominance$id_schema$Immediate Dom-  inance$initial descr$Infectious Diseases$	3
When viewed in this way, a can be regarded as  an ID into these vectors that specifies which  value is relevant o the particular choice of an-  tecedent.	ID	is numbered$information density$identification$index$immediate dominance$immediate  dominance$id_schema$Immediate Dom-  inance$initial descr$Infectious Diseases$	3
are constants for the  binary relations ID (par-  ent relation), dominance (reflexive transitive  closure of ID) and linear  precedence.	ID	is numbered$information density$identification$index$immediate dominance$immediate  dominance$id_schema$Immediate Dom-  inance$initial descr$Infectious Diseases$	4
The export format explicitly  encodes tokens, categories and edge labels,  linear precedence between leaves and the par-  ent (ID) relation.	ID	is numbered$information density$identification$index$immediate dominance$immediate  dominance$id_schema$Immediate Dom-  inance$initial descr$Infectious Diseases$	4
We do not restrict the dependency only to the  ID, but the words on a deeper  level share less of the value E(wz).	ID	is numbered$information density$identification$index$immediate dominance$immediate  dominance$id_schema$Immediate Dom-  inance$initial descr$Infectious Diseases$	4
pl signifies ID of  the first node over the second, p2 immedi-  ate dominance of the second over the first,  d l  dominance of the first over the second,  etc.	ID	is numbered$information density$identification$index$immediate dominance$immediate  dominance$id_schema$Immediate Dom-  inance$initial descr$Infectious Diseases$	4
are the  192  SIMPX  I I  D D  VF MF  I I  1 I  NX NX  I I  I I  PWS NE  wen Maria  I  E3  SIMPX \]  I I  D \[3  I I  LK MF LK  I I I  I I I  VXFIN NX VXFIN  I I I  I I I  WFIN PPER VVFIN  glaubst du liebt  Figure 2: Annotation of (3) in Verbmobil for-  mat  binary relations ID (par-  ent), dominance and linear precedence, # is  a function assigning syntactic categories or  part-of-speech tags to nodes, r/ is a function  mapping edges to grammatical functions, and  a assigns tokens to the leaves (i.e. the nodes  that do not dominate any other node).	ID	is numbered$information density$identification$index$immediate dominance$immediate  dominance$id_schema$Immediate Dom-  inance$initial descr$Infectious Diseases$	4
A tree is often def ined as a set of  elements, cal led "nodes", on which two  relat ions are defined, ID  (D) and l inear precedence (<), which are  required to have certa in propert ies to  the effect that a tree has exact ly one  root node, which dominates every other  node ( immediate ly  or indirect ly);  that  every node in a tree has exact ly  one  "mother" node, etc. (	ID	is numbered$information density$identification$index$immediate dominance$immediate  dominance$id_schema$Immediate Dom-  inance$initial descr$Infectious Diseases$	5
IDiption isgenerated, a mechanism allows the  user to request he information that was omitted for  brevity sake (see Paris 1987).	ID	is numbered$information density$identification$index$immediate dominance$immediate  dominance$id_schema$Immediate Dom-  inance$initial descr$Infectious Diseases$	8
A false start occurs when the  speaker goofs on his IDiption, stops, and then  restarts the description (also see Polanyi (1978) on false  starts).	ID	is numbered$information density$identification$index$immediate dominance$immediate  dominance$id_schema$Immediate Dom-  inance$initial descr$Infectious Diseases$	8
A respecification of the IDiption in more  detail.	ID	is numbered$information density$identification$index$immediate dominance$immediate  dominance$id_schema$Immediate Dom-  inance$initial descr$Infectious Diseases$	8
For example, in the case of the descriptions the  thing that is flared at the top and the main tube which is  the biggest ube, the relative clauses are needed because  the IDiptions are too general to distinguish  any one object.	ID	is numbered$information density$identification$index$immediate dominance$immediate  dominance$id_schema$Immediate Dom-  inance$initial descr$Infectious Diseases$	8
then the later  references will be some subset of IDiption  (like ?	ID	is numbered$information density$identification$index$immediate dominance$immediate  dominance$id_schema$Immediate Dom-  inance$initial descr$Infectious Diseases$	8
which is de- fined as the prefix of the IDiption.	ID	is numbered$information density$identification$index$immediate dominance$immediate  dominance$id_schema$Immediate Dom-  inance$initial descr$Infectious Diseases$	8
SPIN builds a task representation  from a top level goal and an IDiption of the  world using a hierarchical non-linear planning technique.	ID	is numbered$information density$identification$index$immediate dominance$immediate  dominance$id_schema$Immediate Dom-  inance$initial descr$Infectious Diseases$	8
Using Internet Searches for Influenza Sur-veillance, Clinical ID Vol.	ID	is numbered$information density$identification$index$immediate dominance$immediate  dominance$id_schema$Immediate Dom-  inance$initial descr$Infectious Diseases$	9
4.2 ID The ID track differs from the Genia track in two important ways.	ID	is numbered$information density$identification$index$immediate dominance$immediate  dominance$id_schema$Immediate Dom-  inance$initial descr$Infectious Diseases$	9
This project has been funded in whole or in part with Fed- eral funds from the National Institute of Allergy and ID, National Institutes of Health, Department of Health and Human Services, under Contract No.	ID	is numbered$information density$identification$index$immediate dominance$immediate  dominance$id_schema$Immediate Dom-  inance$initial descr$Infectious Diseases$	9
We use custom versions of these (except for ID where we use those from Stenetorp et al (2011)).	ID	is numbered$information density$identification$index$immediate dominance$immediate  dominance$id_schema$Immediate Dom-  inance$initial descr$Infectious Diseases$	9
Our system performed competitively, obtain- ing 3rd place in the ID track (50.6% f-score), 5th place in Epigenetics and Post-translational Modifications (31.2%), and 7th place in Genia (50.0%).	ID	is numbered$information density$identification$index$immediate dominance$immediate  dominance$id_schema$Immediate Dom-  inance$initial descr$Infectious Diseases$	9
Recently there has been a surge of interest in extracting PAes from online data due to the rapid growth of E-Commerce.	PA	Press Association$product attribut$Phrase Analysi$Philadelphia$Ph i lade lph ia$Percent Agreemen$	1
These sentiment factors may also be factors based on PAes.	PA	Press Association$product attribut$Phrase Analysi$Philadelphia$Ph i lade lph ia$Percent Agreemen$	1
An unsupervised ap- proach to PAe extraction.	PA	Press Association$product attribut$Phrase Analysi$Philadelphia$Ph i lade lph ia$Percent Agreemen$	1
c?2011 Association for Computational Linguistics Bootstrapped Named Entity Recognition for Product Attribute Extraction Duangmanee (Pew) Putthividhya eBay Inc. 2065 Hamilton Ave San Jose, CA 95125 dputthividhya@ebay.com Junling Hu eBay Inc. 2065 Hamilton Ave San Jose, CA 95125 juhu@ebay.com Abstract We present a named entity recognition (NER) system for extracting PAes and values from listing titles.	PA	Press Association$product attribut$Phrase Analysi$Philadelphia$Ph i lade lph ia$Percent Agreemen$	1
Extracting PAes from such short titles faces the following challenges: ?	PA	Press Association$product attribut$Phrase Analysi$Philadelphia$Ph i lade lph ia$Percent Agreemen$	1
Then there came up with research works shifting focus from overall document sentiment to sentiment analysis based on PAes (Hu and Liu, 2004; Popescu and Etzioni, 2005; Ding and Liu, 2007; Liu et al, 2005).	PA	Press Association$product attribut$Phrase Analysi$Philadelphia$Ph i lade lph ia$Percent Agreemen$	1
1 Introduction At present a classical Chinese NLU architec- ture usually includes several components, such as Word Segmentation (Word-Seg), POS Tag- ging, PAs, Parsing, Word Sense Dis- ambiguation (WSD) and so on.	PA	Press Association$product attribut$Phrase Analysi$Philadelphia$Ph i lade lph ia$Percent Agreemen$	2
085  0.038 0.060  +13%  +31%  +26%  +33%  +58%  +32%  Average Search Precision for  Query Statement PAs  (CACM Collection, 25 Queries)  Table 5  The special processing described up to now  is user related in the sense that user query  formulations and user relevance assessments are  utilized to improve the retrieval procedures.	PA	Press Association$product attribut$Phrase Analysi$Philadelphia$Ph i lade lph ia$Percent Agreemen$	2
3.3 Sentence and PAs Tech- niques  Annotation of real text requires various tech- niques to be applied.	PA	Press Association$product attribut$Phrase Analysi$Philadelphia$Ph i lade lph ia$Percent Agreemen$	2
Word Formation Approach to Noun  PAs for Thai,  Proceedings of  SNLP2002.	PA	Press Association$product attribut$Phrase Analysi$Philadelphia$Ph i lade lph ia$Percent Agreemen$	2
Word Formation Approach and Noun PAs for Thai? ?,	PA	Press Association$product attribut$Phrase Analysi$Philadelphia$Ph i lade lph ia$Percent Agreemen$	2
In the future we are going to add the PAs, WSD (Word Sense Disambiguation) and Semantic Analysis components into CUP, because it is impossible to analyze some sen- tences correctly without semantic understand- ing and the PAs helps to en- hance the performance of Parsing.	PA	Press Association$product attribut$Phrase Analysi$Philadelphia$Ph i lade lph ia$Percent Agreemen$	2
In Proceedings of the 2005 International Conference on Acoustics, Speech, and Signal Process- ing (ICASSP 2005), PA, Pennsylvania.	PA	Press Association$product attribut$Phrase Analysi$Philadelphia$Ph i lade lph ia$Percent Agreemen$	3
In EMNLP-02, pages 46?51, PA, USA.	PA	Press Association$product attribut$Phrase Analysi$Philadelphia$Ph i lade lph ia$Percent Agreemen$	3
John Benjamins Publishing Company, Amsterdam/PA.	PA	Press Association$product attribut$Phrase Analysi$Philadelphia$Ph i lade lph ia$Percent Agreemen$	3
PA, US.	PA	Press Association$product attribut$Phrase Analysi$Philadelphia$Ph i lade lph ia$Percent Agreemen$	3
Semantic orientation applied to unsupervised clas- sification of reviews, Proceedings of ACL-02,  PA, Pennsylvania, 417-424  Wiebe, Janyce, Bruce M., Rebecca F., and Thomas P.  O'Hara.	PA	Press Association$product attribut$Phrase Analysi$Philadelphia$Ph i lade lph ia$Percent Agreemen$	3
1044  AN AUTOMATIC  METHOD OF F INDING TOP IC   BOUNDARIES   Jeffrey C. Reynar*  Depar tment  of Computer  and In format ion  Sc ience  Un ivers i ty  of  Pennsy lvan ia   PA ,  Pennsy lvan ia ,  USA  j c reynar@unag i .c i s .upenn.edu   Abstract  This article outlines a new method of locating discourse  boundaries based on lexical cohesion and a graphical  technique called dotplotting.	PA	Press Association$product attribut$Phrase Analysi$Philadelphia$Ph i lade lph ia$Percent Agreemen$	4
PA ,  1980.	PA	Press Association$product attribut$Phrase Analysi$Philadelphia$Ph i lade lph ia$Percent Agreemen$	4
1190  GRADED UNIF ICAT ION:  A FRAMEWORK FOR  INTERACTIVE  PROCESSING  Alber t  K im *  Depar tment  of  Computer  and  In fo rmat ion  Sciences  Un ivers i ty  of  Pennsy lvan ia   PA ,  Pennsy lvan ia ,  USA  email :  a lk im?unagi ,  cis.	PA	Press Association$product attribut$Phrase Analysi$Philadelphia$Ph i lade lph ia$Percent Agreemen$	4
I~oo~ of  the  18th  Meet in~ of  the  Assoc ia t ion  fo r  Computat iona l  L in~ulet??so   PA ,  1980  Kwasny 8o and Sondheimer N. "Ungrsmmat ioa l i ty  and Ex~rn-  ~ammat ica l i ty  in  Natura l  Lant~age Unders tand ing  Syst -   ems"?	PA	Press Association$product attribut$Phrase Analysi$Philadelphia$Ph i lade lph ia$Percent Agreemen$	4
93   A Simple Rule-Based Part of Speech Tagger  Er ic  Br i l l  *  Depar tment  of Computer  Sc ience  Un ivers i ty  of  Pennsy lvan ia   PA ,  Pennsy lvan ia  19104  U.S.A.  br i l l@unag i .c i s .upenn.edu   Abst ract   Automatic part of speech tagging is an area  of natural anguage processing where statistical  techniques have been more successful than rule-  based methods.	PA	Press Association$product attribut$Phrase Analysi$Philadelphia$Ph i lade lph ia$Percent Agreemen$	4
Inter -Coder  Agreement:  Inter-coder agree-  ment is a direct measure of consistency among  20  PAt  Speech-act 82.14  Dialog-act 65.48  Concept lists 88.00  Argument lists I 85.79  Table 2: Inter-coder Agreement between CMU  and IRST  C-STAR partners.	PA	Press Association$product attribut$Phrase Analysi$Philadelphia$Ph i lade lph ia$Percent Agreemen$	5
to judge single terms without any context: they had to think about all the senses of Metric PAt Kappa HN 0.909 0.465 N 0.796 0.368 P 0.714 0.281 HP 0.846 0 N+HN 0.829 0.396 P+HP 0.728 0.280 ALL 0.766 0.318 Table 6: Inter-annotator agreement on checking the trian- gulated list.	PA	Press Association$product attribut$Phrase Analysi$Philadelphia$Ph i lade lph ia$Percent Agreemen$	5
8 113 112 46 151 85 94 56  Al~reement 87 82 91 89 89 90 90 90 90 88 92 90 91 89 85 89 92 91 91 86  Boundary  21 16 7 10 6 5 11 5 8 22 13 17 9 11 8 7 15 11 10 6  Agreement  74 70 76 77 60 80 79 69 75 70 74 75 73 71 68 73 77 71 80 74  Non-Boundary   % Agreement   117 105 48 53 63 78 79 45 88 173 97 143 99 102 104 39 136 74 84 50  89 84 93 91 92 91 92 92 92 90 95 91 93 91 87 92 93 94 93 88  Table 1: PAt with the Majority Opinion  of boundaries assigned by subject i. In the case of  narrative 9 (j=96), one of the subjects assigned 8  boundaries.	PA	Press Association$product attribut$Phrase Analysi$Philadelphia$Ph i lade lph ia$Percent Agreemen$	5
Metric PAt Kappa HN 0.804 0.523 N 0.765 0.545 P 0.686 0.405 HP 0.855 0.669 N+HN 0.784	PA	Press Association$product attribut$Phrase Analysi$Philadelphia$Ph i lade lph ia$Percent Agreemen$	5
In ad- dition, annotators had to judge single terms without any context: they had to think about all the senses of Metric PAt Kappa HN 0.909 0.465 N 0.796 0.368 P 0.714 0.281 HP 0.846 0 N+HN 0.829 0.396 P+HP 0.728 0.280 ALL 0.766 0.318 Table 6: Inter-annotator agreement on checking the trian- gulated list.	PA	Press Association$product attribut$Phrase Analysi$Philadelphia$Ph i lade lph ia$Percent Agreemen$	5
Metric PAt Kappa HN 0.804 0.523 N 0.765 0.545 P 0.686 0.405 HP 0.855 0.669 N+HN 0.784 0.553 P+HP 0.783 0.559 ALL 0.826 0.614 Table 7: Inter-annotator agreement on checking the can- didates.	PA	Press Association$product attribut$Phrase Analysi$Philadelphia$Ph i lade lph ia$Percent Agreemen$	5
3.2.4 PAt.	PA	Press Association$product attribut$Phrase Analysi$Philadelphia$Ph i lade lph ia$Percent Agreemen$	5
c?2009 AssociACL A Markov Logic Approach to Bio-Molecular Event Extraction Sebastian Riedel ??	ACL	ation for Computational Linguistics$Association for Computational Linguistics$address changes to Betty Walker$Association for Computational Linguistic$-$Approaches to Semitic Languages$Association for Computational Lin-  guistics$	0
2007 AssociACL Computational Linguistics Volume 33, Number 1 Research in QA has been developed from two different scientific perspectives, artificial intelligence (AI) and information retrieval (IR).	ACL	ation for Computational Linguistics$Association for Computational Linguistics$address changes to Betty Walker$Association for Computational Linguistic$-$Approaches to Semitic Languages$Association for Computational Lin-  guistics$	0
c?2016 AssociACL Solving and Generating Chinese Character Riddles Chuanqi Tan? ?	ACL	ation for Computational Linguistics$Association for Computational Linguistics$address changes to Betty Walker$Association for Computational Linguistic$-$Approaches to Semitic Languages$Association for Computational Lin-  guistics$	0
Associ- ACL.	ACL	ation for Computational Linguistics$Association for Computational Linguistics$address changes to Betty Walker$Association for Computational Linguistic$-$Approaches to Semitic Languages$Association for Computational Lin-  guistics$	0
c?2010 AssociACL Features for Detecting Hedge Cues Nobuyuki Shimizu Information Technology Center The University of Tokyo shimizu@r.dl.itc.u-tokyo.ac.jp Hiroshi Nakagawa Information Technology Center The University of Tokyo n3@dl.itc.u-tokyo.ac.jp Abstract We present a sequential labeling approach to hedge cue detection submitted to the bi- ological portion of task 1 for the CoN	ACL	ation for Computational Linguistics$Association for Computational Linguistics$address changes to Betty Walker$Association for Computational Linguistic$-$Approaches to Semitic Languages$Association for Computational Lin-  guistics$	0
In Janyce Wiebe, editor, Proceedings of the 1st Meeting of the North American Chapter of the AssociACL, Seattle, WA.	ACL	ation for Computational Linguistics$Association for Computational Linguistics$address changes to Betty Walker$Association for Computational Linguistic$-$Approaches to Semitic Languages$Association for Computational Lin-  guistics$	0
2007 ACL Computational Linguistics Volume 33, Number 1 Research in QA has been developed from two different scientific perspectives, artificial intelligence (AI) and information retrieval (IR).	ACL	ation for Computational Linguistics$Association for Computational Linguistics$address changes to Betty Walker$Association for Computational Linguistic$-$Approaches to Semitic Languages$Association for Computational Lin-  guistics$	1
c?2009 ACL A Markov Logic Approach to Bio-Molecular Event Extraction Sebastian Riedel ??	ACL	ation for Computational Linguistics$Association for Computational Linguistics$address changes to Betty Walker$Association for Computational Linguistic$-$Approaches to Semitic Languages$Association for Computational Lin-  guistics$	1
c?2016 ACL A General Regularization Framework for Domain Adaptation Wei Lu1 and Hai Leong Chieu2 and Jonathan Lo?fgren3 1Singapore University of Technology and Design 2DSO National Laboratories 3Uppsala University luwei@sutd.edu.sg, chaileon@dso.org.sg, lofgren021@gmail.com Abstract We propose a domain adaptation framework, and formally prove that it generalizes the	ACL	ation for Computational Linguistics$Association for Computational Linguistics$address changes to Betty Walker$Association for Computational Linguistic$-$Approaches to Semitic Languages$Association for Computational Lin-  guistics$	1
c?2016 ACL Solving and Generating Chinese Character Riddles Chuanqi Tan? ?	ACL	ation for Computational Linguistics$Association for Computational Linguistics$address changes to Betty Walker$Association for Computational Linguistic$-$Approaches to Semitic Languages$Association for Computational Lin-  guistics$	1
c?2010 ACL Features for Detecting Hedge Cues Nobuyuki Shimizu Information Technology Center The University of Tokyo shimizu@r.dl.itc.u-tokyo.ac.jp Hiroshi Nakagawa Information Technology Center The University of Tokyo n3@dl.itc.u-tokyo.ac.jp Abstract We present a sequential labeling approach to hedge cue detection submitted to the bi- ological portion of task 1 for t	ACL	ation for Computational Linguistics$Association for Computational Linguistics$address changes to Betty Walker$Association for Computational Linguistic$-$Approaches to Semitic Languages$Association for Computational Lin-  guistics$	1
In Janyce Wiebe, editor, Proceedings of the 1st Meeting of the North American Chapter of the ACL, Seattle, WA.	ACL	ation for Computational Linguistics$Association for Computational Linguistics$address changes to Betty Walker$Association for Computational Linguistic$-$Approaches to Semitic Languages$Association for Computational Lin-  guistics$	1
2007 ACLs Computational Linguistics Volume 33, Number 1 Research in QA has been developed from two different scientific perspectives, artificial intelligence (AI) and information retrieval (IR).	ACL	ation for Computational Linguistics$Association for Computational Linguistics$address changes to Betty Walker$Association for Computational Linguistic$-$Approaches to Semitic Languages$Association for Computational Lin-  guistics$	3
c?2009 ACLs A Markov Logic Approach to Bio-Molecular Event Extraction Sebastian Riedel ??	ACL	ation for Computational Linguistics$Association for Computational Linguistics$address changes to Betty Walker$Association for Computational Linguistic$-$Approaches to Semitic Languages$Association for Computational Lin-  guistics$	3
c?2016 ACLs A General Regularization Framework for Domain Adaptation Wei Lu1 and Hai Leong Chieu2 and Jonathan Lo?fgren3 1Singapore University of Technology and Design 2DSO National Laboratories 3Uppsala University luwei@sutd.edu.sg, chaileon@dso.org.sg, lofgren021@gmail.com Abstract We propose a domain adaptation framework, and formally prove that it generalizes the	ACL	ation for Computational Linguistics$Association for Computational Linguistics$address changes to Betty Walker$Association for Computational Linguistic$-$Approaches to Semitic Languages$Association for Computational Lin-  guistics$	3
c?2016 ACLs Solving and Generating Chinese Character Riddles Chuanqi Tan? ?	ACL	ation for Computational Linguistics$Association for Computational Linguistics$address changes to Betty Walker$Association for Computational Linguistic$-$Approaches to Semitic Languages$Association for Computational Lin-  guistics$	3
c?2010 ACLs Features for Detecting Hedge Cues Nobuyuki Shimizu Information Technology Center The University of Tokyo shimizu@r.dl.itc.u-tokyo.ac.jp Hiroshi Nakagawa Information Technology Center The University of Tokyo n3@dl.itc.u-tokyo.ac.jp Abstract We present a sequential labeling approach to hedge cue detection submitted to the bi- ological portion of task 1 for t	ACL	ation for Computational Linguistics$Association for Computational Linguistics$address changes to Betty Walker$Association for Computational Linguistic$-$Approaches to Semitic Languages$Association for Computational Lin-  guistics$	3
In Janyce Wiebe, editor, Proceedings of the 1st Meeting of the North American Chapter of the ACLs, Seattle, WA.	ACL	ation for Computational Linguistics$Association for Computational Linguistics$address changes to Betty Walker$Association for Computational Linguistic$-$Approaches to Semitic Languages$Association for Computational Lin-  guistics$	3
brown, edu  Abst ract   This paper presents an algorithm for identiACL  fying pronominal anaphora and two experiACL  ments based upon this algorithm.	ACL	ation for Computational Linguistics$Association for Computational Linguistics$address changes to Betty Walker$Association for Computational Linguistic$-$Approaches to Semitic Languages$Association for Computational Lin-  guistics$	4
We incorpoACL  rate multiple anaphora resolution factors into  a statistical framework ACL ACL  specifically the disACL  tance between the pronoun and the proposed  antecedent, gender/number/animaticity of the  proposed antecedent, governing head informaACL  tion and noun phrase repetition.	ACL	ation for Computational Linguistics$Association for Computational Linguistics$address changes to Betty Walker$Association for Computational Linguistic$-$Approaches to Semitic Languages$Association for Computational Lin-  guistics$	4
In Proceedings of the Association for Computational Linguistics Workshop on Computational ACL.	ACL	ation for Computational Linguistics$Association for Computational Linguistics$address changes to Betty Walker$Association for Computational Linguistic$-$Approaches to Semitic Languages$Association for Computational Lin-  guistics$	5
Compu- tational ACL, pp.	ACL	ation for Computational Linguistics$Association for Computational Linguistics$address changes to Betty Walker$Association for Computational Linguistic$-$Approaches to Semitic Languages$Association for Computational Lin-  guistics$	5
In Pro- ceedings of the Workshop on Computational ACL, pages 27?	ACL	ation for Computational Linguistics$Association for Computational Linguistics$address changes to Betty Walker$Association for Computational Linguistic$-$Approaches to Semitic Languages$Association for Computational Lin-  guistics$	5
In Proceedings of the Workshop on Compu- tational ACL, pages 9?	ACL	ation for Computational Linguistics$Association for Computational Linguistics$address changes to Betty Walker$Association for Computational Linguistic$-$Approaches to Semitic Languages$Association for Computational Lin-  guistics$	5
In Proceedings of the Workshop on Computational ACL, pages 9?16, Prague, Czech Republic.	ACL	ation for Computational Linguistics$Association for Computational Linguistics$address changes to Betty Walker$Association for Computational Linguistic$-$Approaches to Semitic Languages$Association for Computational Lin-  guistics$	5
The user should choose the area so that  it contains necessary and sufficient words to be one  ME.	ME	meaningful expression$Microelectronics$	0
These subjective ex- pressions could then be added to a subjectivity lex- icon (Esuli and Sebastiani, 2005), and used to gain understanding about which types of complex fea- tures capture MEs that are im- portant for opinion recognition.	ME	meaningful expression$Microelectronics$	0
Figure 1: The flow of finding the correspondences of word sequences  number of other interesting and ME that should be translated in a specific way.	ME	meaningful expression$Microelectronics$	0
I  distinguish - as usual - two kinds of MEs in SRL, terms and  formulas.	ME	meaningful expression$Microelectronics$	0
tional restrictions and so serve to delimit the set of  MEs in the language.	ME	meaningful expression$Microelectronics$	0
Therefore, it makes sense to restrict our attention to a  finite subset E of A, containing all expressions of  length less than or equal to an appropriately fixed  integer n.  Let L be the set of all MEs of a  natural language, that is, of all expressions to which  humans attach a meaning.	ME	meaningful expression$Microelectronics$	0
One such tool was the template-filling tool devel - oped by Bill Ogden and Jim Cowie at New Mexico State University (known as Locke in the version designed fo r English ME) .	ME	meaningful expression$Microelectronics$	1
54  COMPARING HUMAN AND MACHINE PERFORMANCE FO R NATURAL LANGUAGE INFORMATION EXTRACTION : Results for English ME from the MUC-5 Evaluation Craig A. Will Institute for Defense Analyses Computer and Software Engineering Division 1801 N. Beauregard Street Alexandria, VA 2231 1 INTRODUCTION In evaluating the state of technology for extracting information from natural language text by machine, it i s valuable to compare the performance of machine extraction systems with that achieved by humans pe	ME	meaningful expression$Microelectronics$	1
This paper discusses preparation of templates and presents results for human and machine per- formance for English ME ; a companion paper [1] presents additional experimental results.	ME	meaningful expression$Microelectronics$	1
To improve quality and consistency, three steps were taken : First, a set of relatively detailed rules for extracting information from articles and structuring it as an object - oriented template was developed (with the rules for English ME a 40-page, single-spaced document) .	ME	meaningful expression$Microelectronics$	1
Previous experience with the English and Japanese Joint Ventures corpus had made it clear that producin g templates with a high degree of quality and consistency is a difficult and time-consuming task 11l, and we attempte d 54 to make the best use of what had been learned in that effort in producing templates for English ME with quality and consistency appropriate to both the needs of the project and the resources we had available .	ME	meaningful expression$Microelectronics$	1
THE PREPARATION OF TEMPLATE S The development of templates for the English ME corpus began in the fall of 1992 .	ME	meaningful expression$Microelectronics$	1
4.1 Classifiers We use a NBian model as in Kupiec, Pedersen, and Chen?s (1995) experiment (cf.	NB	na??ve Bayes$Na??ve Bayesian$na?ve Bayesian$Naive Baye$Naive Bayes$Na?ve  Baye$Na??ve Bayes$Na?ve Bayes$na?ve Bayes$NaiveBayes$	0
To test if such a simple approach would be enough, we performed a text categorization experiment, using the Rainbow implementation of a NB term frequency times inverse document frequency (TF*IDF) method (McCallum 1997) and considering each sentence as a ?	NB	na??ve Bayes$Na??ve Bayesian$na?ve Bayesian$Naive Baye$Naive Bayes$Na?ve  Baye$Na??ve Bayes$Na?ve Bayes$na?ve Bayes$NaiveBayes$	0
In this paper we generalize the tri-training al- gorithm and use three different learning algo- rithms rather than bootstrap samples to create diversity: a NB algorithm (no smooth- ing), random forests (Breiman, 2001) (with 100 unpruned decision trees) and an algorithm that induces unpruned decision trees.	NB	na??ve Bayes$Na??ve Bayesian$na?ve Bayesian$Naive Baye$Naive Bayes$Na?ve  Baye$Na??ve Bayes$Na?ve Bayes$na?ve Bayes$NaiveBayes$	0
In their exper- iments, they consider classifiers based on deci- sion trees, BP neural networks and NB inference.	NB	na??ve Bayes$Na??ve Bayesian$na?ve Bayesian$Naive Baye$Naive Bayes$Na?ve  Baye$Na??ve Bayes$Na?ve Bayes$na?ve Bayes$NaiveBayes$	0
We have experimented with a maxi- mum entropy model, Repeated Incremental Pruning to Produce Error Reduction (RIP- PER), and decision trees; preliminary results do not show significant improvement over the NBian model.	NB	na??ve Bayes$Na??ve Bayesian$na?ve Bayesian$Naive Baye$Naive Bayes$Na?ve  Baye$Na??ve Bayes$Na?ve Bayes$na?ve Bayes$NaiveBayes$	0
This contrasts with typical semi-supervised learning methods for text categorization that com- bine unlabeled and labeled data within a genera- tive model, such as multinomial NB, via expectation-maximization (Nigam et al, 2000) or semi-supervised frequency estimation (Su et al, 2011).	NB	na??ve Bayes$Na??ve Bayesian$na?ve Bayesian$Naive Baye$Naive Bayes$Na?ve  Baye$Na??ve Bayes$Na?ve Bayes$na?ve Bayes$NaiveBayes$	0
For the NBian classification method, indeed, it is most important that the features be as independent of each other as possible.	NB	na??ve Bayes$Na??ve Bayesian$na?ve Bayesian$Naive Baye$Naive Bayes$Na?ve  Baye$Na??ve Bayes$Na?ve Bayes$na?ve Bayes$NaiveBayes$	0
We used the NLTK (Bird, 2006) implementation of the NB classifier for all our experiments.	NB	na??ve Bayes$Na??ve Bayesian$na?ve Bayesian$Naive Baye$Naive Bayes$Na?ve  Baye$Na??ve Bayes$Na?ve Bayes$na?ve Bayes$NaiveBayes$	1
ME classifiers outperform their generative counterparts (e.g., NB clas- sifiers) because they can easily handle overlapping, probably dependent features which might be con- tained in rich feature sets.	NB	na??ve Bayes$Na??ve Bayesian$na?ve Bayesian$Naive Baye$Naive Bayes$Na?ve  Baye$Na??ve Bayes$Na?ve Bayes$na?ve Bayes$NaiveBayes$	1
For rhetorical relations, NB models achieve 84.90% and 57.87% accuracy in classifying NARRATION and BACKGROUND / ELABORATION re- lations respectively (16% and 23% above baseline).	NB	na??ve Bayes$Na??ve Bayesian$na?ve Bayesian$Naive Baye$Naive Bayes$Na?ve  Baye$Na??ve Bayes$Na?ve Bayes$na?ve Bayes$NaiveBayes$	1
Fn?1; P(C): (Overall) probability of category C; P(Fj | C): Probability of feature-value pair Fj, given that the sentence is of target category C; P(Fj): Probability of feature value Fj; Figure 9 NB classifier.	NB	na??ve Bayes$Na??ve Bayesian$na?ve Bayesian$Naive Baye$Naive Bayes$Na?ve  Baye$Na??ve Bayes$Na?ve Bayes$na?ve Bayes$NaiveBayes$	1
So this kind of training is misleading especially for a NB classifier that utilizes the prior prob- ability of classes.	NB	na??ve Bayes$Na??ve Bayesian$na?ve Bayesian$Naive Baye$Naive Bayes$Na?ve  Baye$Na??ve Bayes$Na?ve Bayes$na?ve Bayes$NaiveBayes$	1
Obtaining Cal- ibrated Probability Estimates from Decision Trees and NB Classiers.	NB	na??ve Bayes$Na??ve Bayesian$na?ve Bayesian$Naive Baye$Naive Bayes$Na?ve  Baye$Na??ve Bayes$Na?ve Bayes$na?ve Bayes$NaiveBayes$	1
In  this paper, we integrate punctuation rules, lexicon, and NB models  to recognize creation titles in Chinese documents.	NB	na??ve Bayes$Na??ve Bayesian$na?ve Bayesian$Naive Baye$Naive Bayes$Na?ve  Baye$Na??ve Bayes$Na?ve Bayes$na?ve Bayes$NaiveBayes$	2
An approach  of integrating punctuation rules, lexicon, and NB models is employed to  recognize creation titles in Chinese documents.	NB	na??ve Bayes$Na??ve Bayesian$na?ve Bayesian$Naive Baye$Naive Bayes$Na?ve  Baye$Na??ve Bayes$Na?ve Bayes$na?ve Bayes$NaiveBayes$	2
Another usage of the NB  model in summarization can be found in (Aone et  al.,	NB	na??ve Bayes$Na??ve Bayesian$na?ve Bayesian$Naive Baye$Naive Bayes$Na?ve  Baye$Na??ve Bayes$Na?ve Bayes$na?ve Bayes$NaiveBayes$	2
Section 4 addresses which features  may be adopted in training NB model.	NB	na??ve Bayes$Na??ve Bayesian$na?ve Bayesian$Naive Baye$Naive Bayes$Na?ve  Baye$Na??ve Bayes$Na?ve Bayes$na?ve Bayes$NaiveBayes$	2
The focus of this study is related to different classification techniques such as support-vector machines (SVM), multi-layer perceptron, NB nets, decision trees and random forest.	NB	na??ve Bayes$Na??ve Bayesian$na?ve Bayesian$Naive Baye$Naive Bayes$Na?ve  Baye$Na??ve Bayes$Na?ve Bayes$na?ve Bayes$NaiveBayes$	2
The rest of undetermined  candidates are verified by NB model.	NB	na??ve Bayes$Na??ve Bayesian$na?ve Bayesian$Naive Baye$Naive Bayes$Na?ve  Baye$Na??ve Bayes$Na?ve Bayes$na?ve Bayes$NaiveBayes$	2
However, we find that NBs in con- junction with Dirichlet smoothing (Smucker and Allan, 2006) works at least as well when appropri- ately tuned.	NB	na??ve Bayes$Na??ve Bayesian$na?ve Bayesian$Naive Baye$Naive Bayes$Na?ve  Baye$Na??ve Bayes$Na?ve Bayes$na?ve Bayes$NaiveBayes$	3
Figure 1 shows a choropleth map of the behav- ior of NBs, plotting the rank of cells for 338 Figure 1: Relative NBs rank of cells for ENWIKI13 test document Pennsylvania Avenue (Washington, DC), surrounding the true location.	NB	na??ve Bayes$Na??ve Bayesian$na?ve Bayesian$Naive Baye$Naive Bayes$Na?ve  Baye$Na??ve Bayes$Na?ve Bayes$na?ve Bayes$NaiveBayes$	3
Edlin provides gen- eral machine learning architecture for linear models and a framework with implementations of popular learning algorithms including NBs, percep- tron, maximum entropy, one-best MIRA, and condi- tional random fields (CRF) among others.	NB	na??ve Bayes$Na??ve Bayesian$na?ve Bayesian$Naive Baye$Naive Bayes$Na?ve  Baye$Na??ve Bayes$Na?ve Bayes$na?ve Bayes$NaiveBayes$	3
andard mea- sures such as Kullback-Leibler (KL) divergence (Zhai and Lafferty, 2001), which seeks the cell whose language model is closest to the test doc- ument?s, or NBs (Lewis, 1998), which chooses the cell that assigns the highest probabil- ity to the test document.	NB	na??ve Bayes$Na??ve Bayesian$na?ve Bayesian$Naive Baye$Naive Bayes$Na?ve  Baye$Na??ve Bayes$Na?ve Bayes$na?ve Bayes$NaiveBayes$	3
A test document?s location is then chosen based on the cell with the most sim- ilar language model according to standard mea- sures such as Kullback-Leibler (KL) divergence (Zhai and Lafferty, 2001), which seeks the cell whose language model is closest to the test doc- ument?s, or NBs (Lewis, 1998), which chooses the cell that assigns the highest probabil- ity to the test document.	NB	na??ve Bayes$Na??ve Bayesian$na?ve Bayesian$Naive Baye$Naive Bayes$Na?ve  Baye$Na??ve Bayes$Na?ve Bayes$na?ve Bayes$NaiveBayes$	3
Importantly, this is the first method that improves upon straight uniform-grid NBs on all of these corpora, in contrast with k-d trees (Roller12) and the current state-of-the-art tech- nique for Twitter users of geographically-salient feature selection (Han14).	NB	na??ve Bayes$Na??ve Bayesian$na?ve Bayesian$Naive Baye$Naive Bayes$Na?ve  Baye$Na??ve Bayes$Na?ve Bayes$na?ve Bayes$NaiveBayes$	3
3.2 NBs A geodesic grid of sufficient granularity creates a large decision space, when each cell is viewed as a label to be predicted by some classifier.	NB	na??ve Bayes$Na??ve Bayesian$na?ve Bayesian$Naive Baye$Naive Bayes$Na?ve  Baye$Na??ve Bayes$Na?ve Bayes$na?ve Bayes$NaiveBayes$	3
However, we find that NB in con- junction with Dirichlet smoothing (Smucker and Allan, 2006) works at least as well when appropri- ately tuned.	NB	na??ve Bayes$Na??ve Bayesian$na?ve Bayesian$Naive Baye$Naive Bayes$Na?ve  Baye$Na??ve Bayes$Na?ve Bayes$na?ve Bayes$NaiveBayes$	4
Figure 1 shows a choropleth map of the behav- ior of NB, plotting the rank of cells for 338 Figure 1: Relative NB rank of cells for ENWIKI13 test document Pennsylvania Avenue (Washington, DC), surrounding the true location.	NB	na??ve Bayes$Na??ve Bayesian$na?ve Bayesian$Naive Baye$Naive Bayes$Na?ve  Baye$Na??ve Bayes$Na?ve Bayes$na?ve Bayes$NaiveBayes$	4
Edlin provides gen- eral machine learning architecture for linear models and a framework with implementations of popular learning algorithms including NB, percep- tron, maximum entropy, one-best MIRA, and condi- tional random fields (CRF) among others.	NB	na??ve Bayes$Na??ve Bayesian$na?ve Bayesian$Naive Baye$Naive Bayes$Na?ve  Baye$Na??ve Bayes$Na?ve Bayes$na?ve Bayes$NaiveBayes$	4
andard mea- sures such as Kullback-Leibler (KL) divergence (Zhai and Lafferty, 2001), which seeks the cell whose language model is closest to the test doc- ument?s, or NB (Lewis, 1998), which chooses the cell that assigns the highest probabil- ity to the test document.	NB	na??ve Bayes$Na??ve Bayesian$na?ve Bayesian$Naive Baye$Naive Bayes$Na?ve  Baye$Na??ve Bayes$Na?ve Bayes$na?ve Bayes$NaiveBayes$	4
A test document?s location is then chosen based on the cell with the most sim- ilar language model according to standard mea- sures such as Kullback-Leibler (KL) divergence (Zhai and Lafferty, 2001), which seeks the cell whose language model is closest to the test doc- ument?s, or NB (Lewis, 1998), which chooses the cell that assigns the highest probabil- ity to the test document.	NB	na??ve Bayes$Na??ve Bayesian$na?ve Bayesian$Naive Baye$Naive Bayes$Na?ve  Baye$Na??ve Bayes$Na?ve Bayes$na?ve Bayes$NaiveBayes$	4
Importantly, this is the first method that improves upon straight uniform-grid NB on all of these corpora, in contrast with k-d trees (Roller12) and the current state-of-the-art tech- nique for Twitter users of geographically-salient feature selection (Han14).	NB	na??ve Bayes$Na??ve Bayesian$na?ve Bayesian$Naive Baye$Naive Bayes$Na?ve  Baye$Na??ve Bayes$Na?ve Bayes$na?ve Bayes$NaiveBayes$	4
3.2 NB A geodesic grid of sufficient granularity creates a large decision space, when each cell is viewed as a label to be predicted by some classifier.	NB	na??ve Bayes$Na??ve Bayesian$na?ve Bayesian$Naive Baye$Naive Bayes$Na?ve  Baye$Na??ve Bayes$Na?ve Bayes$na?ve Bayes$NaiveBayes$	4
A  series  of  four  experiments  was  conducted  on  a  baseline  method:  NBs  with varying sets of attributes.	NB	na??ve Bayes$Na??ve Bayesian$na?ve Bayesian$Naive Baye$Naive Bayes$Na?ve  Baye$Na??ve Bayes$Na?ve Bayes$na?ve Bayes$NaiveBayes$	5
We therefore modified our  system to incorporate the position information with  the help of SVMs and we also investigated the  capability of SVMs versus NBs on this  problem.	NB	na??ve Bayes$Na??ve Bayesian$na?ve Bayesian$Naive Baye$Naive Bayes$Na?ve  Baye$Na??ve Bayes$Na?ve Bayes$na?ve Bayes$NaiveBayes$	5
Besides,  the  proposed  method  is  compared  with  three  baselines,  namely  NBs  classifier,  a  language  model  and an approach based on significant collo- cations.	NB	na??ve Bayes$Na??ve Bayesian$na?ve Bayesian$Naive Baye$Naive Bayes$Na?ve  Baye$Na??ve Bayes$Na?ve Bayes$na?ve Bayes$NaiveBayes$	5
As  an  alternative  other  teams  during  the  last  TREC and NTCIR evaluation campaigns have sug- gested  variations  of  NBs  classifier,  lan- guage models and SVM, along with the use of such  heuristics  as  word  order,  punctuation,  sentence  length, etc.	NB	na??ve Bayes$Na??ve Bayesian$na?ve Bayesian$Naive Baye$Naive Bayes$Na?ve  Baye$Na??ve Bayes$Na?ve Bayes$na?ve Bayes$NaiveBayes$	5
Bhalotia et al (2003)  converted this task into a binary classification  problem and trained a NBs classifier with  kernels, achieving 53.04% for CD.	NB	na??ve Bayes$Na??ve Bayesian$na?ve Bayesian$Naive Baye$Naive Bayes$Na?ve  Baye$Na??ve Bayes$Na?ve Bayes$na?ve Bayes$NaiveBayes$	5
Our approach was compared to the three baselines,  namely  NBs  classifier,  language  model  and an approach based on finding significant collo- cations.	NB	na??ve Bayes$Na??ve Bayesian$na?ve Bayesian$Naive Baye$Naive Bayes$Na?ve  Baye$Na??ve Bayes$Na?ve Bayes$na?ve Bayes$NaiveBayes$	5
ML method (Weka) Features Accuracy Decision Trees PMI scores 65.4% Decision Rules PMI scores 65.5% NB PMI scores 52.5% K-Nearest Neighbor PMI scores 64.5% Kernel Density PMI scores 60.5% Boosting (Dec. Stumps) PMI scores 67.7% NB 500 words 68.0% Decision Trees 500 words 67.0% NB PMI + 500 words 66.5% Boosting (Dec. Stumps) PMI + 500 words 69.2% Table 6: Comparative results for the supervised learning method using various ML learning algo- rithms (Weka), averaged ov	NB	na??ve Bayes$Na??ve Bayesian$na?ve Bayesian$Naive Baye$Naive Bayes$Na?ve  Baye$Na??ve Bayes$Na?ve Bayes$na?ve Bayes$NaiveBayes$	6
ML method (Weka) Features Accuracy Decision Trees PMI scores 65.4% Decision Rules PMI scores 65.5% NB PMI scores 52.5% K-Nearest Neighbor PMI scores 64.5% Kernel Density PMI scores 60.5% Boosting (Dec. Stumps) PMI scores 67.7% NB 500 words 68.0% Decision Trees 500 words 67.0% NB PMI + 500 words 66.5% Boosting (Dec. Stumps) PMI + 500 words 69.2% Table 6: Comparative results for the supervised learning method using various ML learning algo- rithms (Weka), averaged over the seven groups of near-synonyms from the Exp1 data set.	NB	na??ve Bayes$Na??ve Bayesian$na?ve Bayesian$Naive Baye$Naive Bayes$Na?ve  Baye$Na??ve Bayes$Na?ve Bayes$na?ve Bayes$NaiveBayes$	6
In particular, when using the 500 word features for each training exam- ple, only the NB algorithm was able to run in reasonable time.	NB	na??ve Bayes$Na??ve Bayesian$na?ve Bayesian$Naive Baye$Naive Bayes$Na?ve  Baye$Na??ve Bayes$Na?ve Bayes$na?ve Bayes$NaiveBayes$	6
The classifiers that use PMI features are Decision Trees, Decision Rules, NB, K-Nearest Neighbor, Kernel Density, and Boosting a weak classifier (De- cision Stumps ?	NB	na??ve Bayes$Na??ve Bayesian$na?ve Bayesian$Naive Baye$Naive Bayes$Na?ve  Baye$Na??ve Bayes$Na?ve Bayes$na?ve Bayes$NaiveBayes$	6
Then, a NB classifier that uses only the word features is presented, and the same type of classifiers with both types of features.	NB	na??ve Bayes$Na??ve Bayesian$na?ve Bayesian$Naive Baye$Naive Bayes$Na?ve  Baye$Na??ve Bayes$Na?ve Bayes$na?ve Bayes$NaiveBayes$	6
We noticed that the NB classifier performs very poorly on PMI features only (55% average accuracy), but performs very well on word features (68% average accuracy).	NB	na??ve Bayes$Na??ve Bayesian$na?ve Bayesian$Naive Baye$Naive Bayes$Na?ve  Baye$Na??ve Bayes$Na?ve Bayes$na?ve Bayes$NaiveBayes$	6
Common Local Temporal Noun expres- sions  3.2 NBian Classifier  A variety of machine learning classifiers are de- signed to resolve the classification problem,  such as SVM classifier, ME classifier and the  Decision Tree family.	NB	na??ve Bayes$Na??ve Bayesian$na?ve Bayesian$Naive Baye$Naive Bayes$Na?ve  Baye$Na??ve Bayes$Na?ve Bayes$na?ve Bayes$NaiveBayes$	7
Pang et al  (2002) adopt the VSM model to represent product  reviews and apply text classification algorithms  such as NB, maximum entropy and sup- port vector machines to predict sentiment polarity  of given product review.	NB	na??ve Bayes$Na??ve Bayesian$na?ve Bayesian$Naive Baye$Naive Bayes$Na?ve  Baye$Na??ve Bayes$Na?ve Bayes$na?ve Bayes$NaiveBayes$	7
Text  Dictionary Lookup  Found  Noun Phrase Head identifier  NB classifier  Best guess cluster Mayo Synonym Clusters M001|cholangeocarcinoma  M001|bile duct cancer  M001|?	NB	na??ve Bayes$Na??ve Bayesian$na?ve Bayesian$Naive Baye$Naive Bayes$Na?ve  Baye$Na??ve Bayes$Na?ve Bayes$na?ve Bayes$NaiveBayes$	7
The ML (Machine Learning) Named Entity  annotator is based on a NB classifier  trained on a combination of the UMLS entry terms  and the MCS where each diagnostic statement is  represented as a bag-of-words and used as a train- ing sample for generating a Naive Bayes classifier  which assigns MCS id?s to noun phrases identified  in the text of clinical notes.	NB	na??ve Bayes$Na??ve Bayesian$na?ve Bayesian$Naive Baye$Naive Bayes$Na?ve  Baye$Na??ve Bayes$Na?ve Bayes$na?ve Bayes$NaiveBayes$	7
So the NBian Classifier  that assumes independence among feature deno- tations is suitable to be applied to our method.	NB	na??ve Bayes$Na??ve Bayesian$na?ve Bayesian$Naive Baye$Naive Bayes$Na?ve  Baye$Na??ve Bayes$Na?ve Bayes$na?ve Bayes$NaiveBayes$	7
From the  NB formula?	NB	na??ve Bayes$Na??ve Bayesian$na?ve Bayesian$Naive Baye$Naive Bayes$Na?ve  Baye$Na??ve Bayes$Na?ve Bayes$na?ve Bayes$NaiveBayes$	7
7.2 Machine Learning Approaches  We evaluated the performance of four different  machine-learning approaches on the DA classifi- cation tasks: memory-based learning (k-Nearest- Neighbor), decision trees, neural networks, and  NB n-gram classifiers.	NB	na??ve Bayes$Na??ve Bayesian$na?ve Bayesian$Naive Baye$Naive Bayes$Na?ve  Baye$Na??ve Bayes$Na?ve Bayes$na?ve Bayes$NaiveBayes$	8
For concept sequence classifi- cation, no learning approach clearly outper- formed any other (with the exception that the  NB n-gram approach performed worse  than other approaches).	NB	na??ve Bayes$Na??ve Bayesian$na?ve Bayesian$Naive Baye$Naive Bayes$Na?ve  Baye$Na??ve Bayes$Na?ve Bayes$na?ve Bayes$NaiveBayes$	8
This time, we compare with a bag-of- words NB system (reported by Hermann and Blunsom (2014)), a system trained on the Polyglot embeddings from Al-Rfou et al (2013) (which are multilingual, but not in a shared rep- resentation space), and the two systems developed by Hermann and Blunsom (2014).	NB	na??ve Bayes$Na??ve Bayesian$na?ve Bayesian$Naive Baye$Naive Bayes$Na?ve  Baye$Na??ve Bayes$Na?ve Bayes$na?ve Bayes$NaiveBayes$	8
Another usage of the NBian  model in summarization can be found in (Aone et  al.,	NB	na??ve Bayes$Na??ve Bayesian$na?ve Bayesian$Naive Baye$Naive Bayes$Na?ve  Baye$Na??ve Bayes$Na?ve Bayes$na?ve Bayes$NaiveBayes$	8
We used Rainbow (McCallum, 1996) for  our NB n-gram classifiers.	NB	na??ve Bayes$Na??ve Bayesian$na?ve Bayesian$Naive Baye$Naive Bayes$Na?ve  Baye$Na??ve Bayes$Na?ve Bayes$na?ve Bayes$NaiveBayes$	8
Although our best- performing classifiers combined word and argu- ment parse information, the NB word  bigram classifier (Rainbow) performed very well  on the SA classification task.	NB	na??ve Bayes$Na??ve Bayesian$na?ve Bayesian$Naive Baye$Naive Bayes$Na?ve  Baye$Na??ve Bayes$Na?ve Bayes$na?ve Bayes$NaiveBayes$	8
NB makes a strict indepen- dence assumption and can be swamped by the sheer number of features we used, but it is a solid baseline and does a decent job of suggesting which features are more powerful.	NB	na??ve Bayes$Na??ve Bayesian$na?ve Bayesian$Naive Baye$Naive Bayes$Na?ve  Baye$Na??ve Bayes$Na?ve Bayes$na?ve Bayes$NaiveBayes$	9
PARAM_CORPUS_PATH, "src/main/resources/tweets.txt"]]), Dimension.create("featureMode", "document"), Dimension.create("learningMode", "singleLabel"), Dimension.create("featureSet", [EmoticonRatioExtractor.name, NumberOfHashTagsExtractor.name]), Dimension.create("dataWriter", WekaDataWriter.name), Dimension.create("classificationArguments", [NB.name, RandomForest.name])], reports: [BatchCrossValidationReport], // collects results from folds numFolds: 10]; Listing 1: Groovy code to configure a DKPro TC cross-validation BatchTask on Twitter data.	NB	na??ve Bayes$Na??ve Bayesian$na?ve Bayesian$Naive Baye$Naive Bayes$Na?ve  Baye$Na??ve Bayes$Na?ve Bayes$na?ve Bayes$NaiveBayes$	9
NB 56.3 76.2 50.7 Table 2: Performance of several machine learn- ing algorithms on the English TempEval-1 train- ing data, with cross-validation.	NB	na??ve Bayes$Na??ve Bayesian$na?ve Bayesian$Naive Baye$Naive Bayes$Na?ve  Baye$Na??ve Bayes$Na?ve Bayes$na?ve Bayes$NaiveBayes$	9
JRip is not without its own limitations but, for our task, it shows better results than NB.	NB	na??ve Bayes$Na??ve Bayesian$na?ve Bayesian$Naive Baye$Naive Bayes$Na?ve  Baye$Na??ve Bayes$Na?ve Bayes$na?ve Bayes$NaiveBayes$	9
The top classifier uses NB.	NB	na??ve Bayes$Na??ve Bayesian$na?ve Bayesian$Naive Baye$Naive Bayes$Na?ve  Baye$Na??ve Bayes$Na?ve Bayes$na?ve Bayes$NaiveBayes$	9
NB 56.3 76.2 50.7 Table 2: Comparing different algorithms (%-acc.	NB	na??ve Bayes$Na??ve Bayesian$na?ve Bayesian$Naive Baye$Naive Bayes$Na?ve  Baye$Na??ve Bayes$Na?ve Bayes$na?ve Bayes$NaiveBayes$	9
Such a rule was used by Soubbotin (2001), who developed a system who obtained the best accuracy in the 2001 TREC (Voorhees, 2001a).	TREC	Text REtrieval Conference$Text Retrieval Conference$	0
TREC.	TREC	Text REtrieval Conference$Text Retrieval Conference$	0
The evaluation consisted of four tasks  designed to assess performance ofautomatic  summaries used in real world tasks and to  leverage off of previous evaluations in IR and IE,  the TRECs and Message  Understanding Conferences, respectively.	TREC	Text REtrieval Conference$Text Retrieval Conference$	0
In TREC, pages 104?	TREC	Text REtrieval Conference$Text Retrieval Conference$	0
1 TREC (http://trec.nist.gov/).	TREC	Text REtrieval Conference$Text Retrieval Conference$	0
First TREC.	TREC	Text REtrieval Conference$Text Retrieval Conference$	0
Continuing Evaluation  During Phase II, TIPSTER became primary  sponsor for both the Message Understanding  Conferences and the TRECs,  based on the belief that these forums for evaluation  of text-processing technologies are essential to  continued success in TIPSTER research and  development.	TREC	Text REtrieval Conference$Text Retrieval Conference$	1
Proceedings of the Twelfth TREC, November 18-21, 2003, Gaithersburg, MD.	TREC	Text REtrieval Conference$Text Retrieval Conference$	1
In Proceedings of the 15th TREC.	TREC	Text REtrieval Conference$Text Retrieval Conference$	1
of TREC, pages 16?19.	TREC	Text REtrieval Conference$Text Retrieval Conference$	1
In pro- ceeding of the Eight TREC 1999,  253-258.	TREC	Text REtrieval Conference$Text Retrieval Conference$	1
In Proceedings of the 10th TREC,  pages 437-446, 2001.	TREC	Text REtrieval Conference$Text Retrieval Conference$	1
This algorithm searches the parse tree in a left-  to-right, breadth-first fashion that obeys the  major reflexive pronoun constraints while giv-  ing a pREF to antecedents hat are closer  to the pronoun.	REF	reference$obj123$references$	0
Since the train-  ing corpus is tawed with REF informa-  tion, the probability P(plWo) is easily obtained.	REF	reference$obj123$references$	0
This implements the observed pREF  for subject position antecedents.	REF	reference$obj123$references$	0
This program differs  from earlier work in its almost complete lack of  hand-crafting, relying instead on a very small  corpus of Penn Wall Street Journal Tree-bank  text (Marcus et al, 1993) that has been marked  with co-REF information.	REF	reference$obj123$references$	0
We collect these probabilities on the  training data, which are marked with REF  links.	REF	reference$obj123$references$	0
The corpus  is manually tagged with REF indices and  referents" repetition numbers.	REF	reference$obj123$references$	0
in some respect other than that it is a panda as opposed to a bear, and looks for something about REF which might distinguish it from previously observed bears.	REF	reference$obj123$references$	1
in s refers to the focused object REF.	REF	reference$obj123$references$	1
1  " " " ' ' ' " |   10 100  Number of REF  O ?	REF	reference$obj123$references$	2
salience(re/)  = -2  log  Making the unrealistic simplifying assumption  that REF of one gender class are com-  pletely independent of REF for another  classes 1, the likelihood function in this case is  just the product over all classes of the probabil-  ities of each class of reference to the power of  the number of observations of this class.	REF	reference$obj123$references$	2
We  would like to know, therefore, whether the pat-  tern of pronoun REF that we observe for  a given referent is the result of our supposed  "hypothesis about pronoun reference" - that is,  the pronoun reference strategy we have provi-  sionally adopted in order to gather statistics -  or whether the result of some other unidentified  process.	REF	reference$obj123$references$	2
Figure 2 shows average  accuracy as a function of number of REF  for a given referent.	REF	reference$obj123$references$	2
The constraints rule out implausible can-  didates and the pREF mphasize the selec-  tion of the most likely antecedent.	REF	reference$obj123$references$	2
Sub- ject(S), verbs(V), and objects(O) are displayed for both VP.	VP	verb phrases$verb phrase$VBD PRT$VBP$verb  phrase$Verb Phrase$Verb Phras$Verb Pair$VP:NP$verbal parent$V "$VP:P$	0
Torisawa et al, (2002) presented a method to detect asso- ciative relationships between VP.	VP	verb phrases$verb phrase$VBD PRT$VBP$verb  phrase$Verb Phrase$Verb Phras$Verb Pair$VP:NP$verbal parent$V "$VP:P$	0
Figure 2: An example of a Chinese sentence with a coordination of VP as predicate.	VP	verb phrases$verb phrase$VBD PRT$VBP$verb  phrase$Verb Phrase$Verb Phras$Verb Pair$VP:NP$verbal parent$V "$VP:P$	0
uffixes for inflec- tion as clues to identify the attachment position of the verb and adjective phrases, because Japanese verbs and adjectives have inflections, which depends 110 (no label) base form cont continuative form attr attributive form neg negative form hyp hypothetical form imp imperative form stem stem Table 2: Inflection tag suffixes on their modifying words and phrases (e.g. noun and VP).	VP	verb phrases$verb phrase$VBD PRT$VBP$verb  phrase$Verb Phrase$Verb Phras$Verb Pair$VP:NP$verbal parent$V "$VP:P$	0
Typically, ambiguous VP of the form v rip1 p rip2 are  resolved through a model which considers values of the four head words (v, nl ,  p and 77,2).	VP	verb phrases$verb phrase$VBD PRT$VBP$verb  phrase$Verb Phrase$Verb Phras$Verb Pair$VP:NP$verbal parent$V "$VP:P$	0
As an option, we also added voice infor- mation (A:active, P:passive and C:causative) to the VP, because it effectively helps to discrim- inate cases.	VP	verb phrases$verb phrase$VBD PRT$VBP$verb  phrase$Verb Phrase$Verb Phras$Verb Pair$VP:NP$verbal parent$V "$VP:P$	0
Parser-based games accept typed-in com- mands from the player, usually in the form of VP, such as ?	VP	verb phrases$verb phrase$VBD PRT$VBP$verb  phrase$Verb Phrase$Verb Phras$Verb Pair$VP:NP$verbal parent$V "$VP:P$	0
Torisawa et al, (2002) presented a method to detect asso- ciative relationships between VPs.	VP	verb phrases$verb phrase$VBD PRT$VBP$verb  phrase$Verb Phrase$Verb Phras$Verb Pair$VP:NP$verbal parent$V "$VP:P$	1
noun and VPs).	VP	verb phrases$verb phrase$VBD PRT$VBP$verb  phrase$Verb Phrase$Verb Phras$Verb Pair$VP:NP$verbal parent$V "$VP:P$	1
uffixes for inflec- tion as clues to identify the attachment position of the verb and adjective phrases, because Japanese verbs and adjectives have inflections, which depends 110 (no label) base form cont continuative form attr attributive form neg negative form hyp hypothetical form imp imperative form stem stem Table 2: Inflection tag suffixes on their modifying words and phrases (e.g. noun and VPs).	VP	verb phrases$verb phrase$VBD PRT$VBP$verb  phrase$Verb Phrase$Verb Phras$Verb Pair$VP:NP$verbal parent$V "$VP:P$	1
Subcategorization and voice Each verb has a subcategorization frame, which is useful for build- ing VP s	VP	verb phrases$verb phrase$VBD PRT$VBP$verb  phrase$Verb Phrase$Verb Phras$Verb Pair$VP:NP$verbal parent$V "$VP:P$	1
This is almost cer- tainly a misleading figure, since those two words do not form a plausible VP; it is much more probable that the very strong, in fact id- iomatic, correlation ?	VP	verb phrases$verb phrase$VBD PRT$VBP$verb  phrase$Verb Phrase$Verb Phras$Verb Pair$VP:NP$verbal parent$V "$VP:P$	1
Subcategorization and voice Each verb has a subcategorization frame, which is useful for build- ing VP structure.	VP	verb phrases$verb phrase$VBD PRT$VBP$verb  phrase$Verb Phrase$Verb Phras$Verb Pair$VP:NP$verbal parent$V "$VP:P$	1
The texts used to de- scribe states and actions could be very different in nature, e.g., a state text could be long, contain- ing sentences with complex linguistic structure, whereas an action text could be very concise or just a VP.	VP	verb phrases$verb phrase$VBD PRT$VBP$verb  phrase$Verb Phrase$Verb Phras$Verb Pair$VP:NP$verbal parent$V "$VP:P$	1
Parser-based games accept typed-in com- mands from the player, usually in the form of VPs, such as ?	VP	verb phrases$verb phrase$VBD PRT$VBP$verb  phrase$Verb Phrase$Verb Phras$Verb Pair$VP:NP$verbal parent$V "$VP:P$	1
Sentence 2, are VP, ROOT 1, Exports NNS, SBJ A1 3, thought VBN, VC PRED 4, to TO, OPRD C-A1 5, have VB, IM 6, risen VBN, VC 7, strongly RB, MNR 8, in IN, TMP 9, August NNP, PMOD [...] Figure 1: a labeled example for the (part of) sentence ?	VP	verb phrases$verb phrase$VBD PRT$VBP$verb  phrase$Verb Phrase$Verb Phras$Verb Pair$VP:NP$verbal parent$V "$VP:P$	3
U.S. to get feature values of -0.8353556 and -2.0460036 per language model (see 41 Feature Example for that-CCs Example for that-less CCs Dependency length and position of CC Position of matrix verb thatCC:mvInd 7.0 noThatCC:mvInd 7.0 Dist between matrix verb & CC thatCC:mvCCDist 1.0 noThatCC:mvCCDist 1.0 Length of CC thatCC:ccLen 29.0 noThatCC:ccLen 28.0 Matrix verb features POS-tag thatCC:mvPos:VP 1.0 noThatCC:mvPos:VP 1.0 Stem thatCC:mvStem:argue 1.0 noThatCC:mvStem:argue 1.0 Form thatCC:mvForm:argue 1.0 noThatCC:mvForm:argue 1.0 CCG supertag thatCC:mvSt:s[dcl]\np/s[em] 1.0 noThatCC:mvSt:s[dcl]\np/s[dcl] 1.0 uniform information density (UID) Average n-gram log probs thatCC:$uidCCMean1 -0.8353556 noThatCC:$uidCCMean1 -2.5177214 of first 2 words of that-less CCs thatCC:$uidCCMean2 -2.04	VP	verb phrases$verb phrase$VBD PRT$VBP$verb  phrase$Verb Phrase$Verb Phras$Verb Pair$VP:NP$verbal parent$V "$VP:P$	3
Therefore, if we take a naive filtering method using the rule dictionary, we Original Symbol Normalized Symbol NNP, NNS, NNPS, PRP NN RBR, RBS RB JJR, JJS, PRP$ JJ VBD, VBZ VP : , ?, ?	VP	verb phrases$verb phrase$VBD PRT$VBP$verb  phrase$Verb Phrase$Verb Phras$Verb Pair$VP:NP$verbal parent$V "$VP:P$	3
Groups Member POS tags Count % Noun NN/NNP/NNS/NNPS 7511 31.30 Verb VBD/VB/VBZ/VBN/VBG/VP 3285 13.69 Adj JJ/JJR/JJS 1718 7.16 Adv RB/RBR 742 3.09 Pronoun CD/PRP/PRP$ 1397 5.82 Content Noun/Verb/Adj/Adv/Pronoun 14653 61.05 Function Other 9347 38.95 Total All 45 POS tags 24K 100.00 Table 3: Group names, members, number and per- centage of the words according to their gold POS tags.	VP	verb phrases$verb phrase$VBD PRT$VBP$verb  phrase$Verb Phrase$Verb Phras$Verb Pair$VP:NP$verbal parent$V "$VP:P$	3
ues of -0.8353556 and -2.0460036 per language model (see 41 Feature Example for that-CCs Example for that-less CCs Dependency length and position of CC Position of matrix verb thatCC:mvInd 7.0 noThatCC:mvInd 7.0 Dist between matrix verb & CC thatCC:mvCCDist 1.0 noThatCC:mvCCDist 1.0 Length of CC thatCC:ccLen 29.0 noThatCC:ccLen 28.0 Matrix verb features POS-tag thatCC:mvPos:VP 1.0 noThatCC:mvPos:VP 1.0 Stem thatCC:mvStem:argue 1.0 noThatCC:mvStem:argue 1.0 Form thatCC:mvForm:argue 1.0 noThatCC:mvForm:argue 1.0 CCG supertag thatCC:mvSt:s[dcl]\np/s[em] 1.0 noThatCC:mvSt:s[dcl]\np/s[dcl] 1.0 uniform information density (UID) Average n-gram log probs thatCC:$uidCCMean1 -0.8353556 noThatCC:$uidCCMean1 -2.5177214 of first 2 words of that-less CCs thatCC:$uidCCMean2 -2.0460036 noThatCC:$uidCCMe	VP	verb phrases$verb phrase$VBD PRT$VBP$verb  phrase$Verb Phrase$Verb Phras$Verb Pair$VP:NP$verbal parent$V "$VP:P$	3
Implicative verbs are a special subclass2 of such verbs which give rise to entailments involving their 1Here, * indicates that VB1 can match any verb form, e.g. VB, VBD, VP, etc.	VP	verb phrases$verb phrase$VBD PRT$VBP$verb  phrase$Verb Phrase$Verb Phras$Verb Pair$VP:NP$verbal parent$V "$VP:P$	3
L ikewise  the  p~ocedure   ~h|ch  pa~ses in f in i t i va l  complements  in   accordance  w i th  $8 accepts  a con junct ion   o f  one or  more VPs  s ta r t ing  ~ i th   in f in i t i ves .	VP	verb phrases$verb phrase$VBD PRT$VBP$verb  phrase$Verb Phrase$Verb Phras$Verb Pair$VP:NP$verbal parent$V "$VP:P$	4
As ~egards  the  F i r s t ,  Par tee   observes  ( (24 \ ] ,  C25\] )  that  a vers ion  o f   51~ which inse~ts  labe l led  bracket ing ,  and  a vers ion  oF $4 sens i t i ve  to  such  bracket ing  and genera l i sed  to  add sub jec t   - agreement  to  the  f i r s t  verb  in  each  con junct  o f  a con jo ined  VP ,  i s   needed in  o~dey .	VP	verb phrases$verb phrase$VBD PRT$VBP$verb  phrase$Verb Phrase$Verb Phras$Verb Pair$VP:NP$verbal parent$V "$VP:P$	4
Without l abe l led  bracket ing ,  PTG has d tFF -   4~  i f   i f   i f   i f   i f   i f   i f   i f   4~  4~  i f   4~  i f   and then  const ra ins  the  pred icate  to  be a  con junct ion  o f  one or  mo~e VPs	VP	verb phrases$verb phrase$VBD PRT$VBP$verb  phrase$Verb Phrase$Verb Phras$Verb Pair$VP:NP$verbal parent$V "$VP:P$	4
An example of a derivation tree in  treebank comparing between CG and CDG A status  of  transformed  CDG  treebank  is  30,340  text  lines  which  include  14,744  sen- tences,  9,651  VPs  or  subject-omitted  sentences and 5,945 noun phrases.	VP	verb phrases$verb phrase$VBD PRT$VBP$verb  phrase$Verb Phrase$Verb Phras$Verb Pair$VP:NP$verbal parent$V "$VP:P$	4
Without l abe l led  bracket ing ,  PTG has d tFF -   4~  i f   i f   i f   i f   i f   i f   i f   i f   4~  4~  i f   4~  i f   and then  const ra ins  the  pred icate  to  be a  con junct ion  o f  one or  mo~e VPs   ident i f i ab le  as commencing ~ i th  concordant   F in i te  Forms.	VP	verb phrases$verb phrase$VBD PRT$VBP$verb  phrase$Verb Phrase$Verb Phras$Verb Pair$VP:NP$verbal parent$V "$VP:P$	4
VP   M\[ remove nontermina l  category   N. add new nontermina l  category   Cont ro l  l ing  access  to  l ingu is t i c   in fo rmat ion  by  means  of menu ensures  that  the   updated  f i les  are appropr ia te ly  recompi led  into  the form used by the program.	VP	verb phrases$verb phrase$VBD PRT$VBP$verb  phrase$Verb Phrase$Verb Phras$Verb Pair$VP:NP$verbal parent$V "$VP:P$	4
a con jo ined  VP ,  i s   needed in  o~dey .	VP	verb phrases$verb phrase$VBD PRT$VBP$verb  phrase$Verb Phrase$Verb Phras$Verb Pair$VP:NP$verbal parent$V "$VP:P$	4
0 1 2 4 5 6  Rule  us ing semant ic  re lat ion  to VP   Candidate judging rule 1  When a candidate r ferent of a case component (azero  pronoun) does not satisfy the semantic marker of the  case component in the case frame, it is given -5.	VP	verb phrases$verb phrase$VBD PRT$VBP$verb  phrase$Verb Phrase$Verb Phras$Verb Pair$VP:NP$verbal parent$V "$VP:P$	4
Ru le  when a pronoun refers to a VP   Like a demonstrative pronoun, a demonstrative adjec-  tive can refer to the meaning of the verb phrase in the  previous sentence.	VP	verb phrases$verb phrase$VBD PRT$VBP$verb  phrase$Verb Phrase$Verb Phras$Verb Pair$VP:NP$verbal parent$V "$VP:P$	4
This can be summarized by the following  table:    Outside SimpleNP Exceptions  Prepositional Phrases  Relative Clauses  VPs  Apposition1  Some conjunctions  (Conjunctions are  marked according to the  TreeBank guidelines)2.	VP	verb phrases$verb phrase$VBD PRT$VBP$verb  phrase$Verb Phrase$Verb Phras$Verb Pair$VP:NP$verbal parent$V "$VP:P$	5
Presented at the  Linguistic Conference on East Asian Languages: VPs, in  Los Angeles, California. (	VP	verb phrases$verb phrase$VBD PRT$VBP$verb  phrase$Verb Phrase$Verb Phras$Verb Pair$VP:NP$verbal parent$V "$VP:P$	5
Robinson, A. E., "Determining VP  Referents in Dialog~", American Journal of  Computatlor~l Linguistics", Jan. 1981  17.	VP	verb phrases$verb phrase$VBD PRT$VBP$verb  phrase$Verb Phrase$Verb Phras$Verb Pair$VP:NP$verbal parent$V "$VP:P$	5
The structure is not only  functional (with- function s/m/ools laloeling the const|tuents instead of  category names like Noun Phrase and VP) but i t  is  multifunctional.	VP	verb phrases$verb phrase$VBD PRT$VBP$verb  phrase$Verb Phrase$Verb Phras$Verb Pair$VP:NP$verbal parent$V "$VP:P$	5
3 Outline  3.1 Corpus, Language Usage and Computa- ble Semantic Properties of VPs  section  Basic Computational Semantic Con- cepts  Theory of Norm and Exploitation of  Language Usage   Corpus Pattern Analysis in  Sketch En- gine  Sense Discriminative Patterns  3.2   Semantic Types and Ontologies          Argument Structures         Frames and Semantic Types    Inducing Semantic Types  Discriminative Patterns  3.3 Statistical Models for Corpus Pattern  Recognition	VP	verb phrases$verb phrase$VBD PRT$VBP$verb  phrase$Verb Phrase$Verb Phras$Verb Pair$VP:NP$verbal parent$V "$VP:P$	5
4 .3  VP  Different from a noun phrase, a verb phrase may have  pre-modifiers and post-modifiers.	VP	verb phrases$verb phrase$VBD PRT$VBP$verb  phrase$Verb Phrase$Verb Phras$Verb Pair$VP:NP$verbal parent$V "$VP:P$	5
This can be summarized by the following  table:    Outside SimpleNP Exceptions  Prepositional Phrases  Relative Clauses  VPes  Apposition1  Some conjunctions  (Conjunctions are  marked according to the  TreeBank guidelines)2.	VP	verb phrases$verb phrase$VBD PRT$VBP$verb  phrase$Verb Phrase$Verb Phras$Verb Pair$VP:NP$verbal parent$V "$VP:P$	6
Presented at the  Linguistic Conference on East Asian Languages: VPes, in  Los Angeles, California. (	VP	verb phrases$verb phrase$VBD PRT$VBP$verb  phrase$Verb Phrase$Verb Phras$Verb Pair$VP:NP$verbal parent$V "$VP:P$	6
Robinson, A. E., "Determining VPe  Referents in Dialog~", American Journal of  Computatlor~l Linguistics", Jan. 1981  17.	VP	verb phrases$verb phrase$VBD PRT$VBP$verb  phrase$Verb Phrase$Verb Phras$Verb Pair$VP:NP$verbal parent$V "$VP:P$	6
The structure is not only  functional (with- function s/m/ools laloeling the const|tuents instead of  category names like Noun Phrase and VPe) but i t  is  multifunctional.	VP	verb phrases$verb phrase$VBD PRT$VBP$verb  phrase$Verb Phrase$Verb Phras$Verb Pair$VP:NP$verbal parent$V "$VP:P$	6
3 Outline  3.1 Corpus, Language Usage and Computa- ble Semantic Properties of VPes  section  Basic Computational Semantic Con- cepts  Theory of Norm and Exploitation of  Language Usage   Corpus Pattern Analysis in  Sketch En- gine  Sense Discriminative Patterns  3.2   Semantic Types and Ontologies          Argument Structures         Frames and Semantic Types    Inducing Semantic Types  Discriminative Patterns  3.3 Statistical Models for Corpus Pattern  Recognition	VP	verb phrases$verb phrase$VBD PRT$VBP$verb  phrase$Verb Phrase$Verb Phras$Verb Pair$VP:NP$verbal parent$V "$VP:P$	6
4 .3  VPe  Different from a noun phrase, a verb phrase may have  pre-modifiers and post-modifiers.	VP	verb phrases$verb phrase$VBD PRT$VBP$verb  phrase$Verb Phrase$Verb Phras$Verb Pair$VP:NP$verbal parent$V "$VP:P$	6
Algorithm for Generating  Clustering-based VPs 5 Results We explored a range of thresholds in the final stage of the algorithm.	VP	verb phrases$verb phrase$VBD PRT$VBP$verb  phrase$Verb Phrase$Verb Phras$Verb Pair$VP:NP$verbal parent$V "$VP:P$	7
Specifically, we add the feature types in this order: YAGO TYPE, YAGO MEANS, Noun Pairs, FrameNet, VPs, and Appositives.	VP	verb phrases$verb phrase$VBD PRT$VBP$verb  phrase$Verb Phrase$Verb Phras$Verb Pair$VP:NP$verbal parent$V "$VP:P$	7
Algorithms for Generating Non-clustering-based VPs scope of the other frame or if the semantic scopes SemFrame?s verb classes list specific LDOCE of the two frames have significant overlap.	VP	verb phrases$verb phrase$VBD PRT$VBP$verb  phrase$Verb Phrase$Verb Phras$Verb Pair$VP:NP$verbal parent$V "$VP:P$	7
3.2 Choice of VPs and Coverage To ensure a wide coverage of a variety of syntactico- semantic phenomena (C1), the choice of verb pairs is steered by two standard semantic resources available online: (1) the USF norms data set3 (Nelson et al, 2004), and (2) the VerbNet verb lexicon4 (Kipper et al.,	VP	verb phrases$verb phrase$VBD PRT$VBP$verb  phrase$Verb Phrase$Verb Phras$Verb Pair$VP:NP$verbal parent$V "$VP:P$	7
5 Conclusions We have examined the utility of three major sources of world knowledge for coreference resolu- tion, namely, large-scale knowledge bases (YAGO, FrameNet), coreference-annotated data (Noun Pairs, VPs), and unannotated data (Appositives), by applying them to two learning-based coreference models, the mention-pair model and the cluster- ranking model, and evaluating them on documents annotated with the ACE and OntoNotes annotation schemes.	VP	verb phrases$verb phrase$VBD PRT$VBP$verb  phrase$Verb Phrase$Verb Phras$Verb Pair$VP:NP$verbal parent$V "$VP:P$	7
Specifically, since each instance now corresponds to 819 an NP, NPk, and a preceding cluster, c, we can gener- ate a noun-pair-based feature by applying the above method to NPk and each of the NPs in c, and its value is the number of times it is applicable to NPk and c. 3.2.2 Features Based on VPs As discussed above, features encoding the seman- tic roles of two NPs and the relatedness of the asso- ciated verbs could be useful for coreference resolu- tion.	VP	verb phrases$verb phrase$VBD PRT$VBP$verb  phrase$Verb Phrase$Verb Phras$Verb Pair$VP:NP$verbal parent$V "$VP:P$	7
In the manner described in Section 5.1, we collected co-occurrence counts between a functor that the given it possesses concatenated with a lemma of its VP and a Czech counterpart having the same functor (denoted as csit).	VP	verb phrases$verb phrase$VBD PRT$VBP$verb  phrase$Verb Phrase$Verb Phras$Verb Pair$VP:NP$verbal parent$V "$VP:P$	9
TIEV: We add normal experts that tie all proba- bilities corresponding to a VP (any par- ent, using the coarse tags of Cohen et al, 2008).	VP	verb phrases$verb phrase$VBD PRT$VBP$verb  phrase$Verb Phrase$Verb Phras$Verb Pair$VP:NP$verbal parent$V "$VP:P$	9
Thus the three mentioned objects are defined as follows in English: new_A = regA "new" ; play_V2 = dirV2 (regVPplay") ; remove_V3 = dirV3 (regVPremove") from_Prep ; Here are the German definitions: 19 new_A = regA "neu" ; play_V2 = dirV2 (regVPspielen") ; remove_V3 = dirV3 (prefixVPheraus" nehmen_V) aus_Prep ; The lexicon definitions are gathered into a separate interface module, which the concrete syntax mod- ule depends on.	VP	verb phrases$verb phrase$VBD PRT$VBP$verb  phrase$Verb Phrase$Verb Phras$Verb Pair$VP:NP$verbal parent$V "$VP:P$	10
above) to: play_V2 = variants { dirV2 (regVPspielen") ; dirV2 (prefixVPab" (regVPspielen")) } ; the extended grammar is able to parse an utterance like ?	VP	verb phrases$verb phrase$VBD PRT$VBP$verb  phrase$Verb Phrase$Verb Phras$Verb Pair$VP:NP$verbal parent$V "$VP:P$	10
UBCAT* NOO('CONV* N)VPyarak ' ) ( '$UB*  A ' r~) )   "yan')C-AGR* 3S;G)(*C/LSE- NOl4)))  kez - Far \[5 CONS C93a cg3a Cg3a C22ad : ( "  e r "  ((*CAT* N)(*R* "yer ' )C*AGg* 3SG)(*CASE* NOH)))  burada - * \ [1\ ]  : ( ' . "	VP	verb phrases$verb phrase$VBD PRT$VBP$verb  phrase$Verb Phrase$Verb Phras$Verb Pair$VP:NP$verbal parent$V "$VP:P$	10
2.2 The Cluster Type Size Zipf Constraint The motivation behind using a Zipfian constraint is the following observation: when a certain statistic is known to affect the quality of the induced clus- tering and it is not explicitly manipulated by the al- gorithm, strong fluctuations in its values are likely to imply that there are UR fluctuations in the quality of the induced clusterings.	UR	uncontrolled$unlabeled recall$	0
An UR  pkediction (used by a canonical top-down parser )  i s  t o  s e l e c t  each such r u l e   as the expansion.	UR	uncontrolled$unlabeled recall$	0
In  PEDAGLOT, t h e  choice of which r u l e  t o  t r y  can be defined as  the  r e s u l t  of  the c a l l  t o  a 'choosef funct ion (or it can be l e f t  UR] , We have  des&ned various approaches t o  such predic t ions  (e.g., a l imi ted  key-word  scan of the incoming s t r i n g ,  and the use of 'language s t a t i s t i c s  such as the   s e t  of rules which can generate the next symbol i n  the  s t r i n g  as t h e i r  l e f t   most symbol).	UR	uncontrolled$unlabeled recall$	0
puts, but at its core is still an UR expo- nential algorithm.	UR	uncontrolled$unlabeled recall$	0
and  on the other hand the UR inclusion in the XML  document of free PCDATA strings, which are written in  a specific language.	UR	uncontrolled$unlabeled recall$	0
We report labeled and UR, preci- sion and F-scores for this experiment.	UR	uncontrolled$unlabeled recall$	1
The Petrov parser has better results by a sta- tistically significant margin for both labeled and UR and unlabeled F-score.	UR	uncontrolled$unlabeled recall$	1
But  2The figure indicates UR and preci-  sion.	UR	uncontrolled$unlabeled recall$	1
Most of the decrease in F1 is due to the drop in UR.	UR	uncontrolled$unlabeled recall$	1
Even with help from the true English trees, the unsu- pervised SITGs underperform PCFGs trained on as few as 32 sentences, with the exception of UR in one experiment.	UR	uncontrolled$unlabeled recall$	1
63.73 82.10 63.20 75.14 90.89 92.79 80.02 81.40 Table 6: Detailed results for SVM; T = transformation; P = unlabeled precision, R = UR costly to train (Sagae and Lavie, 2005).	UR	uncontrolled$unlabeled recall$	1
POS tagging: Each word is labelled with its POS  tag following the UT proposed by  Petrov et al (2011).	UT	Universal Tagset$utterance$Universal Theory$Unified Transition$	0
UMPOS employs a 12-tag UT introduced by Petrov et al. (	UT	Universal Tagset$utterance$Universal Theory$Unified Transition$	0
For the sake of comparability we ap- plied the split to the UT (Petrov et al.,	UT	Universal Tagset$utterance$Universal Theory$Unified Transition$	0
As the UT did not have a separate  118 category for NEs, we chose to label and classify  them as people, locations and organizations.	UT	Universal Tagset$utterance$Universal Theory$Unified Transition$	0
punctuation ADV adverb NUM numeral X everything else Table 1: The 12 tags of the UT.	UT	Universal Tagset$utterance$Universal Theory$Unified Transition$	0
In these chains of UTs, the speaker is not explicitly mentioned because the author relies on the shared understanding with the reader that adja- cent pieces of quoted speech are not independent (Zhang et al.,	UT	Universal Tagset$utterance$Universal Theory$Unified Transition$	1
First is the problem of iden- tifying anaphoric speakers, i.e., in the UT ?	UT	Universal Tagset$utterance$Universal Theory$Unified Transition$	1
Elson and McKeown (2010) describe and address two basic types of UT chains: (i) one-character chains, and (ii) intertwined chains.	UT	Universal Tagset$utterance$Universal Theory$Unified Transition$	1
is the problem of iden- tifying anaphoric speakers, i.e., in the UT ?	UT	Universal Tagset$utterance$Universal Theory$Unified Transition$	1
The second problem is resolving UT chains with implicit speakers.	UT	Universal Tagset$utterance$Universal Theory$Unified Transition$	1
2013) also iden- tified similar chains of UTs and addressed their attribution to characters using a model-based approach.	UT	Universal Tagset$utterance$Universal Theory$Unified Transition$	1
the speakers before their UTs.	UT	Universal Tagset$utterance$Universal Theory$Unified Transition$	1
Elson and McKeown (2010) describe and address two basic types of UT chains: (i) one-character chains, and (ii) intertwined	UT	Universal Tagset$utterance$Universal Theory$Unified Transition$	1
// Vocabulary Acquisition Device // theory-based lexicon L experience OO experience OO Figure 1: (a) The Model of Concepts from the Golden Oldies: used in the present Machine Learning Paradigm; (b) The UT Model of Concepts: necessary for deep lexical acquisition The new framework: UT We have much progress to make: We can de- scribe naive theories precisely; we can describe how theory acquisition occurs; we can describe the map from naive theories to a set of lexicalizable concepts.	UT	Universal Tagset$utterance$Universal Theory$Unified Transition$	2
For the SU component, a German corpus named SAMMIE (SAarbru?cken Multi-Modal Interface Experiment) was collected by Saarland University and DFKI2 using a Wizard of Oz experiment.	SU	speech understanding$system units$System utterance$Segmentation Units$	0
1 Introduction The SU component in a spoken dialogue system consists of an automatic speech recognition (ASR) component and a language un- derstanding (LU) component.	SU	speech understanding$system units$System utterance$Segmentation Units$	0
The correction of ill-formed input  using history-based expectation with  applications to SU."	SU	speech understanding$system units$System utterance$Segmentation Units$	0
Tina: a probabilistic syntactic  parser for SU systems.	SU	speech understanding$system units$System utterance$Segmentation Units$	0
Performance evaluation for SU  is being conducted with the ATIS corpus,  collected from subjects interacting with a  simulated (wizard-based) understanding system  that contains certain data from the Official  Airline Guide (OAG).	SU	speech understanding$system units$System utterance$Segmentation Units$	0
Erdman, L. D. Overview -- of the Hearsay SU research  (Computer Sc iences  Review 1974-1 975) .	SU	speech understanding$system units$System utterance$Segmentation Units$	0
Multiple SU contribute to  multiple model units.	SU	speech understanding$system units$System utterance$Segmentation Units$	1
However, in the h igh   level dialogue description SU S-goals  see/~1 to be more important, while at the low levels  C-goal seem to prevail.	SU	speech understanding$system units$System utterance$Segmentation Units$	1
For example, in an evaluation session an  assessor judged SU S1.1 and S10.4 as  sharing some content with model unit M2.2.	SU	speech understanding$system units$System utterance$Segmentation Units$	1
We believe that metrics  have to take coverage score (C, Section 4.1.1)  into consideration to be reasonable since most  of the content sharing among SU and  model units is partial.	SU	speech understanding$system units$System utterance$Segmentation Units$	1
It also allow s assessors to step  through each model unit, mark all SU  sharing content with the current model unit,  and specify that the marked SU                                                 3 Does a summary follow the rule of English  grammatical rules independent of its content?	SU	speech understanding$system units$System utterance$Segmentation Units$	1
SUs are appropriate.	SU	speech understanding$system units$System utterance$Segmentation Units$	2
Table 2: Dihana database statistics (mean for the five cross-validation partitions) Training Test Dialogues 720 180 Turns 12,330 3,083 User turns 5,024 1,256 System turns 7,206 1,827 Utterances 18,837 4,171 User utterances 7,773 1,406 SUs 11,064 2,765 Running words 162,613 40,765 User running words 42,806 10,815 System running words 119,807 29,950 Vocabulary 832 485 User vocabulary 762 417 System vocabulary 208 174 A cross-validation approach was adopted in Di- hana as well.	SU	speech understanding$system units$System utterance$Segmentation Units$	2
It either waits 500 ms after a SU to speak or interrupts 200 ms after the System con- firms an misrecognized word, which is in line with human reaction time (Fry, 1975).	SU	speech understanding$system units$System utterance$Segmentation Units$	2
The feature set is described by three main subsets: 25 system-utterance-level binary features4 derived from the system dialogue act(s) in the last turn; 17 user-utterance-level binary features5 derived from (a) what the user heard prior to the current turn, or (b) what keywords the system recognised in its 4SU features: heardAck, heardCantHelp, heardExample, heardExplConf, heardGoBackDAT, heard- Hello, heardImplConf, heardMoreBuses, heardRequest, heardRestartDAT, heardSchedule, heardSorry, heardDate, heardFrom, heardRoute, heardTime, heardTo, heardNext, heardPrevious, heardGoBack, heardChoices, heardRestart, heardRepeat, heardDontKnow, lastSystemDialActType.	SU	speech understanding$system units$System utterance$Segmentation Units$	2
SUs have no problem.	SU	speech understanding$system units$System utterance$Segmentation Units$	2
ation  that the user wants to book a room on Wednesday  this week has the highest priority, and then after  that, the interpretation that the user wants to book  a room on Wednesday next week has the highest  Dialogue ) C  s~,,~ Control ontext  Utterance I Response Understanding  (ISSS method) Generation  Wor  /  hypotheses/ ~ i o n   I  peec "eco nition I I   eoc  o uction I  l \  User utterance SU  Figure 3: Architecture of the experimental systems.	SU	speech understanding$system units$System utterance$Segmentation Units$	2
Lin (1999) uses the SUB test 2 and mutual information (MI) to determine the compositionality of the phrase.	SUB	substitution$substance$	0
The numerical value assigned by the func-  tion to a pair of segments i referred to as the cost,  or penalty, of SUB.	SUB	substitution$substance$	0
Section 2.2))  A 'SUB' test can be used to confirm that  NPs that stand in anaphoric relations to NPs of  these types do not corefer with them.	SUB	substitution$substance$	0
For in-  stance~ one may observe that Every man loves  his mother does not mean the same as Every  man loves every man's mother, contrasting with  referring NPs, which do allow such SUBs  (e.g., John loves his mother equals John loves  John's mother).	SUB	substitution$substance$	0
The function's values  for SUBs are listed in the "penalty" column  in Table 2.	SUB	substitution$substance$	0
They report an av- 2 The SUB test aims to replace part of the idiom?s component words with semantically similar words, and test how the co-occurrence frequency changes.	SUB	substitution$substance$	0
Blood is not  tagged as 27 (SUB) or as 13 (food), though it might  well be considered as such in certain contexts.	SUB	substitution$substance$	1
By empirical inspection, concrete nouns are hyponyms of the WordNet synsets physical object.n.01, matter.n.03, or SUB.n.04.	SUB	substitution$substance$	1
Restriction WordNet Synsets animate animate being.n.01 people.n.01 person.n.01 concrete physical object.n.01 matter.n.01 SUB.n.01 organization social group.n.01 district.n.01 Table 2: DAVID?s mappings between some common VerbNet restriction types and WordNet synsets.	SUB	substitution$substance$	1
First, there is information concerning patients, such as the changes in Mr. X?s blood pressure over the past three days, or the SUBs to which Ms. Y is allergic.	SUB	substitution$substance$	1
Blood is not  tagged as 27 (SUB) or as 13 (food), though it might  well be consider	SUB	substitution$substance$	1
ct, animal, man-made artifact, attributes, body parts,  .... SUB, and time), and 15 tags for verbs (from  grooming and dressing verbs, to verbs of weather).	SUB	substitution$substance$	1
to create a general semantic tagging scheme and  to apply it on a large scale: every set of synonymous  senses, synsets, are tagged with one of 45 tags as Word-  Net version 1.5 ~. In their schema, there are3 tags for  adjectives (relational adjectives, participial adjectives  and all others), 1 tag for all adverbs, 26 tags for nouns  (act, animal, man-made artifact, attributes, body parts,  .... SUB, and time), and 15 tags for verbs (from  grooming and dressing verbs, to verbs of weather).	SUB	substitution$substance$	1
Likewise, in Example 4, the paper refers to a certain SUB from made and pulp, and therefore it is a mass noun.	SUB	substitution$substance$	1
Multiple Regression  and the ANOVA and Covariance.	ANOVA	Analysis of Variance$analysis of variance$	0
We carry out an one-way ANOVA which shows significant differences in score as a function of system (p < 0.05, paired t-test).	ANOVA	Analysis of Variance$analysis of variance$	0
Dimension F value Probability r2  1 111.9 p < .001 84.3%  2 32.3 p K .001 60.8%  3 31.9 p < .001 60.5%  4 4.2 p K .001 16.9%  5 28.8 p < .001 58.0%  the results of an ANOVA showing that the registers are significant dis-  criminators for each dimension, and the r 2 values show their strength (r 2 is a direct  measure of the percentage of variation in the dimension score that can be predicted  on the basis of the register distinctions).	ANOVA	Analysis of Variance$analysis of variance$	0
The ANOVA, Wiley- Interscience.	ANOVA	Analysis of Variance$analysis of variance$	0
More references were required to achieve optimal performance for ROUGE based on longer n-grams (using the Kruskal-Wallis test, a non-parametric ANOVA, p < 2.2 ?	ANOVA	Analysis of Variance$analysis of variance$	1
We measured the reliability of the judges in the black  box evaluation by using an ANOVA technique  described in \[16\].	ANOVA	Analysis of Variance$analysis of variance$	1
GAMMs, as implemented in the mgcv package 1.7-28, offer three important advantages for the analysis of EEG data compared to standard linear models and ANOVA.	ANOVA	Analysis of Variance$analysis of variance$	1
We performed an ANOVA to de-  tect significant differences in accuracy consider-  ing algorithm, collocational property, and fea-  ture organization.	ANOVA	Analysis of Variance$analysis of variance$	1
A 3-way ANOVA in- cluding non-filtered scores for proportional thresh- old resulted in F [4, 235] = 31.82, p < 0.01 for WD scores.	ANOVA	Analysis of Variance$analysis of variance$	1
Table 3 shows results from the ANOVA for 10The interpretation of the N-N results in terms of spread- ing activation is also rejected by Hare et al(2009, 163).	ANOVA	Analysis of Variance$analysis of variance$	1
Due to their performative na- ture, we assume that these DAs make the turn they belong to a good candidate for a cor- responding edit-turn-pair.	DA	Differentiable Adjectiv$dialog act$Directed accuracy$definite articles$dative$da$Dependency accuracy$descriptor array$Dialectal Arabic$dialogue act$dialog acts$direct answer$Dialog act$Discovery Assistants$	1
Next a subset of the agent/user telephone calls were transcribed and annotated with simple DAs, and from these the two user models were estimated.	DA	Differentiable Adjectiv$dialog act$Directed accuracy$definite articles$dative$da$Dependency accuracy$descriptor array$Dialectal Arabic$dialogue act$dialog acts$direct answer$Dialog act$Discovery Assistants$	1
Actually, we deviate from Grosz and Sidner here and  keep the recent subDAive near the top of the stack.	DA	Differentiable Adjectiv$dialog act$Directed accuracy$definite articles$dative$da$Dependency accuracy$descriptor array$Dialectal Arabic$dialogue act$dialog acts$direct answer$Dialog act$Discovery Assistants$	1
As future work, we are plan- ning to test our model on DA classification and multimodal behavior recognition tasks.	DA	Differentiable Adjectiv$dialog act$Directed accuracy$definite articles$dative$da$Dependency accuracy$descriptor array$Dialectal Arabic$dialogue act$dialog acts$direct answer$Dialog act$Discovery Assistants$	1
The re- ward function can include distinct costs for differ- ent diagnostic tests, DAions, and for success- ful/unsuccessful task completion.	DA	Differentiable Adjectiv$dialog act$Directed accuracy$definite articles$dative$da$Dependency accuracy$descriptor array$Dialectal Arabic$dialogue act$dialog acts$direct answer$Dialog act$Discovery Assistants$	1
Lexical, prosodic and syntactic cures for DAs.	DA	Differentiable Adjectiv$dialog act$Directed accuracy$definite articles$dative$da$Dependency accuracy$descriptor array$Dialectal Arabic$dialogue act$dialog acts$direct answer$Dialog act$Discovery Assistants$	1
Ferschke et al (2012) suggest four types of explicit perfor- matives in their annotation scheme for DAs of Wikipedia turns.	DA	Differentiable Adjectiv$dialog act$Directed accuracy$definite articles$dative$da$Dependency accuracy$descriptor array$Dialectal Arabic$dialogue act$dialog acts$direct answer$Dialog act$Discovery Assistants$	1
27.8 25.7 8.6 BH 67.5 55.3 44.9 45.6 58.6 27.6 62.7 Mu 29.3 42.4 38.8 51.8 34.5 30.5 33.0 R 12.9 9.4 16.1 21.1 12.1 15.7 2.7 Mc 60.7 47.4 39.9 45.8 36.5 33.9 44.3 RB 17.9 12.4 26.2 36.5 15.3 25.4 1.1 H 19.4 29.3 12.2 22.2 17.3 20.9 10.3 DMVp 54.7 42.0 30.7 30.1 28.9 25.4 24.3 S2 45.2 41.9 44.2 49.8 39.7 25.4 63.8 Mp 67.8 54.3 49.6 59.4 47.7 37.7 49.7 S1 43.1 47.9 36.3 46.7 27.9 23.5 7.6 Table 6: DA results on the Penn treebank, stratified by dependency relation.	DA	Differentiable Adjectiv$dialog act$Directed accuracy$definite articles$dative$da$Dependency accuracy$descriptor array$Dialectal Arabic$dialogue act$dialog acts$direct answer$Dialog act$Discovery Assistants$	2
l l l l l l l l l 2 4 6 8 10 20 30 40 50 60 70 edge length direc ted a ccura cy (%) l TuBCBHMZ?pS?2DMV?p Figure 1: DA on the Penn treebank strat- ified by dependency length.	DA	Differentiable Adjectiv$dialog act$Directed accuracy$definite articles$dative$da$Dependency accuracy$descriptor array$Dialectal Arabic$dialogue act$dialog acts$direct answer$Dialog act$Discovery Assistants$	2
DA is the ratio of cor- rectly predicted dependencies (including direction) over total amount of predicted dependencies.	DA	Differentiable Adjectiv$dialog act$Directed accuracy$definite articles$dative$da$Dependency accuracy$descriptor array$Dialectal Arabic$dialogue act$dialog acts$direct answer$Dialog act$Discovery Assistants$	2
functional 59.6 52.4 41.5 47.4 36.2 30.6 40.0 58.5 20.9 37.2 20.6 19.3 29.2 23.7 lexical 40.6 41.9 28.5 37.3 24.8 27.7 35.5 39.5 23.5 23.1 23.0 14.4 33.1 10.1 oldLTH 41.4 43.6 28.8 37.8 24.6 28.6 36.1 39.5 22.3 23.7 21.8 14.3 32.0 10.7 standard 56.0 50.4 41.0 50.3 37.5 32.8 42.5 55.5 22.3 33.5 21.8 18.4 31.4 20.4 best 59.6 52.4 41.5 50.3 37.5 32.8 42.5 58.5 23.5 37.2 23.0 19.3 33.1 23.7 Table 5: DA results measured against different conversions of the Penn Treebank into dependency trees.	DA	Differentiable Adjectiv$dialog act$Directed accuracy$definite articles$dative$da$Dependency accuracy$descriptor array$Dialectal Arabic$dialogue act$dialog acts$direct answer$Dialog act$Discovery Assistants$	2
20 Universal Dependency Rules 1 HDP-DEP 71.9 50.4 No Rules (Random Init) 2 HDP-DEP 24.9 24.4 3 Headden III et al (2009) 68.8 - English-Specific Parsing Rules 4 Deterministic (rules only) 70.0 62.6 5 HDP-DEP 73.8 66.1 Druck et al (2009) Rules 6 Druck et al (2009) 61.3 - 7 HDP-DEP 64.9 42.2 Table 6: DA of our model (HDP-DEP) on sentences of length 10 or less and 20 or less from WSJ with different rulesets and with no rules, along with vari- ous baselines from the literature.	DA	Differentiable Adjectiv$dialog act$Directed accuracy$definite articles$dative$da$Dependency accuracy$descriptor array$Dialectal Arabic$dialogue act$dialog acts$direct answer$Dialog act$Discovery Assistants$	2
.3 32.0 10.7 standard 56.0 50.4 41.0 50.3 37.5 32.8 42.5 55.5 22.3 33.5 21.8 18.4 31.4 20.4 best 59.6 52.4 41.5 50.3 37.5 32.8 42.5 58.5 23.5 37.2 23.0 19.3 33.1 23.7 Table 5: DA results measured against different conversions of the Penn Treebank into dependency trees.	DA	Differentiable Adjectiv$dialog act$Directed accuracy$definite articles$dative$da$Dependency accuracy$descriptor array$Dialectal Arabic$dialogue act$dialog acts$direct answer$Dialog act$Discovery Assistants$	2
-ptb 75.2 69.8 69.4 73.8 67.2 64.1 71.6 69.8 49.8 67.0 49.6 44.8 53.9 68.1 portuguese 67.5 79.8 75.6 75.7 71.7 66.9 78.2 80.4 62.1 66.6 61.3 51.8 57.3 75.4 slovene 64.4 60.7 56.9 58.9 57.1 55.9 66.7 68.7 49.2 47.3 49.2 38.9 43.8 56.6 swedish 80.1 70.9 73.2 73.8 72.7 66.7 77.0 77.1 64.0 64.0 62.0 56.0 56.5 71.0 averages 71.6 63.1 68.0 68.6 66.9 63.6 68.6 72.1 59.1 59.9 57.6 50.3 57.6 68.1 Table 3: DA, undirected accuracy and NED results for the dependency task (using supplied POS).	DA	Differentiable Adjectiv$dialog act$Directed accuracy$definite articles$dative$da$Dependency accuracy$descriptor array$Dialectal Arabic$dialogue act$dialog acts$direct answer$Dialog act$Discovery Assistants$	2
The strength of Hirst's approach is his attempt o reduce the  presuppositional metric of Craln and Steedman to criteria manipul-  able by basic semantie/lexieal codings, and particularly the contrast  of definite and inDA.	DA	Differentiable Adjectiv$dialog act$Directed accuracy$definite articles$dative$da$Dependency accuracy$descriptor array$Dialectal Arabic$dialogue act$dialog acts$direct answer$Dialog act$Discovery Assistants$	3
Examples are the distinc- tion between definite and inDA, and the distinction between hyphens, slashes, left and right parentheses, quotation marks, and other sym- bols which the Tiger treebank annotates with ?	DA	Differentiable Adjectiv$dialog act$Directed accuracy$definite articles$dative$da$Dependency accuracy$descriptor array$Dialectal Arabic$dialogue act$dialog acts$direct answer$Dialog act$Discovery Assistants$	3
4.2 Adaptation and evaluation of HeidelTime: HTSwe The main modifications required in the adapta- tion of HeidelTime to Swedish (HTSwe) involved handling DA and plurals, e.g. adding eftermiddag(en)?(ar)?(na)? (?	DA	Differentiable Adjectiv$dialog act$Directed accuracy$definite articles$dative$da$Dependency accuracy$descriptor array$Dialectal Arabic$dialogue act$dialog acts$direct answer$Dialog act$Discovery Assistants$	3
2.1 General Web Corpora In a first attempt, we tried to obtain a general German HTML corpus using the mean- ingless query der die das, i.e., the three German DA.	DA	Differentiable Adjectiv$dialog act$Directed accuracy$definite articles$dative$da$Dependency accuracy$descriptor array$Dialectal Arabic$dialogue act$dialog acts$direct answer$Dialog act$Discovery Assistants$	3
Lexi-  cally reiterated items include repeated syn-  onymous noun phrases which may often be  preceded by DA or demon-  stratives.	DA	Differentiable Adjectiv$dialog act$Directed accuracy$definite articles$dative$da$Dependency accuracy$descriptor array$Dialectal Arabic$dialogue act$dialog acts$direct answer$Dialog act$Discovery Assistants$	3
But in abstract anaphora, English prefers demonstratives to personal pronouns and DA (Pas- sonneau, 1989; Navarretta, 2011).1 Demonstra- 1This is not to say that personal pronouns and definite arti- cles do not occur in abstract anaphora, but they are not common.	DA	Differentiable Adjectiv$dialog act$Directed accuracy$definite articles$dative$da$Dependency accuracy$descriptor array$Dialectal Arabic$dialogue act$dialog acts$direct answer$Dialog act$Discovery Assistants$	3
The inlransitive verbs.are  claksified accor .ding to semantic criteria (verbs of motion~ state) or  by their syntactic usa~ (like predicativi~ auxiliaries, urgpers6nal  verbs with DA ).W'e should-notice that he same verb maybe  transitiv 9 or intransitive, accor "ding to its m e.	DA	Differentiable Adjectiv$dialog act$Directed accuracy$definite articles$dative$da$Dependency accuracy$descriptor array$Dialectal Arabic$dialogue act$dialog acts$direct answer$Dialog act$Discovery Assistants$	4
takes three arguments (nomina- tive, accusative and DA cases), it is tagged with VB[nad].	DA	Differentiable Adjectiv$dialog act$Directed accuracy$definite articles$dative$da$Dependency accuracy$descriptor array$Dialectal Arabic$dialogue act$dialog acts$direct answer$Dialog act$Discovery Assistants$	4
We also added suffixes to verb tags to de- note which arguments they require (n:nominative, a:accusative and d: DA).	DA	Differentiable Adjectiv$dialog act$Directed accuracy$definite articles$dative$da$Dependency accuracy$descriptor array$Dialectal Arabic$dialogue act$dialog acts$direct answer$Dialog act$Discovery Assistants$	4
DA?.?	DA	Differentiable Adjectiv$dialog act$Directed accuracy$definite articles$dative$da$Dependency accuracy$descriptor array$Dialectal Arabic$dialogue act$dialog acts$direct answer$Dialog act$Discovery Assistants$	4
takes three arguments: nominative, accusative and DA cases.	DA	Differentiable Adjectiv$dialog act$Directed accuracy$definite articles$dative$da$Dependency accuracy$descriptor array$Dialectal Arabic$dialogue act$dialog acts$direct answer$Dialog act$Discovery Assistants$	4
To the contrary, subjects can occur in the ergative, nominative, DA, genitive, and instrumental cases.	DA	Differentiable Adjectiv$dialog act$Directed accuracy$definite articles$dative$da$Dependency accuracy$descriptor array$Dialectal Arabic$dialogue act$dialog acts$direct answer$Dialog act$Discovery Assistants$	4
Nev-  ertheless it can still give us guiDAnce on which  candiDAtes are more probable than others.	DA	Differentiable Adjectiv$dialog act$Directed accuracy$definite articles$dative$da$Dependency accuracy$descriptor array$Dialectal Arabic$dialogue act$dialog acts$direct answer$Dialog act$Discovery Assistants$	5
Here we would compare the degree to which  each possible candiDAte antecedent (A Japanese  company, television picture tubes, Japan, TV   sets, and Malaysia in this example) could serve  as the direct object of "export".	DA	Differentiable Adjectiv$dialog act$Directed accuracy$definite articles$dative$da$Dependency accuracy$descriptor array$Dialectal Arabic$dialogue act$dialog acts$direct answer$Dialog act$Discovery Assistants$	5
The first piece of useful information we con-  sider is the distance between the pronoun  and the candiDAte antecedent.	DA	Differentiable Adjectiv$dialog act$Directed accuracy$definite articles$dative$da$Dependency accuracy$descriptor array$Dialectal Arabic$dialogue act$dialog acts$direct answer$Dialog act$Discovery Assistants$	5
We collect these probabilities on the  training DAta, which are marked with reference  links.	DA	Differentiable Adjectiv$dialog act$Directed accuracy$definite articles$dative$da$Dependency accuracy$descriptor array$Dialectal Arabic$dialogue act$dialog acts$direct answer$Dialog act$Discovery Assistants$	5
Gener-  ally, a singular pronoun cannot refer to a plural  noun phrase, so that in resolving such a pro-  noun any plural candiDAtes should be ruled out.	DA	Differentiable Adjectiv$dialog act$Directed accuracy$definite articles$dative$da$Dependency accuracy$descriptor array$Dialectal Arabic$dialogue act$dialog acts$direct answer$Dialog act$Discovery Assistants$	5
91.97 96.81 Petrov I-5 85.30 85.87 85.58 92.00 92.61 92.31 96.65 Petrov I-6 84.86 85.46 85.16 91.79 92.44 92.11 96.65 Figure 12: DA on CCGbank dependencies on all sentences from section 00.	DA	Differentiable Adjectiv$dialog act$Directed accuracy$definite articles$dative$da$Dependency accuracy$descriptor array$Dialectal Arabic$dialogue act$dialog acts$direct answer$Dialog act$Discovery Assistants$	6
DA is the ratio of dependency relations correctly identified by the parser, while sentence accuracy is the exact match accuracy of complete dependency relations in a sentence.	DA	Differentiable Adjectiv$dialog act$Directed accuracy$definite articles$dative$da$Dependency accuracy$descriptor array$Dialectal Arabic$dialogue act$dialog acts$direct answer$Dialog act$Discovery Assistants$	6
0  10  20  30  40  50  60  70  80  90  100  80  82  84  86  88  90  92  94  96  98  100 Sen tenc e co vera ge (% ) DA (%) Figure 1: Accuracy-coverage curve on BIO rel dev.	DA	Differentiable Adjectiv$dialog act$Directed accuracy$definite articles$dative$da$Dependency accuracy$descriptor array$Dialectal Arabic$dialogue act$dialog acts$direct answer$Dialog act$Discovery Assistants$	6
Labeled % Unlabeled % Parser R P F R P F C&C Hybrid 84.71 86.35 85.52 90.96 92.72 91.83 Petrov I-5 85.50 86.08 85.79 92.12 92.75 92.44 p-value 0.005 0.189 0.187 < 0.001 0.437 0.001 Figure 13: DA on the section 00 sentences that receive an analysis from both parsers.	DA	Differentiable Adjectiv$dialog act$Directed accuracy$definite articles$dative$da$Dependency accuracy$descriptor array$Dialectal Arabic$dialogue act$dialog acts$direct answer$Dialog act$Discovery Assistants$	6
DA is de- fined as the ratio of correct dependencies over the to- tal number of dependencies in a sentence. (	DA	Differentiable Adjectiv$dialog act$Directed accuracy$definite articles$dative$da$Dependency accuracy$descriptor array$Dialectal Arabic$dialogue act$dialog acts$direct answer$Dialog act$Discovery Assistants$	6
611 Collins Head-Driven Statistical Models for NL Parsing Table 4 DA on section 0 of the treebank with Model 2.	DA	Differentiable Adjectiv$dialog act$Directed accuracy$definite articles$dative$da$Dependency accuracy$descriptor array$Dialectal Arabic$dialogue act$dialog acts$direct answer$Dialog act$Discovery Assistants$	6
Labeled % Unlabeled % Parser R P F R P F Cover C&C Normal Form 84.39 85.28 84.83 90.93 91.89 91.41 98.95 C&C Hybrid 84.53 86.20 85.36 90.84 92.63 91.73 98.95 Petrov I-0 79.87 78.81 79.34 87.68 86.53 87.10 96.45 Petrov I-4 84.76 85.27 85.02 91.69 92.25 91.97 96.81 Petrov I-5 85.30 85.87 85.58 92.00 92.61 92.31 96.65 Petrov I-6 84.86 85.46 85.16 91.79 92.44 92.11 96.65 Figure 12: DA on CCGbank dependencies on all sentences from section 00.	DA	Differentiable Adjectiv$dialog act$Directed accuracy$definite articles$dative$da$Dependency accuracy$descriptor array$Dialectal Arabic$dialogue act$dialog acts$direct answer$Dialog act$Discovery Assistants$	6
Labeled % Unlabeled % Parser R P F R P F C&C Hybrid 85.11 86.46 85.78 91.15 92.60 91.87 Petrov I-5 85.73 86.29 86.01 92.04 92.64 92.34 p-value 0.013 0.278 0.197 < 0.001 0.404 0.005 Figure 14: DA on the section 23 sentences that receive an analysis from both parsers.	DA	Differentiable Adjectiv$dialog act$Directed accuracy$definite articles$dative$da$Dependency accuracy$descriptor array$Dialectal Arabic$dialogue act$dialog acts$direct answer$Dialog act$Discovery Assistants$	6
249  Computational Linguistics Volume 23, Number 2  Figure 2  noun  article  conjunction  preposition  number  left parentheses  non-punctuation character  colon or dash  sentence-ending punctuation  verb  modifier  pronoun  proper noun  comma or semicolon  right parentheses  possessive  abbreviation  others  Elements of the DA assigned to each incoming token.	DA	Differentiable Adjectiv$dialog act$Directed accuracy$definite articles$dative$da$Dependency accuracy$descriptor array$Dialectal Arabic$dialogue act$dialog acts$direct answer$Dialog act$Discovery Assistants$	7
3.4 Classification by a Learning Algorithm  The DAs representing the tokens in the context are used as the input to  a machine learning algorithm.	DA	Differentiable Adjectiv$dialog act$Directed accuracy$definite articles$dative$da$Dependency accuracy$descriptor array$Dialectal Arabic$dialogue act$dialog acts$direct answer$Dialog act$Discovery Assistants$	7
In addition to the 18 category frequencies, the DA also contains two  additional f ags that indicate if the word begins with a capital etter and if it follows a  punctuation mark, for a total of 20 items in each DA.	DA	Differentiable Adjectiv$dialog act$Directed accuracy$definite articles$dative$da$Dependency accuracy$descriptor array$Dialectal Arabic$dialogue act$dialog acts$direct answer$Dialog act$Discovery Assistants$	7
3.3 Descriptor Array Construction  A vector, or DA, is constructed for each token in the input text.	DA	Differentiable Adjectiv$dialog act$Directed accuracy$definite articles$dative$da$Dependency accuracy$descriptor array$Dialectal Arabic$dialogue act$dialog acts$direct answer$Dialog act$Discovery Assistants$	7
The context vectors, which we call DAs, are input to a machine learn-  ing algorithm trained to disambiguate s ntence boundaries.	DA	Differentiable Adjectiv$dialog act$Directed accuracy$definite articles$dative$da$Dependency accuracy$descriptor array$Dialectal Arabic$dialogue act$dialog acts$direct answer$Dialog act$Discovery Assistants$	7
An important component of the Satz system is the lexicon contain-  ing part-of-speech frequency data from which the DAs are constructed.	DA	Differentiable Adjectiv$dialog act$Directed accuracy$definite articles$dative$da$Dependency accuracy$descriptor array$Dialectal Arabic$dialogue act$dialog acts$direct answer$Dialog act$Discovery Assistants$	7
c?2014 Association for Computational Linguistics Named Entity Recognition for DA Ayah Zirikly Department of Computer Science The George Washington University Washington DC, USA ayaz@gwu.edu Mona Diab Department of Computer Science The George Washington University Washington DC, USA mtdiab@gwu.edu Abstract To date, majority of research for Ara- bic Named Entity Recognition (NER) ad- dresses the task for Modern Standard Ara- bic (MSA) and mainly focuses on the n	DA	Differentiable Adjectiv$dialog act$Directed accuracy$definite articles$dative$da$Dependency accuracy$descriptor array$Dialectal Arabic$dialogue act$dialog acts$direct answer$Dialog act$Discovery Assistants$	8
40 6 Current and Future Work We have already utilized the dialect labels to identify dialectal sentences to be translated into English, in an effort to create a DA-to-English par- allel dataset (also taking a crowdsourcing approach) to aid machine translation of dialectal Arabic.	DA	Differentiable Adjectiv$dialog act$Directed accuracy$definite articles$dative$da$Dependency accuracy$descriptor array$Dialectal Arabic$dialogue act$dialog acts$direct answer$Dialog act$Discovery Assistants$	8
However, the dataset contains English and Arabic; in this work we only target DA.	DA	Differentiable Adjectiv$dialog act$Directed accuracy$definite articles$dative$da$Dependency accuracy$descriptor array$Dialectal Arabic$dialogue act$dialog acts$direct answer$Dialog act$Discovery Assistants$	8
2012) show that small amounts of data from the right dialect can have a dramatic impact on the quality of DA Machine Translation systems.	DA	Differentiable Adjectiv$dialog act$Directed accuracy$definite articles$dative$da$Dependency accuracy$descriptor array$Dialectal Arabic$dialogue act$dialog acts$direct answer$Dialog act$Discovery Assistants$	8
5 Conclusion & Future Work In this paper we present DA NER system using state-of-the-art features in addi- tion to proposing new features that improve the performance.	DA	Differentiable Adjectiv$dialog act$Directed accuracy$definite articles$dative$da$Dependency accuracy$descriptor array$Dialectal Arabic$dialogue act$dialog acts$direct answer$Dialog act$Discovery Assistants$	8
4.118 88.889 91.429 83.333 23.81 37.037 88.7255 56.3495 64.233 FEA4={FEA3, EM-GAZ} 94.118 88.889 91.429 72.222 30.952 43.333 83.17 59.9205 67.381 FEA5={FEA4, PM-GAZ} 94.118 88.889 91.429 73.684 33.333 45.902 83.901 61.111 68.666 FEA6={FEA5, LVM-GAZ} 94.118 88.889 91.429 78.947 35.714 49.18 86.533 62.302 70.305 FEA7={FEA6, BC} 93.333 77.778 84.849 77.778 33.333 46.667 85.556 55.556 65.758 Table 3: DA NER the test data.	DA	Differentiable Adjectiv$dialog act$Directed accuracy$definite articles$dative$da$Dependency accuracy$descriptor array$Dialectal Arabic$dialogue act$dialog acts$direct answer$Dialog act$Discovery Assistants$	8
discourse or dialogue generation  Franck Panaget*  Centre National d'Etudes des T@l@communications (CNET)  LAA/TSS/RCP  Route de Tr@gastel -- 22300 Lannion - France  email: panaget@lannion.cnet.fr / tel: +33 96.05.28.52 / fax: +33 96.05.35.30  Abst ract   A natural language generation system is typically con-  stituted by two main components: a content planning  component (e.g., text planner or DA planner)  and a linguistic realization component.	DA	Differentiable Adjectiv$dialog act$Directed accuracy$definite articles$dative$da$Dependency accuracy$descriptor array$Dialectal Arabic$dialogue act$dialog acts$direct answer$Dialog act$Discovery Assistants$	9
Sneaker's belief set  For every condition C in DA performed:  default_ascribe(Speaker, H arer, believe(C))  That is, for every condition in the speech act, the speaker  must ascribe abelief to the hearer that the condition is satis-  fied.	DA	Differentiable Adjectiv$dialog act$Directed accuracy$definite articles$dative$da$Dependency accuracy$descriptor array$Dialectal Arabic$dialogue act$dialog acts$direct answer$Dialog act$Discovery Assistants$	9
Furthermore, we will incorporate past work on DA tag- ging in chat (Wu et al, 2005) to both improve sum- marization and create a framework for the next two steps.	DA	Differentiable Adjectiv$dialog act$Directed accuracy$definite articles$dative$da$Dependency accuracy$descriptor array$Dialectal Arabic$dialogue act$dialog acts$direct answer$Dialog act$Discovery Assistants$	9
Instead, a theory of speech acLs should be solipsistic in  that it refers olely to finite belief representations of either  the speaker or hearer of the DA.	DA	Differentiable Adjectiv$dialog act$Directed accuracy$definite articles$dative$da$Dependency accuracy$descriptor array$Dialectal Arabic$dialogue act$dialog acts$direct answer$Dialog act$Discovery Assistants$	9
The update  rule for the heater is the converse of the speaker's:  Update on the He.arer's belie, f set  For every condition C in DA performed:  default ascribe(Hearer;Speaker, C)  That is, given that he speaker has performed an infolxn  act, the hearer can ascribe to the speaker the preconditions of  the inform act assuming that he speaker isbeing coopera-  tive.	DA	Differentiable Adjectiv$dialog act$Directed accuracy$definite articles$dative$da$Dependency accuracy$descriptor array$Dialectal Arabic$dialogue act$dialog acts$direct answer$Dialog act$Discovery Assistants$	9
To overcome this, we plan to use an unsupervised learn- ing approach to discover DAs (Ritter et al, 2010).	DA	Differentiable Adjectiv$dialog act$Directed accuracy$definite articles$dative$da$Dependency accuracy$descriptor array$Dialectal Arabic$dialogue act$dialog acts$direct answer$Dialog act$Discovery Assistants$	9
Due to their performative na- ture, we assume that these DA make the turn they belong to a good candidate for a cor- responding edit-turn-pair.	DA	Differentiable Adjectiv$dialog act$Directed accuracy$definite articles$dative$da$Dependency accuracy$descriptor array$Dialectal Arabic$dialogue act$dialog acts$direct answer$Dialog act$Discovery Assistants$	10
Next a subset of the agent/user telephone calls were transcribed and annotated with simple DA, and from these the two user models were estimated.	DA	Differentiable Adjectiv$dialog act$Directed accuracy$definite articles$dative$da$Dependency accuracy$descriptor array$Dialectal Arabic$dialogue act$dialog acts$direct answer$Dialog act$Discovery Assistants$	10
The average length of the topics (measured using the number of DA) among all the meetings is 172.5, with a high standard devi- ation of 236.8.	DA	Differentiable Adjectiv$dialog act$Directed accuracy$definite articles$dative$da$Dependency accuracy$descriptor array$Dialectal Arabic$dialogue act$dialog acts$direct answer$Dialog act$Discovery Assistants$	10
These logs include information about the system DA, the N-best speech recognition hypotheses, and the hypothesized interpretation (including confidence estimates) of the user?s spo- ken utterances as provided by the dialog system?s Spoken Language Understanding (SLU) module.	DA	Differentiable Adjectiv$dialog act$Directed accuracy$definite articles$dative$da$Dependency accuracy$descriptor array$Dialectal Arabic$dialogue act$dialog acts$direct answer$Dialog act$Discovery Assistants$	10
Lexical, prosodic and syntactic cures for DA.	DA	Differentiable Adjectiv$dialog act$Directed accuracy$definite articles$dative$da$Dependency accuracy$descriptor array$Dialectal Arabic$dialogue act$dialog acts$direct answer$Dialog act$Discovery Assistants$	10
Ferschke et al (2012) suggest four types of explicit perfor- matives in their annotation scheme for DA of Wikipedia turns.	DA	Differentiable Adjectiv$dialog act$Directed accuracy$definite articles$dative$da$Dependency accuracy$descriptor array$Dialectal Arabic$dialogue act$dialog acts$direct answer$Dialog act$Discovery Assistants$	10
This increased the need for Question Answering (QA) systems that provide the users with DAs to their questions.	DA	Differentiable Adjectiv$dialog act$Directed accuracy$definite articles$dative$da$Dependency accuracy$descriptor array$Dialectal Arabic$dialogue act$dialog acts$direct answer$Dialog act$Discovery Assistants$	11
Max Planck Institute for Informatics myahya@mpi-inf.mpg.de Steven Euijong Whang, Rahul Gupta, Alon Halevy Google Research {swhang,grahul,halevy}@google.com Abstract Search engines are increasingly relying on large knowledge bases of facts to provide DAs to users?	DA	Differentiable Adjectiv$dialog act$Directed accuracy$definite articles$dative$da$Dependency accuracy$descriptor array$Dialectal Arabic$dialogue act$dialog acts$direct answer$Dialog act$Discovery Assistants$	11
In both cases, users can ask  natural language questions about, the contents of the  texts, and the system responds with DAs  along with the original text.	DA	Differentiable Adjectiv$dialog act$Directed accuracy$definite articles$dative$da$Dependency accuracy$descriptor array$Dialectal Arabic$dialogue act$dialog acts$direct answer$Dialog act$Discovery Assistants$	11
This paper describes a sys-  tem that attempts to retrieve a much smaller section  of text, namely, a DA to a user's question.	DA	Differentiable Adjectiv$dialog act$Directed accuracy$definite articles$dative$da$Dependency accuracy$descriptor array$Dialectal Arabic$dialogue act$dialog acts$direct answer$Dialog act$Discovery Assistants$	11
This would deliver DAs to the query rather than having the user sort through list of keyword results.	DA	Differentiable Adjectiv$dialog act$Directed accuracy$definite articles$dative$da$Dependency accuracy$descriptor array$Dialectal Arabic$dialogue act$dialog acts$direct answer$Dialog act$Discovery Assistants$	11
6 Evaluation 6.1 The Evaluation Settings Evaluation has been carried out to determine whether Topic Indexing and Retrieval using a sim- ple and efficient IR technique for DA re- trieval can indeed make for an accurate QA sys- tem.	DA	Differentiable Adjectiv$dialog act$Directed accuracy$definite articles$dative$da$Dependency accuracy$descriptor array$Dialectal Arabic$dialogue act$dialog acts$direct answer$Dialog act$Discovery Assistants$	11
DA (Abbreviation)  acceptance (ace)  query (query)  rejection (rej)  request comment (re-c)  request suggestion (re-s)  statement (state)  date/loc, suggestion (sag)  miscellaneous (mist)  Example  That would be \[ine  1)o you know ttamburg  This is /,oo late for mc  Is that possible  When wouM it be ok  Right, it's a Tuesday  \[ propose April 13th  So long, bye  Table I: DAs and e	DA	Differentiable Adjectiv$dialog act$Directed accuracy$definite articles$dative$da$Dependency accuracy$descriptor array$Dialectal Arabic$dialogue act$dialog acts$direct answer$Dialog act$Discovery Assistants$	12
DA (Abbreviation)  acceptance (ace)  query (query)  rejection (rej)  request comment (re-c)  request suggestion (re-s)  statement (state)  date/loc, suggestion (sag)  miscellaneous (mist)  Example  That would be \[ine  1)o you know ttamburg  This is /,oo late for mc  Is that possible  When wouM it be ok  Right, it's a Tuesday  \[ propose April 13th  So long, bye  Table I: DAs and examples  For example, in our example turu below there  arc several utterances and each of them has a par-  ticular dialog act as shown below.	DA	Differentiable Adjectiv$dialog act$Directed accuracy$definite articles$dative$da$Dependency accuracy$descriptor array$Dialectal Arabic$dialogue act$dialog acts$direct answer$Dialog act$Discovery Assistants$	12
The content word over- 1 Manual annotations for DA tags and adjacency pairs are available for the AMI corpus.	DA	Differentiable Adjectiv$dialog act$Directed accuracy$definite articles$dative$da$Dependency accuracy$descriptor array$Dialectal Arabic$dialogue act$dialog acts$direct answer$Dialog act$Discovery Assistants$	12
DAs  aCC  state  misc  query  r~j  sug  re-c  re-s  Total  'Ih:aining Test  88.9 72.0  90.0 90.9  54.5 73.7  40.0 0.0  9\]..7 85.7  90.3 92.9  0.0 0.0  90.0 82.4  82.0 79.4  Table 6: Performance of simple recurrent network  with dialog plausibility vectors in percent  Table 6 shows the results for our training and  test utterances.	DA	Differentiable Adjectiv$dialog act$Directed accuracy$definite articles$dative$da$Dependency accuracy$descriptor array$Dialectal Arabic$dialogue act$dialog acts$direct answer$Dialog act$Discovery Assistants$	12
DAs in this domain consist of a list of component acts of the form acttype(slot=value) where the slot=value pair is optional.	DA	Differentiable Adjectiv$dialog act$Directed accuracy$definite articles$dative$da$Dependency accuracy$descriptor array$Dialectal Arabic$dialogue act$dialog acts$direct answer$Dialog act$Discovery Assistants$	12
Meeting recorder project: DA labeling guide.	DA	Differentiable Adjectiv$dialog act$Directed accuracy$definite articles$dative$da$Dependency accuracy$descriptor array$Dialectal Arabic$dialogue act$dialog acts$direct answer$Dialog act$Discovery Assistants$	12
1.3 DA, Cooperative Observatories  The development of interactive nvironments for  monolingnal writers leads to modelling new functions for  documentation, self-documentation, self-learning and  management of indivitlualized personal knowledge bases,  to bc pooled into opcn encyclopaedic 'discovery  environments', pecific components for NLP systems.	DA	Differentiable Adjectiv$dialog act$Directed accuracy$definite articles$dative$da$Dependency accuracy$descriptor array$Dialectal Arabic$dialogue act$dialog acts$direct answer$Dialog act$Discovery Assistants$	13
Our research aims at developing such Linguistic  DA by merging hyperdocumcnts, Data  Base Management Systems and interpretive adaptive  interfaces.	DA	Differentiable Adjectiv$dialog act$Directed accuracy$definite articles$dative$da$Dependency accuracy$descriptor array$Dialectal Arabic$dialogue act$dialog acts$direct answer$Dialog act$Discovery Assistants$	13
The ASR  hypothesis is first classified into domain-specific  DAs (DA).	DAs	Domain Acts$Dialog acts$Dialogue Acts$	0
1 10.4 5.7 58.1 10.4 WORD-UNG 43.1 29.1 34.7 63.0 39.5 48.6 PN,MN,FV,DA 66.7 48.8 56.4 72.3 54.7 62.3 PN,MN,DA 64.5 46.5 54.1 75.8 58.1 65.8 LN,PN,MN,FV 64.4 44.2 52.4 65.2 50.0 56.6 Table 2: Results Class Imbalance Handling: InstWeight: Instance weighting and SigThresh: Sigmoid thresholding Features: WORD-UNG: Word unigrams, LN: Lemma ngrams, PN: POS ngrams, MN: Mixed ngrams, FV: First verb, DA: DAs bution on positive and negative training instances.	DAs	Domain Acts$Dialog acts$Dialogue Acts$	1
Dialog act (Abbreviation)  acceptance (ace)  query (query)  rejection (rej)  request comment (re-c)  request suggestion (re-s)  statement (state)  date/loc, suggestion (sag)  miscellaneous (mist)  Example  That would be \[ine  1)o you know ttamburg  This is /,oo late for mc  Is that possible  When wouM it be ok  Right, it's a Tuesday  \[ propose April 13th  So long, bye  Table I: DAs and examples  For example, in our example turu below there  arc several utterances and each of them has a par-  ticular dialog act as shown below.	DAs	Domain Acts$Dialog acts$Dialogue Acts$	1
Topics are DAs while documents are utterances ; we used the S-Space Package http://code.google.com/p/ airhead-research/wiki/RandomIndexing 9.	DAs	Domain Acts$Dialog acts$Dialogue Acts$	1
DAs, according to the classic speech act theory (Austin, 1962; Searle, 1969), represent the meaning of an utterance at the level of illocutionary force, i.e. a dialog act label con- cisely characterizes the intention and the role of a contribution in a dialog.	DAs	Domain Acts$Dialog acts$Dialogue Acts$	1
N-Best List of DAs Let?s Go Score Bayesian Score inform(from=mill street) 7.8E-4 3.5998527E-16 inform(from=mission street) 0.015577 3.5998527E-16 inform(from=osceola street) 0.0037 3.5998527E-16 inform(from=robinson township) 0.007292 3.5998527E-16 inform(from=sheraden station) 0.001815 3.1346254E-8 inform(from=brushton) 2.45E-4 3.5998527E-16 inform(from=jefferson) 0.128727 0.0054255757 inform(from=mck	DAs	Domain Acts$Dialog acts$Dialogue Acts$	2
Predicting DAs for a Speech-To-Speech  Translation System.	DAs	Domain Acts$Dialog acts$Dialogue Acts$	2
N-Best List of DAs Let?s Go Score Bayesian Score inform(route=46a) 3.33E-4 1.9236763E-6 inform(route=46b) 1.0E-6 1.5243509E-16 inform(route=46d) 0.096107 7.030841E-4 inform(route=46k) 0.843685 4.9941495E-10 silence() NA 0 User input: ?	DAs	Domain Acts$Dialog acts$Dialogue Acts$	2
c?2009 Association for Computational Linguistics Unsupervised Classification of DAs using a Dirichlet Process Mixture Model Nigel Crook, Ramon Granell, and Stephen Pulman Oxford University Computing Laboratory Wolfson Building Parks Road, OXFORD, UK nigc@comlab.ox.ac.uk ramg@comlab.ox.ac.uk sgp@clg.ox.ac.uk Abstract In recent years DAs have be- come a popular means of modelling the communicative intentions of human and machine utterances in many modern di-	DAs	Domain Acts$Dialog acts$Dialogue Acts$	2
3 Re-Ranking DAs Using Multiple Bayesian Networks Figure 1 shows an illustration of our dialogue act re-ranker within a pipeline architecture.	DAs	Domain Acts$Dialog acts$Dialogue Acts$	2
References   Adreani, G., Di Fabbrizio, G., Gilbert, M., Gillick, D.,  Hakkani-Tur, D., and Lemon, O., 2006 Let?s DiS- CoH: Collecting an Annotated Open Corpus with  DAs and Reward Signals for Natural Lan- guage Helpdesk, in Proceedings of IEEE SLT-2006  Workshop, Aruba Beach, Aruba.	DAs	Domain Acts$Dialog acts$Dialogue Acts$	2
N-Best List of DAs Let?s Go Score Bayesian Score inform(from=mill street) 7.8E-4 3.5998527E-16 inform(from=mission street) 0.0	DAs	Domain Acts$Dialog acts$Dialogue Acts$	2
Verbs that take dative rather than OAs are a particu- lar problem, such as mistaking accusative for dative feminine objects (10.6% of occurrences) or dative for accusative feminine objects (11.9%).	OA	accusative object$Obligatory Adjoining$of a$	0
the rele- vant point here is that the gap between the results (23% for sub- jects, 35% for OAs) warrants further attention in the context of comparing parsing results across corpora.	OA	accusative object$Obligatory Adjoining$of a$	0
and OA such as ?	OA	accusative object$Obligatory Adjoining$of a$	0
OA?	OA	accusative object$Obligatory Adjoining$of a$	0
The frames in GermaNet use comple-mentation codes provided with the German version of the CELEX Lexical Database (Baayen et al, 2005) such as NN.AN for transitive verbs with OAs.	OA	accusative object$Obligatory Adjoining$of a$	0
In order to find structures like (1) in a Ger-  man corpus, one needs to search for  (a) a prepositional phrase modifying the ac-  cusative object and preceding the finite  verb (i.e. in the so-called vorfeld), and  (b) an OA between finite verb  and infinite verb forms (i.e. in the so-  called rnittelfeld)  Obviously, two things need to be available  in order to enable such a search.	OA	accusative object$Obligatory Adjoining$of a$	0
Most no-  tably, the Hobbs algorithm depends on the ex-  istence OAn/~" parse-tree node that is absent  from the Penn Tree-bank trees.	OA	accusative object$Obligatory Adjoining$of a$	2
In particular, the scheme infers the gender OA  referent from the gender of the pronouns that  161  refer to it and selects referents using the pro-  noun anaphora program.	OA	accusative object$Obligatory Adjoining$of a$	2
We also ob-  serve that the position OA pronoun in a story  influences the mention count of its referent.	OA	accusative object$Obligatory Adjoining$of a$	2
We present some typ-  ical results as well as the more rigorous results  OA blind evaluation of its output.	OA	accusative object$Obligatory Adjoining$of a$	2
However a singular noun phrase can be the ref-  erent OA plural pronoun, as illustrated by the  following example:  "I think if I tell Viacom I need more  time, they will take 'Cosby' across the  street," says the general manager ol a  network a~liate.	OA	accusative object$Obligatory Adjoining$of a$	2
When viewed in this way, a can be regarded as  an index into these vectors that specifies which  value is relevant o the particular choice OAn-  tecedent.	OA	accusative object$Obligatory Adjoining$of a$	2
ap-afzk durum  INT-clear situation  'Very clear situation'  uzun yol  long road  ' l ong  road'  bu- ndan ba~ka  this-ABLATIVE other  'other than this'  ktz kedi-yi gSr-dii  girl cat-ACC see-TENSE  or   ktz g6rdii kediyi  'The girl saw the cat'  Figure 2: OPs in the proposed model.	OP	Operator$prepositional object$overparsing$	0
Table 3: Paraphrasability of OPs Paraphrase pair P ?? (	OP	Operator$prepositional object$overparsing$	0
OP Morp.	OP	Operator$prepositional object$overparsing$	0
OPs occur with different frequencies.	OP	Operator$prepositional object$overparsing$	0
3 Mu l t i -domain  Combinat ion   OP   Oehrle (1988) describes a model of multi-dimen-  sional composition in which every domain Di has  an algebra with a finite set of primitive operations  1Derived and basic categories in the examples are in  fact feature structures; ee section 4.	OP	Operator$prepositional object$overparsing$	0
Component-wise OPs Mitchell and Lap- ata (2008) investigate using component-wise op- erators to combine the vectors of verbs and their intransitive subjects.	OP	Operator$prepositional object$overparsing$	0
So, for example the wh-NP "whom" in COMP-  position can only be coindexed with an empty NP  bearingthe grammatical function of direct object or  OP for example.	OP	Operator$prepositional object$overparsing$	1
He argues with her arg1 arg2 (9) 6In the ID trees, pobj stands for a OP, pcomp for the complement of a preposition, pmod for a prepositional modifier, and det for a determiner.	OP	Operator$prepositional object$overparsing$	1
Consider for example (17) where the  OP (realized as where) may be merged  into the clause he put the book by a Ioncl-distance  movement equation or by a simple equation of the form  (t  FOCUS) = (~ ON).	OP	Operator$prepositional object$overparsing$	1
For OPs, the semantic type of the noun within the  prepositional phrase in also indicated as one of the semantic noun class-  es mentioned earlier.	OP	Operator$prepositional object$overparsing$	1
For example if we assume that in English  adjuncts follow OPs, for the sentence  in (11) the parser develops 19 readings whereas without  a coherence-check it had to pursue 42 different paths.	OP	Operator$prepositional object$overparsing$	1
In TPP, the lexicographer develops three pieces of information about each sense: a semantic relation name, the properties of the OP, and the properties of the word to which the prepositional phrase is attached.	OP	Operator$prepositional object$overparsing$	1
To the extent hat underparsing and OP are  avoided, the description is said to be faithful to the  input.	OP	Operator$prepositional object$overparsing$	2
Thus, a block of cells  (the set of cells each covering the same input sub-  string) is interdependent with respect o OP  operations, meaning that an OP operation  trying to fill one cell in the block is adding structure  to a partial description from a different cell in the  same block.	OP	Operator$prepositional object$overparsing$	2
The first consequence of this is that the  OP operations must be considered after the  underparsing and parsing operations for that block.	OP	Operator$prepositional object$overparsing$	2
The second consequence is that OP oper-  ations may need to be considered more than once,  because the result of one OP operation (if it  fills a cell) could be the source	OP	Operator$prepositional object$overparsing$	2
There are three main types of opera-  tions, corresponding to underparsing, parsing, and  OP actions.	OP	Operator$prepositional object$overparsing$	2
Crucial to Optimality Theory are faithful-  ness constraints, which are violated by underparsing  and OP.	OP	Operator$prepositional object$overparsing$	2
PD for Multilingual Word Sense Disambiguation.	PD	Peripheral Diversity$Precision$predicate$predicates$	0
150 80% 74%  > 150 74% 70%  Table 8: PD of the ATR method   (with the usage of a stoplist)    The majority of remaining errors originate  from the ambiguous POS tagging (more than  50%, problematic words being naziv(a), igra,  kod, etc.).	PD	Peripheral Diversity$Precision$predicate$predicates$	1
50  55  60  65  70  75  80  85  90  95  100  0  10  20  30  40  50  60  70  80  90  100 Pe rc e n ta ge Window Size PD Recall F-Measure   Figure 6: Relationship between the window size of the  snippet and recall/precision on the John Smith corpus.	PD	Peripheral Diversity$Precision$predicate$predicates$	1
PD-focused textual inference.	PD	Peripheral Diversity$Precision$predicate$predicates$	1
150 52% 58%  > 150 69% 68%  Table 6: PD of the ATR method   (without the usage of a stoplist)    In the first 50 terms for the domain of mathe- matical analysis, there was only one false term  candidate (specijalna klasa neprekidnih pres- likavanja), which contained an ?	PD	Peripheral Diversity$Precision$predicate$predicates$	1
PD is the proportion of mentions in HC that are  also in TC and recall is the proportion of mentions in TC  that are also in HC.	PD	Peripheral Diversity$Precision$predicate$predicates$	1
Figure 2: PD using honorific scoring  scheme with syntactic Hobbs algorithm  for the last-noun method and 70.3% for the  Hobbs method.	PD	Peripheral Diversity$Precision$predicate$predicates$	1
Although adjectives are PDs, they have a much more limited distribution than verbs, and do not present long-distance dependencies.	PD	Peripheral Diversity$Precision$predicate$predicates$	2
The same applies to event adjectives, this time being PDs (flipping coin - a coin flips).	PD	Peripheral Diversity$Precision$predicate$predicates$	2
They are further characterised by not occur- ing as PDs (low value for -1ve).	PD	Peripheral Diversity$Precision$predicate$predicates$	2
Adjectives are PDs, equivalent to verbs when appearing in predicative environments.	PD	Peripheral Diversity$Precision$predicate$predicates$	2
The other main function of the adjective is that of PD in a copular sentence (6% of the tokens).	PD	Peripheral Diversity$Precision$predicate$predicates$	2
The arity is a basic parameter for the seman- tic characterisation of any PD.	PD	Peripheral Diversity$Precision$predicate$predicates$	2
Although adjectives are PD, they have a much more limited distribution than verbs, and do not present long-distance dependencies.	PD	Peripheral Diversity$Precision$predicate$predicates$	3
The same applies to event adjectives, this time being PD (flipping coin - a coin flips).	PD	Peripheral Diversity$Precision$predicate$predicates$	3
Verbless clauses: nouns, adjectives, and adverbs, lexical or derived, functioning as PD ?	PD	Peripheral Diversity$Precision$predicate$predicates$	3
They are further characterised by not occur- ing as PD (low value for -1ve).	PD	Peripheral Diversity$Precision$predicate$predicates$	3
Adjectives are PD, equivalent to verbs when appearing in predicative environments.	PD	Peripheral Diversity$Precision$predicate$predicates$	3
The SE problem is quite similar to the  Information Extraction (IE) task, in that in both cases  we are interested only in certain PD and their  argument bindings and not in full understanding.	PD	Peripheral Diversity$Precision$predicate$predicates$	3
The narrate  verbs require OCjects or objects of normalization phrases, the basic structures  are ?	OC	clausal ob$Clausal Object$clausal object$	0
NP+[AG]+[NP+VP]+VP(NAR), or [NP+VP] +NP +[AG]+VP(NAR), in which the  OCjects of citation verbs unusually attaches clause particle ?	OC	clausal ob$Clausal Object$clausal object$	0
For each verb, each grammatical role ( stC`A is the set of such roles) is instantiated from the stock of all con- stituents ( /8u wv x`v5y , which includes all np and pp constituents but also the verbs as potential heads of OCjects).	OC	clausal ob$Clausal Object$clausal object$	0
Note that in multiclausal  constructions there is the corresponding subordination  of different OCliqueness hlerarchles (for the salve  or comparalgility with diagrams (3) involving time  arrow, Hasse dm~ams for obliqueness are displayed  with a turn of 90~right):  (8) Kim said Lee saw Max.	OC	clausal ob$Clausal Object$clausal object$	0
but there is no frame that combines a subject, a prepositional object and a OCject.	OC	clausal ob$Clausal Object$clausal object$	0
The worst example in our tests was a verb that receives from the maxent classifier two sub- jects and three OCjects.	OC	clausal ob$Clausal Object$clausal object$	0
The narrate  verbs require OCs or objects of normalization phrases, the basic structures  are ?	OC	clausal ob$Clausal Object$clausal object$	2
verbal predicates with a pronominal subject  and a OC  In order to determine whether our results hold  on sample documents of smaller size, we con- ducted a second round of experiments where  document length was scaled down to five sen- tences per document.	OC	clausal ob$Clausal Object$clausal object$	2
NP+[AG]+[NP+VP]+VP(NAR), or [NP+VP] +NP +[AG]+VP(NAR), in which the  OCs of citation verbs unusually attaches clause particle ?	OC	clausal ob$Clausal Object$clausal object$	2
For each verb, each grammatical role ( stC`A is the set of such roles) is instantiated from the stock of all con- stituents ( /8u wv x`v5y , which includes all np and pp constituents but also the verbs as potential heads of OCs).	OC	clausal ob$Clausal Object$clausal object$	2
but there is no frame that combines a subject, a prepositional object and a OC.	OC	clausal ob$Clausal Object$clausal object$	2
The worst example in our tests was a verb that receives from the maxent classifier two sub- jects and three OCs.	OC	clausal ob$Clausal Object$clausal object$	2
Deter- mining the correct label for a single textual en- tailment example requires human analysts to make many smaller, LOC decisions which may de- pend on each other.	LOC	localized$locative$LOCATION$locus$locations$Location$	0
Each latent concept is then modeled as a LOC Gaussian distribution over the embedding space.	LOC	localized$locative$LOCATION$locus$locations$Location$	0
Our approach is to augment model training compared to the clean baseline by adding non-English, mixed-language, and non-language material, and to augment the model?s feature set with language- identification features more LOC than the sentence-level perplexity described above, as well as other features designed primarily to distinguish non- language material such as mark-up codes.	LOC	localized$locative$LOCATION$locus$locations$Location$	0
These are generic systems that can be LOC to specific domains.	LOC	localized$locative$LOCATION$locus$locations$Location$	0
3.2 Linguistic events  Russian gaps are LOC to second conjugation  non-past verb forms, so productions of these forms  are the focus of interest.	LOC	localized$locative$LOCATION$locus$locations$Location$	0
One of the two Twitter datasets is primarily LOC to the United States, while the remaining datasets cover the whole world.	LOC	localized$locative$LOCATION$locus$locations$Location$	0
Nouns are inflected based  on number (singular, plural), article and case [k?- raka] (nominative, accusative, instrumental, dative,  ablative, genitive, LOC and vocative).	LOC	localized$locative$LOCATION$locus$locations$Location$	1
Phenomenon Occurrence Agreement Named Entity 91.67% 0.856 LOC 17.62% 0.623 Numerical Quantity 14.05% 0.905 temporal 5.48% 0.960 nominalization 4.05% 0.245 implicit relation 1.90% 0.651 Table 4: Occurrence statistics for hypothesis struc- ture features.	LOC	localized$locative$LOCATION$locus$locations$Location$	1
The problem arises during the parsing  process with the fact that the " to"  argument of  "prefer" in Italian may occur before the verb, and the  LOC preposition " in"  is a, the same word as the  marking preposition corresponding to "to".	LOC	localized$locative$LOCATION$locus$locations$Location$	1
LOC-, con- junctive-, and objective particles) ?? (	LOC	localized$locative$LOCATION$locus$locations$Location$	1
Typical target positions  for transitions corresponding to noun phrase mod-  ification (noun phrases are head-final in Chinese)  are as follows:  head: 0 ( f l ight )   nominal: -1 (a i r l ine)   ad ject ive:  -3 (cheap)  possessive: -5 (Continental~s)  relative: -6 (that leaves NYC)  LOC: -8 (from NYC)  temporal: -9 (before one pm)  classifier: -I0 (pint)  specifier: -11 (a11)  cardinal: -II (five)  ordinal: -11 (first)  DE: -2, -4, or -6  The position for transitions emitting the Chi-  nese particle pronounced DE may be either -2, -4,  or -6, depending on the transducer states for the  transition.	LOC	localized$locative$LOCATION$locus$locations$Location$	1
But even the ostensibly disambiguating prepo-  sition by, is itself ambiguous, since it might introduce  a manner or LOC phrase consistent with the main  clause analysis.	LOC	localized$locative$LOCATION$locus$locations$Location$	1
n+ 1 2 )2 (1) In order to make pairwise comparisons between samples and to establish the LOC of the difference, we rely on the Mann?Whitney test.	LOC	localized$locative$LOCATION$locus$locations$Location$	3
Biological proof PMID 1744050 : DNA sequencing established that spoIIIF and spoVB are a single monocistronic LOC encoding a 518-amino-acid polypeptide with features of an integral membrane protein.	LOC	localized$locative$LOCATION$locus$locations$Location$	3
C11b Subfact: p19ARF induces cell cycle arrest in a p53-dependent manner INK4a/ARF is perhaps the second most commonly disrupted LOC in cancer cells.	LOC	localized$locative$LOCATION$locus$locations$Location$	3
In  what follows then we will concentrate on terms  and their meaning as expressed in definitions,  the typical LOC tbr conceptual meaning  information.	LOC	localized$locative$LOCATION$locus$locations$Location$	3
The lesson learned can then be  used to LOC on what knowledge needs malmal an-  notation.	LOC	localized$locative$LOCATION$locus$locations$Location$	3
In this way, reformulations iden- tify the LOC of any error, and hence the existence of an error.	LOC	localized$locative$LOCATION$locus$locations$Location$	3
A constraint that de- scribes this individual word pair would be trivial to write, but it is not feasible to model the general phenomenon in this way; thousands of constraints would be needed just to reflect the more impor- tant colLOC in a language, and the exact set of collocating words is impossible to predict ac- curately.	LOC	localized$locative$LOCATION$locus$locations$Location$	4
In another  approach, she used regular expression patterns to  extract term colLOC from a morpho- syntactically tagged corpus.	LOC	localized$locative$LOCATION$locus$locations$Location$	4
The ex- tracted colLOC were filtered with a stop- word list, and only colLOC containing sin- gle-word terms (devised previously by bilingual  alignment) were accepted as relevant.	LOC	localized$locative$LOCATION$locus$locations$Location$	4
For ex- ample, Vintar (2000) presented two methods for  extraction of terminological colLOC in order  to assist the translation process in Slovene.	LOC	localized$locative$LOCATION$locus$locations$Location$	4
Extracting Terms and Terminological  ColLOC from the ELAN Slovene-English Par- allel Corpus.	LOC	localized$locative$LOCATION$locus$locations$Location$	4
defined as follows: RuleID: ON; TriggeringCondition: (DMOVE:specifyCommand, TYPE:SwitchOn); DeclareExpectations: { LOC, DeviceType } ActionsExpectations: { [DeviceType] => {NLG(DeviceType);} } PostActions: { ExecuteAction(@is-ON); } The DTAC obtained for switch on triggers the dialogue rule ON.	LOC	localized$locative$LOCATION$locus$locations$Location$	5
LOC prediction in social media based on tie strength.	LOC	localized$locative$LOCATION$locus$locations$Location$	5
However, since two declared expectations are still missing (LOC and De- viceType), the dialogue manager will activate the ActionExpectations and prompt the user for the kind of device she wants to switch on, by means of a call to the natural language generation mod- ule NLG(DeviceType).	LOC	localized$locative$LOCATION$locus$locations$Location$	5
Using our detailed dependency annotations we can also determine how many instances need addi- 175 LOC Main Fact Subfact Synonym Extra Title 3.3 ( 0.2) 1.9 ( 0.7) 0.0 ( 0.0) 0.8 ( 0.8) Abstract 19.1 (10.1) 9.3 ( 5.1) 36.2 (21.7) 25.8 (14.8) Introduction 11.3 ( 5.2) 8.3 ( 3.4) 30.4 (17.4) 17.2 ( 7.8) Results 31.0 (13.8) 37.6 (16.1) 20.3 (15.9) 32.0 (12.5) Discussion 21.8 ( 7.3) 19.5 ( 6.6) 2.9 ( 1.4) 9.4 ( 3.1) Figure Heading 5.0 ( 0.6) 10.7 ( 3.8) 1.4 ( 1.4) 2.3 ( 0.0) Figure Legend 3.1 (	LOC	localized$locative$LOCATION$locus$locations$Location$	5
However, since two declared expectations are still missing (LOC and De- viceType), the dialogue manager will activate the ActionExpectations and prompt the user for the kind of device she wants to switch on, b	LOC	localized$locative$LOCATION$locus$locations$Location$	5
FrameNet II contains                                                    5 http://www.wjh.harvard.edu/~inquirer/homecat.htm  Table 1: Example of opinion related frames  and lexical units  Frame  name Lexical units Frame elements  Desiring want, wish, hope,  eager, desire,  interested,  Event,  Experiencer,  LOC_of_event Emotion _directed agitated, amused,  anguish, ashamed,  angry, annoyed,  Event, Topic  Experiencer,  Expressor,  Mental  _property absurd, brilliant,  careless, crazy,  cunning, foolish  Behavior,  Protagonist,  Domain, Degree  Subject  _stimulus delightful, amazing,  annoying, amusing,  aggravating,  Stimulus, Degree Experiencer,  Circumstances,     Figure 1: An overview of our	LOC	localized$locative$LOCATION$locus$locations$Location$	5
DMOVE specifyCommand TYPE SwitchOn ARGS [ LOC, DeviceType ] META INFO ?	LOC	localized$locative$LOCATION$locus$locations$Location$	5
defined as follows: RuleID: ON; TriggeringCondition: (DMOVE:specifyCommand, TYPE:SwitchOn); DeclareExpectations: { LOC, DeviceType } ActionsExpectations: { [DeviceType] => {NLG(DeviceType);} } PostActions: { ExecuteAction(@	LOC	localized$locative$LOCATION$locus$locations$Location$	5
salience(re/)  = -2  LO  Making the unrealistic simplifying assumption  that references of one gender class are com-  pletely indepe	LO	log$Lexical Overlap$	0
salience(re/)  = -2  LO  Making the unrealistic simplifying assumption  that references of one gender class are com-  pletely independent of references for another  classes 1, the likelihood function in this case is  just the product over all classes of the probabil-  ities of each class of reference to the power of  the number of observations of this class.	LO	log$Lexical Overlap$	0
2006), gen- erate diaLOs uttered by different characters using synthetic voices appropriate for each character?s gender, age and personality (Greene et al.,	LO	log$Lexical Overlap$	0
HatzivassiLOlou and McKeown, 1997)  for another application in which no explicit  indicators are available in the stream).	LO	log$Lexical Overlap$	0
This decision is made by ranking the refer-  ents by LO-likelihood ratio, termed salience, for  each referent.	LO	log$Lexical Overlap$	0
Shared story reading with parents or teachers helps children to learn about vocabulary, syntax and phonoLOy, and to develop narrative comprehension and awareness of the concepts of print, all of which are linked to developing reading and writing skills (National Early Literacy Panel 2008).	LO	log$Lexical Overlap$	0
Vasileios HatzivassiLOlou and Kathleen R.  McKeown.	LO	log$Lexical Overlap$	0
Textual Entailment Through Ex- tended LO.	LO	log$Lexical Overlap$	1
Literary News Subtitles Parallel Text Articles Corpus # Tokens 1879 3379 1632 1581 # Unique Tokens 811 1473 824 609 # Shared Tokens 519 1125 402 354 LO 72.5 82.9 63.2 62.7 LO 68.4 67.2 48.6 45 (lem.	LO	log$Lexical Overlap$	1
Textual Entailment through Ex- tended LO and Lexico-Semantic  Matching.	LO	log$Lexical Overlap$	1
c?2007 Association for Computational Linguistics Textual Entailment Through Extended LO and Lexico-Semantic Matching Rod Adams, Gabriel Nicolae, Cristina Nicolae and Sanda Harabagiu Human Language Technology Research Institute University of Texas at Dallas Richardson, Texas {rod, gabriel, cristina, sanda}@hlt.utdallas.edu Abstract This paper presents two systems for textual entailment, both employing decision trees as a supervised learning algorithm.	LO	log$Lexical Overlap$	1
6.4 Evaluating Summaries using LO ROUGE (Lin 2004) is a package for automatically evaluating summaries.	LO	log$Lexical Overlap$	1
P e t r i c k ,  Trans fo rmat iona l  Analysis, II  I n  NLP, ed.	NLP	Natural  Language Processing$Natural Language Processing$natural anguage processing$natural language processor$Natural language processin$Natural language processing$Natural Language Processin$natural language processin$natural language processing$	0
3.3 NLP Tools  We used the N I,P COml)onents of kl/ l ' - .	NLP	Natural  Language Processing$Natural Language Processing$natural anguage processing$natural language processor$Natural language processin$Natural language processing$Natural Language Processin$natural language processin$natural language processing$	0
204  Bruce and Wiebe Decomposable Model ing in NLP  Data Set Naive Model  Word Number (10% used as a test Entropy Bayes Selection MS - NB Majority Best  of Senses set on each fold) (NB) (MS) Classifier Model  Tagged Word  Instances Count  sick 14 659 15066 2.969 56.8 65.1 +8.3 30.8 67.4  storm 18 752 20806 2.895 63.4 71.6 +8.2 39.6 73.6  drift 17 520 13484 2.889 56.0 63.3 +7.3 31.7 66.0  curious 3 459 12950 0.833 83.0	NLP	Natural  Language Processing$Natural Language Processing$natural anguage processing$natural language processor$Natural language processin$Natural language processing$Natural Language Processin$natural language processin$natural language processing$	0
In J. Vicedo, P. Martnez- Barco, R. Muoz, and M. Saiz Noeda, editors, Ad- vances in NLP, volume 3230 of Lecture Notes in Computer Science, pages 82?90.	NLP	Natural  Language Processing$Natural Language Processing$natural anguage processing$natural language processor$Natural language processin$Natural language processing$Natural Language Processin$natural language processin$natural language processing$	1
He  has worked on Machine Learning in the context of NLP and  has published papers in several conferences.	NLP	Natural  Language Processing$Natural Language Processing$natural anguage processing$natural language processor$Natural language processin$Natural language processing$Natural Language Processin$natural language processin$natural language processing$	1
In Proceedings of the Human Language Technology Conference and Conference on Empirical Methods in NLP, pages 779?786, Vancouver.	NLP	Natural  Language Processing$Natural Language Processing$natural anguage processing$natural language processor$Natural language processin$Natural language processing$Natural Language Processin$natural language processin$natural language processing$	1
In Proceedings of 2010 Workshop on Do- main Adaptation for NLP.	NLP	Natural  Language Processing$Natural Language Processing$natural anguage processing$natural language processor$Natural language processin$Natural language processing$Natural Language Processin$natural language processin$natural language processing$	1
940  Proceedings of the 2016 Conference on Empirical Methods in NLP, pages 950?954, Austin, Texas, November 1-5, 2016.	NLP	Natural  Language Processing$Natural Language Processing$natural anguage processing$natural language processor$Natural language processin$Natural language processing$Natural Language Processin$natural language processin$natural language processing$	1
Role-playing games  System by Microsoft Research (Kacmarcik 2005) 61   Proceedings of the 2016 Conference on Empirical Methods in NLP, pages 846?855, Austin, Texas, November 1-5, 2016.	NLP	Natural  Language Processing$Natural Language Processing$natural anguage processing$natural language processor$Natural language processin$Natural language processing$Natural Language Processin$natural language processin$natural language processing$	1
CONCI.USIONS  There are many important sources of infor-  mation for NLP.	NLP	Natural  Language Processing$Natural Language Processing$natural anguage processing$natural language processor$Natural language processin$Natural language processing$Natural Language Processin$natural language processin$natural language processing$	2
& Whitney R.A., A general organization of ~he  knowledge for NLP: the 1lehman  Upper Model, ISI research report, California, 1990.	NLP	Natural  Language Processing$Natural Language Processing$natural anguage processing$natural language processor$Natural language processin$Natural language processing$Natural Language Processin$natural language processin$natural language processing$	2
1 In t roduct ion   This paper describes a new discourse module within  our multilingual NLP system  which has been used for understanding texts in En-  glish, Spanish and Japanese (el.	NLP	Natural  Language Processing$Natural Language Processing$natural anguage processing$natural language processor$Natural language processin$Natural language processing$Natural Language Processin$natural language processin$natural language processing$	2
t r   Abst rac t   Automatic text tagging is an important  component in higher level analysis of text  corpora, and its output can be used in  many NLP applica-  tions.	NLP	Natural  Language Processing$Natural Language Processing$natural anguage processing$natural language processor$Natural language processin$Natural language processing$Natural Language Processin$natural language processin$natural language processing$	2
It should be noted that  all the processing steps, those performed by the backbone system,  and those performed by the NLP com-  ponents, are fully automated, and no human intervention rmanual  encoding isrequired.	NLP	Natural  Language Processing$Natural Language Processing$natural anguage processing$natural language processor$Natural language processin$Natural language processing$Natural Language Processin$natural language processin$natural language processing$	2
This environment is based on a  set of NLP compo-  nents, at the morphologic, syntactic and  semantic levels.	NLP	Natural  Language Processing$Natural Language Processing$natural anguage processing$natural language processor$Natural language processin$Natural language processing$Natural Language Processin$natural language processin$natural language processing$	2
We believe that we have presented a viable approach to the automatic generation  of a NLP.	NLP	Natural  Language Processing$Natural Language Processing$natural anguage processing$natural language processor$Natural language processin$Natural language processing$Natural Language Processin$natural language processin$natural language processing$	3
CONCLUSIONS The LOLITA system is a NLP that has a core functionality on which a number of different applications can be built.	NLP	Natural  Language Processing$Natural Language Processing$natural anguage processing$natural language processor$Natural language processin$Natural language processing$Natural Language Processin$natural language processin$natural language processing$	3
To interpret a comparative xpression (CE) a  'NLP must determine (1)  the entities to he compared, and (2) the at-  tribute(s) of those entities to consider in per-  forming the comparison.	NLP	Natural  Language Processing$Natural Language Processing$natural anguage processing$natural language processor$Natural language processin$Natural language processing$Natural Language Processin$natural language processin$natural language processing$	3
We believe  -that automatic, or semi-automatic acquisition of  the lexicon is a critical factor in determining how  widespread the use of NLPs  will be in the next few years. '	NLP	Natural  Language Processing$Natural Language Processing$natural anguage processing$natural language processor$Natural language processin$Natural language processing$Natural Language Processin$natural language processin$natural language processing$	3
We believe that this approach to machine learning of  a NLP that involves a 1/uman in-  formant in an elicit-generate-test loop and uses scaffold-  ing provided by the human informant in machine learn-  ing, is a very viable approach that avoids the noise and  opaqueness of other induction schemes.	NLP	Natural  Language Processing$Natural Language Processing$natural anguage processing$natural language processor$Natural language processin$Natural language processing$Natural Language Processin$natural language processin$natural language processing$	3
A Method for the Acquisition and Interpretation of a Semantic Lexicon  Our research on lexical acquisition from corpora started in 1988, when a first version  of the system was built as utility for the DANTE NLP (Velardi  1989), a system that analyzes press agency releases on finance and economics.	NLP	Natural  Language Processing$Natural Language Processing$natural anguage processing$natural language processor$Natural language processin$Natural language processing$Natural Language Processin$natural language processin$natural language processing$	3
INTRODUCTION  NLPg systems  need much larger lexicons than those  available today.	NLP	Natural  Language Processing$Natural Language Processing$natural anguage processing$natural language processor$Natural language processin$Natural language processing$Natural Language Processin$natural language processin$natural language processing$	4
1 Introduction  1.1 Genera l i t ies   NLPg is nowadays trongly  related to Cognitive Science, since linguistics, psy-  chology and computer science have to collaborate  to produce systems that are useful for man-machine  communication.	NLP	Natural  Language Processing$Natural Language Processing$natural anguage processing$natural language processor$Natural language processin$Natural language processing$Natural Language Processin$natural language processin$natural language processing$	4
NLPg (almost) from scratch.	NLP	Natural  Language Processing$Natural Language Processing$natural anguage processing$natural language processor$Natural language processin$Natural language processing$Natural Language Processin$natural language processin$natural language processing$	4
\[Shinghal 92\] Shinghal R., NLPg:  a prescriptive grammar, In Formal conceptions in Arti-  ficial Intelligence, pp 131-232, Chapman & Hall, 1992.	NLP	Natural  Language Processing$Natural Language Processing$natural anguage processing$natural language processor$Natural language processin$Natural language processing$Natural Language Processin$natural language processin$natural language processing$	4
INTRODUCTION  NLP systems  need much larger lexicons than those  available today.	NLP	Natural  Language Processing$Natural Language Processing$natural anguage processing$natural language processor$Natural language processin$Natural language processing$Natural Language Processin$natural language processin$natural language processing$	5
1 Introduction  1.1 Genera l i t ies   NLP is nowadays trongly  related to Cognitive Science, since linguistics, psy-  chology and computer science have to collaborate  to produce systems that are useful for man-machine  communication.	NLP	Natural  Language Processing$Natural Language Processing$natural anguage processing$natural language processor$Natural language processin$Natural language processing$Natural Language Processin$natural language processin$natural language processing$	5
NLP (almost) from scratch.	NLP	Natural  Language Processing$Natural Language Processing$natural anguage processing$natural language processor$Natural language processin$Natural language processing$Natural Language Processin$natural language processin$natural language processing$	5
\[Shinghal 92\] Shinghal R., NLP:  a prescriptive grammar, In Formal conceptions in Arti-  ficial Intelligence, pp 131-232, Chapman & Hall, 1992.	NLP	Natural  Language Processing$Natural Language Processing$natural anguage processing$natural language processor$Natural language processin$Natural language processing$Natural Language Processin$natural language processin$natural language processing$	5
In J. Vicedo, P. Martnez- Barco, R. Muoz, and M. Saiz Noeda, editors, Ad- vances in NLPg, volume 3230 of Lecture Notes in Computer Science, pages 82?90.	NLP	Natural  Language Processing$Natural Language Processing$natural anguage processing$natural language processor$Natural language processin$Natural language processing$Natural Language Processin$natural language processin$natural language processing$	6
He  has worked on Machine Learning in the context of NLPg and  has published papers in several conferences.	NLP	Natural  Language Processing$Natural Language Processing$natural anguage processing$natural language processor$Natural language processin$Natural language processing$Natural Language Processin$natural language processin$natural language processing$	6
In Proceedings of the Human Language Technology Conference and Conference on Empirical Methods in NLPg, pages 779?786, Vancouver.	NLP	Natural  Language Processing$Natural Language Processing$natural anguage processing$natural language processor$Natural language processin$Natural language processing$Natural Language Processin$natural language processin$natural language processing$	6
In Proceedings of 2010 Workshop on Do- main Adaptation for NLPg.	NLP	Natural  Language Processing$Natural Language Processing$natural anguage processing$natural language processor$Natural language processin$Natural language processing$Natural Language Processin$natural language processin$natural language processing$	6
940  Proceedings of the 2016 Conference on Empirical Methods in NLPg, pages 950?954, Austin, Texas, November 1-5, 2016.	NLP	Natural  Language Processing$Natural Language Processing$natural anguage processing$natural language processor$Natural language processin$Natural language processing$Natural Language Processin$natural language processin$natural language processing$	6
Role-playing games  System by Microsoft Research (Kacmarcik 2005) 61   Proceedings of the 2016 Conference on Empirical Methods in NLPg, pages 846?855, Austin, Texas, November 1-5, 2016.	NLP	Natural  Language Processing$Natural Language Processing$natural anguage processing$natural language processor$Natural language processin$Natural language processing$Natural Language Processin$natural language processin$natural language processing$	6
2 Motivation PP attachment disambiguation has often been studied as a benchmark test for empirical meth- ods in NLPg.	NLP	Natural  Language Processing$Natural Language Processing$natural anguage processing$natural language processor$Natural language processin$Natural language processing$Natural Language Processin$natural language processin$natural language processing$	7
Mitigating the paucity- of-data problem: Exploring the effect of training corpus size on classifier performance for NLPg.	NLP	Natural  Language Processing$Natural Language Processing$natural anguage processing$natural language processor$Natural language processin$Natural language processing$Natural Language Processin$natural language processin$natural language processing$	7
Roth has published broadly in machine learning, NLPg,  knowledge representation and reasoning and received several paper, teaching and  research awards.	NLP	Natural  Language Processing$Natural Language Processing$natural anguage processing$natural language processor$Natural language processin$Natural language processing$Natural Language Processin$natural language processin$natural language processing$	7
Web-based models for NLPg.	NLP	Natural  Language Processing$Natural Language Processing$natural anguage processing$natural language processor$Natural language processin$Natural language processing$Natural Language Processin$natural language processin$natural language processing$	7
In practical NLPg applic- ations the fan-out of the grammar is typically bounded by some small number.	NLP	Natural  Language Processing$Natural Language Processing$natural anguage processing$natural language processor$Natural language processin$Natural language processing$Natural Language Processin$natural language processin$natural language processing$	7
Nevertheless, as current NLPg 54 Molla?	NLP	Natural  Language Processing$Natural Language Processing$natural anguage processing$natural language processor$Natural language processin$Natural language processing$Natural Language Processin$natural language processin$natural language processing$	7
2 Motivation PP attachment disambiguation has often been studied as a benchmark test for empirical meth- ods in NLP.	NLP	Natural  Language Processing$Natural Language Processing$natural anguage processing$natural language processor$Natural language processin$Natural language processing$Natural Language Processin$natural language processin$natural language processing$	8
Mitigating the paucity- of-data problem: Exploring the effect of training corpus size on classifier performance for NLP.	NLP	Natural  Language Processing$Natural Language Processing$natural anguage processing$natural language processor$Natural language processin$Natural language processing$Natural Language Processin$natural language processin$natural language processing$	8
Roth has published broadly in machine learning, NLP,  knowledge representation and reasoning and received several paper, teaching and  research awards.	NLP	Natural  Language Processing$Natural Language Processing$natural anguage processing$natural language processor$Natural language processin$Natural language processing$Natural Language Processin$natural language processin$natural language processing$	8
Web-based models for NLP.	NLP	Natural  Language Processing$Natural Language Processing$natural anguage processing$natural language processor$Natural language processin$Natural language processing$Natural Language Processin$natural language processin$natural language processing$	8
In practical NLP applic- ations the fan-out of the grammar is typically bounded by some small number.	NLP	Natural  Language Processing$Natural Language Processing$natural anguage processing$natural language processor$Natural language processin$Natural language processing$Natural Language Processin$natural language processin$natural language processing$	8
Nevertheless, as current NLP 54 Molla?	NLP	Natural  Language Processing$Natural Language Processing$natural anguage processing$natural language processor$Natural language processin$Natural language processing$Natural Language Processin$natural language processin$natural language processing$	8
PS Metaphorical Literal0 1 Vs.	PS	Previous Sentence$prefer-  ence score$phrase structure$Phrase structure$point of speech$Plastic Surgeon$possible$	0
Ad- ditionally, PS Arg1 model perfor- mance is much lower than that of the other models due to the fact that it only considers immediately previous sentence; which, as was mentioned ear- lier, covers only 71.7% of the inter-sentential re- lations.	PS	Previous Sentence$prefer-  ence score$phrase structure$Phrase structure$point of speech$Plastic Surgeon$possible$	0
PS 0 1 MetaphoricalLiteral T0 T1 T2 T3 T4 T5 T6 T7 T8 T9 Vs.	PS	Previous Sentence$prefer-  ence score$phrase structure$Phrase structure$point of speech$Plastic Surgeon$possible$	0
PS l l l l l l ll Metaphorical Literal0 1 Vs.	PS	Previous Sentence$prefer-  ence score$phrase structure$Phrase structure$point of speech$Plastic Surgeon$possible$	0
models in all but PS Arg2 category.	PS	Previous Sentence$prefer-  ence score$phrase structure$Phrase structure$point of speech$Plastic Surgeon$possible$	0
In anal--  ogy to a eontext-flee PS rule, a DATR  sentence has a left hand side that consists of exactly  one non-terminal symbol (i.e. a node-path pair) and  a right hand side that consists of an arbitrary num-  ber of non-terminal and terminal symbols (i.e. DATR  atoms).	PS	Previous Sentence$prefer-  ence score$phrase structure$Phrase structure$point of speech$Plastic Surgeon$possible$	2
Us- ing a stochastic variant of Constraint Dependency Grammar (Wang and Harper, 2004) reached a 92.4% labelled F-score on the Penn Treebank, which slightly outperforms (Collins, 1999) who reports 92.0% on dependency structures automati- cally derived from PS results.	PS	Previous Sentence$prefer-  ence score$phrase structure$Phrase structure$point of speech$Plastic Surgeon$possible$	2
I I l  contrast to context-free PS  grmmnar, DATR nonterminals are not atomic sym-  hols, but highly structured complex objects.	PS	Previous Sentence$prefer-  ence score$phrase structure$Phrase structure$point of speech$Plastic Surgeon$possible$	2
Subcategorization and voice Each verb has a subcategorization frame, which is useful for build- ing verb PS.	PS	Previous Sentence$prefer-  ence score$phrase structure$Phrase structure$point of speech$Plastic Surgeon$possible$	2
Classifier VB Verb ADJ Adjective ADNOM Adnominal adjective ADV Adverb PCS Case particle PBD Binding particle PADN Adnominal particle PCO Parallel particle PCJ Conjunctive particle PEND Sentence-ending particle P Particle (others) AUX Auxiliary verb CONJ Conjunction PNC Punctuation PAR Parenthesis SYM Symbol FIL Filler Table 1: Preterminal tags automatically converted from dependency structure to PS by the previously described method (Uematsu et al 2013), and conversion er- rors of structures and tags were manually corrected.	PS	Previous Sentence$prefer-  ence score$phrase structure$Phrase structure$point of speech$Plastic Surgeon$possible$	2
Keyaki Treebank: PS with functional informa- tion for Japanese.	PS	Previous Sentence$prefer-  ence score$phrase structure$Phrase structure$point of speech$Plastic Surgeon$possible$	2
Combining treebank transfor- mation techniques with a suffix analysis, (Dubey, 2005) trained a probabilistic parser and reached a labelled F-score of 76.3% on PS an- notations for a subset of the sentences used here (with a maximum length of 40).	PS	Previous Sentence$prefer-  ence score$phrase structure$Phrase structure$point of speech$Plastic Surgeon$possible$	2
The Penn Chinese Treebank: PS annotation of a large corpus.	PS	Previous Sentence$prefer-  ence score$phrase structure$Phrase structure$point of speech$Plastic Surgeon$possible$	3
PS surprisal and phase structure en- tropy reduction are good predictors of the sort of human parsing difficulty that is measured by re- gression path duration, for these sentence types.	PS	Previous Sentence$prefer-  ence score$phrase structure$Phrase structure$point of speech$Plastic Surgeon$possible$	3
6.2 PS surprisal PS surprisal predicted that the am- biguous cases would be harder then the unambigu- ous cases; and that the disadvantage of sentence type 1 in the ambiguous cases would turn around into a disadvantage of sentence type 2 in the unam- biguous conditions.	PS	Previous Sentence$prefer-  ence score$phrase structure$Phrase structure$point of speech$Plastic Surgeon$possible$	3
PS  grammars are entirely concerned with assigning termi-  nal strings to categories and determining dominance and  precedence between constituents on the basis of their  categories.	PS	Previous Sentence$prefer-  ence score$phrase structure$Phrase structure$point of speech$Plastic Surgeon$possible$	3
6.3 PS entropy reduction The directions of the entropy reduction hypothesis predictions were the same as for phrase structure surprisal, although there was a relatively greater difficulty with the type 2 cases versus surprisal.	PS	Previous Sentence$prefer-  ence score$phrase structure$Phrase structure$point of speech$Plastic Surgeon$possible$	3
5.1 The basic elements  The minimal machinery necessary for generating tense is  composed of a time axis with a PS (now)  and a means of locating an action/event with respect to  this PS (Figure 5).	PS	Previous Sentence$prefer-  ence score$phrase structure$Phrase structure$point of speech$Plastic Surgeon$possible$	4
PS  o  o  time axis O  Figure 5  Reichenbach used the following concepts to characterize  tense: a PS (S), a point of reference (R) and a  point of event (E).	PS	Previous Sentence$prefer-  ence score$phrase structure$Phrase structure$point of speech$Plastic Surgeon$possible$	4
where additional information may be added (e.g.  structure can only he added at leaves and nodes of  type COMPOSITE; furthermore, in an incremental  pipeline architecture such as this, information can  only be added ahead of the PS)  ?	PS	Previous Sentence$prefer-  ence score$phrase structure$Phrase structure$point of speech$Plastic Surgeon$possible$	4
The point of reference can be used in order  to represent some instant which plays the role of a  translated PS (ht 1980 Paul had already  visited Paris twice.).	PS	Previous Sentence$prefer-  ence score$phrase structure$Phrase structure$point of speech$Plastic Surgeon$possible$	4
Note  that 17r ", d'~ and A~ are vector quantities in which  each entry corresponds to a PS antecedent.	PS	Previous Sentence$prefer-  ence score$phrase structure$Phrase structure$point of speech$Plastic Surgeon$possible$	6
Here we would compare the degree to which  each PS candidate antecedent (A Japanese  company, television picture tubes, Japan, TV   sets, and Malaysia in this example) could serve  as the direct object of "export".	PS	Previous Sentence$prefer-  ence score$phrase structure$Phrase structure$point of speech$Plastic Surgeon$possible$	6
It is quite PS that a  word appears in the test data that the program  never saw in the training data and low which it  hence has no P(plwo) probability.	PS	Previous Sentence$prefer-  ence score$phrase structure$Phrase structure$point of speech$Plastic Surgeon$possible$	6
Given the above PS sources of informar  tion, we arrive at the following equation, where  F(p) denotes a function from pronouns to their  antecedents:  F(p) = argmaxP( A(p) = alp, h, l~', t, l, so, d~ A~')  where A(p) is a random variable denoting the  referent of the pronoun p and a is a proposed  antecedent.	PS	Previous Sentence$prefer-  ence score$phrase structure$Phrase structure$point of speech$Plastic Surgeon$possible$	6
We  also transform our trees under certain condi-  tions to meet Hobbs' assumptions as much as  PS.	PS	Previous Sentence$prefer-  ence score$phrase structure$Phrase structure$point of speech$Plastic Surgeon$possible$	6
Once we have the trees in the proper form  (to the degree this is PS) we run Hobbs'  algorithm repeatedly for each pronoun until it  has proposed n (= 15 in our experiment) can-  didates.	PS	Previous Sentence$prefer-  ence score$phrase structure$Phrase structure$point of speech$Plastic Surgeon$possible$	6
23-28, 1992  in Southern Sweden, Sun in  Southern Sweden, Fair in Southern  Sweden,  or descriptive It will be  sunny.., There will be sun .... The  weather will be fair...  The generation process may be  set to generate telegraphic or full  utterances, single or coordinated  utterances, texts where the area is  kept in focus, e.g. Wales will get sun  and light winds or CC  with different areas such as Wales   will get sun, but Cornwall will get  rain.	CC	coordinations$correlation coefficient$Connected Component$Coordinating Conjunction$Concept Class$coordinating conjunction$Contrastive Connectives$computerized conference$CAUSE-EFFECT$coordinating conjunctions$Chinese Contex$Chief Complaint$Control Construction$	0
In addition to these  simple sentences, difficult problems are also han-  dled: clitics, complex determiners, completives, var-  ious forms of questions, extraction and non limited  dependancies, CC, comparatives.	CC	coordinations$correlation coefficient$Connected Component$Coordinating Conjunction$Concept Class$coordinating conjunction$Contrastive Connectives$computerized conference$CAUSE-EFFECT$coordinating conjunctions$Chinese Contex$Chief Complaint$Control Construction$	0
For binary trees, the coordination is represented by a left-branching tree, which is a conjunction or a con- junction particle that first joined a left hand con- stituent; the phrase is marked as a modifier consist- ing of coordination (-NCOORD and -VCOORD for NP and VP CC), as shown on the left side of Figure 3.	CC	coordinations$correlation coefficient$Connected Component$Coordinating Conjunction$Concept Class$coordinating conjunction$Contrastive Connectives$computerized conference$CAUSE-EFFECT$coordinating conjunctions$Chinese Contex$Chief Complaint$Control Construction$	0
Indeed, according to this approach  (Chomsky, 1957; Banfield, 1981), only coordination  of sentences are basic and other syntagmatic coordi-  nations should be considered as CC of re-  duced sentences, the reduction being performed by  deleting repeated elements.	CC	coordinations$correlation coefficient$Connected Component$Coordinating Conjunction$Concept Class$coordinating conjunction$Contrastive Connectives$computerized conference$CAUSE-EFFECT$coordinating conjunctions$Chinese Contex$Chief Complaint$Control Construction$	0
However, this solution cannot be applied gener-  ally because all CC have not such "natu-  ral" intersection (see (2)).	CC	coordinations$correlation coefficient$Connected Component$Coordinating Conjunction$Concept Class$coordinating conjunction$Contrastive Connectives$computerized conference$CAUSE-EFFECT$coordinating conjunctions$Chinese Contex$Chief Complaint$Control Construction$	0
Last, note that gapping the verb is less compati-  ble with head-driven mechanisms (and the comma in  (4f) could be such a head mark, see (BEF, 1996) for  an analysis of Gapping CC).	CC	coordinations$correlation coefficient$Connected Component$Coordinating Conjunction$Concept Class$coordinating conjunction$Contrastive Connectives$computerized conference$CAUSE-EFFECT$coordinating conjunctions$Chinese Contex$Chief Complaint$Control Construction$	0
denote the CC between the relevance ranking and the performance ranking for a given question.	CC	coordinations$correlation coefficient$Connected Component$Coordinating Conjunction$Concept Class$coordinating conjunction$Contrastive Connectives$computerized conference$CAUSE-EFFECT$coordinating conjunctions$Chinese Contex$Chief Complaint$Control Construction$	1
The results in Figure 2 shows strong correlation be- tween the confidence measure and the alignment F-score, with the CCs equals to -0.69.	CC	coordinations$correlation coefficient$Connected Component$Coordinating Conjunction$Concept Class$coordinating conjunction$Contrastive Connectives$computerized conference$CAUSE-EFFECT$coordinating conjunctions$Chinese Contex$Chief Complaint$Control Construction$	1
For a sample of size n, the n raw scores X i , Y i are converted to ranks x i , y i , and the Spearman CC ?	CC	coordinations$correlation coefficient$Connected Component$Coordinating Conjunction$Concept Class$coordinating conjunction$Contrastive Connectives$computerized conference$CAUSE-EFFECT$coordinating conjunctions$Chinese Contex$Chief Complaint$Control Construction$	1
Pearson CC with hu- man annotations was computed individually for each test set and a weighted sum of the correlations was used as the final evaluation metric (the weight for each dataset was proportional to its size).	CC	coordinations$correlation coefficient$Connected Component$Coordinating Conjunction$Concept Class$coordinating conjunction$Contrastive Connectives$computerized conference$CAUSE-EFFECT$coordinating conjunctions$Chinese Contex$Chief Complaint$Control Construction$	1
Regarding the interpretation of the absolute value of (Pearson?s) CCs, both here and in the rest of the paper, we adopt Cohen?s scale (Co- hen, 1988) for use in human judgements, given in Table 1; we use this as most of this work is to do with human judgements of fluency.	CC	coordinations$correlation coefficient$Connected Component$Coordinating Conjunction$Concept Class$coordinating conjunction$Contrastive Connectives$computerized conference$CAUSE-EFFECT$coordinating conjunctions$Chinese Contex$Chief Complaint$Control Construction$	1
We adopt the Spearman?s rank CC as the test measure.	CC	coordinations$correlation coefficient$Connected Component$Coordinating Conjunction$Concept Class$coordinating conjunction$Contrastive Connectives$computerized conference$CAUSE-EFFECT$coordinating conjunctions$Chinese Contex$Chief Complaint$Control Construction$	1
This might be due to the slow convergence of SimRank for higher values of C. Figure 1 shows that by varying thresholds the im- provement of the CCs Cluster- ing over the random clustering baseline at the same granularity first increases and then decreases.	CC	coordinations$correlation coefficient$Connected Component$Coordinating Conjunction$Concept Class$coordinating conjunction$Contrastive Connectives$computerized conference$CAUSE-EFFECT$coordinating conjunctions$Chinese Contex$Chief Complaint$Control Construction$	2
55  0.6  0.65 FSco re Threshold CCs ClusteringRandom Clustering (a)  0.6  0.65  0.7  0.75  0.8  0.85  0.9  0.95  1  0.35  0.4  0.45  0.5  0.55  0.6  0.65  0.7  0.75 FSco re Threshold CCs ClusteringRandom Clustering (b) Figure 1: Improvement in (a) average performance of best 3 Supervised Systems and (b) performance of best Unuper- vised System in Senseval-3 using CC Clustering Vs Random Clustering at the same granularity with C = 0.6 for experimentation.	CC	coordinations$correlation coefficient$Connected Component$Coordinating Conjunction$Concept Class$coordinating conjunction$Contrastive Connectives$computerized conference$CAUSE-EFFECT$coordinating conjunctions$Chinese Contex$Chief Complaint$Control Construction$	2
Definition (CC): Let G = (V,E) be an undirected graph where V is the set of vertices and E is the set of edges.	CC	coordinations$correlation coefficient$Connected Component$Coordinating Conjunction$Concept Class$coordinating conjunction$Contrastive Connectives$computerized conference$CAUSE-EFFECT$coordinating conjunctions$Chinese Contex$Chief Complaint$Control Construction$	2
uster senses but the generic nature of the similarity gives us the flexibility to use other clustering algorithms 17  0.6  0.65  0.7  0.75  0.8  0.85  0.9  0.95  1  0.25  0.3  0.35  0.4  0.45  0.5  0.55  0.6  0.65 FSco re Threshold CCs ClusteringRandom Clustering (a)  0.6  0.65  0.7  0.75  0.8  0.85  0.9  0.95  1  0.35  0.4  0.45  0.5  0.55  0.6  0.65  0.7  0.75 FSco re Threshold CCs ClusteringRandom Clustering (b) Figure 1: Improvement in (a) average performance of best 3 Supervised Systems and (b) performance of best Unuper- vised System in Senseval-3 using CC Clustering Vs Random Clustering at the same granularity with C = 0.6 for experimentation.	CC	coordinations$correlation coefficient$Connected Component$Coordinating Conjunction$Concept Class$coordinating conjunction$Contrastive Connectives$computerized conference$CAUSE-EFFECT$coordinating conjunctions$Chinese Contex$Chief Complaint$Control Construction$	2
For coarsening senses, we used one of the simplest approaches to cluster senses but the generic nature of the similarity gives us the flexibility to use other clustering algorithms 17  0.6  0.65  0.7  0.75  0.8  0.85  0.9  0.95  1  0.25  0.3  0.35  0.4  0.45  0.5  0.55  0.6  0.65 FSco re Threshold CCs ClusteringRandom Clustering (a)  0.6  0.65  0.7  0.75  0.8  0.85  0.9  0.95  1  0.35  0.4  0.45  0.5  0.55  0.6  0.65  0.7  0.75 FSco re Threshold CCs ClusteringRandom Clustering (b) Figure 1: Improvement in (a) average performance of best 3 Supervised Systems and (b) performance of best Unuper- vised System in Senseval-3 using CC Clustering V	CC	coordinations$correlation coefficient$Connected Component$Coordinating Conjunction$Concept Class$coordinating conjunction$Contrastive Connectives$computerized conference$CAUSE-EFFECT$coordinating conjunctions$Chinese Contex$Chief Complaint$Control Construction$	2
6 0.52 0.8453 0.7864 0.0589 SenseLearner 0.7104 0.49 0.8541 0.8097 0.0444 KOC University 0.7191 0.52 0.8448 0.7911 0.0538 IRST-DDD 0.6367 0.49 0.7970 0.7402 0.0568 0.8 GAMBL 0.7116 0.59 0.8419 0.7843 0.0577 SenseLearner 0.7104 0.56 0.8439 0.7984 0.0455 KOC University 0.7191 0.59 0.8414 0.7879 0.0535 IRST-DDD 0.6367 0.47 0.8881 0.8324 0.0557 Table 4: Improvement in Senseval-3 WSD performance using CC Clustering Vs Random Cluster- ing at the same granularity credit if it belongs to the cluster of the correct an- swer.	CC	coordinations$correlation coefficient$Connected Component$Coordinating Conjunction$Concept Class$coordinating conjunction$Contrastive Connectives$computerized conference$CAUSE-EFFECT$coordinating conjunctions$Chinese Contex$Chief Complaint$Control Construction$	2
3.1 Argument Position Classification Discourse connectives have a very strong prefer- ence on the location of the Arg1 with respect to their syntactic category (Subordinating Conjunc- tion, CC, and Discourse Adverbial) and position in the sentence (sentence initial or sentence medial); thus, classification of discourse connectives into inter-sentential or intra- sentential is an easy task yielding high supervised machine learning performance (Stepanov and Ric- cardi, 2013; Lin et al.,	CC	coordinations$correlation coefficient$Connected Component$Coordinating Conjunction$Concept Class$coordinating conjunction$Contrastive Connectives$computerized conference$CAUSE-EFFECT$coordinating conjunctions$Chinese Contex$Chief Complaint$Control Construction$	3
6.5 The Punctuation and CC Parameter Classes 6.5.1 Inconsistent Model.	CC	coordinations$correlation coefficient$Connected Component$Coordinating Conjunction$Concept Class$coordinating conjunction$Contrastive Connectives$computerized conference$CAUSE-EFFECT$coordinating conjunctions$Chinese Contex$Chief Complaint$Control Construction$	3
5.2.1.2 CC.	CC	coordinations$correlation coefficient$Connected Component$Coordinating Conjunction$Concept Class$coordinating conjunction$Contrastive Connectives$computerized conference$CAUSE-EFFECT$coordinating conjunctions$Chinese Contex$Chief Complaint$Control Construction$	3
6.7 Handling CCs Headless constructions such as coordinating conjunctions have been one of the weaker points of dependency grammar approaches.	CC	coordinations$correlation coefficient$Connected Component$Coordinating Conjunction$Concept Class$coordinating conjunction$Contrastive Connectives$computerized conference$CAUSE-EFFECT$coordinating conjunctions$Chinese Contex$Chief Complaint$Control Construction$	3
The 114 English question Frequency Is Next Preposition 1523 Is Prev Determiner 1444 Is Prev Preposition 1209 Is Prev Adjective 864 Is Next Noun Singular Mass 772 Is Prev Noun Singular Mass 690 Is Next Noun Plural 597 Is Next Noun 549 Arabic question Frequency Is Prev Preposition 1110 Is Next Preposition 993 Is Prev Noun 981 Is Next Noun 912 Is Prev CC 627 Is Prev Noun SingularMass 607 Is Next Punctuation 603 Is Next Adjective Adverb 559 Table 1: Most frequent root node context questions models were then evaluated using AER at each train- ing iteration.	CC	coordinations$correlation coefficient$Connected Component$Coordinating Conjunction$Concept Class$coordinating conjunction$Contrastive Connectives$computerized conference$CAUSE-EFFECT$coordinating conjunctions$Chinese Contex$Chief Complaint$Control Construction$	3
CC: We will assume that a coordinating conjunction like but may link spans within a text-clause, or across text-clauses and text- sentences, so that no constraint on the levels of A and B results.	CC	coordinations$correlation coefficient$Connected Component$Coordinating Conjunction$Concept Class$coordinating conjunction$Contrastive Connectives$computerized conference$CAUSE-EFFECT$coordinating conjunctions$Chinese Contex$Chief Complaint$Control Construction$	3
Miyoshi, H.; Ogino, T. and Sugiyarna, K. (1997)  EDR's CCification and Description for  Interlingual Representation.	CC	coordinations$correlation coefficient$Connected Component$Coordinating Conjunction$Concept Class$coordinating conjunction$Contrastive Connectives$computerized conference$CAUSE-EFFECT$coordinating conjunctions$Chinese Contex$Chief Complaint$Control Construction$	4
ncept Dictionary is one of the five types of EDR  dictionaries, the others are the Word Dmtlonarms  for English and Japanese the Blhngual Dictio-  nary, the Coocurrence Dictionary, and the Tech-  mcal Telmmology Dxctlonar} The EDR Con-  cept Dictionary consists of three sub-dmuonanes  the Headconcept Dlctxonaz} contains concept ex-  planations m natural language (both m Engh~h  and Japanese),~the CCification Dmuo-  nar} contains a set of ls-a relationships, and the  Concept Description Dictionary contains pairs of  concepts that have certain semantic relationships  other than ls-a relationship 1 e object, agent  9oal, zmplement a-object (object of a particular at-  tribute), place, scene and cause  The CCification Dmtlonar~ classifies  all the 400 000 concepts based o	CC	coordinations$correlation coefficient$Connected Component$Coordinating Conjunction$Concept Class$coordinating conjunction$Contrastive Connectives$computerized conference$CAUSE-EFFECT$coordinating conjunctions$Chinese Contex$Chief Complaint$Control Construction$	4
NP1 NP1 Concept Vs Vs Concept VPc VPc  concept VPe VPe CC  Naringi  crenulata herb 	   use as  cure  poison be- antipyretic 	   relieve muscle  pain n  Asiatic  Pennyworth herb leaf 	   use as    apply  externally apply  topically  heal wound y  ! red onion herb    is "# excrete be-lexative 		!	CC	coordinations$correlation coefficient$Connected Component$Coordinating Conjunction$Concept Class$coordinating conjunction$Contrastive Connectives$computerized conference$CAUSE-EFFECT$coordinating conjunctions$Chinese Contex$Chief Complaint$Control Construction$	4
c?2010 Association for Computational Linguistics CCification with Bayesian Multi-task Learning Marcel van Gerven Radboud University Nijmegen Intelligent Systems Heyendaalseweg 135 6525 AJ Nijmegen, The Netherlands marcelge@cs.ru.nl Irina Simanova Max Planck Institute for Psycholinguistics Wundtlaan 1 6525 XD Nijmegen, The Netherlands irina.simanova@mpi.nl Abstract Multivariate analysis allows decoding of sin- gle trial data in individ	CC	coordinations$correlation coefficient$Connected Component$Coordinating Conjunction$Concept Class$coordinating conjunction$Contrastive Connectives$computerized conference$CAUSE-EFFECT$coordinating conjunctions$Chinese Contex$Chief Complaint$Control Construction$	4
Cross-Document Summarization by  CCification.	CC	coordinations$correlation coefficient$Connected Component$Coordinating Conjunction$Concept Class$coordinating conjunction$Contrastive Connectives$computerized conference$CAUSE-EFFECT$coordinating conjunctions$Chinese Contex$Chief Complaint$Control Construction$	4
Real-time Visual CCification.	CC	coordinations$correlation coefficient$Connected Component$Coordinating Conjunction$Concept Class$coordinating conjunction$Contrastive Connectives$computerized conference$CAUSE-EFFECT$coordinating conjunctions$Chinese Contex$Chief Complaint$Control Construction$	4
The position we argue for in this article, is that whereas adjacency and explicit conjunction (CCs such as and, or, so, and but; subordinating con- junctions such as although, whereas, and when) imply discourse relations between (the interpretation of) adjacent or conjoined discourse units, discourse adverbials such as then, otherwise, nevertheless, and instead are anaphors, signaling a relation between the interpretation of their matrix clause and an entity in or deri	CC	coordinations$correlation coefficient$Connected Component$Coordinating Conjunction$Concept Class$coordinating conjunction$Contrastive Connectives$computerized conference$CAUSE-EFFECT$coordinating conjunctions$Chinese Contex$Chief Complaint$Control Construction$	5
CCs: tokens morpho- logical tag of which starts with the pair J?	CC	coordinations$correlation coefficient$Connected Component$Coordinating Conjunction$Concept Class$coordinating conjunction$Contrastive Connectives$computerized conference$CAUSE-EFFECT$coordinating conjunctions$Chinese Contex$Chief Complaint$Control Construction$	5
The conjuncts are sister nodes separated by CCs; we call these configu- rations coordination domains.	CC	coordinations$correlation coefficient$Connected Component$Coordinating Conjunction$Concept Class$coordinating conjunction$Contrastive Connectives$computerized conference$CAUSE-EFFECT$coordinating conjunctions$Chinese Contex$Chief Complaint$Control Construction$	5
interjection D determiner P pre- or postposition, or subordinating conjunction & CC T verb particle X existential there, predeterminers Y X + verbal # hashtag (indicates topic/category for tweet) @ at-mention (indicates a user as a recipient of a tweet) ~ discourse marker, indications of continuation across multiple tweets U URL or email address E emoticon $ numeral , punctuation G other abbreviations, foreign words, possessive endings, symbols, garbage T	CC	coordinations$correlation coefficient$Connected Component$Coordinating Conjunction$Concept Class$coordinating conjunction$Contrastive Connectives$computerized conference$CAUSE-EFFECT$coordinating conjunctions$Chinese Contex$Chief Complaint$Control Construction$	5
We  used  parts  of  speech  often  found  in  G?TA systems:  V  verb,  N  noun,  A adjunct,  R  pronoun,  S 51 subordination (preposition,  subordinating conjunction and linking word),  C CC.	CC	coordinations$correlation coefficient$Connected Component$Coordinating Conjunction$Concept Class$coordinating conjunction$Contrastive Connectives$computerized conference$CAUSE-EFFECT$coordinating conjunctions$Chinese Contex$Chief Complaint$Control Construction$	5
Coor- dinating elements are commata and CCs.	CC	coordinations$correlation coefficient$Connected Component$Coordinating Conjunction$Concept Class$coordinating conjunction$Contrastive Connectives$computerized conference$CAUSE-EFFECT$coordinating conjunctions$Chinese Contex$Chief Complaint$Control Construction$	5
c?2008 Association for Computational Linguistics Learning CC in Sentence Realization Ranking Crystal Nakatsu Department of Linguistics The Ohio State Univeristy Columbus, OH, USA cnakatsu@ling.osu.edu Abstract We look at the average frequency of con- trastive connectives in the SPaRKy Restaurant Corpus with respect to realization ratings by human judges.	CC	coordinations$correlation coefficient$Connected Component$Coordinating Conjunction$Concept Class$coordinating conjunction$Contrastive Connectives$computerized conference$CAUSE-EFFECT$coordinating conjunctions$Chinese Contex$Chief Complaint$Control Construction$	6
3 CC Usage 3.1 Usage Restrictions Previous work on contrastive connectives have found that these connectives often have different re- strictions on their location in the discourse struc- ture, with respect to maintaining discourse coher- ance (Quirk et al, 1972; Grote et al, 1995).	CC	coordinations$correlation coefficient$Connected Component$Coordinating Conjunction$Concept Class$coordinating conjunction$Contrastive Connectives$computerized conference$CAUSE-EFFECT$coordinating conjunctions$Chinese Contex$Chief Complaint$Control Construction$	6
Face to face vs. CCs:  A controlled experiment.	CC	coordinations$correlation coefficient$Connected Component$Coordinating Conjunction$Concept Class$coordinating conjunction$Contrastive Connectives$computerized conference$CAUSE-EFFECT$coordinating conjunctions$Chinese Contex$Chief Complaint$Control Construction$	7
CC Earplugs relieve the discomfort from traveling with a cold allergy or sinus condition.	CC	coordinations$correlation coefficient$Connected Component$Coordinating Conjunction$Concept Class$coordinating conjunction$Contrastive Connectives$computerized conference$CAUSE-EFFECT$coordinating conjunctions$Chinese Contex$Chief Complaint$Control Construction$	8
A total of 10 relations were used includ- ing CC, COMPONENT-WHOLE, CONTENT-CONTAINER, ENTITY-ORIGIN, ENTITY-DESTINATION, INSTRUMENT-AGENCY, MEMBER-COLLECTION, MESSAGE-TOPIC, OTHER, and PRODUCT-PRODUCER.	CC	coordinations$correlation coefficient$Connected Component$Coordinating Conjunction$Concept Class$coordinating conjunction$Contrastive Connectives$computerized conference$CAUSE-EFFECT$coordinating conjunctions$Chinese Contex$Chief Complaint$Control Construction$	8
Seven relations were used in the task: CC, INSTRUMENT-AGENCY, PRODUCT-PRODUCER, ORIGIN-ENTITY, THEME- TOOL, PART-WHOLE and CONTENT-CONTAINER.	CC	coordinations$correlation coefficient$Connected Component$Coordinating Conjunction$Concept Class$coordinating conjunction$Contrastive Connectives$computerized conference$CAUSE-EFFECT$coordinating conjunctions$Chinese Contex$Chief Complaint$Control Construction$	8
From this table it can be observed that the PRODUCT-PRODUCER, INSTRUMENT-AGENCY, and CC rela- tions were detected with a relatively very high per- formance score, whereas the THEME-TOOL relation classification yielded a relatively small score.	CC	coordinations$correlation coefficient$Connected Component$Coordinating Conjunction$Concept Class$coordinating conjunction$Contrastive Connectives$computerized conference$CAUSE-EFFECT$coordinating conjunctions$Chinese Contex$Chief Complaint$Control Construction$	8
User-centric decompositions are based on the idea that each user generates a sequence of questions that repre- sents a path from the complex question to a series of ques- tions that are connected through coherence relations of the type ELABORATION or CC.	CC	coordinations$correlation coefficient$Connected Component$Coordinating Conjunction$Concept Class$coordinating conjunction$Contrastive Connectives$computerized conference$CAUSE-EFFECT$coordinating conjunctions$Chinese Contex$Chief Complaint$Control Construction$	8
R1 R2 R3 R4 R5 R6 R7 before 682 1200 913 898 861 849 677 after 13 19 10 15 15 8 16 Table 4: The number of features before and af- ter Weka selection, for each semantic relation dataset: R1 CC, R2 INSTRUMENT- AGENCY, R3 PRODUCT-PRODUCER, R4 ORIGIN- ENTITY, R5 THEME-TOOL, R6 PART-WHOLE, and R7 CONTENT-CONTAINER.	CC	coordinations$correlation coefficient$Connected Component$Coordinating Conjunction$Concept Class$coordinating conjunction$Contrastive Connectives$computerized conference$CAUSE-EFFECT$coordinating conjunctions$Chinese Contex$Chief Complaint$Control Construction$	8
The position we argue for in this article, is that whereas adjacency and explicit conjunction (CC such as and, or, so, and but; subordinating con- junctions such as although, whereas, and when) imply discourse relations between (the interpretation of) adjacent or conjoined discourse units, discourse adverbials such as then, otherwise, nevertheless, and instead are anaphors, signaling a relation between the interpretation of their matrix clause and an entity in or deri	CC	coordinations$correlation coefficient$Connected Component$Coordinating Conjunction$Concept Class$coordinating conjunction$Contrastive Connectives$computerized conference$CAUSE-EFFECT$coordinating conjunctions$Chinese Contex$Chief Complaint$Control Construction$	9
CC: tokens morpho- logical tag of which starts with the pair J?	CC	coordinations$correlation coefficient$Connected Component$Coordinating Conjunction$Concept Class$coordinating conjunction$Contrastive Connectives$computerized conference$CAUSE-EFFECT$coordinating conjunctions$Chinese Contex$Chief Complaint$Control Construction$	9
The conjuncts are sister nodes separated by CC; we call these configu- rations coordination domains.	CC	coordinations$correlation coefficient$Connected Component$Coordinating Conjunction$Concept Class$coordinating conjunction$Contrastive Connectives$computerized conference$CAUSE-EFFECT$coordinating conjunctions$Chinese Contex$Chief Complaint$Control Construction$	9
ent speakers 2.1 2 2 6 7.5 Turns 242 276 25 96 140 Sentences 280 366 101 281 304 Sentences per turn 1.2 1.3 4.1 2.9 2.2 Questions (in %) 3.7 6.4 6.3 9.8 4.0 False starts (in %) 12.1 11.0 2.0 7.2 13.9 Words 1685 1905 1224 3165 2355 Words per sentence 6.0 5.2 12.1 11.3 7.7 Disfluent (in %) 16.0 16.3 5.1 4.2 13.2 Disfluencies 222 259 48 95 266 Disfluencies per sentence 0.79 0.71 0.48 0.34 0.87 Empty CC (in %) 30.3 30.4 64.8 50.7 24.3 Lexicalized filled pauses (in %) 18.8 21.0 17.2 23.5 13.9 Editing terms (in %) 3.6 1.6 3.4 5.7 3.3 Nonlexicalized filled pauses (in %) 20.8 29.9 0.7 2.3 29.5 Repairs (in %) 26.6 17.1 13.8 17.8 29.0 (8E-CH) and four dialogues for the eval set (4E-CH).4 These are recordings of phone conversations between two family members or friends, typical	CC	coordinations$correlation coefficient$Connected Component$Coordinating Conjunction$Concept Class$coordinating conjunction$Contrastive Connectives$computerized conference$CAUSE-EFFECT$coordinating conjunctions$Chinese Contex$Chief Complaint$Control Construction$	9
Coor- dinating elements are commata and CC.	CC	coordinations$correlation coefficient$Connected Component$Coordinating Conjunction$Concept Class$coordinating conjunction$Contrastive Connectives$computerized conference$CAUSE-EFFECT$coordinating conjunctions$Chinese Contex$Chief Complaint$Control Construction$	9
CC that don?t serve their usual connective role, but act more as links between subsequent speech acts of a speaker (e.g., and then; we call these empty CC in this work) 2.	CC	coordinations$correlation coefficient$Connected Component$Coordinating Conjunction$Concept Class$coordinating conjunction$Contrastive Connectives$computerized conference$CAUSE-EFFECT$coordinating conjunctions$Chinese Contex$Chief Complaint$Control Construction$	9
1 Introduction: Language Variations in the CCt  Commonly dichotomy of language and dialect is not easily maintained in the context of Chinese  language(s).	CC	coordinations$correlation coefficient$Connected Component$Coordinating Conjunction$Concept Class$coordinating conjunction$Contrastive Connectives$computerized conference$CAUSE-EFFECT$coordinating conjunctions$Chinese Contex$Chief Complaint$Control Construction$	10
c?2007 Association for Computational Linguistics Extending a Thesaurus in the Pan-CCt  Oi Yee Kwong and Benjamin K. Tsou  Language Information Sciences Research Centre  City University of Hong Kong  Tat Chee Avenue, Kowloon, Hong Kong  {rlolivia,rlbtsou}@cityu.edu.hk    Abstract  In this paper, we address a unique problem  in Chinese language processing and report  on our study on extending a Chinese the- saurus with region-specific words, mostly  from the financial	CC	coordinations$correlation coefficient$Connected Component$Coordinating Conjunction$Concept Class$coordinating conjunction$Contrastive Connectives$computerized conference$CAUSE-EFFECT$coordinating conjunctions$Chinese Contex$Chief Complaint$Control Construction$	10
Interpretation  of Biblical Texts in CCts?,	CC	coordinations$correlation coefficient$Connected Component$Coordinating Conjunction$Concept Class$coordinating conjunction$Contrastive Connectives$computerized conference$CAUSE-EFFECT$coordinating conjunctions$Chinese Contex$Chief Complaint$Control Construction$	10
Kwong, O.Y. and Tsou, B.K. (2007)  Extending a  Thesaurus in the Pan-CCt.	CC	coordinations$correlation coefficient$Connected Component$Coordinating Conjunction$Concept Class$coordinating conjunction$Contrastive Connectives$computerized conference$CAUSE-EFFECT$coordinating conjunctions$Chinese Contex$Chief Complaint$Control Construction$	10
section headers (History of Present Illness, CCs, Physical  Examination, Laboratory Data etc.).	CC	coordinations$correlation coefficient$Connected Component$Coordinating Conjunction$Concept Class$coordinating conjunction$Contrastive Connectives$computerized conference$CAUSE-EFFECT$coordinating conjunctions$Chinese Contex$Chief Complaint$Control Construction$	11
Re- cent works (Halasz et al, 2006) also utilize NGram techniques to automatically create CCs classifiers based on ICD9 groupings.	CC	coordinations$correlation coefficient$Connected Component$Coordinating Conjunction$Concept Class$coordinating conjunction$Contrastive Connectives$computerized conference$CAUSE-EFFECT$coordinating conjunctions$Chinese Contex$Chief Complaint$Control Construction$	11
MT Summit XII, Beyond TMs: New Tools for Translators Workshop, pp 20?27.	TMs	Translation Memories$Topic models$	0
c?2012 Association for Computational Linguistics Language Resources Factory: case study on the acquisition of TMs?	TMs	Translation Memories$Topic models$	0
In Proceedings of Machine Translation Summit XII - Workshop: Beyond TMs: New Tools for Translators.	TMs	Translation Memories$Topic models$	0
Beyond TMs: Finding similar documents in comparable corpora.	TMs	Translation Memories$Topic models$	0
MT Summit XII-Workshop: Beyond TMs: New Tools for Translators MT.	TMs	Translation Memories$Topic models$	0
A Compact Data Structure for  Searchable TMs.	TMs	Translation Memories$Topic models$	0
TMs are used in computational biology, com- puter vision, music, and, of course, text analysis.	TMs	Translation Memories$Topic models$	1
TMs for word sense disambiguation and token-based idiom detection.	TMs	Translation Memories$Topic models$	1
TMs can also be inspected man- ually by a human to understand the themes of the underlying corpus.	TMs	Translation Memories$Topic models$	1
TMs posit doc- ument collection exhibits multiple latent semantic topics where each topic is represented as a multino- mial distribution over a given vocabulary and each document is a mixture of hidden topics.	TMs	Translation Memories$Topic models$	1
TMs for meaning similarity in context.	TMs	Translation Memories$Topic models$	1
1 Introduction TMs, exemplified by latent Dirichlet aloca- tion (LDA) (Blei et al, 2003), discover latent themes present in text collections. ?	TMs	Translation Memories$Topic models$	1
2016 Association for Computational Linguistics A Latent Concept TMl for Robust Topic Inference Using Word Embeddings Weihua Hu ?	TM	Topic Mode$terns$translation memory$term  frequency$text mining$translation model$Text Mining$Task  Manager$	0
Supervised TMls.	TM	Topic Mode$terns$translation memory$term  frequency$text mining$translation model$Text Mining$Task  Manager$	0
c?2015 Association for Computational Linguistics Lifelong Machine Learning for TMling and Beyond Zhiyuan Chen Department of Computer Science University of Illinois at Chicago czyuanacm@gmail.com Abstract Machine learning has been popularly used in numerous natural language processing tasks.	TM	Topic Mode$terns$translation memory$term  frequency$text mining$translation model$Text Mining$Task  Manager$	0
Leveraging  Multi-Domain Prior Knowledge in TMls.	TM	Topic Mode$terns$translation memory$term  frequency$text mining$translation model$Text Mining$Task  Manager$	0
A TMl for Word Sense Disambigua- tion.	TM	Topic Mode$terns$translation memory$term  frequency$text mining$translation model$Text Mining$Task  Manager$	0
Software Framework for TMlling with Large Corpora.	TM	Topic Mode$terns$translation memory$term  frequency$text mining$translation model$Text Mining$Task  Manager$	0
The used patTM are: 1) (DT|CD) (NN|NNS), 2) DT JJ (NN|NNS), 3) NN POS (NN|NNS), and 4) PRP$ JJ (NN|NNS).	TM	Topic Mode$terns$translation memory$term  frequency$text mining$translation model$Text Mining$Task  Manager$	1
These POS-based patTM are quite generic, al- lowing for the creation of large sets of characters.	TM	Topic Mode$terns$translation memory$term  frequency$text mining$translation model$Text Mining$Task  Manager$	1
using syntactic patTM.	TM	Topic Mode$terns$translation memory$term  frequency$text mining$translation model$Text Mining$Task  Manager$	1
Variations of the following basic patTM (Elson and McKe- own, 2010) were used: 1) QT SV CH, 2) QT CH SV, and 3) CH SV QT,	TM	Topic Mode$terns$translation memory$term  frequency$text mining$translation model$Text Mining$Task  Manager$	1
Variations of the following basic patTM (Elson and McKe- own, 2010) were used: 1) QT SV CH, 2) QT CH SV, and 3) CH SV QT, where QT denotes a quote boundary and CH stands for a story character.	TM	Topic Mode$terns$translation memory$term  frequency$text mining$translation model$Text Mining$Task  Manager$	1
Several syntac- tic patTM were applied to associate quotes with explicit mention of speakers in their vicinity to characters from the pruned list of story charac- ters.	TM	Topic Mode$terns$translation memory$term  frequency$text mining$translation model$Text Mining$Task  Manager$	1
2) a set of part-of-speech patTM was used for the extraction of human and non-human characters that were not represented by proper names, e.g., ?	TM	Topic Mode$terns$translation memory$term  frequency$text mining$translation model$Text Mining$Task  Manager$	1
These patTM were developed around SV.	TM	Topic Mode$terns$translation memory$term  frequency$text mining$translation model$Text Mining$Task  Manager$	1
The NLP technology known as TM finds its place here.	TM	Topic Mode$terns$translation memory$term  frequency$text mining$translation model$Text Mining$Task  Manager$	2
5 Experiments We performed experiments on translation from En- glish into Swedish and Danish on two different cor- pora, an automotive corpus collected from a propri- etary TM, and on Europarl (Koehn, 2005) for the merging experiments.	TM	Topic Mode$terns$translation memory$term  frequency$text mining$translation model$Text Mining$Task  Manager$	2
A stand-alone sub- sentential alignment module however, is also use- ful for human translators if incorporated in CAT- tools, e.g. sophisticated bilingual concordance sys- tems, or in sub-sentential TM sys- tems (Gotti et al, 2005).	TM	Topic Mode$terns$translation memory$term  frequency$text mining$translation model$Text Mining$Task  Manager$	2
The au- thor automatically derived from the Hansard corpus what he calls a TM: ac- tually a collection of pairs of source and target word sequences that are in a translation rela- tion according to the viterbi alignment run with an IBM4 model that was also trained on the Hansard corpus.	TM	Topic Mode$terns$translation memory$term  frequency$text mining$translation model$Text Mining$Task  Manager$	2
The GREYC Translation Memory for the IWSLT 2009 Evaluation Campaign: one step be- yond TM.	TM	Topic Mode$terns$translation memory$term  frequency$text mining$translation model$Text Mining$Task  Manager$	2
It resembles algorithms used in TM search for locating orthographically similar sentences.	TM	Topic Mode$terns$translation memory$term  frequency$text mining$translation model$Text Mining$Task  Manager$	2
Table 1 lists three  common local weighting methods, namely raw term frequency (tf ), term presence (tp) and augmented  TM (atf ).	TM	Topic Mode$terns$translation memory$term  frequency$text mining$translation model$Text Mining$Task  Manager$	3
ABSTRACT We present an adaptation for the French TM challenge (DEFT 2012) of the KX system for multilingual unsupervised key-concept extraction.	TM	Topic Mode$terns$translation memory$term  frequency$text mining$translation model$Text Mining$Task  Manager$	4
As well as serving as a dataset for future tool de- velopment, our corpus is an excellent case study pro- viding valuable guidance to developers of biomedi- cal TM and retrieval systems.	TM	Topic Mode$terns$translation memory$term  frequency$text mining$translation model$Text Mining$Task  Manager$	4
This work provides important empirical guidance for developers of biomedical TM systems.	TM	Topic Mode$terns$translation memory$term  frequency$text mining$translation model$Text Mining$Task  Manager$	4
1 Introduction  With the rapidly increasing biomedical literature,  TM has become popular for finding bio- medical information in text.	TM	Topic Mode$terns$translation memory$term  frequency$text mining$translation model$Text Mining$Task  Manager$	4
dical Literature by  Combining High-Precision Gene Identifiers      Sun Kim, Won Kim, Don Comeau, and W. John Wilbur  National Center for Biotechnology Information  National Library of Medicine, National Institutes of Health  Bethesda, MD 20894, USA  {sun.kim,won.kim,donald.comeau,john.wilbur}@nih.gov              Abstract  Gene name identification is a fundamental  step to solve more complicated TM  problems such as gene normalization and pro- tein-protein interactions.	TM	Topic Mode$terns$translation memory$term  frequency$text mining$translation model$Text Mining$Task  Manager$	4
5 Discussion and Future Work We have developed a dataset for IFLOW in the con- text of financial TM and demonstrated it is a Features P (%) R (%) F (%) Radford et al (2009) 90.9 88.1 89.5 + Metadata Features 91.1 ??	TM	Topic Mode$terns$translation memory$term  frequency$text mining$translation model$Text Mining$Task  Manager$	4
For the efficient computation of the de- nominator, we use the lexical TM.	TM	Topic Mode$terns$translation memory$term  frequency$text mining$translation model$Text Mining$Task  Manager$	5
Generat- ing chinese classical poems with statistical machine TMs.	TM	Topic Mode$terns$translation memory$term  frequency$text mining$translation model$Text Mining$Task  Manager$	5
In order to improve the efficiency of a human translator, the k-best output of a translation system could be displayed as word or phrase choices which are color coded based on the confidence value assigned by the TM.	TM	Topic Mode$terns$translation memory$term  frequency$text mining$translation model$Text Mining$Task  Manager$	5
Here, n sign means the number of sign words in 69 Figure 2: Two ways of learning TMs the alignment pair, and n JP means the number of Japanese words in the alignment pair.	TM	Topic Mode$terns$translation memory$term  frequency$text mining$translation model$Text Mining$Task  Manager$	5
The source-to- target lexical TM p(t|s) and target- to-source model p(s|t) can be obtained through IBM Model-1 or HMM training.	TM	Topic Mode$terns$translation memory$term  frequency$text mining$translation model$Text Mining$Task  Manager$	5
Although no longer competitive as end-to-end TMs, the IBM Models, as well as the hidden Markov model (HMM) of Vogel, Ney, and Tillmann (1996), are still widely used for word alignment.	TM	Topic Mode$terns$translation memory$term  frequency$text mining$translation model$Text Mining$Task  Manager$	5
National Centre for TM, UK {sebastian,chun,takagi}@dbcls.rois.ac.jp tsujii@is.s.u-tokyo.ac.jp Abstract In this paper we describe our entry to the BioNLP 2009 Shared Task regarding bio- molecular event extraction.	TM	Topic Mode$terns$translation memory$term  frequency$text mining$translation model$Text Mining$Task  Manager$	6
TM Tools: Instruments for  Scientific Discovery, in IMA TM Work- shop, Institute for	TM	Topic Mode$terns$translation memory$term  frequency$text mining$translation model$Text Mining$Task  Manager$	6
National Centre for TM, University of Manchester, Manchester, UK ?	TM	Topic Mode$terns$translation memory$term  frequency$text mining$translation model$Text Mining$Task  Manager$	6
TM as Integration of Several Re- lated Research Areas, KDD 2000 Workshop on  TM, Boston, USA  Hatzivassiloglou V., Duboue P. and Rzetsky A. 2001.	TM	Topic Mode$terns$translation memory$term  frequency$text mining$translation model$Text Mining$Task  Manager$	6
TM Tools: Instruments for  Scientific Discovery, in IMA TM Work- shop, Institute for Mathematics and its Applica- tions, Minneapolis, USA, 2000  Jacquemin C. 2001.	TM	Topic Mode$terns$translation memory$term  frequency$text mining$translation model$Text Mining$Task  Manager$	6
ML method (Weka) Features Accuracy DT PMI scores 65.4% Decision Rules PMI scores 65.5% Na??ve Bayes PMI scores 52.5% K-Nearest Neighbor PMI scores 64.5% Kernel Density PMI scores 60.5% Boosting (Dec. Stumps) PMI scores 67.7% Na??ve Bayes 500 words 68.0% DT 500 words 67.0% Na??ve Bayes PMI + 500 words 66.5% Boosting (Dec. Stumps) PMI + 500 words 69.2% Table 6: Comparative results for the supervised learning method using various ML learning algo- rithms (Weka), averaged over the seven groups of near-synonyms from the Exp1 data set.	DT	Decision Trees$date or time$different topics$Date or Time$dependency tree$Dependency tree$Decision Tree$	0
1345  Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 777?784 Manchester, August 2008 Estimation of Conditional Probabilities With DT and an Application to Fine-Grained POS Tagging Helmut Schmid and Florian Laws IMS, University of Stuttgart {schmid,lawsfn}@ims.uni-stuttgart.de Abstract We present a HMM part-of-speech tag- ging method which is particularly suited for POS tagsets with a large number of fine-grained tags.	DT	Decision Trees$date or time$different topics$Date or Time$dependency tree$Dependency tree$Decision Tree$	0
3 Constructing a Classifier  3.1 Sequential Minimal Optimization  Although any of a number of machine learning  algorithms, including DT, might be  equally applicable here, Support Vector Ma- chines (Vapnik, 1995) have been extensively  used in text classification  problems and with  considerable success (Dumais 1998; Dumais et  al.,	DT	Decision Trees$date or time$different topics$Date or Time$dependency tree$Dependency tree$Decision Tree$	0
Helmut Schmid, 1994, Probabilistic Part-of-Speech Tag- ging Using DT, Intl.	DT	Decision Trees$date or time$different topics$Date or Time$dependency tree$Dependency tree$Decision Tree$	0
ML method (Weka) Features Accuracy DT PMI scores 65.4% Decision Rules PMI scores 65.5% Na??ve Bayes PMI scores 52.5% K-Nearest Neighbor PMI scores 64.5% Kernel Density PMI scores 60.5% Boosting (Dec. Stumps) PMI scores 67.7% Na??ve Bayes 500 words 68.0% DT 500 words 67.0% Na??ve Bayes PMI + 500 words 66.5% Boosting (Dec. Stumps) PMI + 500 words 69.2% Table 6: Comparative results for the supervised learning m	DT	Decision Trees$date or time$different topics$Date or Time$dependency tree$Dependency tree$Decision Tree$	0
The classifiers that use PMI features are DT, Decision Rules, Na??ve Bayes, K-Nearest Neighbor, Kernel Density, and Boosting a weak classifier (De- cision Stumps ?	DT	Decision Trees$date or time$different topics$Date or Time$dependency tree$Dependency tree$Decision Tree$	0
Named Ent i ty  (NE) -- Insert SGML  tags into the text to mark each string that  represents a person, organization, or location  name, or a DT stamp, or a currency or  percentage figure.	DT	Decision Trees$date or time$different topics$Date or Time$dependency tree$Dependency tree$Decision Tree$	1
Named Entity Task [NE]: Insert SGML tags into the text to mark each string that represents a person, organization, or location name, or a DT stamp, or a currency or percentage figure ?	DT	Decision Trees$date or time$different topics$Date or Time$dependency tree$Dependency tree$Decision Tree$	1
Every  normalized expression is made up of two dates  although it refers to a concrete DT.	DT	Decision Trees$date or time$different topics$Date or Time$dependency tree$Dependency tree$Decision Tree$	1
The system applies different functions to get a valid ISO DT in a valid gran- ularity from DCT3 dates.	DT	Decision Trees$date or time$different topics$Date or Time$dependency tree$Dependency tree$Decision Tree$	1
ISO: Apply rules to convert any-format ex- plicit DT into a valid ISO 8601 stan- dard date.	DT	Decision Trees$date or time$different topics$Date or Time$dependency tree$Dependency tree$Decision Tree$	1
which inquire about the DT when certain events affected certain properties of the instances everquest and radon re- spectively.	DT	Decision Trees$date or time$different topics$Date or Time$dependency tree$Dependency tree$Decision Tree$	1
already available on the DT in the given domain.	DT	Decision Trees$date or time$different topics$Date or Time$dependency tree$Dependency tree$Decision Tree$	2
We propose a new method to identify the author of a document on a topic using a predictive model trained on examples from DT.	DT	Decision Trees$date or time$different topics$Date or Time$dependency tree$Dependency tree$Decision Tree$	2
Therefore, we try to improve the performance of cross-topic AA (CTAA), one of the dimensions of cross-domain AA (CDAA) where training and test data come from DT.	DT	Decision Trees$date or time$different topics$Date or Time$dependency tree$Dependency tree$Decision Tree$	2
For example, people on Social Media might talk about DT dur- ing and after work on weekdays, talk every Friday about the weekend ahead, or comment about their favorite weekly TV show during its air time.	DT	Decision Trees$date or time$different topics$Date or Time$dependency tree$Dependency tree$Decision Tree$	2
And do DT benefit from different indicators?	DT	Decision Trees$date or time$different topics$Date or Time$dependency tree$Dependency tree$Decision Tree$	2
Many differences between the male and female lists can be linked to men and women talking about DT, or to differ- ent people.	DT	Decision Trees$date or time$different topics$Date or Time$dependency tree$Dependency tree$Decision Tree$	2
Cotterill 2010 Li & Roth 2002 % Person(s) HUM{individual,title} 2.53 Group or Organisation HUM{group} 0.17 Descriptive text HUM{description} 11.51 DESC{manner, definition, description} Reason DESC{reason} 1.57 DT NUM{date, period} 3.57 Numeric NUM{weight, volume/size, ordinal, percentage, count, speed, money, temperature, distance, other} 1.92 Phone NUM{code}1 0.40 URL 0.17 Email 0.17 Place LOC{country, state, city, mountain, other} 0.96 Animal ENTY{animal} 0.00 Physical Object ENTY{instrument, plant, body part, vehicle, food, product, substance} 0.30 Concept ENTY{language, religion, letter, c	DT	Decision Trees$date or time$different topics$Date or Time$dependency tree$Dependency tree$Decision Tree$	3
For instance, general projec- tivity rules ensure that the DT corre- sponds to a properly nested syntax structure with- out crossing brackets1 .	DT	Decision Trees$date or time$different topics$Date or Time$dependency tree$Dependency tree$Decision Tree$	4
4.4 Results We trained the PP attachment predictor both with the counts acquired from the DTbank (supervised) and those from the newspaper cor- pus (unsupervised).	DT	Decision Trees$date or time$different topics$Date or Time$dependency tree$Dependency tree$Decision Tree$	4
4.3 Corpus For our parsing experiments, we used the first 1,000 sentences of technical newscasts from the DTbank mentioned above.	DT	Decision Trees$date or time$different topics$Date or Time$dependency tree$Dependency tree$Decision Tree$	4
Weighted Constraint Dependency Grammar (Schro?der, 2002) models syntax structure as la- belled DTs as shown in the exam- ple.	DT	Decision Trees$date or time$different topics$Date or Time$dependency tree$Dependency tree$Decision Tree$	4
How- ever, comparable results are also achieved when applying the parser to the standard test set from the NEGRA corpus of German, as used by (Schiehlen, 2004; Foth et al, 2005): adding the PP predic- tor trained on our DTbank raises the overall attachment accuracy from 89.3% to 90.6%.	DT	Decision Trees$date or time$different topics$Date or Time$dependency tree$Dependency tree$Decision Tree$	4
4 Experiments 4.1 Sources To obtain the counts to base our estimates of at- traction on, we first turned to the DT- bank that accompanies the WCDG parsing suite.	DT	Decision Trees$date or time$different topics$Date or Time$dependency tree$Dependency tree$Decision Tree$	4
DT-based sentiment classifica- tion using CRFs with hidden variables.	DT	Decision Trees$date or time$different topics$Date or Time$dependency tree$Dependency tree$Decision Tree$	5
DT kernels for relation extrac- tion.	DT	Decision Trees$date or time$different topics$Date or Time$dependency tree$Dependency tree$Decision Tree$	5
Perceptron with DT kernel.	DT	Decision Trees$date or time$different topics$Date or Time$dependency tree$Dependency tree$Decision Tree$	5
le  imnl  quitte/l  ~g>~do~  Ne pas/l l me~12  Figure 7: DT for Ne me quitte pas t  4 A Computational Rendering  A close approximation of the described stochastic  model of dependency syntax has been realized as a  type of prohabilistic bottom-up chart parser.	DT	Decision Trees$date or time$different topics$Date or Time$dependency tree$Dependency tree$Decision Tree$	5
1 The architecture of our parser  (i)Preprocessor (neighboring  relation tagger) (ii)Get contextual features (iii)Estimate dependency attachment by SVM (iv)Tag label by MaxEnt Construct Subtree No more construction DT False True Left or Right attachment None Input sentence (word tokens) 191 Fig.	DT	Decision Trees$date or time$different topics$Date or Time$dependency tree$Dependency tree$Decision Tree$	5
This means  that the dependency links establish a tree structure,  main main  ate ate  ~bj~d~j  ~bj~su~i  John beans beans John  Fignre 1: DTs for John ate beans.	DT	Decision Trees$date or time$different topics$Date or Time$dependency tree$Dependency tree$Decision Tree$	5
DT Pars-  ing using a Hidden Derivation Model.	DT	Decision Trees$date or time$different topics$Date or Time$dependency tree$Dependency tree$Decision Tree$	6
3 Constructing a Classifier  3.1 Sequential Minimal Optimization  Although any of a number of machine learning  algorithms, including DTs, might be  equally applicable here, Support Vector Ma- chines (Vapnik, 1995) have been extensively  used in text classification  problems and with  considerable success (Dumais 1998; Dumais et  al.,	DT	Decision Trees$date or time$different topics$Date or Time$dependency tree$Dependency tree$Decision Tree$	6
Kettei-gi o mochiita ni-  hongo kakariuke kaiseki (A Japanese Dependency  Parser Using A DT).	DT	Decision Trees$date or time$different topics$Date or Time$dependency tree$Dependency tree$Decision Tree$	6
Helmut Schmid, 1994, Probabilistic Part-of-Speech Tag- ging Using DTs, Intl.	DT	Decision Trees$date or time$different topics$Date or Time$dependency tree$Dependency tree$Decision Tree$	6
ML method (Weka) Features Accuracy DTs PMI scores 65.4% Decision Rules PMI scores 65.5% Na??ve Bayes PMI scores 52.5% K-Nearest Neighbor PMI scores 64.5% Kernel Density PMI scores 60.5% Boosting (Dec. Stumps) PMI scores 67.7% Na??ve Bayes 500 words 68.0% DTs 500 words 67.0% Na??ve Bayes PMI + 500 words 66.5% Boosting (Dec. Stumps) PMI + 500 words 69.2% Table 6: Comparative results for the supervised learning m	DT	Decision Trees$date or time$different topics$Date or Time$dependency tree$Dependency tree$Decision Tree$	6
112   Discourse Parsing: A DT Approach  Tadash i  Nomoto   Advanced Research  Laboratory   H i tach i  L td .	DT	Decision Trees$date or time$different topics$Date or Time$dependency tree$Dependency tree$Decision Tree$	6
such graphs are called DET curves.	DET	detection error tradeoff$Determiner$Determine$deter$Detai l izat ion$	0
The rules are the following:  (defrule NPRule ; NP --> DET Noun  (STATUS active)  (CNTXTLORNOPR NIL)  (PRODUCTION (NP (DET Noun)))  (SYN-TESTS T)  (SEM-TESTS T)  (SYN-ACTIONS  (ralsef "(* DefiniteneSS DET)  ;raise the values of the specified features from the  ;son node into the parent node  "(* * Noun)l}}  ; second *means all features of the son node  ;first * means the storing of the features as they are  ;in the son node into t	DET	detection error tradeoff$Determiner$Determine$deter$Detai l izat ion$	1
The rules are the following:  (defrule NPRule ; NP --> DET Noun  (STATUS active)  (CNTXTLORNOPR NIL)  (PRODUCTION (NP (DET Noun)))  (SYN-TESTS T)  (SEM-TESTS T)  (SYN-ACTIONS  (ralsef "(* DefiniteneSS DET)  ;raise the values of the specified features from the  ;son node into the parent node  "(* * Noun)l}}  ; second *means all features of the son node  ;first * means the storing of the features as they are  ;in the son node into the parent node  (defrule VPRule ; VP --> Verb NP NP  (STATUS active)  (	DET	detection error tradeoff$Determiner$Determine$deter$Detai l izat ion$	1
The rules are the following:  (defrule NPRule ; NP --> DET Noun  (STATUS active)  (CNTXTLORNOPR NIL)  (PRODUCTION (NP (DET Noun)))  (SYN-TESTS T)  (SEM-TESTS T)  (SYN-ACTIONS  (ralsef "(* DefiniteneSS DET)  ;raise the values of the specified features from the  ;son node into the parent node  "(* * Noun)l}}  ; second *means all features of the son node  ;first * means the storing of the features as they are  ;in the son node into the parent node  (defrule VPRule ; VP --> Verb NP NP  (STATUS active)  (CNTXTLORN OPR NIL)  (PRODUCTION (Via (Verb NP NP)))  (SYN-TESTS T)  (SEM-TESTS T)  (SYN-A	DET	detection error tradeoff$Determiner$Determine$deter$Detai l izat ion$	1
AN EXAMPLE  The example concerns a simple fragment of a LFG  written in SAIL according to the CGU model, Our example  is taken f rom/Kap lan  1982/and/Winograd  1983/.   The lexical entries for this grammar in SAIL are the  following:  a {(DET NIL (Definiteness) (Indefinite)  {Number) (singular)))  baby ((Noun NIL (Number) (Singular)  (Predicate) {Baby)}}  girl {(Noun NIL {Number} (Singular}  (Predicate) {Girl)l}  handed ((Verb NIL (Tense) (Past)  (Predicate) (Hand)))  the ((DET NIL (Definiteness) {Definite)))  toys ((Noun NIL (Number) (Plural}  (Predicate) (Toys)))  Rules in SAIL are written using a def~ttle format wh	DET	detection error tradeoff$Determiner$Determine$deter$Detai l izat ion$	1
DETs were inserted before nouns result- ing in queries of the type story/stories about and about the/a/0 war/wars for the compound war story.	DET	detection error tradeoff$Determiner$Determine$deter$Detai l izat ion$	1
to the CGU model, Our example  is taken f rom/Kap lan  1982/and/Winograd  1983/.   The lexical entries for this grammar in SAIL are the  following:  a {(DET NIL (Definiteness) (Indefinite)  {Number) (singular)))  baby ((Noun NIL (Number) (Singular)  (Predicate) {Baby)}}  girl {(Noun NIL {Number} (Singular}  (Predicate) {Girl)l}  handed ((Verb NIL (Tense) (Past)  (Predicate) (Hand)))  the ((DET NIL (Definiteness) {Definite)))  toys ((Noun NIL (Number) (Plural}  (Predicate) (Toys)))  Rules in SAIL are written using a def~ttle format where  all the fields appearing in the CGUs can be defined; in  addit ion two fields are devoted to the state definition  (STATUS field) and the rule type definition, that  is ff the  rule is a standard rule or a contextual  or a NOP rule  (CNTXTLOR	DET	detection error tradeoff$Determiner$Determine$deter$Detai l izat ion$	1
DET the canonical function of these argu- ments (canonicalization).	DET	detection error tradeoff$Determiner$Determine$deter$Detai l izat ion$	2
Combining  Finite  State Automata and a Greedy Learning Algorithm  to DET the Syntactic Roles of Commas.	DET	detection error tradeoff$Determiner$Determine$deter$Detai l izat ion$	2
3 DETd by lexical information (question marks, dis/agreement indications and sentence length) 2.3.1 Relevance Decisions Our raw representation allows as many as six pre- vious turns to be relevant to the classification de- cision, however not all turns are indeed relevant, and even relevant turns may consist only of a handful of relevant sentences.	DET	detection error tradeoff$Determiner$Determine$deter$Detai l izat ion$	2
Query  Classi f icat ion DET what category of  entity the question is asking for.	DET	detection error tradeoff$Determiner$Determine$deter$Detai l izat ion$	2
AN EXAMPLE  The example concerns a simple fragment of a LFG  written in SAIL according to the CGU model, Our example  is taken f rom/Kap lan  1982/and/Winograd  1983/.   The lexical entries for this grammar in SAIL are the  following:  a {(DETr NIL (Definiteness) (Indefinite)  {Number) (singular)))  baby ((Noun NIL (Number) (Singular)  (Predicate) {Baby)}}  girl {(Noun NIL {Number} (Singular}  (Predicate) {Girl)l}  handed ((Verb NIL (Tense) (Past)  (Predicate) (Hand)))  the ((DETr NIL (Definiteness) {Definite)))  toys ((Noun NIL (Number) (Plural}  (Predicate) (Toys)))  Rules in SAIL are written using a def~ttle format wh	DET	detection error tradeoff$Determiner$Determine$deter$Detai l izat ion$	2
DETrs were inserted before nouns result- ing in queries of the type story/stories about and about the/a/0 war/wars for the compound war story.	DET	detection error tradeoff$Determiner$Determine$deter$Detai l izat ion$	2
1 In t roduct ion   We present a statistical method for DETmin-  ing pronoun anaphora.	DET	detection error tradeoff$Determiner$Determine$deter$Detai l izat ion$	3
Finally we attempted a fully automatic di-  rect test of the accuracy of both pronoun meth-  ods for gender DETmination.	DET	detection error tradeoff$Determiner$Determine$deter$Detai l izat ion$	3
A statistical approach is present in the dis-  course module only where it is used to DET-  mine the probability that a noun (verb) phrase  is the center of a sentence.	DET	detection error tradeoff$Determiner$Determine$deter$Detai l izat ion$	3
One can judge the pro-  gram informally by simply examining the re-  sults and DETmining if the program's gender  decisions are correct (occasionally ooking at the  text for difficult cases).	DET	detection error tradeoff$Determiner$Determine$deter$Detai l izat ion$	3
First, as one might expect given the al-  ready noted superior performance of the Hobbs  scheme over last-noun, Hobbs also performs bet-  ter at DETmining ender.	DET	detection error tradeoff$Determiner$Determine$deter$Detai l izat ion$	3
This transducer is non-DETministic, producing more than  one sequence of nlarks Ior a given input.	DET	detection error tradeoff$Determiner$Determine$deter$Detai l izat ion$	3
2.30 TB2 Most likely for each P 81.73 FN Average human, headwords (Ratnaparkhi et al, 1994) 88.2 RRR Average human, whole sentence (Ratnaparkhi et al, 1994) 93.2 RRR Maximum Likelihood-based (Hindle and Rooth, 1993) 79.7 AP Maximum entropy, words (Ratnaparkhi et al, 1994) 77.7 RRR Maximum entropy, words & classes (Ratnaparkhi et al, 1994) 81.6 RRR Decision trees (Ratnaparkhi et al, 1994) 77.7 RRR TBL (Brill and Resnik, 1994) 81.8 WordNet Maximum-Likelihood based (Collins and Brooks, 1995) 84.5 RRR Maximum-Likelihood based (Collins and Brooks, 1995) 86.1 TB2 Decision trees & WSD (Stetina and Nagao, 1997) 88.1 RRR WordNet Memory-based Learning (Zavrel et al, 1997) 84.4 RRR LexSpace Maximum entropy, unsupervised (Ratnaparkhi, 1998) 81.9 Maximum entropy, supervised (R	TBL	Transformation based learnin$Transformation-Based Learning$Transformation Based Learning$	1
Computing Dialogue Acts from Features  with TBL.	TBL	Transformation based learnin$Transformation-Based Learning$Transformation Based Learning$	1
Text Chunking Using TBL.	TBL	Transformation based learnin$Transformation-Based Learning$Transformation Based Learning$	1
For instance, the general principle of  the TBL (Brill, 1993) is  to assign to each element its most f requent  category,  and then to learn transformation rules which cor-  rect its initial categorisation.	TBL	Transformation based learnin$Transformation-Based Learning$Transformation Based Learning$	1
Text  Chunking Using TBL.	TBL	Transformation based learnin$Transformation-Based Learning$Transformation Based Learning$	2
TBL and Data- Driven Lexical Disambiguation: Syntactic and Semantic Ambiguity Resolution.	TBL	Transformation based learnin$Transformation-Based Learning$Transformation Based Learning$	2
Part-Of-Speech  Tagging and Chunking using Conditional Random  Fields and TBL.	TBL	Transformation based learnin$Transformation-Based Learning$Transformation Based Learning$	2
Avinesh PVS and Karthik G. Part-Of-Speech Tagging  and Chunking Using Conditional Random Fields and  TBL.	TBL	Transformation based learnin$Transformation-Based Learning$Transformation Based Learning$	2
Methods using both sources of information for tagging are: Hidden Markov Modeling, Maximum Entropy modeling, and TBL (Brill, 1995).	TBL	Transformation based learnin$Transformation-Based Learning$Transformation Based Learning$	2
Screening for PTSD using verbal features in self narratives: A text min- ing approach.	PTSD	posttraumatic stress disorder$Posttraumatic stress disorder$post traumatic stress disorder$	0
Measuring PTSD in Twitter.	PTSD	posttraumatic stress disorder$Posttraumatic stress disorder$post traumatic stress disorder$	2
Measuring PTSD in twitter.	PTSD	posttraumatic stress disorder$Posttraumatic stress disorder$post traumatic stress disorder$	2
Brown?s algorithm in 2) finds clusters such that PMI terms are maximized; in 3), we not only maximize the mutual information, but we also reduce the effective V by ensuring that each cluster (or state) specializes and represents as few words as possible.	PMI	pairwise mutual information$pointwise mutual information$Pointwise Mutual Information$	0
62 -,52 0   confidence , ,2 0  PMI ,    	 ,   -,2 .	PMI	pairwise mutual information$pointwise mutual information$Pointwise Mutual Information$	0
Generating Object-Trouble Pairs To generate and rank object-trouble pairs we use a variant of PMI that scores an object- trouble pair ?	PMI	pairwise mutual information$pointwise mutual information$Pointwise Mutual Information$	0
Other approaches for query similarity use sta- tistical translation models (Riezler et al, 2008), analysing search engine logs (Jones et al, 2006), looking for different anchor texts pointing to the same pages (Kraft and Zien, 2004), or replacing query words with other words that have the high- est PMI (Terra and Clarke, 2004).	PMI	pairwise mutual information$pointwise mutual information$Pointwise Mutual Information$	1
We then convert the raw counts to positive PMI scores, which has been shown to improve word similarity correlation results (Turney and Pantel, 2010).	PMI	pairwise mutual information$pointwise mutual information$Pointwise Mutual Information$	1
Espresso takes advantage of PMI (pmi) (Manning and Schu?tze, 1999) between instances and patterns to evaluate their reliability.	PMI	pairwise mutual information$pointwise mutual information$Pointwise Mutual Information$	1
Medians and maxima of pairwise collocation statistics for tokens for a particular size of ngram motifs: we use the following statis- tics: PMI, Chi- square statistic, and conditional probability.	PMI	pairwise mutual information$pointwise mutual information$Pointwise Mutual Information$	1
Stream- ing PMI.	PMI	pairwise mutual information$pointwise mutual information$Pointwise Mutual Information$	1
The strength of co-occurrence (as measured by PMI) be- tween two contrasting word pairs is taken to be the degree of antonymy.	PMI	pairwise mutual information$pointwise mutual information$Pointwise Mutual Information$	1
We also study the behavior of approxi- mate PMI and Log Likeli- hood Ratio for the sketches.	PMI	pairwise mutual information$pointwise mutual information$Pointwise Mutual Information$	2
2013), we use the top 10K most frequent content lemmas as context features, PMI as weight- ing method and we reduce the dimensionality of the data by both Non-negative Matrix Factoriza- tion (NMF, Lee and Seung (2000)) and Singular Value Decomposition (SVD).	PMI	pairwise mutual information$pointwise mutual information$Pointwise Mutual Information$	2
Streaming PMI.	PMI	pairwise mutual information$pointwise mutual information$Pointwise Mutual Information$	2
III, 2011a), computing approximate as- sociation scores like PMI (Li et al 2008; Van Durme and Lall, 2009b; Goyal and Daume?	PMI	pairwise mutual information$pointwise mutual information$Pointwise Mutual Information$	2
Sev- eral sources of information are used to identify the best label including PMI scores, WordNet hypernymy relations and distribu- tional similarity.	PMI	pairwise mutual information$pointwise mutual information$Pointwise Mutual Information$	2
Thus, the author computes  the PMI score between  seed words and new words on the basis of the  number of AltaVista hits returned when querying  the seed word and the word to be classified with  the ?	PMI	pairwise mutual information$pointwise mutual information$Pointwise Mutual Information$	2
Unsupervised Large Vocabulary WSDn with Graph-based Al- gorithms for Sequence Data Labeling, In Proc.	WSD	Word Sense Disambiguatio$Word Sense Disambiguation$Word sense disambiguation$word sense disambiguation$word-sense disambiguation$	0
Combin- ing Lexical and Syntactic Features for Supervised WSDn. (	WSD	Word Sense Disambiguatio$Word Sense Disambiguation$Word sense disambiguation$word sense disambiguation$word-sense disambiguation$	0
In Proceedings of the ACL Work- shop on WSDn: Recent Successes and Future Directions, pages 40?46, Philadelphia.	WSD	Word Sense Disambiguatio$Word Sense Disambiguation$Word sense disambiguation$word sense disambiguation$word-sense disambiguation$	0
70  Proceedings of the 4th International Workshop on Cross Lingual Information Access at COLING 2010, page 1, Beijing, August 2010 WSDn and IR Pushpak Bhattacharya Department of Computer Science & Engineering, Indian Institute of Technology Bombay, Powai, Mumbai 400076, India pb@cse.iitb.ac.in 1                                                                 Edmonton, May-June 2003                                                                      Tutorials , pg.	WSD	Word Sense Disambiguatio$Word Sense Disambiguation$Word sense disambiguation$word sense disambiguation$word-sense disambiguation$	0
c?2013 Association for Computational Linguistics DALE: A WSDn System for Biomedical Documents Trained using Automatically Labeled Examples Judita Preiss and Mark Stevenson Department of Computer Science, University of Sheffield Regent Court, 211 Portobello Sheffield S1 4DP, United Kingdom j.preiss,m.stevenson@dcs.shef.ac.uk Abstract Automatic interpretation of documents is hampered by the fact that language contains terms which have	WSD	Word Sense Disambiguatio$Word Sense Disambiguation$Word sense disambiguation$word sense disambiguation$word-sense disambiguation$	0
Unsupervised Graph- based WSDn Using Measures of Word Semantic Similarity.	WSD	Word Sense Disambiguatio$Word Sense Disambiguation$Word sense disambiguation$word sense disambiguation$word-sense disambiguation$	0
Unsupervised Large Vocabulary WSD with Graph-based Al- gorithms for Sequence Data Labeling, In Proc.	WSD	Word Sense Disambiguatio$Word Sense Disambiguation$Word sense disambiguation$word sense disambiguation$word-sense disambiguation$	1
Combin- ing Lexical and Syntactic Features for Supervised WSD. (	WSD	Word Sense Disambiguatio$Word Sense Disambiguation$Word sense disambiguation$word sense disambiguation$word-sense disambiguation$	1
In Proceedings of the ACL Work- shop on WSD: Recent Successes and Future Directions, pages 40?46, Philadelphia.	WSD	Word Sense Disambiguatio$Word Sense Disambiguation$Word sense disambiguation$word sense disambiguation$word-sense disambiguation$	1
70  Proceedings of the 4th International Workshop on Cross Lingual Information Access at COLING 2010, page 1, Beijing, August 2010 WSD and IR Pushpak Bhattacharya Department of Computer Science & Engineering, Indian Institute of Technology Bombay, Powai, Mumbai 400076, India pb@cse.iitb.ac.in 1                                                                 Edmonton, May-June 2003                                                                      Tutorials , pg.	WSD	Word Sense Disambiguatio$Word Sense Disambiguation$Word sense disambiguation$word sense disambiguation$word-sense disambiguation$	1
c?2013 Association for Computational Linguistics DALE: A WSD System for Biomedical Documents Trained using Automatically Labeled Examples Judita Preiss and Mark Stevenson Department of Computer Science, University of Sheffield Regent Court, 211 Portobello Sheffield S1 4DP, United Kingdom j.preiss,m.stevenson@dcs.shef.ac.uk Abstract Automatic interpretation of documents is hampered by the fact that language contains terms which have	WSD	Word Sense Disambiguatio$Word Sense Disambiguation$Word sense disambiguation$word sense disambiguation$word-sense disambiguation$	1
Unsupervised Graph- based WSD Using Measures of Word Semantic Similarity.	WSD	Word Sense Disambiguatio$Word Sense Disambiguation$Word sense disambiguation$word sense disambiguation$word-sense disambiguation$	1
WSD: A survey.	WSD	Word Sense Disambiguatio$Word Sense Disambiguation$Word sense disambiguation$word sense disambiguation$word-sense disambiguation$	2
WSD using statistical models of Roget's categories trained on large cor- pora.	WSD	Word Sense Disambiguatio$Word Sense Disambiguation$Word sense disambiguation$word sense disambiguation$word-sense disambiguation$	2
WSD: MWEs tend to be less polysemous than simple words.	WSD	Word Sense Disambiguatio$Word Sense Disambiguation$Word sense disambiguation$word sense disambiguation$word-sense disambiguation$	2
WSD using conceptual density.	WSD	Word Sense Disambiguatio$Word Sense Disambiguation$Word sense disambiguation$word sense disambiguation$word-sense disambiguation$	2
WSD using a second language monolingual corpus.	WSD	Word Sense Disambiguatio$Word Sense Disambiguation$Word sense disambiguation$word sense disambiguation$word-sense disambiguation$	2
A com- parative study of support vector machines applied  to the supervised WSD prob- lem in the medical domain.	WSD	Word Sense Disambiguatio$Word Sense Disambiguation$Word sense disambiguation$word sense disambiguation$word-sense disambiguation$	3
Similarity-based meth- ods for WSD.	WSD	Word Sense Disambiguatio$Word Sense Disambiguation$Word sense disambiguation$word sense disambiguation$word-sense disambiguation$	3
Graph- based WSD of biomedical docu- ments.	WSD	Word Sense Disambiguatio$Word Sense Disambiguation$Word sense disambiguation$word sense disambiguation$word-sense disambiguation$	3
Artificial sense tagged corpora is used  to evaluate WSD algorithms and is  created by adding ambiguity to a corpus.	WSD	Word Sense Disambiguatio$Word Sense Disambiguation$Word sense disambiguation$word sense disambiguation$word-sense disambiguation$	3
It makes sense: A wide- coverage WSD system for free text.	WSD	Word Sense Disambiguatio$Word Sense Disambiguation$Word sense disambiguation$word sense disambiguation$word-sense disambiguation$	3
One has to realize that even though such a performance might be adequate for some tasks (such as WSD), for many other (such as parsing or translation) the implied sentence error rate at 50% or more is sim- ply too much to deal with.	WSD	Word Sense Disambiguatio$Word Sense Disambiguation$Word sense disambiguation$word sense disambiguation$word-sense disambiguation$	3
We are not aware of any studies on the impact of WSD on restricted domains and certainly this area is wo	WSD	Word Sense Disambiguatio$Word Sense Disambiguation$Word sense disambiguation$word sense disambiguation$word-sense disambiguation$	4
A statistical model for parsing and WSD.	WSD	Word Sense Disambiguatio$Word Sense Disambiguation$Word sense disambiguation$word sense disambiguation$word-sense disambiguation$	4
Constru(:ting bayesian etworks from Word-  Net for WSD: I/.el)resenta-  tional and I)rocessing issues.	WSD	Word Sense Disambiguatio$Word Sense Disambiguation$Word sense disambiguation$word sense disambiguation$word-sense disambiguation$	4
We are not aware of any studies on the impact of WSD on restricted domains and certainly this area is worth exploring.	WSD	Word Sense Disambiguatio$Word Sense Disambiguation$Word sense disambiguation$word sense disambiguation$word-sense disambiguation$	4
This sense of print is not available in WordNet5 and therefore it is not possible to apply WSD techniques to find the appropriate sense.	WSD	Word Sense Disambiguatio$Word Sense Disambiguation$Word sense disambiguation$word sense disambiguation$word-sense disambiguation$	4
and Vicedo Question Answering in Restricted Domains: An Overview it should be noted that some words would still have several senses available and therefore WSD still plays a role.	WSD	Word Sense Disambiguatio$Word Sense Disambiguation$Word sense disambiguation$word sense disambiguation$word-sense disambiguation$	4
The impact of WSD is possibly reduced in RDQA, though 46 Molla?	WSD	Word Sense Disambiguatio$Word Sense Disambiguation$Word sense disambiguation$word sense disambiguation$word-sense disambiguation$	4
For the latter, we use seven types of information generally accepted to be personally II in- formation (McCallister, 2010), as listed in the left column of Table 2.	II	identifiable$itlll pluase eltil$is finished$is worth noting$	0
For this reason, such an item can always be referred to as  "established," or "recoverable," or "II" in the terminology of Halliday (1967) or Chafe (1976).	II	identifiable$itlll pluase eltil$is finished$is worth noting$	0
THE S-RELATORS OF IVIR EEl" AL  In a study unfortunately ittle known, \[Ivir et al1973\]  categorize with uncommon completeness numerous "S-  relators" -- syntactic structures associated with II  semantic links among the meanings of sentences or clauses.	II	identifiable$itlll pluase eltil$is finished$is worth noting$	0
We will term this kind of composition, where the  same resource means different things in different  contexts, "non-linear" composition; this is in contrast to  "linear" composition, where each resource contributes an  II part of the whole and what it contributes i not context dependent.	II	identifiable$itlll pluase eltil$is finished$is worth noting$	0
Guide to protecting the confi- dentiality of personally II information.	II	identifiable$itlll pluase eltil$is finished$is worth noting$	0
Each Turker is II only through an anonymous ID like A23KO2TP7I4KK2.)	II	identifiable$itlll pluase eltil$is finished$is worth noting$	0
Engaged - it is committed a conversation The most obvious case of engagement is when the person and the machine are having a conversation - that is Listening and Talking to each other, how- ever even if the conversation II, the sys- tem may still want to keep the context of the recent discussion.	II	identifiable$itlll pluase eltil$is finished$is worth noting$	2
When the root node is traversed, the trans- lating II.	II	identifiable$itlll pluase eltil$is finished$is worth noting$	2
When the student II with the translation, she clicks on a ?	II	identifiable$itlll pluase eltil$is finished$is worth noting$	2
Once the input II, it appears in text on the left side of the screen.	II	identifiable$itlll pluase eltil$is finished$is worth noting$	2
Thus, by the time Stage 2  processing II, all information about the surface lin-  guistic form is gone.	II	identifiable$itlll pluase eltil$is finished$is worth noting$	2
In (2) it is the case that the paper II, but it would be hard to claim that anything or anyone is up.	II	identifiable$itlll pluase eltil$is finished$is worth noting$	2
It II that the TED accuracy (Tsarfaty et al 2011) for the lattices is 0.8305 which is ranked second.	II	identifiable$itlll pluase eltil$is finished$is worth noting$	3
It II that the experiments with Mal- tOptimizer do not take so long.	II	identifiable$itlll pluase eltil$is finished$is worth noting$	3
Finally it II that the TED accuracy 3DASHTAG comes from the original constituent data, when a DASHTAG was present in a head node label, this feature was kept in the Catib corpus.	II	identifiable$itlll pluase eltil$is finished$is worth noting$	3
The MaltOptimizer process was sped up following heuristics derived from deep proven experience (Nivre and Hall, 2010), which means that there are several combinations that are untested; however, it II that these heuristics re- sulted in similar performance to more exhaustive search for a big set of languages (Ballesteros, 2013).	II	identifiable$itlll pluase eltil$is finished$is worth noting$	3
It II that it can be applied to any treebank in CoNLL data format.1 The rest of the paper is organized as follows.	II	identifiable$itlll pluase eltil$is finished$is worth noting$	3
It II that meaningless iteration of  words (especially, of function words) has less influ-  ence on the text similarity:  a("It is a dog.",	II	identifiable$itlll pluase eltil$is finished$is worth noting$	3
EL1 was  a semantically driven parser which maps English language  sentences into the CD \[6\]  representations of their meanings, it made extensive  use of the semantic properties of the words being  processed, but interacted only slightly with the rest of  the understanding processes it was a part of.	CD	Conceptual Dependency$cardinal$cardi-  nal$Conceptual Dependenc$Conceptual Density$Contrastive Divergence$cardinal number$content determination$	0
It 3 s  not apparent  t h a t  d e f i n i t i o n s  of  themes can be con t ro l l ed  i n  CD Theory, i n  t h a t  if  s t a t e d  i n  terms of  primitives, each a b s t r a c t  term could become ex-  tremelv l a rge .	CD	Conceptual Dependency$cardinal$cardi-  nal$Conceptual Dependenc$Conceptual Density$Contrastive Divergence$cardinal number$content determination$	0
Shank R.C. CD: a theory of natnral language understanding.	CD	Conceptual Dependency$cardinal$cardi-  nal$Conceptual Dependenc$Conceptual Density$Contrastive Divergence$cardinal number$content determination$	0
it would  pass o f f  a completed CD  representation of each sentence to SAM or PAM which  would try to incorporate it into an overall story  r	CD	Conceptual Dependency$cardinal$cardi-  nal$Conceptual Dependenc$Conceptual Density$Contrastive Divergence$cardinal number$content determination$	0
It seems to me not untrue historically to claim that RDF, the representational base of the SW, is a return of the level of representation that Schank (under the name CD, in Schank [1975]) and I (under the name Preference Semantics) developed in the late 1960s and early 1970s (Wilks 1975).	CD	Conceptual Dependency$cardinal$cardi-  nal$Conceptual Dependenc$Conceptual Density$Contrastive Divergence$cardinal number$content determination$	0
The story is generated using the notation  of CD and is fed to another program which  translates it into English.	CD	Conceptual Dependency$cardinal$cardi-  nal$Conceptual Dependenc$Conceptual Density$Contrastive Divergence$cardinal number$content determination$	0
At Stanford as a post-doc, I was on the same corridor asWinograd, just arrived from MIT; Schank, then starting to build his CD empire; and Colby and his large team building the PARRY dialogue system, which included Larry Tesler, later the Apple software architect.	CD	Conceptual Dependency$cardinal$cardi-  nal$Conceptual Dependenc$Conceptual Density$Contrastive Divergence$cardinal number$content determination$	0
one where all five systems were involved, the other where subsets of CDity 2 of these crowds were re-combined.10 4 Results As already described in Section 2.2, we performed two types of experiments.	CD	Conceptual Dependency$cardinal$cardi-  nal$Conceptual Dependenc$Conceptual Density$Contrastive Divergence$cardinal number$content determination$	1
c?2007 Association for Computational Linguistics Real-Time Correction of Closed-Captions P. Cardinal, G. Boulianne, M. Comeau, M. Boisvert Centre de recherche Informatique de Montreal (CRIM) Montreal, Canada patrick.CD@crim.ca Abstract Live closed-captions for deaf and hard of hearing audiences are currently produced by stenographers, or by voice writers us- ing speech recognition.	CD	Conceptual Dependency$cardinal$cardi-  nal$Conceptual Dependenc$Conceptual Density$Contrastive Divergence$cardinal number$content determination$	1
| denotes the CDity of the set, and ?	CD	Conceptual Dependency$cardinal$cardi-  nal$Conceptual Dependenc$Conceptual Density$Contrastive Divergence$cardinal number$content determination$	1
It will be used to associate some approxi- mate CDity to the set of elements selected in the Loc- Geo, allowing to compute some ?	CD	Conceptual Dependency$cardinal$cardi-  nal$Conceptual Dependenc$Conceptual Density$Contrastive Divergence$cardinal number$content determination$	1
The higher role CD- ity models were not possible to create because of the very high computational requirements.	CD	Conceptual Dependency$cardinal$cardi-  nal$Conceptual Dependenc$Conceptual Density$Contrastive Divergence$cardinal number$content determination$	1
They are summarized here:  I name I class operations  cent ains/uses  1,2...N relation CDity  inherits  Figure 1: symboles used in the figures  1.2 Extens ions  to  LFG fo rmal i sm  Four types of equations are defined in classical LFG  (Bresnan and Kaplan, 1981):  1.	CD	Conceptual Dependency$cardinal$cardi-  nal$Conceptual Dependenc$Conceptual Density$Contrastive Divergence$cardinal number$content determination$	1
EL1 was  a semantically driven parser which maps English language  sentences into the CDy \[6\]  representations of their meanings, it made extensive  use of the semantic properties of the words being  processed, but interacted only slightly with the rest of  the understanding processes it was a part of.	CD	Conceptual Dependency$cardinal$cardi-  nal$Conceptual Dependenc$Conceptual Density$Contrastive Divergence$cardinal number$content determination$	3
It 3 s  not apparent  t h a t  d e f i n i t i o n s  of  themes can be con t ro l l ed  i n  CDy Theory, i n  t h a t  if  s t a t e d  i n  terms of  primitives, each a b s t r a c t  term could become ex-  tremelv l a rge .	CD	Conceptual Dependency$cardinal$cardi-  nal$Conceptual Dependenc$Conceptual Density$Contrastive Divergence$cardinal number$content determination$	3
Shank R.C. CDy: a theory of natnral language understanding.	CD	Conceptual Dependency$cardinal$cardi-  nal$Conceptual Dependenc$Conceptual Density$Contrastive Divergence$cardinal number$content determination$	3
it would  pass o f f  a completed CDy  representation of each sentence to SAM or PAM which  would try to incorporate it into an overall story  r	CD	Conceptual Dependency$cardinal$cardi-  nal$Conceptual Dependenc$Conceptual Density$Contrastive Divergence$cardinal number$content determination$	3
It seems to me not untrue historically to claim that RDF, the representational base of the SW, is a return of the level of representation that Schank (under the name CDy, in Schank [1975]) and I (under the name Preference Semantics) developed in the late 1960s and early 1970s (Wilks 1975).	CD	Conceptual Dependency$cardinal$cardi-  nal$Conceptual Dependenc$Conceptual Density$Contrastive Divergence$cardinal number$content determination$	3
The story is generated using the notation  of CDy and is fed to another program which  translates it into English.	CD	Conceptual Dependency$cardinal$cardi-  nal$Conceptual Dependenc$Conceptual Density$Contrastive Divergence$cardinal number$content determination$	3
At Stanford as a post-doc, I was on the same corridor asWinograd, just arrived from MIT; Schank, then starting to build his CDy empire; and Colby and his large team building the PARRY dialogue system, which included Larry Tesler, later the Apple software architect.	CD	Conceptual Dependency$cardinal$cardi-  nal$Conceptual Dependenc$Conceptual Density$Contrastive Divergence$cardinal number$content determination$	3
54  Word Sense Disambiguation  using CD  Eneko Agirre*  Lengoaia eta Sistema Informatikoak saila.	CD	Conceptual Dependency$cardinal$cardi-  nal$Conceptual Dependenc$Conceptual Density$Contrastive Divergence$cardinal number$content determination$	4
The automatic decision procedure for lexical  ambiguity resolution presented in this paper is based  on an elaboration of the conceptual distance among  concepts: CD \[Agirre & Rigau 95\].	CD	Conceptual Dependency$cardinal$cardi-  nal$Conceptual Dependenc$Conceptual Density$Contrastive Divergence$cardinal number$content determination$	4
Word  Sense Disambiguation using CD.	CD	Conceptual Dependency$cardinal$cardi-  nal$Conceptual Dependenc$Conceptual Density$Contrastive Divergence$cardinal number$content determination$	4
The method  relies on the use oil' the wide-coverage noun  taxonomy of WordNet and the notion of  conceptual distance among concepts, captured by  a CD formula developed for this  purpose.	CD	Conceptual Dependency$cardinal$cardi-  nal$Conceptual Dependenc$Conceptual Density$Contrastive Divergence$cardinal number$content determination$	4
3_(5  2 CD and Word  Sense Disambiguation  Conceptual distance tries to provide a basis for  measuring closeness in meaning among words, taking  as reference a structured hierarchical net.	CD	Conceptual Dependency$cardinal$cardi-  nal$Conceptual Dependenc$Conceptual Density$Contrastive Divergence$cardinal number$content determination$	4
Our system tries to resolve the lexical  ambiguity ot' nouns by finding the combination of  senses from a set of contiguous nouns that  maximises the CD among senses.	CD	Conceptual Dependency$cardinal$cardi-  nal$Conceptual Dependenc$Conceptual Density$Contrastive Divergence$cardinal number$content determination$	4
e conceptual distance among  concepts: CD \[Agirre & Rigau 95\].	CD	Conceptual Dependency$cardinal$cardi-  nal$Conceptual Dependenc$Conceptual Density$Contrastive Divergence$cardinal number$content determination$	4
Though exact inference with this class of models is infeasible we use an effi- cient approximation (Bengio and Delalleau, 2007), which can be regarded either as a mean-field approx- imation to the reconstruction error or a determinis- tic version of the CD sampling method (Hinton, 2002).	CD	Conceptual Dependency$cardinal$cardi-  nal$Conceptual Dependenc$Conceptual Density$Contrastive Divergence$cardinal number$content determination$	5
We use 1-step CD (Hinton, 2002) to update the parameters by performing gradient ascent: ?	CD	Conceptual Dependency$cardinal$cardi-  nal$Conceptual Dependenc$Conceptual Density$Contrastive Divergence$cardinal number$content determination$	5
Traditional learning strategies such as CD are very inef- ficient.	CD	Conceptual Dependency$cardinal$cardi-  nal$Conceptual Dependenc$Conceptual Density$Contrastive Divergence$cardinal number$content determination$	5
Usually many steps of Gibbs-Sampling are necessary to get an unbiased sample from the distribution, but in the CD algorithm only one step of sampling is performed (Hinton, 2002).	CD	Conceptual Dependency$cardinal$cardi-  nal$Conceptual Dependenc$Conceptual Density$Contrastive Divergence$cardinal number$content determination$	5
We also tried a CD based training procedure for TRBM instead of equation 7, but that resulted in about an absolute 10% lower LAS.	CD	Conceptual Dependency$cardinal$cardi-  nal$Conceptual Dependenc$Conceptual Density$Contrastive Divergence$cardinal number$content determination$	5
The training objective is to learn a set of weights that maximize the likelihood of training observations, and given the independences inherent, in the model it can be trained quickly and effectively via CD.	CD	Conceptual Dependency$cardinal$cardi-  nal$Conceptual Dependenc$Conceptual Density$Contrastive Divergence$cardinal number$content determination$	5
Noun phrases with certain words, such as non-article determiners (e.g., this car), possessive pronouns (e.g., his car), CDs (e.g., one car) or quantifiers (e.g., some cars), also fall into this category.	CD	Conceptual Dependency$cardinal$cardi-  nal$Conceptual Dependenc$Conceptual Density$Contrastive Divergence$cardinal number$content determination$	6
The quantification can  be by a CD, or by a more vague  expression, like several or many.	CD	Conceptual Dependency$cardinal$cardi-  nal$Conceptual Dependenc$Conceptual Density$Contrastive Divergence$cardinal number$content determination$	6
e Classifier (HP?Obj) High?Precision Subjective Sentence Classifier (HP?Subj) Unannotated Text Collection unlabeled sentences unlabeled sentences unlabeled sentences Pattern?based Subjective Sentence Classifier Extraction Pattern Learner subjective sentences subjective sentences objective sentences subjective patterns subjective patterns Figure 1: Bootstrapping Process jective class (examples are CDs (Wiebe et al.,	CD	Conceptual Dependency$cardinal$cardi-  nal$Conceptual Dependenc$Conceptual Density$Contrastive Divergence$cardinal number$content determination$	6
Some of the  prenominal elements coming before a noun head  are CDs, ordinal numbers, superla- tive adjectives, and indefinite determiners; post- nominal elements are nouns and noun phrases,  adjectives and adjectival phrases, adjectival  clauses with conjunctions, indefinite post- determiners, prepositional phrases, adverbs of  place and time, ordinal numbers, possessive ad- jectives, and Ezafeh.	CD	Conceptual Dependency$cardinal$cardi-  nal$Conceptual Dependenc$Conceptual Density$Contrastive Divergence$cardinal number$content determination$	6
  CD except one  ?	CD	Conceptual Dependency$cardinal$cardi-  nal$Conceptual Dependenc$Conceptual Density$Contrastive Divergence$cardinal number$content determination$	6
e word form of the adjective that modifies the head noun ModByAdjTag The POS tag of the adjective that modifies the head noun ModByPrep Indicates that the head noun is modified by a preposition ModByPrepWord The word form of the preposition that modifies the head noun ModByPossesive Indicates that the head noun is modified by a possesive ModByCardinal Indicates that the head noun is modified by a CD ModByRelative Indicates that the head noun is modified by a relative clause Table 2: Feature templates for English determiner correction.	CD	Conceptual Dependency$cardinal$cardi-  nal$Conceptual Dependenc$Conceptual Density$Contrastive Divergence$cardinal number$content determination$	6
Lexicalisation is often decomposed into two different stages: lexical choice, which occurs dur- ing CD, where the lexical term chosen, which may still be underspecified, is depen- dent on reasoning procedures, the knowledge base contents, and grammatical constraints; and lexical variation which is the choice of a particular word or form among possible synonyms or paraphrases.	CD	Conceptual Dependency$cardinal$cardi-  nal$Conceptual Dependenc$Conceptual Density$Contrastive Divergence$cardinal number$content determination$	7
For instance, distinguish two levels of CD, ?	CD	Conceptual Dependency$cardinal$cardi-  nal$Conceptual Dependenc$Conceptual Density$Contrastive Divergence$cardinal number$content determination$	7
After the command is executed, the CD module constructs the se- mantic representation of the effects that were ap- plied, updates the player KB with it and passes it to the next module for its verbalization (so that the player knows what changed in the world).	CD	Conceptual Dependency$cardinal$cardi-  nal$Conceptual Dependenc$Conceptual Density$Contrastive Divergence$cardinal number$content determination$	7
nce  E Reiter and R Dale 			 Building Natural Language Generation Systems Cambridge University Press  http  xmlapacheorgxalanjindexhtml H Somers 			 Machine translation In R Dale H Moisl and H Somers editors Handbook of Nat  ural Language Processing chapter  pages    Marcel Dekker S G Sripada E Reiter J Hunter and J Yu 		 A twostage model for CD In Proc of the th European Workshop on Natural Language Generation associated to ACL th Ann Meeting and th Conf of the European Chapter pages 	 Toulouse France July   A The Implementation Code in Document Planning Stage A Code Fragment in Document Structuring if  fxoBasketOpen Host   if  fxoBasketLogin Alias UserID Password   if  fxoBasket	CD	Conceptual Dependency$cardinal$cardi-  nal$Conceptual Dependenc$Conceptual Density$Contrastive Divergence$cardinal number$content determination$	7
This is a challenge for both CD and mi- croplanning.	CD	Conceptual Dependency$cardinal$cardi-  nal$Conceptual Dependenc$Conceptual Density$Contrastive Divergence$cardinal number$content determination$	7
Danlos's system  thus performs CD and lexical choice before discourse organization  and syntactic realization (a very original position).	CD	Conceptual Dependency$cardinal$cardi-  nal$Conceptual Dependenc$Conceptual Density$Contrastive Divergence$cardinal number$content determination$	7
14 Qiu et al Opinion Word Expansion and Target Extraction through DP 4.1 Propagation Rules Defined Based on Relations In our propagation, there are four subtasks: (1) extracting targets using opinion words; (2) extracting targets using the extracted targets; (3) extracting opinion words using the extracted targets; (4) extracting opinion words using both the given and the extracted opinion words.	DP	Double Propagation$Dynamic programming$dependency relationships$dynamic programming$dependency parsing$Document Processor$Dirichlet process$Dependency Parsing$Discriminatory Power$Dirichlet Process$	0
10 Qiu et al Opinion Word Expansion and Target Extraction through DP 2.1 Opinion Word Extraction Extensive work has been done on sentiment analysis at word, expression (Breck, Choi, and Cardie 2007; Takamura, Inui, and Okumura 2007), sentence (Yu and Hatzivassiloglou 2003; Kim and Hovy 2004) and document (Pang, Lee, and Vaithyanathan 2002; Turney 2002) levels.	DP	Double Propagation$Dynamic programming$dependency relationships$dynamic programming$dependency parsing$Document Processor$Dirichlet process$Dependency Parsing$Discriminatory Power$Dirichlet Process$	0
16 Qiu et al Opinion Word Expansion and Target Extraction through DP Figure 3 The propagation algorithm.	DP	Double Propagation$Dynamic programming$dependency relationships$dynamic programming$dependency parsing$Document Processor$Dirichlet process$Dependency Parsing$Discriminatory Power$Dirichlet Process$	0
18 Qiu et al Opinion Word Expansion and Target Extraction through DP In this work, we filter non-targets based on frequency.	DP	Double Propagation$Dynamic programming$dependency relationships$dynamic programming$dependency parsing$Document Processor$Dirichlet process$Dependency Parsing$Discriminatory Power$Dirichlet Process$	0
Relation Identification As stated previously, identification of the relations between opinion words/targets and other opinion words/targets is the key to our opinion lexicon expansion and target 12 Qiu et al Opinion Word Expansion and Target Extraction through DP extraction methods.	DP	Double Propagation$Dynamic programming$dependency relationships$dynamic programming$dependency parsing$Document Processor$Dirichlet process$Dependency Parsing$Discriminatory Power$Dirichlet Process$	0
1510  Opinion Word Expansion and Target Extraction through DP Guang Qiu?	DP	Double Propagation$Dynamic programming$dependency relationships$dynamic programming$dependency parsing$Document Processor$Dirichlet process$Dependency Parsing$Discriminatory Power$Dirichlet Process$	0
Figure 4: DP estimation of q?.	DP	Double Propagation$Dynamic programming$dependency relationships$dynamic programming$dependency parsing$Document Processor$Dirichlet process$Dependency Parsing$Discriminatory Power$Dirichlet Process$	1
1 Introduction DP algorithms, also known as tabular or chart-based algorithms, are at the core of many applications in natural language processing.	DP	Double Propagation$Dynamic programming$dependency relationships$dynamic programming$dependency parsing$Document Processor$Dirichlet process$Dependency Parsing$Discriminatory Power$Dirichlet Process$	1
DP for linear-time incremental parsing.	DP	Double Propagation$Dynamic programming$dependency relationships$dynamic programming$dependency parsing$Document Processor$Dirichlet process$Dependency Parsing$Discriminatory Power$Dirichlet Process$	1
1.1 DP as deduction The ?	DP	Double Propagation$Dynamic programming$dependency relationships$dynamic programming$dependency parsing$Document Processor$Dirichlet process$Dependency Parsing$Discriminatory Power$Dirichlet Process$	1
DP search for continuous speech recognition.	DP	Double Propagation$Dynamic programming$dependency relationships$dynamic programming$dependency parsing$Document Processor$Dirichlet process$Dependency Parsing$Discriminatory Power$Dirichlet Process$	1
Possible kinds of LEs are: word forms, parts-of-speech, DP, syntactic phrases, named entities, semantic roles, etc.	DP	Double Propagation$Dynamic programming$dependency relationships$dynamic programming$dependency parsing$Document Processor$Dirichlet process$Dependency Parsing$Discriminatory Power$Dirichlet Process$	2
Dependency Relations (DR) restrains word co- occurrence to words that are reachable from the multi-sense word via a syntactic parse composed of DP limited by some length (Pado?	DP	Double Propagation$Dynamic programming$dependency relationships$dynamic programming$dependency parsing$Document Processor$Dirichlet process$Dependency Parsing$Discriminatory Power$Dirichlet Process$	2
The logical metaf~nction is responsible for the con-  struction of composite semantic entities using the re-  sources of interdependency; it is manifested in grammar  by DP such as those that hold be-  tween the head of a phrase and its dependents and the  association of concepts to be expressed with particu-  lar heads in the sentence structure.	DP	Double Propagation$Dynamic programming$dependency relationships$dynamic programming$dependency parsing$Document Processor$Dirichlet process$Dependency Parsing$Discriminatory Power$Dirichlet Process$	2
The strategy only considers adjectives in successive sentences and does not use features or any DP.	DP	Double Propagation$Dynamic programming$dependency relationships$dynamic programming$dependency parsing$Document Processor$Dirichlet process$Dependency Parsing$Discriminatory Power$Dirichlet Process$	2
Next, by as- suming DP between the bunsetsus, candidate dependency trees are con- structed.	DP	Double Propagation$Dynamic programming$dependency relationships$dynamic programming$dependency parsing$Document Processor$Dirichlet process$Dependency Parsing$Discriminatory Power$Dirichlet Process$	2
To do this, we first identify nsubj and nsubjpass DP.	DP	Double Propagation$Dynamic programming$dependency relationships$dynamic programming$dependency parsing$Document Processor$Dirichlet process$Dependency Parsing$Discriminatory Power$Dirichlet Process$	2
To solve riddles, we utilize a DP algorithm to combine the identified metaphors based on the alignments and rules to obtain the candidate solutions.	DP	Double Propagation$Dynamic programming$dependency relationships$dynamic programming$dependency parsing$Document Processor$Dirichlet process$Dependency Parsing$Discriminatory Power$Dirichlet Process$	3
As with hidden Markov models (Rabiner, 1989), yw(x) can be com- puted efficiently for suitable feature functions using DP.	DP	Double Propagation$Dynamic programming$dependency relationships$dynamic programming$dependency parsing$Document Processor$Dirichlet process$Dependency Parsing$Discriminatory Power$Dirichlet Process$	3
A CKY style DP  chart parser is used to find the maximum probability  tree for each sentence (see figure 6).	DP	Double Propagation$Dynamic programming$dependency relationships$dynamic programming$dependency parsing$Document Processor$Dirichlet process$Dependency Parsing$Discriminatory Power$Dirichlet Process$	3
Then, in the solving phase, we utilize a DP method to combine the identified metaphors to obtain candidate solutions.	DP	Double Propagation$Dynamic programming$dependency relationships$dynamic programming$dependency parsing$Document Processor$Dirichlet process$Dependency Parsing$Discriminatory Power$Dirichlet Process$	3
A combination of these two types of methods was developed by Kikuchi et al (2003), where summarization is performed in two steps: first, sentence extraction is done through feature combination; second, compaction is done by scoring the words in each sentence and then a DP technique is applied to select the words that will remain in the sentence to be included in the summary.	DP	Double Propagation$Dynamic programming$dependency relationships$dynamic programming$dependency parsing$Document Processor$Dirichlet process$Dependency Parsing$Discriminatory Power$Dirichlet Process$	3
Second, in the solving phase, we utilize a DP algorithm on the basis of the alignments and rules to figure out the candidate solutions.	DP	Double Propagation$Dynamic programming$dependency relationships$dynamic programming$dependency parsing$Document Processor$Dirichlet process$Dependency Parsing$Discriminatory Power$Dirichlet Process$	3
This setting includes a  broad range of structured prediction problems such as semantic role labeling, named  entity and relation recognition, co-reference resolution, DP and  semantic parsing.	DP	Double Propagation$Dynamic programming$dependency relationships$dynamic programming$dependency parsing$Document Processor$Dirichlet process$Dependency Parsing$Discriminatory Power$Dirichlet Process$	4
Non-projective DP using spanning tree algorithms.	DP	Double Propagation$Dynamic programming$dependency relationships$dynamic programming$dependency parsing$Document Processor$Dirichlet process$Dependency Parsing$Discriminatory Power$Dirichlet Process$	4
The passive char- acters were identified via the following relations extracted by DP: nsubjpass (passive nominal subject) and pobj (object of a preposition).	DP	Double Propagation$Dynamic programming$dependency relationships$dynamic programming$dependency parsing$Document Processor$Dirichlet process$Dependency Parsing$Discriminatory Power$Dirichlet Process$	4
For the task of unsupervised DP, Smith and Eisner (2006) add a constraint of the form ?	DP	Double Propagation$Dynamic programming$dependency relationships$dynamic programming$dependency parsing$Document Processor$Dirichlet process$Dependency Parsing$Discriminatory Power$Dirichlet Process$	4
We show that the combination of data-driven and rule-based components can reduce the number of all parsing errors by 14% and raise the attachment accuracy for DP of German to an unprecedented 92%.	DP	Double Propagation$Dynamic programming$dependency relationships$dynamic programming$dependency parsing$Document Processor$Dirichlet process$Dependency Parsing$Discriminatory Power$Dirichlet Process$	4
This includes (i) tokenization, (ii) sen- tence splitting and identification of paragraph boundaries, (iii) part-of-speech (POS) tagging, (iv) lemmatization, (v) named entity recognition, (vi) DP, and (vii) co-reference analysis.	DP	Double Propagation$Dynamic programming$dependency relationships$dynamic programming$dependency parsing$Document Processor$Dirichlet process$Dependency Parsing$Discriminatory Power$Dirichlet Process$	4
DP Knowledge Resources Lexicon Resources Grammars Process Manager Tokenlist Legend Output Manager Source Document NLP/IE Processor(s)Tokenizer Tokenlist Lexicon Lookup  POS Tagging Named Entity Detection Shallow Parsing Deep Parsing Relationship Detection Document pool NE CE EP SVO Time Normalization Alias and Coreference Profile/Event Linking/Merging Abbreviations POS = Part of Sp	DP	Double Propagation$Dynamic programming$dependency relationships$dynamic programming$dependency parsing$Document Processor$Dirichlet process$Dependency Parsing$Discriminatory Power$Dirichlet Process$	5
DP Knowledge Resources Lexicon Resources Grammars Process Manager Tokenlist Legend Output Manager Source Document Linguistic Processor(s)Tokenizer Tokenlist Lexicon Lookup Pragmatic Filtering  POS Tagging Named Entity Detection Shallow Parsing Deep Parsing Relationship Detection Document pool NE CE EP SVO Time Normalization Alias/Coreference Linking Profile/Event Linking Profile/Ev	DP	Double Propagation$Dynamic programming$dependency relationships$dynamic programming$dependency parsing$Document Processor$Dirichlet process$Dependency Parsing$Discriminatory Power$Dirichlet Process$	5
DP Knowledge Resources Lexicon Resources Grammars Process Manager Tokenlist Legend Output Manager Source Document Linguistic Processor(s)Tokenizer Tokenlist Lexicon Lookup  POS Tagging NE Tagging Shallow Parsing Relationship Extraction Document pool NE CE EP SVO Time Normalization Profile/Event Consolidation Event Extraction Abbreviations NE = Named Entity CE = Correlated Entity EP	DP	Double Propagation$Dynamic programming$dependency relationships$dynamic programming$dependency parsing$Document Processor$Dirichlet process$Dependency Parsing$Discriminatory Power$Dirichlet Process$	5
DPes, Chinese restaurant processes and all that.	DP	Double Propagation$Dynamic programming$dependency relationships$dynamic programming$dependency parsing$Document Processor$Dirichlet process$Dependency Parsing$Discriminatory Power$Dirichlet Process$	6
Both the hierarchical DP and the hierarchi- cal Pitman-Yor process are examples of Bayesian nonparametric processes.	DP	Double Propagation$Dynamic programming$dependency relationships$dynamic programming$dependency parsing$Document Processor$Dirichlet process$Dependency Parsing$Discriminatory Power$Dirichlet Process$	6
Hierarchical DPes.	DP	Double Propagation$Dynamic programming$dependency relationships$dynamic programming$dependency parsing$Document Processor$Dirichlet process$Dependency Parsing$Discriminatory Power$Dirichlet Process$	6
The hierarchical Pitman-Yor process is a natural generalization of the recently proposed hierarchi- cal DP (Teh et al, 2006).	DP	Double Propagation$Dynamic programming$dependency relationships$dynamic programming$dependency parsing$Document Processor$Dirichlet process$Dependency Parsing$Discriminatory Power$Dirichlet Process$	6
The hier- archical DP was proposed to solve a different problem?that of clustering, and it is interesting to note that such a direct generaliza- tion leads us to a good language model.	DP	Double Propagation$Dynamic programming$dependency relationships$dynamic programming$dependency parsing$Document Processor$Dirichlet process$Dependency Parsing$Discriminatory Power$Dirichlet Process$	6
Three New Probabilistic Models for  DP: An Exploration.	DP	Double Propagation$Dynamic programming$dependency relationships$dynamic programming$dependency parsing$Document Processor$Dirichlet process$Dependency Parsing$Discriminatory Power$Dirichlet Process$	7
Exploring Automatic Feature Selection for Transition-Based DP.	DP	Double Propagation$Dynamic programming$dependency relationships$dynamic programming$dependency parsing$Document Processor$Dirichlet process$Dependency Parsing$Discriminatory Power$Dirichlet Process$	7
2.1 Syntactic DP We use a shift-reduce scheme to implement syn- tactic dependency parsing as in (Nivre, 2003).	DP	Double Propagation$Dynamic programming$dependency relationships$dynamic programming$dependency parsing$Document Processor$Dirichlet process$Dependency Parsing$Discriminatory Power$Dirichlet Process$	7
2.2 Semantic DP Assuming no predicates overtly known, we keep using a word-pair classifier to perform semantic parsing through a single-stage processing.	DP	Double Propagation$Dynamic programming$dependency relationships$dynamic programming$dependency parsing$Document Processor$Dirichlet process$Dependency Parsing$Discriminatory Power$Dirichlet Process$	7
Exploring Morphosyntactic Annotation Over a Spanish Corpus for DP .	DP	Double Propagation$Dynamic programming$dependency relationships$dynamic programming$dependency parsing$Document Processor$Dirichlet process$Dependency Parsing$Discriminatory Power$Dirichlet Process$	7
45  CoNLL 2008: Proceedings of the 12th Conference on Computational Natural Language Learning, pages 233?237 Manchester, August 2008 Semantic DP using N-best Semantic Role Sequences and Roleset Information Joo-Young Lee, Han-Cheol Cho, and Hae-Chang Rim Natural Language Processing Lab.	DP	Double Propagation$Dynamic programming$dependency relationships$dynamic programming$dependency parsing$Document Processor$Dirichlet process$Dependency Parsing$Discriminatory Power$Dirichlet Process$	7
DP ?	DP	Double Propagation$Dynamic programming$dependency relationships$dynamic programming$dependency parsing$Document Processor$Dirichlet process$Dependency Parsing$Discriminatory Power$Dirichlet Process$	8
DP.	DP	Double Propagation$Dynamic programming$dependency relationships$dynamic programming$dependency parsing$Document Processor$Dirichlet process$Dependency Parsing$Discriminatory Power$Dirichlet Process$	8
DP We consider output of three types of MT system (Phrase-based, Syntax- based and Rule-based) to attempt to gain insight into the different types of semantic information preserved by the different systems.	DP	Double Propagation$Dynamic programming$dependency relationships$dynamic programming$dependency parsing$Document Processor$Dirichlet process$Dependency Parsing$Discriminatory Power$Dirichlet Process$	8
stands for DP adaptor (b = 0), PY stands for Pitman-Yor adaptor (b optimized), and PY+inc.	DP	Double Propagation$Dynamic programming$dependency relationships$dynamic programming$dependency parsing$Document Processor$Dirichlet process$Dependency Parsing$Discriminatory Power$Dirichlet Process$	9
Hierarchical DPes.?	DP	Double Propagation$Dynamic programming$dependency relationships$dynamic programming$dependency parsing$Document Processor$Dirichlet process$Dependency Parsing$Discriminatory Power$Dirichlet Process$	9
6.1 Model 0: Simple Mixture Model In our first model, based on Haghighi and Klein?s baseline DP model, each image i corresponds to the set of observed mentions wi from across its captions.	DP	Double Propagation$Dynamic programming$dependency relationships$dynamic programming$dependency parsing$Document Processor$Dirichlet process$Dependency Parsing$Discriminatory Power$Dirichlet Process$	9
con- trols the probability of branching via the per-node DP, and L is the fixed tree depth.	DP	Double Propagation$Dynamic programming$dependency relationships$dynamic programming$dependency parsing$Document Processor$Dirichlet process$Dependency Parsing$Discriminatory Power$Dirichlet Process$	9
Since our generative models are based on DP priors, they are de- signed to favor a small number of unique entities per image.	DP	Double Propagation$Dynamic programming$dependency relationships$dynamic programming$dependency parsing$Document Processor$Dirichlet process$Dependency Parsing$Discriminatory Power$Dirichlet Process$	9
t indicate the variance of corresponding entity dis- tribution respectively, and u r,m is the m-th com- ponent translation vector of relation r. Chinese Restaurant Process (CRP) is a DP and it can automatically detect semantic compo- nents.	DP	Double Propagation$Dynamic programming$dependency relationships$dynamic programming$dependency parsing$Document Processor$Dirichlet process$Dependency Parsing$Discriminatory Power$Dirichlet Process$	9
ICDM and Knowledge Discovery.	ICDM	In Data Mining$International Conference on Data Mining$ing$	0
ICDM and Knowledge Discovery, pages 259-275.	ICDM	In Data Mining$International Conference on Data Mining$ing$	0
ICDM and Knowledge Discovery for Big Data, pages 1?40.	ICDM	In Data Mining$International Conference on Data Mining$ing$	0
ICDM, Eighth IEEE International Con- ference on, pages 1025?1030.	ICDM	In Data Mining$International Conference on Data Mining$ing$	0
ICDM in Time Series Databases.	ICDM	In Data Mining$International Conference on Data Mining$ing$	0
ICDM for Security Ap- plications.	ICDM	In Data Mining$International Conference on Data Mining$ing$	0
of  the SIAM ICDM.	ICDM	In Data Mining$International Conference on Data Mining$ing$	1
In SIAM ICDM (SDM).	ICDM	In Data Mining$International Conference on Data Mining$ing$	1
In  SIAM ICDM.	ICDM	In Data Mining$International Conference on Data Mining$ing$	1
In proceedings of the Text Mining Workshop, at the 3rd SIAM ICDM.	ICDM	In Data Mining$International Conference on Data Mining$ing$	1
In Pro- ceedings of the ICDM, pages 629?632.	ICDM	In Data Mining$International Conference on Data Mining$ing$	1
In Proceedings of the Second SIAM ICDM, Arlington, VA, USA, April 11-13, 2002, pages 158?	ICDM	In Data Mining$International Conference on Data Mining$ing$	1
1 In t roduct ion   We present a statistical method for determin-  ICDM pronoun anaphora.	ICDM	In Data Mining$International Conference on Data Mining$ing$	2
We incorpo-  rate multiple anaphora resolution factors into  a statistical framework - -  specifically the dis-  tance between the pronoun and the proposed  antecedent, gender/number/animaticity of the  proposed antecedent, governICDM head informa-  tion and noun phrase repetition.	ICDM	In Data Mining$International Conference on Data Mining$ing$	2
We combine  them into a sICDMle probability t	ICDM	In Data Mining$International Conference on Data Mining$ing$	2
We present some experiments il-  lustratICDM the accuracy of the method	ICDM	In Data Mining$International Conference on Data Mining$ing$	2
city of the  proposed antecedent, governICDM head informa-  tion and noun phrase repetition.	ICDM	In Data Mining$International Conference on Data Mining$ing$	2
We present some experiments il-  lustratICDM the accuracy of the method and note  that with this information added, our pronoun  resolution method achieves 84.2% accuracy.	ICDM	In Data Mining$International Conference on Data Mining$ing$	2
brown, edu  Abst ract   This paper presents an algorithm for identi-  fyICDM pronominal anaphora and two experi-  ments based upon this algorithm.	ICDM	In Data Mining$International Conference on Data Mining$ing$	2
The second  experiment investigates a method for unsuper-  vised learnICDM of gender/number/animaticity  inform	ICDM	In Data Mining$International Conference on Data Mining$ing$	2
This program differs  from earlier work in its almost complete lack of  hand-craftICDM, relyICDM instead on a very small  corpus of Penn Wall Street Journal Tree-bank  text (Marcus	ICDM	In Data Mining$International Conference on Data Mining$ing$	2
We combine  them into a sICDMle probability that enables us  to identify the referent.	ICDM	In Data Mining$International Conference on Data Mining$ing$	2
thm for identi-  fyICDM pronominal anaphora and two experi-  ments based upon this algorithm.	ICDM	In Data Mining$International Conference on Data Mining$ing$	2
This program differs  from earlier work in its almost complete lack of  hand-craftICDM, relyICDM instead on a very small  corpus of Penn Wall Street Journal Tree-bank  text (Marcus et al, 1993) that has been marked  with co-reference information.	ICDM	In Data Mining$International Conference on Data Mining$ing$	2
The second  experiment investigates a method for unsuper-  vised learnICDM of gender/number/animaticity  information.	ICDM	In Data Mining$International Conference on Data Mining$ing$	2
This program differs  from earlier work in its almost complete lack of  hand-craftICDM, relyi	ICDM	In Data Mining$International Conference on Data Mining$ing$	2
c?2014 Association for Computational Linguistics Word-Based Dialog State Tracking with RNNs Matthew Henderson, Blaise Thomson and Steve Young Department of Engineering, University of Cambridge, U.K. {mh521, brmt2, sjy}@eng.cam.ac.uk Abstract Recently discriminative methods for track- ing the state of a spoken dialog have been shown to outperform traditional generative models.	RNN	Recurrent Neural Network$recursive neural network$recurrent neural network$	0
Specifically for RNNs, Lample et al (2016) and Huang et.	RNN	Recurrent Neural Network$recursive neural network$recurrent neural network$	0
3 LSTM RNNs Our work is based on recurrent neural networks.	RNN	Recurrent Neural Network$recursive neural network$recurrent neural network$	0
Exten- sions of RNN Language Model.	RNN	Recurrent Neural Network$recursive neural network$recurrent neural network$	0
c?2016 Association for Computational Linguistics Keyphrase Extraction Using Deep RNNs on Twitter Qi Zhang, Yang Wang, Yeyun Gong, Xuanjing Huang Shanghai Key Laboratory of Data Science School of Computer Science, Fudan University Shanghai, P.R. China {qz, ywang14, yygong12, xjhuang}@fudan.edu.cn Abstract Keyphrases can provide highly condensed and valuable information that allows users to quickly acquire the main ideas.	RNN	Recurrent Neural Network$recursive neural network$recurrent neural network$	0
c?2014 Association for Computational Linguistics Translation Modeling with Bidirectional RNNs Martin Sundermeyer 1 , Tamer Alkhouli 1 , Joern Wuebker 1 , and Hermann Ney 1,2 1 Human Language Technology and Pattern Recognition Group RWTH Aachen University, Aachen, Germany 2 Spoken Language Processing Group Univ.	RNN	Recurrent Neural Network$recursive neural network$recurrent neural network$	0
Better word representa- tions with RNNs for morphol- ogy.	RNN	Recurrent Neural Network$recursive neural network$recurrent neural network$	1
Better word representations with RNNs for morphology.	RNN	Recurrent Neural Network$recursive neural network$recurrent neural network$	1
2011), regression models by Guevara (2010), and RNN based solutions by Socher et al (2012) and Collobert et al (2011) have been proposed.	RNN	Recurrent Neural Network$recursive neural network$recurrent neural network$	1
Transition-based dependency parsing using RNNs.	RNN	Recurrent Neural Network$recursive neural network$recurrent neural network$	1
Parsing natural scenes and nat- ural language with RNNs.	RNN	Recurrent Neural Network$recursive neural network$recurrent neural network$	1
Exten- sions of RNN language model.	RNN	Recurrent Neural Network$recursive neural network$recurrent neural network$	2
Chinese poetry generation with RNNs.	RNN	Recurrent Neural Network$recursive neural network$recurrent neural network$	2
A RNN language model (Mikolov et al, 2010) aims to esti- mate the probability of observing a word given its preceding context.	RNN	Recurrent Neural Network$recursive neural network$recurrent neural network$	2
RNN language modeling toolkit.	RNN	Recurrent Neural Network$recursive neural network$recurrent neural network$	2
Incremen- tal RNN dependency parser with search-based discriminative training.	RNN	Recurrent Neural Network$recursive neural network$recurrent neural network$	2
Incre- mental RNN dependency parser with search-based discriminative training.	RNN	Recurrent Neural Network$recursive neural network$recurrent neural network$	2
Since large sense-tagged corpora were not available, we simulated the baseline method with a modified version of the proposed method; namely, for each polysemous word, the sense that maximizes the sum of correlations with all clues was selected as the MFS.	MFS	most frequent sense$Maximal Freq Sequences(d1$Most Frequent Sense$most?frequent?sense$	0
Both sys- tems achieved comparable accuracy (0.851 and 0.857), which outperforms considerably the MFS baseline (0.787).	MFS	most frequent sense$Maximal Freq Sequences(d1$Most Frequent Sense$most?frequent?sense$	0
The decision trees learned by our system fall back on the MFS in case the identified features are unable to disambiguate the target word.	MFS	most frequent sense$Maximal Freq Sequences(d1$Most Frequent Sense$most?frequent?sense$	0
Automatic Annotation Results (fine-grained score) The baseline results were obtained running a sim- ple algorithm that assigned to the instances of the test set the MFS of each lemma in the training set.	MFS	most frequent sense$Maximal Freq Sequences(d1$Most Frequent Sense$most?frequent?sense$	0
Comparison with a baseline method, which selects the MFS of each polysemous word independently of contexts, was also done.	MFS	most frequent sense$Maximal Freq Sequences(d1$Most Frequent Sense$most?frequent?sense$	0
A majority classifier which always chooses the MFS of a word in the training data, achieves an accuracy of 56.5%.	MFS	most frequent sense$Maximal Freq Sequences(d1$Most Frequent Sense$most?frequent?sense$	0
We calculated a baseline of 0.788 for the Lexical Sample sub-task, using the MFS for each lemma in the training data.	MFS	most frequent sense$Maximal Freq Sequences(d1$Most Frequent Sense$most?frequent?sense$	2
5.2 MFS WordNet keeps track of the frequency of each word meaning within a sense-annotated corpus.	MFS	most frequent sense$Maximal Freq Sequences(d1$Most Frequent Sense$most?frequent?sense$	2
657 Metric Correlation MSRpar 0.591 MSRvid 0.726 SMTeuroparl 0.485 Table 2: Correlation scores across individual cor- pora using Path Distance and MFS.	MFS	most frequent sense$Maximal Freq Sequences(d1$Most Frequent Sense$most?frequent?sense$	2
Comparison of word sense disambiguation strategies and semantic similarity measures on the training data showed that the best results were ob- tained using the Path Distance Measure combined 658 with the MFS approach (see Ta- bles 1 and 2) and these were used for the official run.	MFS	most frequent sense$Maximal Freq Sequences(d1$Most Frequent Sense$most?frequent?sense$	2
System Accuracy MFS Baseline 7.7 Best SMT system 33.0 UMD unsupervised system 44.5 WSD naive Bayes 60.4 WSD KPCA 63.6 WSD Boosting 64.1 WSD Maximum Entropy 64.4 WSD Ensemble (current best Senseval-3 model) 66.2 6.1 The dedicated supervised WSD models all significantly outperform SMT Table 2 clearly shows that even the best of the SMT model considered performs significantly worse than any of the de	MFS	most frequent sense$Maximal Freq Sequences(d1$Most Frequent Sense$most?frequent?sense$	2
However, this induction step has proven to be greatly chal- lenging, in the most recent shared tasks, induction systems either appear to perform poorly or fail to outperform the simple MFS baseline (Agirre and Soroa, 2007a; Manandhar et al, 2010).	MFS	most frequent sense$Maximal Freq Sequences(d1$Most Frequent Sense$most?frequent?sense$	2
However, not so much attention has been paid on learning class-based classifiers from other available sense?groupings such as WordNet Domains (Magnini and Cavaglia`, 2000), SUMO labels (Niles and Pease, 2001), EWN Base Concepts (Vossen et al, 1998), Top Con- cept Ontology labels (Alvez et al, 2008) or Ba- sic Level Concepts (Izquierdo et al, 2007).	EWN	EuroWordNet$English WordNet$	0
For this purpose, an architecture similar to the one used by EWN (Vossen, 2000) was implemented, in order to obtain knowledge databases for the different languages, but all of them connected though a unit denominated TER-ILI or Temporal Expression Rules Interlingua Index.	EWN	EuroWordNet$English WordNet$	0
The success of Word- Net led to the adoption of its model by lexical re- sources in different languages, such as the ones in the EWN project (Vossen, 1997), or WordNet.	EWN	EuroWordNet$English WordNet$	0
EWN: Building a Multilin- gual Database with WordNets in 8 European Lan- guages.	EWN	EuroWordNet$English WordNet$	0
P. Vossen, and W. Peters, 1997 Multilingual design  of EWN, Proceedings of the Delos  workshop on Cross-language Information  Retrieval.	EWN	EuroWordNet$English WordNet$	0
We can find ontologies ranging from general world knowledge resources, such as WordNet (Fellbaum 1998), EWN (Vossen 1998), Cyc (Lenat and Guha 1990), and FrameNet (Johnson and Fillmore 2000, to very specific domain knowledge, such as the medical domain (Lindberg, Humphreys, and McCray 1993) or the chemistry domain (Barker et al 2004).	EWN	EuroWordNet$English WordNet$	0
The result of this phase is the ex-  tension of the EWN with the Italian synsets.	EWN	EuroWordNet$English WordNet$	1
We used a con-  straint satisfaction algorithm (relaxation label-  ing) to select -among all the candidate trans-  lations proposed by a bilingual dictionary- the  right EWN synset for each sense in  a taxonomy automatically derived from a Span-  ish monolingua\] dictionary.	EWN	EuroWordNet$English WordNet$	1
To remove the  ambiguities we develop new word sense  disambiguation heuristics and automatic mapping  method to construct Korean WordNet based on  the existing EWN.	EWN	EuroWordNet$English WordNet$	1
A set of  automatic WSD techniques i described for  linking Korean words collected from a  bilingual MRD to EWN synsets.	EWN	EuroWordNet$English WordNet$	1
The method has been applied to  link Korean words from a bilingual dictionary to  EWN synsets.	EWN	EuroWordNet$English WordNet$	1
Miller et al, 1994) found that automatic  I We use EWN version 1.6  - L  143  assignment of polysemous words in Brown  Corpus to senses in WordNet was 58% correct  with a heuristic of most frequently occurring  sense.	EWN	EuroWordNet$English WordNet$	1
it changes to a NNS	NNS	noun ?$plural noun$	0
in which the head NNS	NNS	noun ?$plural noun$	0
against, about, versus) with the NNS	NNS	noun ?$plural noun$	0
attaches to the NNS	NNS	noun ?$plural noun$	0
6  Table 2: POS Groups  ProNNS	NNS	noun ?$plural noun$	0
cannot be detected even if whether the NNS	NNS	noun ?$plural noun$	0
Delete a right paren to the left of a NNS.	NNS	noun ?$plural noun$	1
cannot refer to an NP headed by a NNS.	NNS	noun ?$plural noun$	1
NNSs do not take an indefinite article?,	NNS	noun ?$plural noun$	1
Delete a left paren to the left of a NNS.	NNS	noun ?$plural noun$	1
6-grams : [SP]-[O]-[SP]-[O]-[S]-[JK], where S and P correspond to singular or NNSs, O to the prepositions (also in combination with the article), and J and K to singular or plural adjectives (e.g. ?	NNS	noun ?$plural noun$	1
The parsing  stage also includes classification of NNSs.	NNS	noun ?$plural noun$	1
ISOQuest?s NetOwl and IBM?s Textract attempted  to determine whether multiple NEs refer to  the same entity but neither had the ability to distinguish  different entities with the same name.	NE	named entitie$named entity$Named Ent i ty$Named entity$Named entit$Negative Entity$name$amed Entity$	0
are a disguise of multiple NEs  such as ?	NE	named entitie$named entity$Named Ent i ty$Named entity$Named entit$Negative Entity$name$amed Entity$	0
Evaluation       Given a collection of NEs from  documents, the coreferencing task is to put them into  equivalence classes, where every mention in the same  class refers to the same entity (person, location,  organization, and so on).	NE	named entitie$named entity$Named Ent i ty$Named entity$Named entit$Negative Entity$name$amed Entity$	0
A clear candidate is the use of NEs, but the creation of templates has also been tried in open domains (Srihari and Li 2000) and restricted domains (Weischedel, Xu, and Licuanan 2004).	NE	named entitie$named entity$Named Ent i ty$Named entity$Named entit$Negative Entity$name$amed Entity$	0
Nowadays it is possible to parse the entire corpus used in the QA track of TREC and to extract all its NEs.	NE	named entitie$named entity$Named Ent i ty$Named entity$Named entit$Negative Entity$name$amed Entity$	0
It is therefore feasible to parse and extract the NEs of corpora of restricted domains.	NE	named entitie$named entity$Named Ent i ty$Named entity$Named entit$Negative Entity$name$amed Entity$	0
Documents, paragraphs, sentences and tokens are uniquely identified and lemmata, part-of-speech (POS), NE information and morphological analyses have been automatically annotated in the data.	NE	named entitie$named entity$Named Ent i ty$Named entity$Named entit$Negative Entity$name$amed Entity$	1
We used two approaches for identifying story characters motivated by (Elson and McKeown, 2010): 1) NE recognition was used for identifying proper names, e.g., ?	NE	named entitie$named entity$Named Ent i ty$Named entity$Named entit$Negative Entity$name$amed Entity$	1
Two broad approaches for the iden- tification of story characters were followed: (i) NE recognition, and (ii) identification of character nominals, e.g., ?	NE	named entitie$named entity$Named Ent i ty$Named entity$Named entit$Negative Entity$name$amed Entity$	1
This includes (i) tokenization, (ii) sen- tence splitting and identification of paragraph boundaries, (iii) part-of-speech (POS) tagging, (iv) lemmatization, (v) NE recognition, (vi) dependency parsing, and (vii) co-reference analysis.	NE	named entitie$named entity$Named Ent i ty$Named entity$Named entit$Negative Entity$name$amed Entity$	1
Then, we ran the  documents through Identifinder, a NE  extraction system developed by BBN, to tag the named  entities in the documents.	NE	named entitie$named entity$Named Ent i ty$Named entity$Named entit$Negative Entity$name$amed Entity$	1
F 14 NE, especially a protein name.	NE	named entitie$named entity$Named Ent i ty$Named entity$Named entit$Negative Entity$name$amed Entity$	1
2 Japanese NE   Recogn i t ion   2.1 Task of the IREX Workshop  The task of named entity recognition of the  IREX workshop is to recognize ight named en-  tity types in Table 1 (IREX Conmfittee, 1999).	NE	named entitie$named entity$Named Ent i ty$Named entity$Named entit$Negative Entity$name$amed Entity$	2
Unsurprisingly, the Miscellaneous and Other  NE  categories are problematic; unfortu-  nately, they are also rather frequent.	NE	named entitie$named entity$Named Ent i ty$Named entity$Named entit$Negative Entity$name$amed Entity$	2
... Mm(<3 )  1" (2)  (Current Position)  4 Superv ised Learning for Japanese  NE  Recogn i t ion   This section describes how to apply tile deci-  sion list learning method to chunking/tagging  named entities.	NE	named entitie$named entity$Named Ent i ty$Named entity$Named entit$Negative Entity$name$amed Entity$	2
J apanese  NE  Ext rac t ion  Eva luat ion   - Ana lys i s  o f  Resu l t s  -  Satosh i  Sek ine   Computer  Science Depar tment   New York University  715 Broadway, 7th floor  New York, NY 10003, USA  sekine@cs, nyu.	NE	named entitie$named entity$Named Ent i ty$Named entity$Named entit$Negative Entity$name$amed Entity$	2
M I M M \] M  0 0RG_I 0 L0C_I LOC_I LOC_I LOC_B 0  0 0RG_U 0 LOC_S L0C_C LOC_E LOC_U 0  V Mo i - l )hemes  to 1 NE \ ]   <ORGANIZATION>  .... Roshia gun -..   ( S<,s,~i,.	NE	named entitie$named entity$Named Ent i ty$Named entity$Named entit$Negative Entity$name$amed Entity$	2
NE extraction from noisy input: speech and OCR.	NE	named entitie$named entity$Named Ent i ty$Named entity$Named entit$Negative Entity$name$amed Entity$	3
NE recognition through classifier combination.	NE	named entitie$named entity$Named Ent i ty$Named entity$Named entit$Negative Entity$name$amed Entity$	3
NE recognition in tweets: an experimental study.	NE	named entitie$named entity$Named Ent i ty$Named entity$Named entit$Negative Entity$name$amed Entity$	3
NE recognition through classifier combina- tion.	NE	named entitie$named entity$Named Ent i ty$Named entity$Named entit$Negative Entity$name$amed Entity$	3
NE disam- biguation by leveraging Wikipedia semantic knowl- edge.	NE	named entitie$named entity$Named Ent i ty$Named entity$Named entit$Negative Entity$name$amed Entity$	3
NE extraction from broadcast news.	NE	named entitie$named entity$Named Ent i ty$Named entity$Named entit$Negative Entity$name$amed Entity$	3
NEy extraction from noisy input: speech and OCR.	NE	named entitie$named entity$Named Ent i ty$Named entity$Named entit$Negative Entity$name$amed Entity$	4
NEy recognition through classifier combination.	NE	named entitie$named entity$Named Ent i ty$Named entity$Named entit$Negative Entity$name$amed Entity$	4
NEy recognition in tweets: an experimental study.	NE	named entitie$named entity$Named Ent i ty$Named entity$Named entit$Negative Entity$name$amed Entity$	4
NEy recognition through classifier combina- tion.	NE	named entitie$named entity$Named Ent i ty$Named entity$Named entit$Negative Entity$name$amed Entity$	4
NEy disam- biguation by leveraging Wikipedia semantic knowl- edge.	NE	named entitie$named entity$Named Ent i ty$Named entity$Named entit$Negative Entity$name$amed Entity$	4
NEy extraction from broadcast news.	NE	named entitie$named entity$Named Ent i ty$Named entity$Named entit$Negative Entity$name$amed Entity$	4
We used two approaches for identifying story characters motivated by (Elson and McKeown, 2010): 1) NEd entity recognition was used for identifying proper NEs, e.g., ?	NE	named entitie$named entity$Named Ent i ty$Named entity$Named entit$Negative Entity$name$amed Entity$	6
2) a set of part-of-speech patterns was used for the extraction of human and non-human characters that were not represented by proper NEs, e.g., ?	NE	named entitie$named entity$Named Ent i ty$Named entity$Named entit$Negative Entity$name$amed Entity$	6
To that end, we  devised a more objective test, useful only for  scoring the subset of referents that are NEs  of people.	NE	named entitie$named entity$Named Ent i ty$Named entity$Named entit$Negative Entity$name$amed Entity$	6
This includes (i) tokenization, (ii) sen- tence splitting and identification of paragraph boundaries, (iii) part-of-speech (POS) tagging, (iv) lemmatization, (v) NEd entity recognition, (vi) dependency parsing, and (vii) co-reference analysis.	NE	named entitie$named entity$Named Ent i ty$Named entity$Named entit$Negative Entity$name$amed Entity$	6
Two broad approaches for the iden- tification of story characters were followed: (i) NEd entity recognition, and (ii) identification of character nominals, e.g., ?	NE	named entitie$named entity$Named Ent i ty$Named entity$Named entit$Negative Entity$name$amed Entity$	6
The utilization of available resources containing associations between person NEs and gender was followed in (Elson and 41 McKeown, 2010).	NE	named entitie$named entity$Named Ent i ty$Named entity$Named entit$Negative Entity$name$amed Entity$	6
We first ex- tract named entities from scattered opinions DT using Stanford NNE Recognizer (Finkel et al, 2005).	NE	named entitie$named entity$Named Ent i ty$Named entity$Named entit$Negative Entity$name$amed Entity$	7
Workshop on NNE Recognition.	NE	named entitie$named entity$Named Ent i ty$Named entity$Named entit$Negative Entity$name$amed Entity$	7
We are studying to combine WordNet with a NNE Recogniser to produce generalised rules.	NE	named entitie$named entity$Named Ent i ty$Named entity$Named entit$Negative Entity$name$amed Entity$	7
The Third International Chi- nese Language Processing Bakeoff: Word Seg- mentation and NNE Recognition, In  Proceedings of SIGHAN5 the 3rd International  Chinese Language Processing Bakeoff at Col- ing/ACL 2006, July, Sydney, Australia, 108-117.	NE	named entitie$named entity$Named Ent i ty$Named entity$Named entit$Negative Entity$name$amed Entity$	7
NNE recogni- tion using a HMM-based chunk tagger.	NE	named entitie$named entity$Named Ent i ty$Named entity$Named entit$Negative Entity$name$amed Entity$	7
3) We employed IE methods (including pattern sets  and NNE Recognition) as initial extraction  steps.	NE	named entitie$named entity$Named Ent i ty$Named entity$Named entit$Negative Entity$name$amed Entity$	7
RST: A framework for the analysis of texts.	RST	Rhetorical structure theory$Rhetorical Structure Theory$Rough Set Theory$	0
RST: Toward a func-  tional theory of text organization.	RST	Rhetorical structure theory$Rhetorical Structure Theory$Rough Set Theory$	0
RST: Toward a functional theory of text organization.	RST	Rhetorical structure theory$Rhetorical Structure Theory$Rough Set Theory$	0
RST: Toward a functional the- ory of text organization.	RST	Rhetorical structure theory$Rhetorical Structure Theory$Rough Set Theory$	0
RST: Toward a functional the-  ory of text organization.	RST	Rhetorical structure theory$Rhetorical Structure Theory$Rough Set Theory$	0
RST: Toward a Functional  Theory of Text Organization.	RST	Rhetorical structure theory$Rhetorical Structure Theory$Rough Set Theory$	1
Mann W. C. and Thompson S. A (1988)  "RST: Towards a  Functional Theory of Text Organization."	RST	Rhetorical structure theory$Rhetorical Structure Theory$Rough Set Theory$	1
In  both respects, our proposals will contrast with those of  RST \[Mann and Thompson\].	RST	Rhetorical structure theory$Rhetorical Structure Theory$Rough Set Theory$	1
RST: Toward a Functional Theory of Text Organization.	RST	Rhetorical structure theory$Rhetorical Structure Theory$Rough Set Theory$	1
In Olle ma.jor theory of discourse  structure, RST (Mann &: Thompson 1988; Imrea.l'ter simply RS'T), the  smallest possible linguistic units that can lmrtMl)ate in a rhetorical rela.tion a,re called units,  and "are essentially clauses, except that clausal subjects a.nd complenlents a.nd restricte(l rel-  ative clauses are considered as parts of their host clause units rather than as sepa, rate units"  \[p.	RST	Rhetorical structure theory$Rhetorical Structure Theory$Rough Set Theory$	1
RST : A Theory of Text  Organization.	RST	Rhetorical structure theory$Rhetorical Structure Theory$Rough Set Theory$	1
The Task Trajectory and ED features are based on computing a token-level edit distance from a student?s program with respect to that student?s final correct solution.	ED	Edit Distance$Emergency Department$	0
Answer Extraction as Se- quence Tagging with Tree ED.	ED	Edit Distance$Emergency Department$	0
In addition, both feature sets include a feature related to the task progress of the student: Task Trajectory for ECR and ED for SR.	ED	Edit Distance$Emergency Department$	0
SMO has been de- 1 The pseudocode for SMO may be found in the appendix of Platt (1999) ED (e ?	ED	Edit Distance$Emergency Department$	0
Iteration Initial-State ECR Feature Ordering Mean SR  Feature Ordering 1 Last Action Number of Tutor Moves 2 Task Trajectory ED 3 Current Action Last Action 4 Elapsed Idle Time Current Action 5 Number of Tutor Moves Elapsed Idle Time 6 ED Task Trajectory  Table 2.	ED	Edit Distance$Emergency Department$	0
The fourth dataset was collected in a clinical con- trolled trial at Cincinnati Children?s Hospital Med- ical Center ED.	ED	Edit Distance$Emergency Department$	1
ED  reports and Discharge Summaries contain similar  proportions of historical conditions (17% and 19%  respectively), which might be explained by the fact  that both reports describe a patient?s temporal pro- gression throughout the stay in the Emergency De- partment or the hospital.	ED	Edit Distance$Emergency Department$	1
Examples of report sections include Review of Sys- tems (ED), Findings (Opera- tive Gastrointestinal and Radiology) and  Discharge Diagnosis (ED and  Discharge Summary).	ED	Edit Distance$Emergency Department$	1
4 Methods  4.1 Dataset Generation  We randomly selected seven reports from each of  six genres of clinical reports dictated at the Univer- sity of Pittsburgh Medical Center during 2007  These included Discharge Summaries, Surgical  Pathology, Radiology, Echocardiograms, Opera- tive Gastrointestinal, and ED  reports.	ED	Edit Distance$Emergency Department$	1
status post indi- cates the reason for the transfer as an inpatient  from the ED and the condition  is recent.	ED	Edit Distance$Emergency Department$	1
Answers with the getByCategory function 1709 Model k P@1(5) P@1(10) RG - 20.0 10.0 NBOW 50 63.9 47.6 single LSTM 50 68.2 53.9 parallel LSTMs 50 66.9 52.1 Attention LSTMs 50 73.5 62.0 Attention LSTMs (w-by-w) 50 75.1 64.0 LC-LSTMs (Single Direction) 50 75.4 63.0 LC-LSTMs 50 76.1 64.1 three stacked LC-LSTMs 50 78.5 66.2 TC-LSTMs (Single Direction) 50 74.3 62.4 TC-LSTMs 50 74.9 62.9 three stacked TC-LSTMs 50 77.0 65.3 Table 4: Results on Yahoo question-answ	RG	Random Guess$response generation$Rubenstein and Goodenough$Roget$reverse gap$Reverse Gap$	0
The logistic regression model signifi- cantly outperforms the baselines, but underperforms 2389 Exact Match F1 Dev Test Dev Test RG 1.1% 1.3% 4.1% 4.3% Sliding Window 13.2% 12.5% 20.2% 19.7% Sliding Win.	RG	Random Guess$response generation$Rubenstein and Goodenough$Roget$reverse gap$Reverse Gap$	0
query to retrieval top 1, 000 re- 1040 Female  gymnast  warm  up  before  a  competition  Gymnast  get  ready  for  a  competition (a) A   female  gymnast  in  black   and  red  being  coach ed  on  bar  s k il l s   T h e  female  gymnast  is  tr a ining (b) Figure 5: Examples of external memory positions attended when encoding the next word pair (bold and marked by a box) Model k P@1(5) P@1(10) RG - 20.0 10.0 NBOW 50 63.9 47.6 single LSTM 50 68.2 53.9 parallel LSTMs 50 66.9 52.1 Attention LSTMs 50 73.5 62.0 Attention(w-by-w) LSTMs 50 75.1 64.0 DF-LSTMs 50 76.5 65.0 Table 4: Results of our proposed model against other neural models on Yahoo!	RG	Random Guess$response generation$Rubenstein and Goodenough$Roget$reverse gap$Reverse Gap$	0
Madnani et al (2012) used a combination of three basic MT metrics (BLEU, NIST and TER) and five complex MT met- rics (TERp, METEOR, BADGER, MAXISIM, model acc RG 20.00 DeepMatch 34.17 WordEmbed 38.28 SENMLP 34.57 SENNA+MLP 42.09 URAE+MLP 27.41 ARC-I 45.04 ARC-II 50.18 MultiGranCNN 56.27 Table 1: Performance on clause coherence test set.	RG	Random Guess$response generation$Rubenstein and Goodenough$Roget$reverse gap$Reverse Gap$	0
Incorporating time features further improves the relative accuracy by 104 Model Overall Accuracy RG 10.0% Unigram NLLR 24.1% Filtered NLLR 29.1% MaxEnt Unigram 45.1% MaxEnt Time 48.3% MaxEnt Time+NER 51.4% Joint 53.4% Table 3: Performance as measured by accuracy.	RG	Random Guess$response generation$Rubenstein and Goodenough$Roget$reverse gap$Reverse Gap$	0
First, the topic of email RG in the context of customer care has been investigated by (Coch, 1996; Lapalme and Kosseim, 2003; Zukerman and Marom, 2007).	RG	Random Guess$response generation$Rubenstein and Goodenough$Roget$reverse gap$Reverse Gap$	1
The typed input is sent to NUBEE which queries the domain reasoner, BEER, and dialogue history to help build a logical form which it sends to the dialogue manager of the system, the central component of BEETLE?s RG.	RG	Random Guess$response generation$Rubenstein and Goodenough$Roget$reverse gap$Reverse Gap$	1
In this paper we focus on just two aspects of  RG that rely on our model of the  learner's generation process.	RG	Random Guess$response generation$Rubenstein and Goodenough$Roget$reverse gap$Reverse Gap$	1
Keywords  tailoring RG, user modeling,  computer-assisted language learning  1 Introduction  Our long-term goal is to develop a computer-  assisted language learning (CALL) tool to help  deaf students leam written English.	RG	Random Guess$response generation$Rubenstein and Goodenough$Roget$reverse gap$Reverse Gap$	1
We argue that  these two models can be used to "explain" the stu-  dent's sentence generation capabilities and should  affect the system's RG as well.	RG	Random Guess$response generation$Rubenstein and Goodenough$Roget$reverse gap$Reverse Gap$	1
In contrast, in this paper, due to constraints from the deployment environment, we rely on a template-based approach to RG.	RG	Random Guess$response generation$Rubenstein and Goodenough$Roget$reverse gap$Reverse Gap$	1
These word pairs are a subset of 65 word pairs used by (RG, 1965), in a similar study almost 25 years earlier.	RG	Random Guess$response generation$Rubenstein and Goodenough$Roget$reverse gap$Reverse Gap$	2
Human similarity ratings: We use the dataset of RG (1965), consist- ing of averages of subject similarity ratings for 65 noun pairs.	RG	Random Guess$response generation$Rubenstein and Goodenough$Roget$reverse gap$Reverse Gap$	2
the Miller and Charles study as well as the RG research.	RG	Random Guess$response generation$Rubenstein and Goodenough$Roget$reverse gap$Reverse Gap$	2
License details: http: //creativecommons.org/licenses/by/4.0/ of RG (1965) measure similarity between word pairs, while the data sets of Navigli (2006) and Kilgarriff (2001) offer a bi- nary similar-dissimilar distinction between senses.	RG	Random Guess$response generation$Rubenstein and Goodenough$Roget$reverse gap$Reverse Gap$	2
Second, there are two gold standards for the Miller and Charles (1991) set: one has the scores assigned during the original experiment run by RG (1965), the other has the scores assigned during Miller and Charles (1991)?s own experiment.	RG	Random Guess$response generation$Rubenstein and Goodenough$Roget$reverse gap$Reverse Gap$	2
Gurevych (2005) conducted experiments with two datasets: i) a German translation of the English dataset by RG (1965) (Gur65), and ii) a larger dataset containing 350 word pairs (Gur350).	RG	Random Guess$response generation$Rubenstein and Goodenough$Roget$reverse gap$Reverse Gap$	2
They used the test data consisting of 10 En- glish and Japanese verbs taken from RG?s The- saurus and BGH (Bunrui Goi Hyo) (BGH, 1989).	RG	Random Guess$response generation$Rubenstein and Goodenough$Roget$reverse gap$Reverse Gap$	3
2.2 A Thesaurus -based  Approach   Morris and Hirst \[1991\] used RG's thesaurus as  knowledge base for determining whether or not two  words are semantically related.	RG	Random Guess$response generation$Rubenstein and Goodenough$Roget$reverse gap$Reverse Gap$	3
cats(n2) P(t1|p)P(t2|p)(7) Here, t1 and t2 represent concepts in RG?s thesaurus.	RG	Random Guess$response generation$Rubenstein and Goodenough$Roget$reverse gap$Reverse Gap$	3
Fre- quencies for the two models were obtained from the same corpus and from RG?s thesaurus (version 1911) by counting pairs of nouns that are either strictly adjacent or co-occur within a window of a fixed size (e.g., two, three, fifty, or hundred words).	RG	Random Guess$response generation$Rubenstein and Goodenough$Roget$reverse gap$Reverse Gap$	3
An alternative to manual classification is  using on-line thesaura, such as RG's and  Wordnet categories in English 3.	RG	Random Guess$response generation$Rubenstein and Goodenough$Roget$reverse gap$Reverse Gap$	3
The choices could come from various inventories of near-synonyms or similar words, for example the RG thesaurus (RG, 1852), dictionaries of syn- onyms (Hayakawa, 1994), or clusters acquired from corpora (Lin, 1998).	RG	Random Guess$response generation$Rubenstein and Goodenough$Roget$reverse gap$Reverse Gap$	3
By collaps- ing monotone gap and RG into neutral, it can be thought of as a local reordering model sim- ilar to the block orientation bigram (Tillmann and Zhang, 2005).	RG	Random Guess$response generation$Rubenstein and Goodenough$Roget$reverse gap$Reverse Gap$	4
Since non-local reorderings such as monotone gap and RG are more frequent in Japanese to English translations, they are worth modeling ex- plicitly in this reordering model.	RG	Random Guess$response generation$Rubenstein and Goodenough$Roget$reverse gap$Reverse Gap$	4
Combination of  FSA and Neural Network for  Spoken Language Understanding.	FSA	Finite State Automata$finite state acceptors$finite state automation$Finite State Automato$	0
FSA The fsa module defines a data type for encod- ing finite state automata; and an interface for creating automata from regular expressions.	FSA	Finite State Automata$finite state acceptors$finite state automation$Finite State Automato$	0
From Signatures to  FSA.	FSA	Finite State Automata$finite state acceptors$finite state automation$Finite State Automato$	0
Language Independent Text Correction us- ing FSA.	FSA	Finite State Automata$finite state acceptors$finite state automation$Finite State Automato$	0
Language Independent Text Correction using FSA Ahmed Hassan?	FSA	Finite State Automata$finite state acceptors$finite state automation$Finite State Automato$	0
007 Mixed Initiative in Dialogue: An Investigation into Discourse Segmentation (ACL90), M. Walker, S. Whittaker 63 9504017 A Uniform Treatment of Pragmatic Inferences in Simple and Complex Utterances and Sequences of Utterances (ACL95), D. Marcu, G. Hirst 64 9504024 A Morphographemic Model for Error Correction in Nonconcatenative Strings (ACL95), T. Bowden, G. Kiraz 65 9504026 The Intersection of FSA and Definite Clause Grammars (ACL95), G. van Noord 66 9504027 An Efficient Generation Algorithm for Lexicalist MT (ACL95), V. Poznanski et al 67 9504030 Statistical Decision-Tree Models for Parsing (ACL95), D. Magerman 68 9504033 Corpus Statistics Meet the Noun Compound: Some Empirical Results (ACL95), M. Lauer 79 9504034 Bayesian Grammar Induction for Language Modeling (ACL9	FSA	Finite State Automata$finite state acceptors$finite state automation$Finite State Automato$	0
These FSA are then converted into transducers by modifying the arcs: split the labels of each arc, x:y, making x the input label for that arc, and y the output label.	FSA	Finite State Automata$finite state acceptors$finite state automation$Finite State Automato$	1
The  output associated by v to an input string, z, is obtained by concatenating the output strings  of the edges of r that are used to parse the successive symbols of z.  One problem of using Finite State Transducers in our framework is that the problem of  learning of general Finite State Transducers i at least as hard as the problem of learning  a general FSAn, which is well known to be probably intractable.	FSA	Finite State Automata$finite state acceptors$finite state automation$Finite State Automato$	3
Hobbs, Douglas Appelt, Mabry Tyson, John Bear, and David Israel SRI International Menlo Park, California 94025 hobbs?ai .sri .com (415) 859-222 9 INTRODUCTIO N FASTUS is a (slightly permuted) acronym for FSAn Text Understanding System .	FSA	Finite State Automata$finite state acceptors$finite state automation$Finite State Automato$	3
E-Systems' engineers address the overall  military message traffic processing problem, while  SRI Interation~l's computational linguists adapt  their FSAn Text Understanding  System (FASTUS) technology to the domain of mil-  itary message free-text.	FSA	Finite State Automata$finite state acceptors$finite state automation$Finite State Automato$	3
An example of FSAn to recognize open series compounds.	FSA	Finite State Automata$finite state acceptors$finite state automation$Finite State Automato$	3
Since each position set can be adjacent to at most four other position sets, such a DS has size O(|p|).	DS	Domain stacking$data structure$Dataset$Distributional Similarity$discourse$Discharge Summary$Direct responses$discourse segment$	1
We further adopted an efficient DS, spectral bloom filter, to estimate the gradients for the candidate features without generating them.	DS	Domain stacking$data structure$Dataset$Distributional Similarity$discourse$Discharge Summary$Direct responses$discourse segment$	1
The main advantage of this approach  is the extreme simplicity both of its DSs  and of their interpretation.	DS	Domain stacking$data structure$Dataset$Distributional Similarity$discourse$Discharge Summary$Direct responses$discourse segment$	1
For example, the  lexical atoms extracted by this process from the  CACM corpus (about 1 MB) include "operating  system", "DS", "decision table", "data  base", "real time", "natural anguage", "on line",  "least squares", "numerical integration", and "fi-  nite state automaton", among others.	DS	Domain stacking$data structure$Dataset$Distributional Similarity$discourse$Discharge Summary$Direct responses$discourse segment$	1
An example of 4 blocks obtained from the training data is shown in 1To apply the restrictions exhaustively, we have imple- mented tree-based DSs to store the 23 million blocks with phrases of up to length 8 in about 1.6 gigabyte of RAM.	DS	Domain stacking$data structure$Dataset$Distributional Similarity$discourse$Discharge Summary$Direct responses$discourse segment$	1
in our DSs Ve and Va is then per- formed in constant time, as described above.	DS	Domain stacking$data structure$Dataset$Distributional Similarity$discourse$Discharge Summary$Direct responses$discourse segment$	1
Since players are en- couraged to produce as many terms per image, the dataset?s increased coverage is at the expense of accuracy in the word-to-image mapping: a dog in a field with a house in the background might be a golden retriever in ImageNet and could have tags dog, golden retriever, grass, field, house, door in the ESP DS.	DS	Domain stacking$data structure$Dataset$Distributional Similarity$discourse$Discharge Summary$Direct responses$discourse segment$	2
5 Experimental Study 5.1 DS We crawl 77,308 character riddles including riddle descriptions with its solution from the Web.	DS	Domain stacking$data structure$Dataset$Distributional Similarity$discourse$Discharge Summary$Direct responses$discourse segment$	2
URL: http: //ebiquity.umbc.edu/resource/html/ id/212/Splog-Blog-DS.	DS	Domain stacking$data structure$Dataset$Distributional Similarity$discourse$Discharge Summary$Direct responses$discourse segment$	2
For evaluation, 10-fold cross valida- DS Relaxed Exact lex pos lex pos Baseline 0.625 0.250 QUOTES1 0.869 0.883 0.445 0.373 QUOTES2 0.877 0.831 0.450 0.435 BOTH 0.886 0.858 0.464 0.383 Table 4: Age estimation: average accuracy.	DS	Domain stacking$data structure$Dataset$Distributional Similarity$discourse$Discharge Summary$Direct responses$discourse segment$	2
4.1 DSs Used The datasets used for our experiments along with the related tasks are presented in Table 2.	DS	Domain stacking$data structure$Dataset$Distributional Similarity$discourse$Discharge Summary$Direct responses$discourse segment$	2
3.2 DSs  To construct our corpus, we collected news arti- cles from news clusters on the World Wide Web.	DS	Domain stacking$data structure$Dataset$Distributional Similarity$discourse$Discharge Summary$Direct responses$discourse segment$	2
Measures of DS.	DS	Domain stacking$data structure$Dataset$Distributional Similarity$discourse$Discharge Summary$Direct responses$discourse segment$	3
4.4 DS Distributional similarity is based on the distribu- tional hypothesis that similar terms appear in simi- 3Consider only those pairs in which one word appears in the seed list and the other word appears in the test set.	DS	Domain stacking$data structure$Dataset$Distributional Similarity$discourse$Discharge Summary$Direct responses$discourse segment$	3
3.2 DS Here, we present the computation of the distribu- tional similarity between terms using three graphs.	DS	Domain stacking$data structure$Dataset$Distributional Similarity$discourse$Discharge Summary$Direct responses$discourse segment$	3
Explicit Semantic Analysis (ESA) (Gabrilovich and Markovitch, 2007) 0.74 (reimplemented in (Yeh et al 2009)) 0.71 Compact Hierarchical ESA (Liberman and Markovitch, 2009) 0.71 Hyperlink Graph (Milne and Witten, 2008) 0.69 Graph Traversal (Agirre et al 2009)) 0.66 DS (Agirre et al 2009)) 0.65 Latent Semantic Analysis (Finkelstein et al 2002) 0.56 Random Graph Walk (Hughes and Ramage, 2007) 0.55 Normalized Path-length (lch) (Strube and Ponzetto, 2006) 0.55 cPMId(? :	DS	Domain stacking$data structure$Dataset$Distributional Similarity$discourse$Discharge Summary$Direct responses$discourse segment$	3
Improving DS with Lessons Learned from Word Embeddings.	DS	Domain stacking$data structure$Dataset$Distributional Similarity$discourse$Discharge Summary$Direct responses$discourse segment$	3
4 DS Various similarity measures have been proposed and used for NLP tasks (Korhonen, 2002).	DS	Domain stacking$data structure$Dataset$Distributional Similarity$discourse$Discharge Summary$Direct responses$discourse segment$	3
Attention, intentions, and the structure of  DS.	DS	Domain stacking$data structure$Dataset$Distributional Similarity$discourse$Discharge Summary$Direct responses$discourse segment$	4
References  by pronouns are closely related to the topic or  the center of the DS.	DS	Domain stacking$data structure$Dataset$Distributional Similarity$discourse$Discharge Summary$Direct responses$discourse segment$	4
The method is a very simple mechanism  for harvesting the kind of gender information  present in DS fragments like "Kim slept.	DS	Domain stacking$data structure$Dataset$Distributional Similarity$discourse$Discharge Summary$Direct responses$discourse segment$	4
They use Japanese newspa-  per articles tagged with DS information  as training examples for a machine-learning al- gorithm which is the C4.5 decision-tree algo-  rithm by Quinlan (1993).	DS	Domain stacking$data structure$Dataset$Distributional Similarity$discourse$Discharge Summary$Direct responses$discourse segment$	4
However, any syntax-only pronoun resolution  strategy will be wrong some of the time - these  methods know nothing about DS bound-  aries, intentions, or real-world knowledge.	DS	Domain stacking$data structure$Dataset$Distributional Similarity$discourse$Discharge Summary$Direct responses$discourse segment$	4
The system  is not entirely "statistical" in that it consists of  various types of rule-based knowledge -- syn-  tactic, semantic, domain, DS, and heuris-  tic.	DS	Domain stacking$data structure$Dataset$Distributional Similarity$discourse$Discharge Summary$Direct responses$discourse segment$	4
Examples of report sections include Review of Sys- tems (Emergency Department), Findings (Opera- tive Gastrointestinal and Radiology) and  Discharge Diagnosis (Emergency Department and  DS).	DS	Domain stacking$data structure$Dataset$Distributional Similarity$discourse$Discharge Summary$Direct responses$discourse segment$	5
Excerpt  of  DS 55   the patient noted that he had a recurrence of this  56   vague chest discomfort as he was sitting and  57   talking to friends.	DS	Domain stacking$data structure$Dataset$Distributional Similarity$discourse$Discharge Summary$Direct responses$discourse segment$	5
un ruhe 'anyway'   shouxian 'first', qici "next"  huan ju hua shuo 'in other words'  zhengru 'just as'  nandao ('does it mean... ')   kexi 'unfortunately'  and Associated Rhetorical Relations in Chinese  It may be noted that our analysis of Chinese  has yielded about 150 discourse markers, and that  on the average, argumentative t xt (e.g. editorials)  in Chinese shows more than one third of the  DSs to contain discourse markers.	DS	Domain stacking$data structure$Dataset$Distributional Similarity$discourse$Discharge Summary$Direct responses$discourse segment$	7
be a linear sequence of clauses and sentences, it  has "long been recognized by linguists that these  clauses and sentences tend to cluster together into  units, called DSs, that are related  pragmatically toform a hierarchical structure.	DS	Domain stacking$data structure$Dataset$Distributional Similarity$discourse$Discharge Summary$Direct responses$discourse segment$	7
From the above tagging, we can obtain the  following discourse structure with embedding  relations:  A dversativity ( &F (14 ),  Sufficiency(F rontClause(15),  BackClause(15)))  where &F(n) denotes the Front DS  of an inter-sentence rhetorical relation whose  sequence number is n. We can define &B(n)  similarly.	DS	Domain stacking$data structure$Dataset$Distributional Similarity$discourse$Discharge Summary$Direct responses$discourse segment$	7
The application of dot-  plotting to DSation can be performed ei-  ther manually, by examining a graph, or automatically,  using an optimization algorithm.	DS	Domain stacking$data structure$Dataset$Distributional Similarity$discourse$Discharge Summary$Direct responses$discourse segment$	7
The function of discourse analysis is to  divide a text into DSs, and to  recognize and re-construct the discourse structure  of the text as intended by its author.	DS	Domain stacking$data structure$Dataset$Distributional Similarity$discourse$Discharge Summary$Direct responses$discourse segment$	7
cognized by linguists that these  clauses and sentences tend to cluster together into  units, called DSs, that are related  pragmatically toform a hierarchical structure.	DS	Domain stacking$data structure$Dataset$Distributional Similarity$discourse$Discharge Summary$Direct responses$discourse segment$	7
The function of discourse analysis is to  divide a text into DSs, and to  recognize and re-construct the discourse structure  of the text as intend	DS	Domain stacking$data structure$Dataset$Distributional Similarity$discourse$Discharge Summary$Direct responses$discourse segment$	7
We do not argue that DSs can never take on a  hierarchical form.	DS	Domain stacking$data structure$Dataset$Distributional Similarity$discourse$Discharge Summary$Direct responses$discourse segment$	7
Springer Verlag, Berlin, Heidelberg, New York, AI edition, 2000.	AI	artificial Intelligence$artificial intelligence$	0
artificial AI,?	AI	artificial Intelligence$artificial intelligence$	1
There is hardly a  literature to cite, unless it be that unruly assemblage we  have come to call "AI."	AI	artificial Intelligence$artificial intelligence$	1
Having discovered this venue, many researchers in the fields of AI and machine learning see MTurk as a valuable and effective source of annotations, labels, and data, namel	AI	artificial Intelligence$artificial intelligence$	1
They were selected  on the criteria that they (1) have demonstrated problem-solving skills by having suc-  cessfully completed a computer science course and having enrolled in another, (2) not  have excessive familiarity with AI or natural language processing as  would occur, for example, if they had had a course on one of these topics, and (3)  not be an electrical engineering major (in which case they could probably repair the  circuits without aid).	AI	artificial Intelligence$artificial intelligence$	1
Some philosophical problems from the standpoint of AI.	AI	artificial Intelligence$artificial intelligence$	1
Having discovered this venue, many researchers in the fields of AI and machine learning see MTurk as a valuable and effective source of annotations, labels, and data, namely the kind requiring human knowledge.	AI	artificial Intelligence$artificial intelligence$	1
Discourse in computational linguistics and AI.	AI	artificial Intelligence$artificial intelligence$	1
In Proceedings,  5th National Conference ofthe American  AAAI.	AAAI	Association for Artificial Intelligence$Association for the Advancement of Artificial Intelligence$American Association .for  Artificial Intelligence$	0
of the Italian AAAI, Reggio Emilia, Italy.	AAAI	Association for Artificial Intelligence$Association for the Advancement of Artificial Intelligence$American Association .for  Artificial Intelligence$	0
In Proceedings of the American AAAI, September.	AAAI	Association for Artificial Intelligence$Association for the Advancement of Artificial Intelligence$American Association .for  Artificial Intelligence$	0
In Proceedings of EVALITA Workshop, 11th Congress of Italian AAAI, Reggie Emilia.	AAAI	Association for Artificial Intelligence$Association for the Advancement of Artificial Intelligence$American Association .for  Artificial Intelligence$	0
of the American AAAI, pages 984-989.	AAAI	Association for Artificial Intelligence$Association for the Advancement of Artificial Intelligence$American Association .for  Artificial Intelligence$	0
Of the 17th Annual Confer- ence of the American AAAI.	AAAI	Association for Artificial Intelligence$Association for the Advancement of Artificial Intelligence$American Association .for  Artificial Intelligence$	0
Erom a psychological point of view, however, there  are strong reasons for maintaining the distinction  between these two kinds of knowledge (Anderson,  1976:116-119):  - the DK seems possessed in  all-or-none manner whereas i t  is possible to  possess procedural knowledge only partial ly;  - the DK is acquired suddenly  by being told whereas the procedural knowledge can  be acquired only gradually by performing a skit1;  - i t  is possible to communicate verbally the  declarative but not the procedural knowledge.	DK	declarative kernels$declarative knowledge$Daily Kos$Daily Ko$	1
Procedural and DK are  inter-related, and are processed by an inference engine  based on a modified "recognize-acts" cycle, which includes  matching, conflict resolution, and execution phases.	DK	declarative kernels$declarative knowledge$Daily Kos$Daily Ko$	1
We will look at  ways in which Constrained Conditional Models can be used to augment  probabilistic models with DK based constraints and how these  support expressive global decisions.	DK	declarative kernels$declarative knowledge$Daily Kos$Daily Ko$	1
However they rely on hand-coded DK bases.	DK	declarative kernels$declarative knowledge$Daily Kos$Daily Ko$	1
e DK seems possessed in  all-or-none manner whereas i t  is possible to  possess procedural knowledge only partial ly;  - the DK is acquired suddenly  by being told whereas the procedural knowledge can  be acquired only gradually by performing a skit1;  - i t  is possible to communicate verbally the  declarative but not the procedural knowledge.	DK	declarative kernels$declarative knowledge$Daily Kos$Daily Ko$	1
In ALICE the DK is  constituted by the information that the system is  able to derive from the texts.	DK	declarative kernels$declarative knowledge$Daily Kos$Daily Ko$	1
7.6%  + Stemming 26.1% 59.5% 36.3%     + Proper Nouns 36.5% 56.8% 44.4%  Named Entities 48.4% 49.1% 48.7%  All Combined 21.1% 65.0% 31.9%  Manual Scoring 67.0% 75.0% 70.8%      Single word SVM 19.0% 30.0% 23.3%  + Stemming 22.0% 30.2% 25.5%     + Proper Nouns 46.3% 54.0% 49.9%  Named Entities 60.1% 41.5% 49.1%  All Combined 20.3% 65.7% 31.0%  Manual Scoring 47.0% 62.0% 53.5%  Figure 1: Results on DK (top) and Red State  (bottom) data.	DK	declarative kernels$declarative knowledge$Daily Kos$Daily Ko$	2
Red State is a conserva- tive political blog whereas DK is a liberal  political blog.	DK	declarative kernels$declarative knowledge$Daily Kos$Daily Ko$	2
We collected a  total of 100,000 blog posts from DK and  70,000 blog posts from Red State and a total of  787,780 tags across both blogs (an average of 4.63  tags per post).	DK	declarative kernels$declarative knowledge$Daily Kos$Daily Ko$	2
ajor political blogs,  DK (www.dailykos.com) and Red State  (www.redstate.com).	DK	declarative kernels$declarative knowledge$Daily Kos$Daily Ko$	2
For instance, in the top- ics induced from DK, a very liberal leaning blog, we see that the most negative topic (i.e. the topic that contributes the most to potential nega- tive comments) talks about the Bush adminstration and Vice President Cheney, which was and remains quite unpopular with people from the left.	DK	declarative kernels$declarative knowledge$Daily Kos$Daily Ko$	2
5 Results and Evaluation  For evaluating our methods, we used 2,681 posts  from DK and 571 posts from Red State.	DK	declarative kernels$declarative knowledge$Daily Kos$Daily Ko$	2
3 Data  We collected data from two major political blogs,  DK (www.dailykos.com) and Red State  (www.redstate.com).	DK	declarative kernels$declarative knowledge$Daily Kos$Daily Ko$	2
We collected a  total of 100,000 blog posts from DK and  70,000 b	DK	declarative kernels$declarative knowledge$Daily Kos$Daily Ko$	2
7.6%  + Stemming 26.1% 59.5% 36.3%     + Proper Nouns 36.5% 56.8% 44.4%  Named Entities 48.4% 49.1% 48.7%  All Combined 21.1% 65.0% 31.9%  Manual Scoring 67.0% 75.0% 70.8%      Single word SVM 19.0% 30.0% 23.3%  + Stemming 22.0% 30.2% 25.5%     + Proper Nouns 46.3% 54.0% 49.9%  Named Entities 60.1% 41.5% 49.1%  All Combined 20.3% 65.7% 31.0%  Manual Scoring 47.0% 62.0% 53.5%  Figure 1: Results on DKs (top) and Red State  (bottom) data.	DK	declarative kernels$declarative knowledge$Daily Kos$Daily Ko$	3
Red State is a conserva- tive political blog whereas DKs is a liberal  political blog.	DK	declarative kernels$declarative knowledge$Daily Kos$Daily Ko$	3
We collected a  total of 100,000 blog posts from DKs and  70,000 blog posts from Red State and a total of  787,780 tags across both blogs (an average of 4.63  tags per post).	DK	declarative kernels$declarative knowledge$Daily Kos$Daily Ko$	3
ajor political blogs,  DKs (www.dailykos.com) and Red State  (www.redstate.com).	DK	declarative kernels$declarative knowledge$Daily Kos$Daily Ko$	3
For instance, in the top- ics induced from DKs, a very liberal leaning blog, we see that the most negative topic (i.e. the topic that contributes the most to potential nega- tive comments) talks about the Bush adminstration and Vice President Cheney, which was and remains quite unpopular with people from the left.	DK	declarative kernels$declarative knowledge$Daily Kos$Daily Ko$	3
5 Results and Evaluation  For evaluating our methods, we used 2,681 posts  from DKs and 571 posts from Red State.	DK	declarative kernels$declarative knowledge$Daily Kos$Daily Ko$	3
3 Data  We collected data from two major political blogs,  DKs (www.dailykos.com) and Red State  (www.redstate.com).	DK	declarative kernels$declarative knowledge$Daily Kos$Daily Ko$	3
We collected a  total of 100,000 blog posts from DKs and  70,000 b	DK	declarative kernels$declarative knowledge$Daily Kos$Daily Ko$	3
Another, and perhaps more serious example concerns so-called -nie/-cie VBGs, i.e., substan- tiva verbalia (Puzynina, 1969) such as pi?c::picie ?	VBG	Verbs Only ING$gerund$verb, gerund or present partici$	1
on a particular understanding of the notion of lex- eme, various deverbal forms, such as participles and VBGs.	VBG	Verbs Only ING$gerund$verb, gerund or present partici$	1
For example, a typical Polish verbal lexeme contains a number of personal forms, a number of impersonal forms, as well as, depending on a particular understanding of the notion of lex- eme, various deverbal forms, such as participles and VBGs.	VBG	Verbs Only ING$gerund$verb, gerund or present partici$	1
it is often not clear whether VBGs are ?	VBG	Verbs Only ING$gerund$verb, gerund or present partici$	1
ts of such lexemes consisting of those forms which have the same inflectional proper- ties: all verbal forms of given lexeme with the inflectional category of person and number are grouped into one flexeme, other forms belong- ing to this lexeme, but with adjectival inflectional properties, are grouped into another flexeme, those forms, which inflect for case but not for gender are grouped into a VBGial flexeme, etc.	VBG	Verbs Only ING$gerund$verb, gerund or present partici$	1
We selected all adjectives in the cor- pus with more than 50 occurences (2283 lemmata), including some VBGs and participles with a pre- dominant modifying function (for more details on the selection criteria, cf.	VBG	Verbs Only ING$gerund$verb, gerund or present partici$	1
These forms have very different morphosyntactic properties: finite non-past tense forms have the inflectional categories of person and number, adjectival participles have the inflec- tional properties of non-gradable adjectives and, additionally, inflect for negation and have aspect, VBGs inflect for case and, at least potentially, for number, but not for person, etc.	VBG	Verbs Only ING$gerund$verb, gerund or present partici$	1
Following Rhetorical Structure Theory (RST; Mann and Thompson 1988), they represent documents by trees whose leaves correspond to EDUs (edus) and whose nodes specify how these and larger units (e.g., multi-sentence segments) are linked to each other by rhetorical relations (e.g., Contrast, Elaboration).	EDUs	elementary discourse units$Elementary Discourse Units$	0
We stud- ied how preferences are linguistically expressed in EDUs on two different cor- pus genres: one already available, the Verbmobil corpus and the Booking corpus purposely built for this project.	EDUs	elementary discourse units$Elementary Discourse Units$	0
Each discourse analy-  sis yielded an average of 52 EDUs.	EDUs	elementary discourse units$Elementary Discourse Units$	0
Table 1 displays average kappa  statistics that reflect the reliability of the annota-  tion of EDUs, k~,, hierarchical  discourse spans, ks, hierarchical nuclearity assign-  ments, k,~, and hierarchical rhetorical relation as-  signments, k~. Kappa figures higher than 0.8 corre-  spond to good agreement; kappa figures higher than  0.6 correspond to acceptable agreement.	EDUs	elementary discourse units$Elementary Discourse Units$	0
4 This is the decision-based parser described in Marcu (2000); it achieves an F1 of 38.2 for the identification of EDUs, 50.0 for hierarchical spans, 39.9 for nuclearity, and 23.4 for relation assignment.	EDUs	elementary discourse units$Elementary Discourse Units$	0
Our Dis- course Parser uses a symbolic approach and pro- duces discourse trees, which include nuclearity,  but lacking rhetorical relation names: intermedi- ate nodes in the discourse tree have no name and  terminal nodes are EDUs,  mainly clauses.	EDUs	elementary discourse units$Elementary Discourse Units$	0
1In the TRAINING portion of the RST Treebank, we found 17213 EDUs (EDU?s).	EDUs	elementary discourse units$Elementary Discourse Units$	1
143  From EDUs to Complex Ones  Holger  Schauer   Computational Linguistics Division  Freiburg University  D-79085 Freiburg, Germany  s c\]~auer@coling, uni-freiburg, de  Abst ract   Coherence relations have usually  been taken to link clauses and larger  units.	EDUs	elementary discourse units$Elementary Discourse Units$	1
The steps carried out for the annotation of the corpora were the following: A. EDUs segmentation.	EDUs	elementary discourse units$Elementary Discourse Units$	1
In particular, we split all reasons into several reason units by simple preprocessing (splitting us- ing Stanford CoreNLP (Manning et al, 2014), seg- mentation into EDUs by RST tools (Surdeanu et al, 2015)) and identified the ref- erenced arguments (A1 or A2) by pattern matching and dependency parsing.	EDUs	elementary discourse units$Elementary Discourse Units$	1
However in the actual situation of  NLU process, complete  knowlegde cannot be given, but only partial  knowledge is available.	NLU	natural language understanding$Natural Language Understanding$	0
1 In t roduct ion   This paper describes a NLU sys-  tem for the domain of naive thermodynamics.	NLU	natural language understanding$Natural Language Understanding$	0
Conceptual dependency: A the- ory of NLU.	NLU	natural language understanding$Natural Language Understanding$	0
Differentiat- ing between implicative and non-implicative verbs is therefore an essential component of NLU, relevant to applications such as textual entailment and summarization.	NLU	natural language understanding$Natural Language Understanding$	0
The system core is the Dialogue Manager, which processes the information coming from the different input modality agents by means of a NLU module and provides output in the appropriate modality.	NLU	natural language understanding$Natural Language Understanding$	0
Nevertheless, they are key to various NLP applications, including those benefiting from deep NLU (e.g., textual inference (Bobrow et al, 2007)), generation of well- formed output (e.g., natural language weather alert systems (Lareau and Wanner, 2007)) or both (as in machine translation (Oepen et al, 2007)).	NLU	natural language understanding$Natural Language Understanding$	0
Hirst G. (1981) "Discourse Oriented Anaphoral  Resolution in NLU:  A Review."	NLU	natural language understanding$Natural Language Understanding$	1
Sabah G~rard 1997, The fundamental role of  pragmatics in NLU  and its implications for modular, cognitively mo-  tivated architectures, Studies in Computational  Pragmatics: Abduction, Belief, and Context, Uni-  versity College Press, to appear, London.	NLU	natural language understanding$Natural Language Understanding$	1
44  Domain Dependent NLU  Klaus Heje Munch  Dept.	NLU	natural language understanding$Natural Language Understanding$	1
Small, S. 1980 Word Expert Parsing: a Theory of Distributed Word-  Based NLU.	NLU	natural language understanding$Natural Language Understanding$	1
Pages  12-20 in Recent Developments and Applications of  NLU, edited by Jeremy  Peckham.	NLU	natural language understanding$Natural Language Understanding$	1
But there is no clear process for identifying potential tasks (other than consensus by a sufficient num- ber of researchers), nor for quantifying their po- tential contribution to existing NLP tasks, let alne to NLU.	NLU	natural language understanding$Natural Language Understanding$	1
We show how such a DM can be used for topic identification of unseen calls.	DM	domain model$discourse model$dialogue move$dialog management$Data mining$DiscourseModel$dialog manager$Default Model$dialogue management$Dialogue Manager$Dialogue in Motion$	0
The DM is comprised of pri- marily a topic taxonomy where every node is char- acterized by topic(s), typical Questions-Answers (Q&As), typical actions and call statistics.	DM	domain model$discourse model$dialogue move$dialog management$Data mining$DiscourseModel$dialog manager$Default Model$dialogue management$Dialogue Manager$Dialogue in Motion$	0
We note that full parsing is applied in all systems, with the specific choice of the parser of Charniak and Johnson (2005) with the biomedical DM of McClosky (2009) and conversion into the Stan- ford Dependency representation (de Marneffe et al, 2006) being adopted by five participants.	DM	domain model$discourse model$dialogue move$dialog management$Data mining$DiscourseModel$dialog manager$Default Model$dialogue management$Dialogue Manager$Dialogue in Motion$	0
Each such domain generally has a DM which is essential to handle customer complaints.	DM	domain model$discourse model$dialogue move$dialog management$Data mining$DiscourseModel$dialog manager$Default Model$dialogue management$Dialogue Manager$Dialogue in Motion$	0
Towards this, we propose an unsupervised technique to generate DMs automati- cally from call transcriptions.	DM	domain model$discourse model$dialogue move$dialog management$Data mining$DiscourseModel$dialog manager$Default Model$dialogue management$Dialogue Manager$Dialogue in Motion$	0
The DM is comprised of pri- marily a topic taxonomy where ever	DM	domain model$discourse model$dialogue move$dialog management$Data mining$DiscourseModel$dialog manager$Default Model$dialogue management$Dialogue Manager$Dialogue in Motion$	0
The links actually shown in tilt; figure are based  on the, procedural relations in the DM.	DM	domain model$discourse model$dialogue move$dialog management$Data mining$DiscourseModel$dialog manager$Default Model$dialogue management$Dialogue Manager$Dialogue in Motion$	0
This, however, appears to run  counter to what we expect from results reported in  prior work on discourse(Kurohashi and Nagao, 1994;  Litman and Passonneau, 1995; Grosz and Sidner,  1986; Marcu, 1997), where the notion of clues or  cue phrases forms an important part of identifying  a structure of discourse7  Table 4 shows how the confidence value (CF) af-  fects the performance of DMs.	DM	domain model$discourse model$dialogue move$dialog management$Data mining$DiscourseModel$dialog manager$Default Model$dialogue management$Dialogue Manager$Dialogue in Motion$	1
We anchor a referring expression  like ref()~D(named(D, Marg)&card(D, 1))) in  tile DM by proving the existence of  an entity in the model which satisfies the prop-  erties specified by the referring expression, in  this case aD(na,~ed(D, Mary)~ea,'d(D, 1)) 2.	DM	domain model$discourse model$dialogue move$dialog management$Data mining$DiscourseModel$dialog manager$Default Model$dialogue management$Dialogue Manager$Dialogue in Motion$	1
Models and  DMs, dournal of Language and  Computation, 1(2):159-174.	DM	domain model$discourse model$dialogue move$dialog management$Data mining$DiscourseModel$dialog manager$Default Model$dialogue management$Dialogue Manager$Dialogue in Motion$	1
63  Semantic analyses can account for these cases if  nominalizations are assumed to evoke event repre-  sentations into the DM.	DM	domain model$discourse model$dialogue move$dialog management$Data mining$DiscourseModel$dialog manager$Default Model$dialogue management$Dialogue Manager$Dialogue in Motion$	1
vx((ho,, e(x) v car(X))  qY (of (Y, AZ(door( Z) ), X)~eard(Y, 1)))  This means that, having used utterance (1)  above to update the DM, we have  the fbllowing amongst the facts in Discourse  State 1:  Discourse State 1  seel(#138)  0(#138, agent, #94)  0(#138, object, #139)  card(#139, 1) house( #139)  ends_be for'e(#4(1), #sat )   door(#46(#139))  entrance( #46( #139 ) )  of(#46(#139),  ~d(door'(A)), #139)  card(#46(#139),  1   aspect(simple, #137, #138)  In updating utterance (2), the bridging descrip-  tion whi	DM	domain model$discourse model$dialogue move$dialog management$Data mining$DiscourseModel$dialog manager$Default Model$dialogue management$Dialogue Manager$Dialogue in Motion$	1
S&H claim that the ellipsis resolution pro-  cess obtains referents from propositional represen-  tations, whereas what they term Model Interpre-  tive Anaphora (MIA) (e.g., 'do it' anaphora) ob-  tains referents from the DM.	DM	domain model$discourse model$dialogue move$dialog management$Data mining$DiscourseModel$dialog manager$Default Model$dialogue management$Dialogue Manager$Dialogue in Motion$	1
1 Introduction Determining when to make a DM is a topic of growing importance in dialogue systems.	DM	domain model$discourse model$dialogue move$dialog management$Data mining$DiscourseModel$dialog manager$Default Model$dialogue management$Dialogue Manager$Dialogue in Motion$	2
Because turn boundaries are not clearly defined or enforced, we apply RL to the problem of when to make a DM, rather than what type of DM to make.	DM	domain model$discourse model$dialogue move$dialog management$Data mining$DiscourseModel$dialog manager$Default Model$dialogue management$Dialogue Manager$Dialogue in Motion$	2
To date, RL has been applied to learn the most effective DM to make, but has not been applied to learning the timings of these moves, although the related con-cept of when to release a turn has been explored (English and Heeman 2005).	DM	domain model$discourse model$dialogue move$dialog management$Data mining$DiscourseModel$dialog manager$Default Model$dialogue management$Dialogue Manager$Dialogue in Motion$	2
It consists of 66 textual dia-logues between human tutors and students, with an average of 90 tutor DMs and 36 student DMs.	DM	domain model$discourse model$dialogue move$dialog management$Data mining$DiscourseModel$dialog manager$Default Model$dialogue management$Dialogue Manager$Dialogue in Motion$	2
This strategy assumes that each individual input can be considered as an independent DM.	DM	domain model$discourse model$dialogue move$dialog management$Data mining$DiscourseModel$dialog manager$Default Model$dialogue management$Dialogue Manager$Dialogue in Motion$	2
Our overall approach is also related to many other DM approaches, including those that construct dialog graphs from dialog data via clustering (Lee et al, 2009), learn information state updates using discriminative classification models (Hakkani-Tur et al, 2012; Mairesse et al, 2009), optimize dialog strategy using reinforce- ment learning (RL) (Scheffler and Young, 2002; Rieser and Lemon, 2008), or combine RL with information state	DM	domain model$discourse model$dialogue move$dialog management$Data mining$DiscourseModel$dialog manager$Default Model$dialogue management$Dialogue Manager$Dialogue in Motion$	3
However, such systems are challenging to build, currently requiring manual engineering of substantial domain-specific task knowl- edge and DM strategies.	DM	domain model$discourse model$dialogue move$dialog management$Data mining$DiscourseModel$dialog manager$Default Model$dialogue management$Dialogue Manager$Dialogue in Motion$	3
3.4 Dialogue Management We have developed servers for DM and to control the application interface for database query.	DM	domain model$discourse model$dialogue move$dialog management$Data mining$DiscourseModel$dialog manager$Default Model$dialogue management$Dialogue Manager$Dialogue in Motion$	3
However, such systems are challenging to build, currently requiring expensive, expert engineering of significant domain-specific task knowledge and DM strategies.	DM	domain model$discourse model$dialogue move$dialog management$Data mining$DiscourseModel$dialog manager$Default Model$dialogue management$Dialogue Manager$Dialogue in Motion$	3
Williams J, Young S (2003) Using Wizard-of-Oz  simulations to bootstrap Reinforcement-Learning- based DM systems.	DM	domain model$discourse model$dialogue move$dialog management$Data mining$DiscourseModel$dialog manager$Default Model$dialogue management$Dialogue Manager$Dialogue in Motion$	3
Re- inforcement learning for spoken DM using least-squares policy iteration and fast feature selection.	DM	domain model$discourse model$dialogue move$dialog management$Data mining$DiscourseModel$dialog manager$Default Model$dialogue management$Dialogue Manager$Dialogue in Motion$	3
5.2 DM of geographically significant local linguistic contexts We currently use data mining on tagged corpora to learn the contexts in which geographic and non-geographic ref- erences occur, the words and phrases leading up to and trailing the name n. The tagged corpora were obtained using the Alembic tagger (Day et al, 1997).	DM	domain model$discourse model$dialogue move$dialog management$Data mining$DiscourseModel$dialog manager$Default Model$dialogue management$Dialogue Manager$Dialogue in Motion$	4
In Proceedings of the  2004 ACM SIGKDD international conference on  Knowledge Discovery and DM.	DM	domain model$discourse model$dialogue move$dialog management$Data mining$DiscourseModel$dialog manager$Default Model$dialogue management$Dialogue Manager$Dialogue in Motion$	4
DM techniques proved  not to be very transparent when digging up  justifications for scores.	DM	domain model$discourse model$dialogue move$dialog management$Data mining$DiscourseModel$dialog manager$Default Model$dialogue management$Dialogue Manager$Dialogue in Motion$	4
The elements of statistical learning:  DM, inference, and prediction.	DM	domain model$discourse model$dialogue move$dialog management$Data mining$DiscourseModel$dialog manager$Default Model$dialogue management$Dialogue Manager$Dialogue in Motion$	4
DM tools for biological se- quences.	DM	domain model$discourse model$dialogue move$dialog management$Data mining$DiscourseModel$dialog manager$Default Model$dialogue management$Dialogue Manager$Dialogue in Motion$	4
In Proceedings of the 17th ACM SIGKDD International Conference on Knowledge Discovery and DM, pages 430?	DM	domain model$discourse model$dialogue move$dialog management$Data mining$DiscourseModel$dialog manager$Default Model$dialogue management$Dialogue Manager$Dialogue in Motion$	4
Asterisks (*) indicate transitions not in the baseline DM.	DM	domain model$discourse model$dialogue move$dialog management$Data mining$DiscourseModel$dialog manager$Default Model$dialogue management$Dialogue Manager$Dialogue in Motion$	6
The key benefit of the POMDP approach is that the DM can exploit the belief state to make better progress in the face of low-confidence or even nonsensical replies, without sacrificing over- all task completion.	DM	domain model$discourse model$dialogue move$dialog management$Data mining$DiscourseModel$dialog manager$Default Model$dialogue management$Dialogue Manager$Dialogue in Motion$	6
A text plan is a set of communicative goals which is assumed to be output by a DM of a spoken dialog system.	DM	domain model$discourse model$dialogue move$dialog management$Data mining$DiscourseModel$dialog manager$Default Model$dialogue management$Dialogue Manager$Dialogue in Motion$	6
We created a state-based DM by hand (called HC) which broadly reflects the agents?	DM	domain model$discourse model$dialogue move$dialog management$Data mining$DiscourseModel$dialog manager$Default Model$dialogue management$Dialogue Manager$Dialogue in Motion$	6
After each step HC checks if the con- nection is working by asking if the network light is green, pinging the modem, then asking the user 6 POMDP HC HC(0) CER 30% 30% 0% N 500 500 500 TCR 96.1% 78.0% 88.6% Length 19.9 76.5 48.5 Return 73.3 8.13 48.8 Table 2: Results for the POMDP and hand-crafted DMs.	DM	domain model$discourse model$dialogue move$dialog management$Data mining$DiscourseModel$dialog manager$Default Model$dialogue management$Dialogue Manager$Dialogue in Motion$	6
... ... ... ... Table 3: Fragment of a conversation with the POMDP DM.	DM	domain model$discourse model$dialogue move$dialog management$Data mining$DiscourseModel$dialog manager$Default Model$dialogue management$Dialogue Manager$Dialogue in Motion$	6
MIMUS follows the Information State Up- date approach to DM, and supports English, German and Spanish, with the possibility of changing language on?the?	DM	domain model$discourse model$dialogue move$dialog management$Data mining$DiscourseModel$dialog manager$Default Model$dialogue management$Dialogue Manager$Dialogue in Motion$	8
3 ISU?based Dialogue Management in MIMUS As pointed out above, MIMUS follows the ISU approach to DM (Larsson and Traum, 2000).	DM	domain model$discourse model$dialogue move$dialog management$Data mining$DiscourseModel$dialog manager$Default Model$dialogue management$Dialogue Manager$Dialogue in Motion$	8
Information State and DM in the TRINDI Dia- logue Move Engine Toolkit.	DM	domain model$discourse model$dialogue move$dialog management$Data mining$DiscourseModel$dialog manager$Default Model$dialogue management$Dialogue Manager$Dialogue in Motion$	8
In language processing, reinforcement learning has been ap- plied to a DM system that con- verses with a human user by taking actions that generate natural language (Scheffler and Young, 2002; Young et al, 2013).	DM	domain model$discourse model$dialogue move$dialog management$Data mining$DiscourseModel$dialog manager$Default Model$dialogue management$Dialogue Manager$Dialogue in Motion$	8
MIMUS follows the Information State Update approach to DM, and has been developed under the EU?funded TALK project (Talk Project, 2004).	DM	domain model$discourse model$dialogue move$dialog management$Data mining$DiscourseModel$dialog manager$Default Model$dialogue management$Dialogue Manager$Dialogue in Motion$	8
Partially observable Markov decision processes with continu- ous observations for DM.	DM	domain model$discourse model$dialogue move$dialog management$Data mining$DiscourseModel$dialog manager$Default Model$dialogue management$Dialogue Manager$Dialogue in Motion$	8
The DAFs are pushed and popped on a single stack, and that simple virtual machine is the DM, where DAFs being pushed, popped, or reentered at a lower stack point are intended to capture the exits from, and returns to, abandoned topics and the movement of conversational initiative between the system and the user.	DM	domain model$discourse model$dialogue move$dialog management$Data mining$DiscourseModel$dialog manager$Default Model$dialogue management$Dialogue Manager$Dialogue in Motion$	9
Integrating OWL Ontologies with a DM.	DM	domain model$discourse model$dialogue move$dialog management$Data mining$DiscourseModel$dialog manager$Default Model$dialogue management$Dialogue Manager$Dialogue in Motion$	9
In our framework, the NLG component must achieve a high-level Communicative Goal from the DM (e.g. to present a number of items) through planning a sequence of lower- level generation steps or actions, for example first to summarize all the items and then to recommend the highest ranking one.	DM	domain model$discourse model$dialogue move$dialog management$Data mining$DiscourseModel$dialog manager$Default Model$dialogue management$Dialogue Manager$Dialogue in Motion$	9
The DM checks the input pool regularly to retrieve the corresponding input.	DM	domain model$discourse model$dialogue move$dialog management$Data mining$DiscourseModel$dialog manager$Default Model$dialogue management$Dialogue Manager$Dialogue in Motion$	9
The in- put to the module is a Communicative Goal supplied by the DM.	DM	domain model$discourse model$dialogue move$dialog management$Data mining$DiscourseModel$dialog manager$Default Model$dialogue management$Dialogue Manager$Dialogue in Motion$	9
The system core is the DM, which processes the information coming from the different input modality agents by means of a natural language understanding module and provides output in the appropriate modality.	DM	domain model$discourse model$dialogue move$dialog management$Data mining$DiscourseModel$dialog manager$Default Model$dialogue management$Dialogue Manager$Dialogue in Motion$	9
This database is  CMPed by an intelligent search procedure  able to handle multimedia representations of  knowledge.	CMP	complement$Cumulative Micro Precision$	0
Subordinate clauses: clausal CMPs of verbs like ?	CMP	complement$Cumulative Micro Precision$	0
The clustering clearly recognizes a majority of objects bearing no CMP and a minority having a regular CMP.	CMP	complement$Cumulative Micro Precision$	0
in the surface structure, as follows: The auxiliary ngiya is subject to the constraints in (2), meaning that it combines with a verb as its first CMP and then the verb?s CMPs as its remaining CMPs.9 The auxiliary can combine with its CMPs in any order, thanks to a series of head- CMP rules which realize the nth element of 6The grammar in fact finds 42 parses for this example.	CMP	complement$Cumulative Micro Precision$	0
Any other mod- ifiers or CMPs (PPs, other adjectives, etc.)	CMP	complement$Cumulative Micro Precision$	0
The PDT method in- troduced in our work is an explicit attempt to build individual translation systems that meet this crite- ria, while being less computationally demanding than the diversity generating techniques explored by Macherey and Och (2007).	PDT	Positive Diversity Tuning$predeterminer$predeter-$	0
The PDT method explored in this work can be used to tune individual systems for any ensemble in which individual models can be fit to multiple extrinsic loss functions.	PDT	Positive Diversity Tuning$predeterminer$predeter-$	0
Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 320?328, Sofia, Bulgaria, August 8-9, 2013 c?2013 Association for Computational Linguistics PDT for Machine Translation System Combination Daniel Cer, Christopher D. Manning and Daniel Jurafsky Stanford University Stanford, CA 94305, USA {danielcer,manning,jurafsky}@stanford.edu Abstract We present PDT, a newmethod for tuningmachine translation models specifically for improved perfor- mance during system combination.	PDT	Positive Diversity Tuning$predeterminer$predeter-$	0
PDT for Machine Translation System Combination.	PDT	Positive Diversity Tuning$predeterminer$predeter-$	0
Lexical cat-  egories include noun, verb, pronoun, propernoun,  adjective, adverb, preposition, particle, conjunction,  determiner, cardinal, ordinal, PDT, noun  modifier, and month.	PDT	Positive Diversity Tuning$predeterminer$predeter-$	1
It surpassed the syntactic model, which did not  provide for a PDT such as "all" preceding  another determiner, such as "your" or "the".	PDT	Positive Diversity Tuning$predeterminer$predeter-$	1
1 would give: the/DT alien/NN would/MD not/RB use/VB my/PRP$ spaceship/NN but/CC the/DT hers/PRP ./. Finally, words are replaced with their correspond- ing POS tags; for the following words, word to- kens are used as their corresponding POS tags: coordinating conjunctions, determiners, preposi- tions, modals, PDTs, possessives, pro- nouns, question adverbs.	PDT	Positive Diversity Tuning$predeterminer$predeter-$	1
Thus we first look for all possible PDTs to.g, 'all')  within the available gal) before the corresponding header word,  Again the succe.	PDT	Positive Diversity Tuning$predeterminer$predeter-$	1
interjection D determiner P pre- or postposition, or subordinating conjunction & coordinating conjunction T verb particle X existential there, PDTs Y X + verbal # hashtag (indicates topic/category for tweet) @ at-mention (indicates a user as a recipient of a tweet) ~ discourse marker, indications of continuation across multiple tweets U URL or email address E emoticon $ numeral , punctuation G other abbreviations, foreign words, possessive endings, symbols, garbage Table 6: POS tagset from Gimpel et al(2011) used in this paper,	PDT	Positive Diversity Tuning$predeterminer$predeter-$	1
The only change we have made is to turn all ncmod GRs with of as the modifier into iobj GRs (unless the ncmod is a par- titive PDT).	PDT	Positive Diversity Tuning$predeterminer$predeter-$	1
Turney (2002) suggested comparing the frequency  of  phrase  co-occurrences  with  words  PDT mined  by  the  sentiment  lexicon.	PDT	Positive Diversity Tuning$predeterminer$predeter-$	2
One such method  is the Linguistic Inquiry and Word Count (LIWC)  tool developed by Pennebaker et al, (2001), which  looks for words that fall into specific, PDT mined categories such as COGNITIVE MECHANISMS and POSITIVE EMOTIONS, then reports the percent  of words in the document that fall into that catego- ry.	PDT	Positive Diversity Tuning$predeterminer$predeter-$	2
If the NL tool fails, the Wizard may revise the request  and try again until it is understood, or until a PDT  mined time limit expires (usually around a minute).	PDT	Positive Diversity Tuning$predeterminer$predeter-$	2
This result is then compared with a set of PDT mined textual answers (for that given question, of course) in order to verify the correctness of the pa- tient?s input.	PDT	Positive Diversity Tuning$predeterminer$predeter-$	2
Firstly, it matches the input to a PDT mined raw emoticon database (with over ten thou- sand emoticons).	PDT	Positive Diversity Tuning$predeterminer$predeter-$	2
-best documents: One could choose a PDT mined number  of the best matching English doc- uments for each Mandarin story.	PDT	Positive Diversity Tuning$predeterminer$predeter-$	2
While phoneme error rates are generally higher than LER, it should be noted that the reference baseforms for the names contain only one or two alternate pronunciations for each name.	LER	letter error rates$letter error rate$	0
Figure 2 shows the number of letters in the ref- erence (top), number of letter errors (middle), and LER (bottom) for each group.	LER	letter error rates$letter error rate$	1
While phoneme error rates are generally higher than LERs, it should be noted that the reference baseforms for the names contain only one or two alternate pronunciations for each name.	LER	letter error rates$letter error rate$	1
Number of letters in reference (top), number of letter er- rors (middle), and LER (bottom) partitioned according to word frequencies.	LER	letter error rates$letter error rate$	1
Testing on a disjoint cor-  pus of 30 embedded and end-point detected words (place  and ship names) gave a 39.3% LER and 21.1%  word accuracy.	LER	letter error rates$letter error rate$	1
Choice of data For CCM, we found that if the full dataset (all SLs) is used in train- ing, then performance degrades when evaluating on sentences of length ?	SL	Saccade Length$sentence length$strictly local$Slovene$source languag$Sign Language$	1
For actions described by natural language text strings, the action space is inherently discrete and potentially unbounded due to the exponential complexity of language with re- spect to SL.	SL	Saccade Length$sentence length$strictly local$Slovene$source languag$Sign Language$	1
`, where ` is the maximal SL being evaluated.	SL	Saccade Length$sentence length$strictly local$Slovene$source languag$Sign Language$	1
These had the effect of performing normali- zation for SL and other factors.	SL	Saccade Length$sentence length$strictly local$Slovene$source languag$Sign Language$	1
This test set has an average SL of 17.7 words, and from previous experiments we estimate that it is comparable in difficulty to the NEGRA corpus to within 1% of accuracy.	SL	Saccade Length$sentence length$strictly local$Slovene$source languag$Sign Language$	1
J} corresponds to a sequence of source word positions, where J is the source SL, and with null representing un- aligned target words.	SL	Saccade Length$sentence length$strictly local$Slovene$source languag$Sign Language$	1
They  may also be useful for encoding SL  information, such as whether or not a gene was attested  in training data or whether it is present in a particular  language model or other local resource.	SL	Saccade Length$sentence length$strictly local$Slovene$source languag$Sign Language$	2
Notably, it is SL (it does not consider the context of the analysed ex- pression), while the localisation can be ambiguous: ?	SL	Saccade Length$sentence length$strictly local$Slovene$source languag$Sign Language$	2
V, {A, V }} which essentially says that the dislocated A immedi- ately precedes the matrix verb V and precedes (not necessarily immediately) the in-situ B and C. 5.2 Local case To begin with, let us see a case where dependency between the preverbal constituent and the matrix verb is SL, taking (1) as an example.	SL	Saccade Length$sentence length$strictly local$Slovene$source languag$Sign Language$	2
case where dependency between the preverbal constituent and the matrix verb is SL, e.g: 1 Ein Buch geben die Eltern dem Sohn.	SL	Saccade Length$sentence length$strictly local$Slovene$source languag$Sign Language$	2
Such anaphora resolution cannot be done without relevant context, which is not available in SL paradigms of sentence compression.	SL	Saccade Length$sentence length$strictly local$Slovene$source languag$Sign Language$	2
The factuality status of a given event cannot be determined from the SL modality and polarity operators scoping over that event alone; rather, if present, other non-local markers must be considered as well to obtain the adequate interpretation.	SL	Saccade Length$sentence length$strictly local$Slovene$source languag$Sign Language$	2
Extracting Terms and Terminological  Collocations from the ELAN SL-English Par- allel Corpus.	SL	Saccade Length$sentence length$strictly local$Slovene$source languag$Sign Language$	3
Second, it is not clear that vari- ous grammatical categories and their values have the same interpretation in each language; for ex- ample, it is rather surprising that only the Roma- nian tagset explicitly mentions strong and weak pronominal forms, it is not clear whether negative pronouns in Romanian, SL, Czech and Bul- garian are negative in the same sense of participat- ing in Negative Concord, it is not clear why Roma- nian has negative adverbs while, say, Czech lacks them, etc.	SL	Saccade Length$sentence length$strictly local$Slovene$source languag$Sign Language$	3
Such correspondences are not exceptional, e.g., the at least three masculine gen- ders of Polish (Man?czak, 1956; Saloni, 1976) are mapped into the single masculine gender of many other languages, the dual and the plural numbers of some languages (SL, Czech) are mapped to plural of other languages, etc.	SL	Saccade Length$sentence length$strictly local$Slovene$source languag$Sign Language$	3
Morphosyntactic Tagging of SL: Eval- uating PoS Taggers and Tagsets.	SL	Saccade Length$sentence length$strictly local$Slovene$source languag$Sign Language$	3
For ex- ample, Vintar (2000) presented two methods for  extraction of terminological collocations in order  to assist the translation process in SL.	SL	Saccade Length$sentence length$strictly local$Slovene$source languag$Sign Language$	3
One admirable standardization effort in the field of Slavic part of speech (POS) tagging has been the Multext-East project (Erjavec, 2001), one of whose aims was to construct mutually compati- ble tagsets for 8 European languages, including 4 Slavic languages (originally Bulgarian, Czech and SL, later extended to Croatian); additionally, a Multext-East-style tagset for Russian was con- structed at the University of T?bingen (http: //www.sfb441.uni-tuebingen.de/c1/ tagset.html).	SL	Saccade Length$sentence length$strictly local$Slovene$source languag$Sign Language$	3
One  1 approach utilizes multiple translations of a sin- gle SLe text, where the source lan- guage text guarantees semantic equivalence in  the target language texts (e.g., Barzilay &  McKeown, 2001; Pang et al, 2003).	SL	Saccade Length$sentence length$strictly local$Slovene$source languag$Sign Language$	4
We consider a parse tree on the SLe as a set of dependency edges to be transferred.	SL	Saccade Length$sentence length$strictly local$Slovene$source languag$Sign Language$	4
Background A word alignment for a parallel sentence pair represents the correspondence between words in a SLe and their translations in a target language (Brown et al 1993b).	SL	Saccade Length$sentence length$strictly local$Slovene$source languag$Sign Language$	4
This system uses a word-aligned corpus and a parser for a resource-rich language (SLe) in order to create a parser for a resource-poor language (target language).	SL	Saccade Length$sentence length$strictly local$Slovene$source languag$Sign Language$	4
A rare word in the SLe links to many words in the target language that we would ideally like to see unaligned, or aligned to other words in the sentence.	SL	Saccade Length$sentence length$strictly local$Slovene$source languag$Sign Language$	4
Figure 4 shows precision/recall curves for the different models on the En-Fr corpus using English as the SLe (left), and on the En-Pt corpus using Portuguese as the source.	SL	Saccade Length$sentence length$strictly local$Slovene$source languag$Sign Language$	4
level Introductory course in  Indian SL?	SL	Saccade Length$sentence length$strictly local$Slovene$source languag$Sign Language$	5
Place names map in Japanese SL in Japan (in Japanese, ???????????)	SL	Saccade Length$sentence length$strictly local$Slovene$source languag$Sign Language$	5
Spanish SL (LSE) translation.	SL	Saccade Length$sentence length$strictly local$Slovene$source languag$Sign Language$	5
A Survey and Critique of  American SL Natural Language Genera- tion and Machine Translation Systems.	SL	Saccade Length$sentence length$strictly local$Slovene$source languag$Sign Language$	5
British SL cor- pus project: open access archives and the observer?s paradox.	SL	Saccade Length$sentence length$strictly local$Slovene$source languag$Sign Language$	5
c?2014 Association for Computational Linguistics Proper Name Machine Translation from Japanese to Japanese SL Taro Miyazaki, Naoto Kato, Seiki Inoue, Shuichi Umeda, Makiko Azuma, Nobuyuki Hiruma NHK Science & Technology Research Laboratories Tokyo, Japan {miyazaki.t-jw, katou.n-ga, inoue.s-li, umeda.s-hg, azuma.m-ia, hiruma.n-dy}@nhk.or.jp Yuji Nagashima Faculty of Information, Kogakuin University Tokyo, japan nagasima@cc.kogakuin.ac.jp Abstract This paper describes machine transla- tion of	SL	Saccade Length$sentence length$strictly local$Slovene$source languag$Sign Language$	5
CON-based  CCG  of  Japanese  Quantifiers.	CON	Continuation$Conclusion$convolution$	0
3CON counts are used for the lower layers.	CON	Continuation$Conclusion$convolution$	0
CONs and clarifications are handled as  standard mechanisms of our interpreter.	CON	Continuation$Conclusion$convolution$	0
CON class  V_Rootl  V.Root2  V_Root3  V_Root4  V_Root5  V_Root6  V_Root7  V_Root8  Applicable rules  none  +ed  +s  +s, +ed  +ing  +ing, +ed  +ing, +s  +in~, +s, +ed  Figure 2: CON classes for verbs  Examples of lexical entries for verbs follow:  admire V~oot8 "V(admire)"  dyeing V_Roo1:1 "V(dye) PROG"  dye V_~oot4 "V(dye)"  zigza~ing V-Root I "V(zigzag) PROG"  z igzagged V-Root1	CON	Continuation$Conclusion$convolution$	0
CON class  V_Rootl  V.Root2  V_Root3  V_Root4  V_Root5  V_Root6  V_Root7  V_Root8  Applicable rules  none  +ed  +s  +s, +ed  +ing  +ing, +ed  +ing, +s  +in~, +s, +ed  Figure 2: CON classes for verbs  Examples of lexical entries for verbs follow:  admire V~oot8 "V(admire)"  dyeing V_Roo1:1 "V(dye) PROG"  dye V_~oot4 "V(dye)"  zigza~ing V-Root I "V(zigzag) PROG"  z igzagged V-Root1 "V(zigzag) PAST WE"  zigzagged V_Rootl "V(zigzag) PPART WE"  z igzag V_Root3 "V(zigzag)"  tangoes V_P.oot;1 "V(tango) 3SG PRES"  t;amgo V_Root6 "V(tango)"  taught V_Rootl "V(teaeh) PAST	CON	Continuation$Conclusion$convolution$	0
CON of this research in the near fnture  consists of the design of an adaptive variant of this  mode\[, one that learns a grammar from examples in  an unsupervised fashion.	CON	Continuation$Conclusion$convolution$	0
5 CONs and future work In this paper we have pursued a line of research that seeks to induce semantic classes for adjectives from distributional evidence.	CON	Continuation$Conclusion$convolution$	1
5 CONs and Future Directions In this paper, we described the development of a multi-step system aimed for story analysis with particular emphasis on analyzing children?s sto- ries.	CON	Continuation$Conclusion$convolution$	1
5 CON This paper has presented a precision, hand-built grammar for the Australian language Wambaya, and through that grammar a case study evaluation of the LinGO Grammar Matrix.	CON	Continuation$Conclusion$convolution$	1
5 CON  In this paper we have presented an approach to  automatic extraction of terminology in a morpho- logically rich language, such as Serbian.	CON	Continuation$Conclusion$convolution$	1
7 CON  We described measurement of semantic similarity be-  tween words.	CON	Continuation$Conclusion$convolution$	1
Perceptron TP FP FN P (%) R (%) F1 (%) 671 128 119 83.98 84.94 84.46 Conditional Random Fields TP FP FN P (%) R (%) F1 (%) 643 78 147 89.18 81.39 85.11 Bayes Point Machines TP FP FN P (%) R (%) F1 (%) 647 79 143 89.12 81.90 85.36 Table 4: Performance of different optimization strategies 6 CON To tackle the hedge cue detection problem posed by the CoNLL-2010 shared task, we utilized a classifier for sequential labeling following previ- ous work (Morante and Daelemans, 2009).	CON	Continuation$Conclusion$convolution$	1
Although transferring CONal net- work features is not a new idea (Driancourt and Bottou, 1990), the simultaneous availability of large datasets and cheap GPU co-processors has contributed to th	CON	Continuation$Conclusion$convolution$	2
eon Bottou Microsoft Research New York leon@bottou.org Abstract We construct multi-modal concept repre- sentations by concatenating a skip-gram linguistic representation vector with a vi- sual concept representation vector com- puted using the feature extraction layers of a deep CONal neural network (CNN) trained on a large labeled object recognition dataset.	CON	Continuation$Conclusion$convolution$	2
SIFT and HOG descriptors pro- duced big performance gains a decade ago, and now deep CONal features are providing a similar	CON	Continuation$Conclusion$convolution$	2
2015) and shown to achieve human level per- formance by applying CONal neural net- works to the raw image pixels.	CON	Continuation$Conclusion$convolution$	2
The CONal layers are then used as mid-level feature extractors on a variety of computer vi- sion tasks (Oquab et al.,	CON	Continuation$Conclusion$convolution$	2
Imagenet classification with deep CONal neu- ral networks.	CON	Continuation$Conclusion$convolution$	2
An  unusually large reduction in 5D suggests the opti-  inal clustering has been obtained 3 (see n = 10 in  3In practice, CON (mask {1,2,4,8,4,2,1}) is first  aI)plied to 5D to smooth out sharp local changes  Rank matrix Step 2  Step 1  cut  Step 3  Figure 5: A working example of the divisive eluster-  ing algorithm.	CON	Continuation$Conclusion$convolution$	2
Although transferring CONal net- work features is not a new idea (Driancourt and Bottou, 1990), the simultaneous availability of large datasets and cheap GPU co-processors has contributed to the achievement of considerable performance gains on a variety computer vision benchmarks: ?	CON	Continuation$Conclusion$convolution$	2
ATR tools have been developed for English  (Frantzi et al, 2000), FR (Jacquemin, 2001),  Japanese (Nakagawa and Mori, 2000), etc.	FR	French$Federal Register$fast reactors$	0
The top row of Figure 1 shows two word alignments between an English?FR sentence pair.	FR	French$Federal Register$fast reactors$	0
There are many reasons why a simple word-to-word (1-to-1) correspondence is not possible for every sentence pair: for instance, auxiliary verbs used in one lan- guage but not the other (e.g., English He walked and FR Il est alle?),	FR	French$Federal Register$fast reactors$	0
483 Computational Linguistics Volume 36, Number 3 Table 1 Test corpora statistics: English?FR, English?Spanish, English?Portuguese, Portuguese?Spanish, Portuguese?FR, and Spanish?FR.	FR	French$Federal Register$fast reactors$	0
The document data consisted of the  Wall Street Journal, San Jose Mercury News, AP  Newswire, Information from Computer Selected disks,  FR, U.S. Patents and short abstracts from the  Department of Energy.	FR	French$Federal Register$fast reactors$	1
In TREC-5, the test data was actual OCR output  of scanned images of the 1994 FR.	FR	French$Federal Register$fast reactors$	1
The  10 subcollections were defined corresponding to the  various dates of the data, i.e., the three different years  of the Wall Street Journal, the two different years of  the AP newswire, the two sets of Ziff documents (one  on each disk), and the three single subcollections (the  FR, the San Jose Mercury News, and the  U.S. Patents).	FR	French$Federal Register$fast reactors$	1
The evaluation corpus used in the HARD track consists of 372,219 documents, and includes three newswire corpora (New York Times,  Associated Press Worldstream and Xinghua English) and two governmental corpora (The Congressional Record and FR).	FR	French$Federal Register$fast reactors$	1
TREC-3 was forced to re-use some of the training  data, and TREC-4 performed routing tests using the  FR (with new data) for 25 of the topics,  and using training data and "net trash" for testing  the other 25 topics.	FR	French$Federal Register$fast reactors$	1
U.S. Govenltnent Ala~zzlol, 19 73- 74, office of the FR, National Archives and  Records Service, General Services Administration, Washington, D.C. 20408, 1974.	FR	French$Federal Register$fast reactors$	1
IG is an  information theoretic concept measuring how  much the probability distributions for a feature dif- fer among the different classes.	IG	Information gain$Information Gain$index grammar$information gain$	0
We first separately select eight words before  and after the target word in a sentence to  constitute the context, expressed as the  following form:  <wd?8, wd?7, wd?6, wd?5, wd?4, wd?3, wd?2, wd?1  , focus-word,  wd+1, wd+2, wd+3, wd+4, wd+5, wd+6, wd+7, wd+8>  Table 1 IG of every position of  context  Left context Right context  Position Information  gain  Position Information  gain  wd?1 3.979 875 wd+1 4.005 737  wd?2 2.800 943 wd+2 2.931 834  wd?3 2.183 287 wd+3 2.287 020  wd?4 1.709 504 wd+4 1.810 530  wd?5 1.361 637 wd+5 1.437 952  wd?6 1.074 606 wd+6 1.137 979  wd?7 0.304 546 wd+7 0.821 330  wd?8 0.298 992 wd+8 0.419 472    The amount of info	IG	Information gain$Information Gain$index grammar$information gain$	0
0.27 25.32 Table 2: Evaluation results Feature Weight part-of-speech pattern 0.1935 CI shortterm 0.1744 Wikipedia keyphraseness 0.1731 CI maxscore 0.1689 CI shortterm normalized 0.1379 ChiInformativeness 0.1122 document frequency (df) 0.1031 tf.idf 0.0870 ChiPhraseness 0.0660 length of phrase 0.0416 named entity heuristic 0.0279 within document frequency 0.0227 term frequency (tf) 0.0209 Table 4: IG feature weight Table 4 shows the weight associated with each feature.	IG	Information gain$Information Gain$index grammar$information gain$	0
We used two measures, both of which in- dicate a similar trend, to calculate feature effec- tiveness: IG (Kullback-Leibler di- vergence) and the chi-square statistic.	IG	Information gain$Information Gain$index grammar$information gain$	0
IG is used to select target words, in this case resulting in 849 words.	IG	Information gain$Information Gain$index grammar$information gain$	0
The  expression D\[f=v\] refers to those patterns in the database that have value v for feature  f; V is the set of possible values for feature f.  H(D~\]) = ~ H(D\[f=v,\]) ID~ vdI (5)  viEV  IG is then obtained by equation (6) and scaled to be used as a weight  for the feature during similarity matching.	IG	Information gain$Information Gain$index grammar$information gain$	0
One limitation of the IG based feature selection used in langid.py is that each feature is scored inde- pendently, and each language receives a binarized score.	IG	Information gain$Information Gain$index grammar$information gain$	1
8.2 43.2 N.NN 22.2 15.3 N.. 16.4 31.7 HV 41.5 46.4 .NN 29.9 22.9 .P 52.2 68.3 NN 86.3 83.0 XN 0.4 0.4 P. 6.6 16.8 H 61.8 65.9 NNNN 6.2 3.2 D. 4.4 12.6 R 61.5 65.5 D 99.2 99.5 ..$ 0.0 0.0 RR 7.2 9.4 NNN 28.3 18.6 J.. 5.0 12.6 NNNN 21.7 18.5 .NNN 6.7 4.0 ..VV 0.9 5.2 .C 15.8 18.8 N.D 58.6 47.8 DN.. 4.2 11.0 ... 0.8 0.3 NX 0.8 0.5 .PD 24.5 36.3 N.C 11.3 13.6 Table 5: Top 10 POS features per-group by IG, along with percentage of sentences in each language in which the feature appears.	IG	Information gain$Information Gain$index grammar$information gain$	1
For the per-group classifiers, runs AO1 and AO2 use a naive Bayes model on a word-level representation, with feature selection by IG.	IG	Information gain$Information Gain$index grammar$information gain$	1
Averbuch et al (2004) develop an IG algorithm for learning negative context patterns in discharge summaries and measure the effect of context identification on the performance of medical information retrieval.	IG	Information gain$Information Gain$index grammar$information gain$	1
using IG (Hall et al.,	IG	Information gain$Information Gain$index grammar$information gain$	1
If we look at the IG Ratio, of all features the rank difference of the verbs in the training data seems to be the strongest feature, and of the external features the frequency difference of the entire phrase containing the NC and the verb.	IG	Information gain$Information Gain$index grammar$information gain$	1
Multiset-valued  linear IGs.	IG	Information gain$Information Gain$index grammar$information gain$	2
Multiset-valued linear IGs: Imposing dominance constraints on derivations.	IG	Information gain$Information Gain$index grammar$information gain$	2
The best test for each node is selected with the standard IG criterion.	IG	Information gain$Information Gain$index grammar$information gain$	3
This test, which maximizes the IG 1 wrt.	IG	Information gain$Information Gain$index grammar$information gain$	3
The recursive tree building process ter- minates if the IG is 0.	IG	Information gain$Information Gain$index grammar$information gain$	3
A node is pruned if the IG of the best test mul- tiplied by the size of the data subsample is below a given threshold.	IG	Information gain$Information Gain$index grammar$information gain$	3
the class, is 1 The IG measures how much the test de- creases the uncertainty about the class.	IG	Information gain$Information Gain$index grammar$information gain$	3
The IG is therefore 0.92 ?	IG	Information gain$Information Gain$index grammar$information gain$	3
Granada: European  LREC Association.	LREC	Language Resources$Language Resource$Language Resources and Evaluation$	0
on LREC and  Evaluation, p. 567-572.	LREC	Language Resources$Language Resource$Language Resources and Evaluation$	0
In Proceedings of the 3rd Work- shop on Asian LREC and International Standardization, COLING 19, Taipei, Taiwan.	LREC	Language Resources$Language Resource$Language Resources and Evaluation$	0
In First International Conference on  LREC 8J Evaluation: Workshop  on Linguistic Coreference.	LREC	Language Resources$Language Resource$Language Resources and Evaluation$	0
LREC and Evaluation, 39(4):267?285.	LREC	Language Resources$Language Resource$Language Resources and Evaluation$	0
Granada: European  LRECs Association.	LREC	Language Resources$Language Resource$Language Resources and Evaluation$	1
on LRECs and  Evaluation, p. 567-572.	LREC	Language Resources$Language Resource$Language Resources and Evaluation$	1
In Proceedings of the 3rd Work- shop on Asian LRECs and International Standardization, COLING 19, Taipei, Taiwan.	LREC	Language Resources$Language Resource$Language Resources and Evaluation$	1
In First International Conference on  LRECs 8J Evaluation: Workshop  on Linguistic Coreference.	LREC	Language Resources$Language Resource$Language Resources and Evaluation$	1
LRECs and Evaluation, 39(4):267?285.	LREC	Language Resources$Language Resource$Language Resources and Evaluation$	1
In Proceedings of the Third LREC Conference.	LREC	Language Resources$Language Resource$Language Resources and Evaluation$	2
In Proceedings of Fifth International Conference on LREC, pages 449?	LREC	Language Resources$Language Resource$Language Resources and Evaluation$	2
In Proceedings of the 1st Interna- tional Conference on LREC, Granada, Spain.	LREC	Language Resources$Language Resource$Language Resources and Evaluation$	2
In Proceedings of the Fifth International Conference on LREC, pages 449?454, Genoa, Italy.	LREC	Language Resources$Language Resource$Language Resources and Evaluation$	2
LREC, (in press).	LREC	Language Resources$Language Resource$Language Resources and Evaluation$	2
LREC.	LREC	Language Resources$Language Resource$Language Resources and Evaluation$	2
Using IC to Eval- uate Semantic Similarity in a Taxonomy.	IC	Information Content$Informational Contribution$information conten$independent CL$information content$Inverse Consultation$Intonation Center$individual constant$	0
The score for each n-gram is computed as its IC (Cover and Thomas, 1991), ie. ?	IC	Information Content$Informational Contribution$information conten$independent CL$information content$Inverse Consultation$Intonation Center$individual constant$	0
Using IC to Evaluate Semantic Similarity in a Taxonomy.	IC	Information Content$Informational Contribution$information conten$independent CL$information content$Inverse Consultation$Intonation Center$individual constant$	0
Using IC to  Evaluate Semantic Similarity in a Taxonomy.	IC	Information Content$Informational Contribution$information conten$independent CL$information content$Inverse Consultation$Intonation Center$individual constant$	0
The word counts required for computing IC were obtained from Google Books Ngrams.3 The DKPro system (Ba?r et al 2012) obtained first place in STS?12 with the second run.	IC	Information Content$Informational Contribution$information conten$independent CL$information content$Inverse Consultation$Intonation Center$individual constant$	0
Using IC to Evalu- ate Semantic Similarity in a Taxonomy.	IC	Information Content$Informational Contribution$information conten$independent CL$information content$Inverse Consultation$Intonation Center$individual constant$	0
IC for selected word pairs  Ix, y\] ;C(x, Ix, y\]) ;C(y, \[x,y \]) fxa n, d~,  \[system,parallel\] 2 910 322  \[system,computation\] 78 910 322  \[path,parallel\] 1 19 14  \[class,grammar\] 5 128 86  \[define,grammar\] 3 131 80  \[class,language\] 1 128 86  \[define,language\] 9 131 80  0.0016 57 24  0.0634 740 201  0.0313 57 24  0.0235 47 34  0.0143 47 34  0.0047 295 116	IC	Information Content$Informational Contribution$information conten$independent CL$information content$Inverse Consultation$Intonation Center$individual constant$	1
In this way, a more uniform rate of information transfer is achieved, because the higher ICt of the un- expected word is stretched over a slightly longer time.	IC	Information Content$Informational Contribution$information conten$independent CL$information content$Inverse Consultation$Intonation Center$individual constant$	2
The  source grammar essentially guides the  elicitation of the sentence semantic structure  into its corresponding UNL structure, by  determining RLs and ALs, always giving  priority to ICt.	IC	Information Content$Informational Contribution$information conten$independent CL$information content$Inverse Consultation$Intonation Center$individual constant$	2
In order to evaluate the relative contribution of WordNet concepts to the ICt of a text as a whole, a node specificity metric was derived based on an empirical analysis of the distribution of topological features of WordNet such as inheritance, hierarchy depth, clustering coefficients and node de- gree and how these features map onto human judg- ments of concept specificity or informativity.	IC	Information Content$Informational Contribution$information conten$independent CL$information content$Inverse Consultation$Intonation Center$individual constant$	2
The various syntactic  frameworks that we examine below can be seen to share  a great deal of their underlying substantive claims about  the ICt of the category label of a  constituent.	IC	Information Content$Informational Contribution$information conten$independent CL$information content$Inverse Consultation$Intonation Center$individual constant$	2
Using ICt to evaluate semantic similarity in a taxonomy.	IC	Information Content$Informational Contribution$information conten$independent CL$information content$Inverse Consultation$Intonation Center$individual constant$	2
Using ICt to  evaluate semantic similarity in a taxonomy."	IC	Information Content$Informational Contribution$information conten$independent CL$information content$Inverse Consultation$Intonation Center$individual constant$	2
(5) The continuous CLs should be cut into  several ICs.	IC	Information Content$Informational Contribution$information conten$independent CL$information content$Inverse Consultation$Intonation Center$individual constant$	3
In our approach towards language-ICIA system, we have developed context aware query translation using Wikipedia.	IC	Information Content$Informational Contribution$information conten$independent CL$information content$Inverse Consultation$Intonation Center$individual constant$	3
In this way, a more uniform rate of information transfer is achieved, because the higher IC of the un- expected word is stretched over a slightly longer time.	IC	Information Content$Informational Contribution$information conten$independent CL$information content$Inverse Consultation$Intonation Center$individual constant$	4
The  source grammar essentially guides the  elicitation of the sentence semantic structure  into its corresponding UNL structure, by  determining RLs and ALs, always giving  priority to IC.	IC	Information Content$Informational Contribution$information conten$independent CL$information content$Inverse Consultation$Intonation Center$individual constant$	4
In order to evaluate the relative contribution of WordNet concepts to the IC of a text as a whole, a node specificity metric was derived based on an empirical analysis of the distribution of topological features of WordNet such as inheritance, hierarchy depth, clustering coefficients and node de- gree and how these features map onto human judg- ments of concept specificity or informativity.	IC	Information Content$Informational Contribution$information conten$independent CL$information content$Inverse Consultation$Intonation Center$individual constant$	4
The various syntactic  frameworks that we examine below can be seen to share  a great deal of their underlying substantive claims about  the IC of the category label of a  constituent.	IC	Information Content$Informational Contribution$information conten$independent CL$information content$Inverse Consultation$Intonation Center$individual constant$	4
Using IC to evaluate semantic similarity in a taxonomy.	IC	Information Content$Informational Contribution$information conten$independent CL$information content$Inverse Consultation$Intonation Center$individual constant$	4
Using IC to  evaluate semantic similarity in a taxonomy."	IC	Information Content$Informational Contribution$information conten$independent CL$information content$Inverse Consultation$Intonation Center$individual constant$	4
This multiple-pivot idea is similar to IC in that multiple pivots are re- quired, but using multiple pivot languages frees it from the dependency on rich input lexicons that contain a variety of synonyms.	IC	Information Content$Informational Contribution$information conten$independent CL$information content$Inverse Consultation$Intonation Center$individual constant$	5
For example, let V  denote the semantic valuation function (with a particu-  lar interpretation and possible world understood) and  let  V(P) = {<a,b,c>, <a,b ,d>,  <e,f ,g>},  V(x) = a, V(y) = b, and V(z) = d,  where P is a triadic predicate symbol, x, y, and z are  ICs or variables, and a, b . . . . .	IC	Information Content$Informational Contribution$information conten$independent CL$information content$Inverse Consultation$Intonation Center$individual constant$	7
The UM part of BGP-MS  consists of all partitions except SB and SW, plus all  representations i  SB (and probably SW) in which an  IC occurs denoting the user (the rest of  SB corresponds to Sparck Jones's world model).	IC	Information Content$Informational Contribution$information conten$independent CL$information content$Inverse Consultation$Intonation Center$individual constant$	7
We can expand  this by defining new predicates, even higher-order  predicates that express, for instance, properties of  or relations between sets, and in doing so we can use  monadic predicates and ICs freely  since we can interpret hese as existentially bound  variables.	IC	Information Content$Informational Contribution$information conten$independent CL$information content$Inverse Consultation$Intonation Center$individual constant$	7
Ain as unary predicate symbols and at1, ...,  at, as ICs; the symbols "^"  and "~ " are the conjunc-  tion and implication sign and the function symbol . . . . .	IC	Information Content$Informational Contribution$information conten$independent CL$information content$Inverse Consultation$Intonation Center$individual constant$	7
2 L~,p - -The  Monad ic  Second-Order   Language o f  T rees   L2K,p is the monadic second-order language over  the signature including a set of ICs  (K), a set of monadic predicates (P), and binary  predicates for immediate domination (,~), domina-  tion (,~*), linear precedence (-~) and equality (..~).	IC	Information Content$Informational Contribution$information conten$independent CL$information content$Inverse Consultation$Intonation Center$individual constant$	7
For instance, the ICs  even the variables are only substantives while verbs are always  predicats one-.	IC	Information Content$Informational Contribution$information conten$independent CL$information content$Inverse Consultation$Intonation Center$individual constant$	7
In Matuszek et al (2014), W was represented as a distribution over properties of tangible objects and U was a CCG parse.	CCG	Combinatory Categorical Grammar$Combinatory Categorial  Grammars$Combinatory Categorial Grammar$combinatory categorial grammar$Combinatory Categorial grammar$Combinatory  Categorial Grammars$	0
Efficient realization of coordinate structures in CCG.	CCG	Combinatory Categorical Grammar$Combinatory Categorial  Grammars$Combinatory Categorial Grammar$combinatory categorial grammar$Combinatory Categorial grammar$Combinatory  Categorial Grammars$	2
The framework is based on  CCGs and it  uses the morpheme as the basic building  block of the categorial lexicon.	CCG	Combinatory Categorical Grammar$Combinatory Categorial  Grammars$Combinatory Categorial Grammar$combinatory categorial grammar$Combinatory Categorial grammar$Combinatory  Categorial Grammars$	2
Natural Language Parsing with  CCGs in a Graph-  Unification-Based Formalism.	CCG	Combinatory Categorical Grammar$Combinatory Categorial  Grammars$Combinatory Categorial Grammar$combinatory categorial grammar$Combinatory Categorial grammar$Combinatory  Categorial Grammars$	2
For instance, Callaway (2003) re- 2CCG ports that the implementation of such a proces- sor for the SURGE realiser was the most time- consuming part of the evaluation with the result- ing component containing 4000 lines of code and 900 rules.	CCG	Combinatory Categorical Grammar$Combinatory Categorial  Grammars$Combinatory Categorial Grammar$combinatory categorial grammar$Combinatory Categorial grammar$Combinatory  Categorial Grammars$	2
Hoffman, in her thesis (Hoffman, 1995a,  Hoffman, 1995b), has used the Multiset-  CCG formalism  (Hoffman, 1992), an extension of Combinatory  Categorial Grammar to handle free word or-  der languages, to develop a generator for Turk-  ish.	CCG	Combinatory Categorical Grammar$Combinatory Categorial  Grammars$Combinatory Categorial Grammar$combinatory categorial grammar$Combinatory Categorial grammar$Combinatory  Categorial Grammars$	2
I integrated  a level of information structure with a unification-based  version of CCGs,  adapted  for free word order languages.	CCG	Combinatory Categorical Grammar$Combinatory Categorial  Grammars$Combinatory Categorial Grammar$combinatory categorial grammar$Combinatory Categorial grammar$Combinatory  Categorial Grammars$	2
Polynomial time parsing of CCGs.	CCG	Combinatory Categorical Grammar$Combinatory Categorial  Grammars$Combinatory Categorial Grammar$combinatory categorial grammar$Combinatory Categorial grammar$Combinatory  Categorial Grammars$	3
Generat- ing with discourse CCG.	CCG	Combinatory Categorical Grammar$Combinatory Categorial  Grammars$Combinatory Categorial Grammar$combinatory categorial grammar$Combinatory Categorial grammar$Combinatory  Categorial Grammars$	3
A left-to-right parser that simultaneously produces sentence-level syntactic and semantic analyses already exists for CCG (Steedman 1996, 583 Webber et al Anaphora and Discourse Structure 2000b; Hockenmaier, Bierner, and Baldridge, forthcoming), and it would seem straight- forward to extend such a parser to computing discourse-level syntax and semantics as well.	CCG	Combinatory Categorical Grammar$Combinatory Categorial  Grammars$Combinatory Categorial Grammar$combinatory categorial grammar$Combinatory Categorial grammar$Combinatory  Categorial Grammars$	3
Multimodal CCG?.	CCG	Combinatory Categorical Grammar$Combinatory Categorial  Grammars$Combinatory Categorial Grammar$combinatory categorial grammar$Combinatory Categorial grammar$Combinatory  Categorial Grammars$	3
Efficient normal-form parsing for CCG.	CCG	Combinatory Categorical Grammar$Combinatory Categorial  Grammars$Combinatory Categorial Grammar$combinatory categorial grammar$Combinatory Categorial grammar$Combinatory  Categorial Grammars$	3
Such nonstandard constituents are also found with coordination, which was one motivation for CCG (Steedman 1996).	CCG	Combinatory Categorical Grammar$Combinatory Categorial  Grammars$Combinatory Categorial Grammar$combinatory categorial grammar$Combinatory Categorial grammar$Combinatory  Categorial Grammars$	3
Szabolcsi, A. 1987 'On CCG.'	CCG	Combinatory Categorical Grammar$Combinatory Categorial  Grammars$Combinatory Categorial Grammar$combinatory categorial grammar$Combinatory Categorial grammar$Combinatory  Categorial Grammars$	4
{ assert : PROP presup : PROP* 3.2 Alternative Sets The concept of alternative sets plays an impor- tant role in the semantics of alternative phrases.	PROP	proposition$proper$	0
If D1 and D2 are adjacent, our PROP is true.	PROP	proposition$proper$	0
concerns), since the PROP is clearly that the concerns pertain to the merger.	PROP	proposition$proper$	0
The MRS in Fig- ure 1 is assigned to the example in (1).6 It in- cludes the basic PROPal structure: a situation of ?	PROP	proposition$proper$	0
An alternative set is a set of PROPs which differ with respect to how one or more argu- ments are filled.	PROP	proposition$proper$	0
We used two approaches for identifying story characters motivated by (Elson and McKeown, 2010): 1) named entity recognition was used for identifying PROP names, e.g., ?	PROP	proposition$proper$	1
Thus, unary adjectives denote PROPties and binary adjec- tives denote relations.	PROP	proposition$proper$	1
2) a set of part-of-speech patterns was used for the extraction of human and non-human characters that were not represented by PROP names, e.g., ?	PROP	proposition$proper$	1
1 Introduction The main hypothesis underlying the tasks in Lex- ical Acquisition is that it is possible to infer lexi- cal PROPties from distributional evidence, taken as a generalisation of a word?s linguistic behaviour in corpora.	PROP	proposition$proper$	1
Thus, we intend to induce semantic PROPties from syntactic distribution.	PROP	proposition$proper$	1
Once we have the trees in the PROP form  (to the degree this is possible) we run Hobbs'  algorithm repeatedly for each pronoun until it  has proposed n (= 15 in our experiment) can-  didates.	PROP	proposition$proper$	1
They presented a simple method for retrieving bigram counts from the web by querying a search engine and demon- strated that web counts (a) correlate with frequencies ob- tained from a carefully edited, balanced corpus such as the 100M words British National Corpus (BNC), (b) cor- relate with frequencies recreated using smoothing meth- ods in the case of unseen bigrams, (c) Rel predict hu- man plausibility judgments, and (d) yield state-of-the-art performance on pseudo-disambiguation tasks.	Rel	reliably$relevant$	0
1 In t roduct ion :  Core ference   Annotat ion   Various practical tasks requiring language tech-  nology including, for example, information ex-  traction and text summarization, can be done  more Rel if it is possible to automatically  find parts of the text containing information  about a given topic.	Rel	reliably$relevant$	0
Arbisi-Kelm (2010), for example, suggest that disfluent repetitions can be identified Rel through the use of pitch, dura- tion, and pause detection (with precision up to 93% (Nakatani, 1993)).	Rel	reliably$relevant$	0
ohashi, 441?8580, Japan Abstract In the framework of bilingual lexicon acquisition from cross-lingually relevant news articles on the Web, it is relatively harder to Rel estimate bilin- gual term correspondences for low frequency terms.	Rel	reliably$relevant$	0
Considering such a situation, this paper proposes to complementarily use much larger monolingual Web documents collected by search engines, as a resource for Rel re-estimating bilingual term correspon- dences.	Rel	reliably$relevant$	0
Department of Information and Computer Sciences, Toyohashi University of Technology Tenpaku-cho, Toyohashi, 441?8580, Japan Abstract In the framework of bilingual lexicon acquisition from cross-lingually relevant news articles on the Web, it is relatively harder to Rel estimate bilin- gual term correspondences for low frequency terms.	Rel	reliably$relevant$	0
y indicators We only consider credibility indicators that avoid making use of the searcher?s or blogger?s identity (i.e., excluding 1a, 1c, 1e, 1f, 2e from Rubin and Liddy?s list), that can be estimated automatically from available test collections only so as to facilitate repeatability of our experiments (ruling out 3e, 4a, 4c, 4d, 4e), that are textual in nature (ruling out 2d), and that can be Rel estimated with state-of-the- art language technology (ruling out 2a, 2b, 2c, 2g).	Rel	reliably$relevant$	0
As for the features that were most Rel for each cluster, listed in Table 6, they confirm the anal- ysis just made and again match the hypotheses dis- cussed in Section 2.	Rel	reliably$relevant$	1
cl high values low values 0 -1cn+1co, -1cn+1cd -1aj+1pe, -1ve+1pe 1 -1ve+1pe, -1co+1pe -1cn+1aj Table 5: Unary/binary: most Rel features (rep- resented as in examples 1 and 2).	Rel	reliably$relevant$	1
3.1 The  gender /an imat ic i ty  stat ist ics   After we have identified the correct antecedents  it is a simple counting procedure to compute  P(p\[wa) where wa is in the correct antecedent  for the pronoun p (Note the pronouns are  grouped by their gender):  \[ wain the antecedent for p \[  P(pl o) =  When there are multiple Rel words in the  antecedent we apply the likelihood test designed  by Dunning (1993) on all the words in the candi-  date NP.	Rel	reliably$relevant$	1
3.1 Feature representation Although we already had some hypotheses with re- spect to what features could be Rel, as dis- cussed in Section 2, we wanted to proceed as empir- ically as possible.	Rel	reliably$relevant$	1
Indeed, the most Rel features for each cluster matched very closely the hypotheses discussed in Section 2.	Rel	reliably$relevant$	1
When viewed in this way, a can be regarded as  an index into these vectors that specifies which  value is Rel o the particular choice of an-  tecedent.	Rel	reliably$relevant$	1
Firstly, The TSAs are constructed in an unsuper- vised learning manner, and optimized by the trans- lation model during the FD process,  without using any statistics and linguistic heuristics  or syntactic constraints.	FD	flat discriminative$forced decoding$F-measure$frequency dictionary$	1
Figure 2 depicts the TSA generation algorithm  in which a phrase-based FD tech- nique is adopted to produce the TSA of each sen- tence pair.	FD	flat discriminative$forced decoding$F-measure$frequency dictionary$	1
Max- violation perceptron and FD for scalable MT training.	FD	flat discriminative$forced decoding$F-measure$frequency dictionary$	1
In this work, we do not apply syntax- based FD (e.g., tree-to-string) because  phrase-based models can achieve the state-of-the- art translation quality with a large amount of train- ing data, and are not limited by any constituent  boundary based constraints for decoding.	FD	flat discriminative$forced decoding$F-measure$frequency dictionary$	1
Apply M to implement phrase-based FD  on each training sentence pair (c, e), and output its  best derivation d* that can transform c into e.   3.	FD	flat discriminative$forced decoding$F-measure$frequency dictionary$	1
Unlike their efforts, this paper presents a simple  approach that automatically builds the translation  span alignment (TSA) of a sentence pair by utiliz- ing a phrase-based FD technique, and  then improves syntactic rule extraction by deleting  spurious links and adding new valuable links based  on bilingual translation span correspondences.	FD	flat discriminative$forced decoding$F-measure$frequency dictionary$	1
Formally, given a sentence pair (c, e), the  phrase-based FD technique aims to  search for the best derivation d* among all consis- tent deriv	FD	flat discriminative$forced decoding$F-measure$frequency dictionary$	1
Here we select the MaxEnt aligner because it has 935 Precision Recall F-score F-content F-function HMM 62.65 48.57 54.72 62.10 34.39 BM 72.76 54.82 62.53 68.64 43.93 ME 72.66 66.17 69.26 72.52 61.41 Link-Select 69.19 72.49 70.81 74.31 60.26 Intersection-Union-Refine 63.34 66.07 64.68 70.15 49.72 Table 2: Link Selection and Combination Results the highest FD among the three aligners, although the algorithm described below can be ap- plied to any aligner.	FD	flat discriminative$forced decoding$F-measure$frequency dictionary$	2
When looking into the alignment links which are removed during the alignment link fil- tering process, we found that 80% of the removed links (1320 out of 1661 links) are incorrect align- ments, For A-E alignment, it increased the pre- cision by 3 points while reducing recall by 0.5 points, and the alignment FD is increased by about 1.5 points absolute,	FD	flat discriminative$forced decoding$F-measure$frequency dictionary$	2
Tables 3 and 4 show the improvement of C-E and A-E alignment FDs with the confidence-based alignment link filtering (ALF).	FD	flat discriminative$forced decoding$F-measure$frequency dictionary$	2
Figure 5 shows the FD graph.	FD	flat discriminative$forced decoding$F-measure$frequency dictionary$	2
Figure 3 provides another comparison of the three  approaches by highlighting how the FD varies   0  10  20  30  40  50  60  70  80  90  100  0  0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9  1 Pe rc e n ta ge Threshold precision recall F-Measure Figure 1: Results of cross-document coreferencing  on the John Smith corpus using the incremental  vector space approach.	FD	flat discriminative$forced decoding$F-measure$frequency dictionary$	2
As a result, 82% of selected alignments have higher F- scores, and the FD of the combined align- ments is increased over the best aligner (the Max- Ent aligner) by 0.8.	FD	flat discriminative$forced decoding$F-measure$frequency dictionary$	2
For C-E alignment, removing low confidence alignment links increased alignment precision by 5.5 point, while decreased recall by 1.8 point, and the overall alignment FD is increased by 1.3 point.	FD	flat discriminative$forced decoding$F-measure$frequency dictionary$	2
As a first step, we have compiled a preliminary list of  the 4000 most frequent German words from an existing  FD (Meier (1967)) and news texts.	FD	flat discriminative$forced decoding$F-measure$frequency dictionary$	3
The icelandic FD.	FD	flat discriminative$forced decoding$F-measure$frequency dictionary$	3
On the basis of the above collected word li- brary, we discriminate manually the positive and  negative meaning, PSAA, NSAA, and inverse  words in 50,000 Chinese words according to  Richard Xiao?s Top 50,000 Chinese Word Fre- quency List, which collects the frequency of the  top 50000 Chinese words covered in the just  published FD of Mandarin  Chinese based on a balanced corpus of ca.	FD	flat discriminative$forced decoding$F-measure$frequency dictionary$	3
If it is recalled, we weight it with its  frequency from the FD.	FD	flat discriminative$forced decoding$F-measure$frequency dictionary$	3
+ = rel rel rel F tsmfactorcc tsscorec tsscore , ,max , 32 1  (3)  2.4 Evaluation  We generated a Japanese-Hungarian dictionary  using selection methods A, B and F; with C, D  and E contributing indirectly through F.  (a) Recall evaluation  We used a Japanese FD that we  generated from the Japanese EDR corpus (Isa- hara, 2007) to weight each Japanese entry.	FD	flat discriminative$forced decoding$F-measure$frequency dictionary$	3
The Icelandic FD.	FD	flat discriminative$forced decoding$F-measure$frequency dictionary$	3
rel rel F tsmfactorcc tsscorec tsscore , ,max , 32 1  (3)  2.4 Evaluation  We generated a Japanese-Hungarian dictionary  using selection methods A, B and F; with C, D  and E contributing indirectly through F.  (a) Recall evaluation  We used a Japanese FD that we  generated from the Japanese EDR corpus (Isa- hara, 2007) to weight each Japanese entry.	FD	flat discriminative$forced decoding$F-measure$frequency dictionary$	3
Set- ting the standard to the FD (its  recall value being 100), we automatically search  each entry from the FD, verify- ing whether or not it is included in the bilingual  dictionary.	FD	flat discriminative$forced decoding$F-measure$frequency dictionary$	3
3.2 GS Recall that we could not use any previously well- established classification.	GS	Gold Standard$Group significant$Gibbs sampling$gold standard$generative system$	0
In the testing phase, we used a different subset with 80 adjectives as GS against which we could compare the clustering results (see Section 3.2 for details on the manual annotation process).	GS	Gold Standard$Group significant$Gibbs sampling$gold standard$generative system$	0
Also, as the present analysis is based on a small sample of manually annotated adjectives, we intend to obtain a larger GS, in order to establish statistically more reliable results.	GS	Gold Standard$Group significant$Gibbs sampling$gold standard$generative system$	0
4.2 Experiments on Testset 2  GS: Two humans 8  annotated 100  sentences randomly selected from news media  texts.	GS	Gold Standard$Group significant$Gibbs sampling$gold standard$generative system$	0
4.1 Experiments on Testset 1  GS: In total, Testset 1 contains 2028  annotated sentences collected from FrameNet  data set. (	GS	Gold Standard$Group significant$Gibbs sampling$gold standard$generative system$	0
We therefore built our own GS, as has been mentioned at the beginning of this section.	GS	Gold Standard$Group significant$Gibbs sampling$gold standard$generative system$	0
The first one is based on GS.	GS	Gold Standard$Group significant$Gibbs sampling$gold standard$generative system$	2
They apply GS or the expectation max- imization algorithm to discover the word classes that are most probable in the context of surround- ing word classes.	GS	Gold Standard$Group significant$Gibbs sampling$gold standard$generative system$	2
Clearly, for the sentence s there is at most Ns(1+Ns) 2 parent changes1.To estimate the parameters of LTLM we apply GS and gradually sample ?	GS	Gold Standard$Group significant$Gibbs sampling$gold standard$generative system$	2
3.4 Inference For posterior inference of SDTM, we use col- lapsed GS which integrates out la- tent random variables ?,	GS	Gold Standard$Group significant$Gibbs sampling$gold standard$generative system$	2
It is orders of magnitude faster than previous MCMC algorithms like GS, making efficient sampling pos- sible on a scale that was previously out of reach.	GS	Gold Standard$Group significant$Gibbs sampling$gold standard$generative system$	2
Note that in the GS equation, we assume that the Dirichlet parameters ?	GS	Gold Standard$Group significant$Gibbs sampling$gold standard$generative system$	2
We created the GS data, verb classes, using IPAL.	GS	Gold Standard$Group significant$Gibbs sampling$gold standard$generative system$	3
We collected seman- tic classes from IPAL Japanese dictionary (IPAL, 1987), and used them as a GS data.	GS	Gold Standard$Group significant$Gibbs sampling$gold standard$generative system$	3
Sense id Pattern Synonyms 1 kare(he) ga(nominative) soba(noodles) wo(accusative) kuu (eat) 2 kare (he) ga(nominative) fukugyo(a part-time job) de(accusative) kurasu (live) Table 2: Examples of test verbs and their polysemic GS senses Id Sense Verb Classes Id Sense Verb Classes 1 treat {ashirau, atsukau} 11 tell {oshieru, shimesu, shiraseru} 2 prey {negau, inoru} 12 persuade {oshieru, satosu} 3 wish {negau, nozomu} 13 congratulate {iwau, syukufukusuru} 4 ask {negau, tanomu} 14 accept {uketoru, ukeru, morau, osameru} 5 leave {saru, hanareru} 15 take {uketoru, toru, kaisyakusuru, miru} 6 move {saru, utsuru} 1	GS	Gold Standard$Group significant$Gibbs sampling$gold standard$generative system$	3
in the GS, 1285 are attached correctly by the parser, while 607 receive an incorrect regent.	GS	Gold Standard$Group significant$Gibbs sampling$gold standard$generative system$	3
These annotated corpora are  then used as a 'GS' against which  the program's achievements can be compared.	GS	Gold Standard$Group significant$Gibbs sampling$gold standard$generative system$	3
Toward a task-based GS for evaluation of NP chunks and tech- nical terms.	GS	Gold Standard$Group significant$Gibbs sampling$gold standard$generative system$	3
Many of the so- lutions they seem to apply (such as providing con- straints over a GS so that it generates only ?	GS	Gold Standard$Group significant$Gibbs sampling$gold standard$generative system$	4
solutions) are intended to avoid problems arising from the large amount of memory that would be required to consider all possible solu- tions provided by the GS.	GS	Gold Standard$Group significant$Gibbs sampling$gold standard$generative system$	4
SEQUITUR (SEQ), a GS based on the joint n-gram approach (Bisani and Ney, 2008).	GS	Gold Standard$Group significant$Gibbs sampling$gold standard$generative system$	4
Most similar to our entity-level approach is the system of Haghighi and Klein (2010), which also uses approximate global inference; however, theirs is an unsupervised, GS and they attempt to directly model multinomials over words in each mention.	GS	Gold Standard$Group significant$Gibbs sampling$gold standard$generative system$	4
Rather, the lexicon can  be seen as a GS, where word senses are related by logical operations  defined by the well-formedness rules of the semantics.	GS	Gold Standard$Group significant$Gibbs sampling$gold standard$generative system$	4
This is simi- lar to asking what one gains from a formal descrip- tion of language as a GS.	GS	Gold Standard$Group significant$Gibbs sampling$gold standard$generative system$	4
Tagger Forward FEEmbedding Layer Hidden Layer w0=fox, w-1=the, ?	FE	Feature Extractor$Frame element$feeling$frame elements$frame element$French - Englis$	0
FE: For each entity page candidate, we extract a set of context and URL features.	FE	Feature Extractor$Frame element$feeling$frame elements$frame element$French - Englis$	0
CDM FE: For those  untagged CDMs, the classification is  carried out through C4.5.	FE	Feature Extractor$Frame element$feeling$frame elements$frame element$French - Englis$	0
Opensmile: The Munich Versatile and Fast Open-Source Audio FE.	FE	Feature Extractor$Frame element$feeling$frame elements$frame element$French - Englis$	0
.5 75.4 438.7 53.8 434.8 57.7 t(ReLIE) t(CRF) 0.067 0.009 0.144 0.244 0.168 0.143 0.091 0.019 Table 2: Average Training/Testing Time (sec)(with 40% data for training) Task(Extra Feature) Data for Training 10% 40% 80% CRF C+RL CRF C+RL CRF C+RL CourseNumberTask(Negative Dictionary) 0.553 0.624 0.644 0.764 0.854 0.845 PhoneNumberTask(Quantifier) 0.695 0.893 0.820 0.937 0.821 0.964 Table 3: ReLIE as FE (C+RL is CRF enhanced with features learned by ReLIE).	FE	Feature Extractor$Frame element$feeling$frame elements$frame element$French - Englis$	0
5.4 ReLIE as FE for CRF To understand the effect of incorporating ReLIE- identified features into CRF, we chose the two tasks (CourseNumberTask and PhoneNumberTask) with the least F-measure in our experiments to determine raw extraction quality.	FE	Feature Extractor$Frame element$feeling$frame elements$frame element$French - Englis$	0
Features  FE identification is a binary classifica- tion problem in which each constituent in a parse  tree is described by a feature vector and, based on  that vector, tagged as either a frame element or not.	FE	Feature Extractor$Frame element$feeling$frame elements$frame element$French - Englis$	1
FrameNet II contains                                                    5 http://www.wjh.harvard.edu/~inquirer/homecat.htm  Table 1: Example of opinion related frames  and lexical units  Frame  name Lexical units FEs  Desiring want, wish, hope,  eager, desire,  interested,  Event,  Experiencer,  Location_of_event Emotion _directed agitated, amused,  anguish, ashamed,  angry, annoyed,  Event, Topic  Experiencer,  Expressor,  Mental  _property absurd, brilliant,  careless, crazy,  cunning, foolish  Behavior,  Protagonist,  Domain, Degree  Subject  _stimulus delightful, amazing,  annoying, amusing,	FE	Feature Extractor$Frame element$feeling$frame elements$frame element$French - Englis$	1
Previous role (r_n): FEs do not  occur in isolation, but rather, depend very  much on the other elements in a sentence.	FE	Feature Extractor$Frame element$feeling$frame elements$frame element$French - Englis$	1
A Subcategoriza-tionFE represents the syntactic valence of a word sense.	FE	Feature Extractor$Frame element$feeling$frame elements$frame element$French - Englis$	1
FEs that have been so designated for a particular sentence appear to be Core frame elements, but not all core frame elements missing from a sentence have designated as null instantiations.	FE	Feature Extractor$Frame element$feeling$frame elements$frame element$French - Englis$	1
3.3 Frame Elements  FE entries are preceded with a ?%?,	FE	Feature Extractor$Frame element$feeling$frame elements$frame element$French - Englis$	1
In this paper, we consider only predicting polarity (positive and neg- ative FE).	FE	Feature Extractor$Frame element$feeling$frame elements$frame element$French - Englis$	2
2008 Association for Computational Linguistics Computational Linguistics Volume 34, Number 4 Early Academic Life My overwhelming emotion on getting this honor was, after surprise, a FE of in- adequacy in measuring up to previous honorees, but nonetheless, I want to grasp at this moment of autobiography, or at what in his own acceptance paper Martin Kay called: ?	FE	Feature Extractor$Frame element$feeling$frame elements$frame element$French - Englis$	2
In the free-text parts of the reviews, the author describes his subjective FEs and views towards the product, and in the sections Pros and cons and Bottomline he summa- rizes the advantages and disadvantages of the prod- uct, usually by providing some keyphrases or short sentences.	FE	Feature Extractor$Frame element$feeling$frame elements$frame element$French - Englis$	2
This is a FE peculiar to mathematics, I think, because the talent range is so much wider than in most subjects, even at the top level.	FE	Feature Extractor$Frame element$feeling$frame elements$frame element$French - Englis$	2
The resolution of this prob- lem can lead to a complete, realistic and coher- ent analysis of the natural language, therefore ma- jor attention is drawn to the opinion, sentiment and emotion analysis, and to the identification of be- liefs, thoughts, FEs and judgments (Quirk et al, 1985), (Wilson and Wiebe, 2005).	FE	Feature Extractor$Frame element$feeling$frame elements$frame element$French - Englis$	2
As the sentence proceeds  after the object Roma, the new word a causes things to  change again and we go back with a FE of surprise  to the first hypothesis.	FE	Feature Extractor$Frame element$feeling$frame elements$frame element$French - Englis$	2
In our study, we build a semantic role  labeling system as an intermediate step to label  opinion holders and topics by training it on opin- ion-bearing frames and their FE in  FrameNet.	FE	Feature Extractor$Frame element$feeling$frame elements$frame element$French - Englis$	3
Expanding the Lexicon  In order to expand our lexicon for each of the  FE, we used the human-built knowledge  base (WordNet (Fellbaum 1998)) and its rich  hierarchical structure.	FE	Feature Extractor$Frame element$feeling$frame elements$frame element$French - Englis$	3
We assume that the FE are  conditionally independent from each other, given the  frame.	FE	Feature Extractor$Frame element$feeling$frame elements$frame element$French - Englis$	3
For  this task, we assume that that the frame is a latent  class variable (whose domain is the set of lexical  units) and the FE are variables whose  domain is the expanded lexicon (FrameNet +  WordNet).	FE	Feature Extractor$Frame element$feeling$frame elements$frame element$French - Englis$	3
schematic  representations of situations involving various  FE such as participants, props, and  other conceptual roles.?	FE	Feature Extractor$Frame element$feeling$frame elements$frame element$French - Englis$	3
The result is a database that contains a set  of frames (related through hierarchy and  composition), a set of FE for each frame,  and a set of frame annotated sentences that covers the  different patterns of usage for lexical units in the  frame.	FE	Feature Extractor$Frame element$feeling$frame elements$frame element$French - Englis$	3
We filtered out all of the non-relevant terms in  all FE lexicons.	FE	Feature Extractor$Frame element$feeling$frame elements$frame element$French - Englis$	4
Figure 1  We then used the sum-product algorithm (Frey  1998) for statistical inference on the FE  induced graph (such as in Figure 1).	FE	Feature Extractor$Frame element$feeling$frame elements$frame element$French - Englis$	4
Expanding the Lexicon  In order to expand our lexicon for each of the  FEs, we used the human-built knowledge  base (WordNet (Fellbaum 1998)) and its rich  hierarchical structure.	FE	Feature Extractor$Frame element$feeling$frame elements$frame element$French - Englis$	4
This gave us a graphical model of the form  shown in Figure 1 for the FrameNet FE  Suspect and WordNet category Thief.	FE	Feature Extractor$Frame element$feeling$frame elements$frame element$French - Englis$	4
For our initial experiments, we built  a graph whose leaf was the enclosing category of the  FrameNet annotated FE.	FE	Feature Extractor$Frame element$feeling$frame elements$frame element$French - Englis$	4
=   4.2 Relevance of a WordNet Nodes  Throughout our experiments with the training  data, we discovered that some infrequent tail terms in  the FE lexicon that might not be filtered  out by the statistical inference algorithm but still are  frequently used in relevant text.	FE	Feature Extractor$Frame element$feeling$frame elements$frame element$French - Englis$	4
n), a set of FEs for each frame,  and a set of frame annotated sentences that covers the  different patterns of usage for lexical units in the  frame.	FE	Feature Extractor$Frame element$feeling$frame elements$frame element$French - Englis$	4
We filtered out all of the non-relevant terms in  all FE	FE	Feature Extractor$Frame element$feeling$frame elements$frame element$French - Englis$	4
on  In order to expand our lexicon for each of the  FEs, we used the human-built knowledge  base (WordNet (Fellbaum 1998)) and its rich  hierarchical structure.	FE	Feature Extractor$Frame element$feeling$frame elements$frame element$French - Englis$	4
The result is a database that contains a set  of frames (related through hierarchy and  composition), a set of FEs for each frame,  and a set of frame annotated sentences that covers the  different patterns of usage for lexical units in the  frame.	FE	Feature Extractor$Frame element$feeling$frame elements$frame element$French - Englis$	4
We com- bined MT systems for FEh and German - English using provided data only.	FE	Feature Extractor$Frame element$feeling$frame elements$frame element$French - Englis$	5
Our experience on choosing pivot languages is that: (1) a pivot language should be a language whose translation quality can be well guaranteed by the MT engines; (2) it is better to choose a pivot language similar to the source lan- guage (e.g., FEh), which is easier to translate; (3) the translation quality of a pivot lan- guage should not vary a lot among the MT en- gines.	FE	Feature Extractor$Frame element$feeling$frame elements$frame element$French - Englis$	5
The 1992 Evaluation tested three research MT systems:  CANDIDE (IBM, FEh) uses a statistical  language modeling technique based on speech recognition  algorithms (see Brown et al, 1990).	FE	Feature Extractor$Frame element$feeling$frame elements$frame element$French - Englis$	5
Spanish-English 976 Number of Words Language Pair Foreign English German - English 42 M 45 M Czech - English 56 M 65 M Spanish - English 232 M 210 M FEh 962 M 827 M Table 3: Training data statistics.	FE	Feature Extractor$Frame element$feeling$frame elements$frame element$French - Englis$	5
Aligning Sentences in Bilingual  Texts FEh and French - Arabic.	FE	Feature Extractor$Frame element$feeling$frame elements$frame element$French - Englis$	5
SYSTRAN Translation Systems Inc. provided output from  a FEh production system and a Spanish -  English pilot prototype.	FE	Feature Extractor$Frame element$feeling$frame elements$frame element$French - Englis$	5
Of the ten prolific work- ers, one was located in the United States, one in the GB, and eight in India.	GB	United Kingdom$Great Britain$Gambling$	0
In 22nd COLING, pages 51?54, Manchester, GB.	GB	United Kingdom$Great Britain$Gambling$	0
c?2010 Association for Computational Linguistics Combining Manual Rules and Supervised Learning for Hedge Cue and Scope Detection Marek Rei Computer Laboratory University of Cambridge GB Marek.Rei@cl.cam.ac.uk Ted Briscoe Computer Laboratory University of Cambridge GB Ted.Briscoe@cl.cam.ac.uk Abstract Hedge cues were detected using a super- vised Conditional Random Field (CRF) classifier exploiting features from the RASP parser.	GB	United Kingdom$Great Britain$Gambling$	0
c?2013 Association for Computational Linguistics DALE: A Word Sense Disambiguation System for Biomedical Documents Trained using Automatically Labeled Examples Judita Preiss and Mark Stevenson Department of Computer Science, University of Sheffield Regent Court, 211 Portobello Sheffield S1 4DP, GB j.preiss,m.stevenson@dcs.shef.ac.uk Abstract Automatic interpretation of documents is hampered by the fact that language contains terms which have multiple meanings.	GB	United Kingdom$Great Britain$Gambling$	0
Automatic Feature Selection for Agenda-Based Dependency Parsing Miguel Ballesteros Natural Language Processing Group Universitat Pompeu Fabra Barcelona, Spain miguel.ballesteros@upf.edu Bernd Bohnet School of Computer Science University of Birmingham Birmingham, GB bohnetb@cs.bham.ac.uk Abstract In this paper we present an in-depth study on automatic feature selection for beam-search depen- dency parsers.	GB	United Kingdom$Great Britain$Gambling$	0
GB?)	GB	United Kingdom$Great Britain$Gambling$	1
Phonetics, Pelican Books, GB.	GB	United Kingdom$Great Britain$Gambling$	1
In: L. J. Cahill & ll,iehard  Coates \[eds.\]: Sussex Papers in General and Com-  putational Linguistics: Presented to the Linguis-  tic Association of GB Conference at  Brighton Polytechnic, 6th-8th April 1992.	GB	United Kingdom$Great Britain$Gambling$	1
There is, in fact, also an informal  personalized style which may be  illustrated by the following quotes  from a British newspaper (the  European):  Players in the Rugby League test  match between GB and  Australia on Saturday may need  longer studs and safe hands to  tackle the tricky conditions at  London 's  Wembley  stadium.	GB	United Kingdom$Great Britain$Gambling$	1
Estimation of  conditional Probabilities with Decision Trees and  an Application to Fine-Grained POS tagging,  COLING 2008, Manchester, GB.	GB	United Kingdom$Great Britain$Gambling$	1
In Proceedings of the 12th Con- ference on Computational Natural Language Learning (CoNLL-2008), Manchester, GB.	GB	United Kingdom$Great Britain$Gambling$	1
9 : Horoscope GB 9.11?	GB	United Kingdom$Great Britain$Gambling$	2
Animals Fishing Plants  Baseball Flight Race  Body Football Religion  Botany GB Sick  Boundary Grasp Size  Chess Health Sound  Color Height Sports  Combustion Light Taste  Cooking Liquid Temperature  Courtship Machine Texture  Cut Maritime Theater  Directional force Money Time of day  Dogs Motion Toxicity  Drug use Mythology Vehicle  Electricity Natural disasters War  Energy source Nuclear Weaponry  Entry Odor Weather	GB	United Kingdom$Great Britain$Gambling$	2
For instance, the polarity for GB increases the number of crimes is ?	GB	United Kingdom$Great Britain$Gambling$	2
In this paper, to reduce computational cost, we restrict that adjacent SIBs in the subtrees must be adjacent in the original tree.	SIB	sibling$Swiss Institute of Bioinformatic$	0
Starting with all the synsets containing a term in 11 a triple, the term is assigned to the synset with higher similarity to the contexts from where the triple was extracted, computed based on the terms in the synset, SIB synsets and direct hyponym synsets.	SIB	sibling$Swiss Institute of Bioinformatic$	0
Thus, we only need to consider two types of words: (i) the children of the rightmost leaf of S r , (ii) the adjacent right SIB of the any node in S r , as shown in Figure 2.	SIB	sibling$Swiss Institute of Bioinformatic$	0
Its possible values include parent, SIB, child, uncle, grand parent etc.	SIB	sibling$Swiss Institute of Bioinformatic$	0
today is not a subtree because He and today are not adjacent SIBs.	SIB	sibling$Swiss Institute of Bioinformatic$	0
For argument, a dependency version of the prun- ing algorithm in (Xue and Palmer, 2004) is used to find, in an iterative way, the current syntactic head and its SIBs in a parse tree in a constituent- based representation.	SIB	sibling$Swiss Institute of Bioinformatic$	0
The total number of subtrees is no greater than L 3 , because the level of a subtree is less than L, and for the children of each node, there are at most L 2 subsequences of SIBs.	SIB	sibling$Swiss Institute of Bioinformatic$	0
University of Geneva  3SIBs, Geneva  4SIM, University Hospital of Geneva  {imad.tbahriti;christine.chichester;frederique.lisacek}@genebio.com - patrick.ruch@epfl.ch   Abstract  The aim of this study is to investigate the  relationships between citations and the  scientific argumentation found in the abstract.	SIB	sibling$Swiss Institute of Bioinformatic$	1
The MRR An- swer Rank (MRAR) is used to compute the over- all performance of the systems participating in the TREC evaluation efZ*[gZ^] _ bihkj b ) _ `La;bdc Qml In ad- dition, TREC-9 imposed the constraint that an an- swer is considered correct only when the textual context from the document that contains it can account for it.	MRR	Mean Reciprocal$Mean Reciprocal Rank$mean reciprocal rank$	0
For eval- uation we employed the averaged top-1 accuracy and MRR Rank (MMR) at finding the real comparable document in the other language.	MRR	Mean Reciprocal$Mean Reciprocal Rank$mean reciprocal rank$	0
As one measure of success, we show that a MRR Rank near 0.5 can be achieved when more than one relevant response exists; this corresponds to a relevant response appearing in the second position of a ranked list, on average (by the harmonic mean).	MRR	Mean Reciprocal$Mean Reciprocal Rank$mean reciprocal rank$	0
5 Evaluation and Experimental Results 5.1 Evaluation Measures We use the following evaluation measures for evalu- ating the results: MRR Rank For a question, the recip- rocal rank RR is	MRR	Mean Reciprocal$Mean Reciprocal Rank$mean reciprocal rank$	0
6http://lucene.apache.org/java/docs/ 48 T -SW T -SW +SC S -SW S -SW +SC L -SW L -SW Preprocessing 50 60 70 80 90 100 Ac cu ra cy T -SW T -SW +SC S -SW S -SW +SC L -SW L -SW Preprocessing 0.5 0.6 0.7 0.8 0.9 1.0 M RR Matching coefficient Overlap coefficient Normalised edit distance Ngram Overlap Term vector similarity Lucene Figure 1: Accuracy (%) and MRR Rank obtained for different question similarity measures and pre- processing strategies: tokens (T), stemming (S), lemmatisation (L), stop words removal (-SW), spelling correction (+SC).	MRR	Mean Reciprocal$Mean Reciprocal Rank$mean reciprocal rank$	0
In order to evaluate Qanda's performance on the  five tests we decided to use the MRR  Answer Rank (MRAR) technique which was used  for evaluating question-answering systems at TREC-  8 (Singhal, 1999).	MRR	Mean Reciprocal$Mean Reciprocal Rank$mean reciprocal rank$	0
We measure the Top-1 accu- racy (i.e., whether the true comparable is the clos- est in the test set), and the MRR Rank of the true comparable, and report the average per- formance over the two retrieval directions.	MRR	Mean Reciprocal$Mean Reciprocal Rank$mean reciprocal rank$	0
RESULTS We entered the TREC-9 short form QA track, and received an overall MRR score of 0.318, which put Webclopedia in essentially tied second place with two others.	MRR	Mean Reciprocal$Mean Reciprocal Rank$mean reciprocal rank$	1
For eval- uation we employed the averaged top-1 accuracy and MRR (MMR) at finding the real comparable document in the other language.	MRR	Mean Reciprocal$Mean Reciprocal Rank$mean reciprocal rank$	1
The MRR score for TFIDF and LDA1 are more than 80%.	MRR	Mean Reciprocal$Mean Reciprocal Rank$mean reciprocal rank$	1
As one measure of success, we show that a MRR near 0.5 can be achieved when more than one relevant response exists; this corresponds to a relevant response appearing in the second position of a ranked list, on average (by the harmonic mean).	MRR	Mean Reciprocal$Mean Reciprocal Rank$mean reciprocal rank$	1
5 Evaluation and Experimental Results 5.1 Evaluation Measures We use the following evaluation measures for evalu- ating the results: MRR For a question, the recip- rocal rank RR is	MRR	Mean Reciprocal$Mean Reciprocal Rank$mean reciprocal rank$	1
6http://lucene.apache.org/java/docs/ 48 T -SW T -SW +SC S -SW S -SW +SC L -SW L -SW Preprocessing 50 60 70 80 90 100 Ac cu ra cy T -SW T -SW +SC S -SW S -SW +SC L -SW L -SW Preprocessing 0.5 0.6 0.7 0.8 0.9 1.0 M RR Matching coefficient Overlap coefficient Normalised edit distance Ngram Overlap Term vector similarity Lucene Figure 1: Accuracy (%) and MRR obtained for different question similarity measures and pre- processing strategies: tokens (T), stemming (S), lemmatisation (L), stop words removal (-SW), spelling correction (+SC).	MRR	Mean Reciprocal$Mean Reciprocal Rank$mean reciprocal rank$	1
We measure the Top-1 accu- racy (i.e., whether the true comparable is the clos- est in the test set), and the MRR of the true comparable, and report the average per- formance over the two retrieval directions.	MRR	Mean Reciprocal$Mean Reciprocal Rank$mean reciprocal rank$	1
We took the MRR (the inverse of the harmonic mean) as a figure of merit for each configuration, and used a paired two-tailed   - test (with p  0.05) to assess the statistical significance of observed differences.	MRR	Mean Reciprocal$Mean Reciprocal Rank$mean reciprocal rank$	2
The accuracy of a list C(w) for an attribute w is measured by a scoring metric that corresponds to a modification (Pas?ca and Alfonseca, 2009) of the MRR score (Voorhees and Tice, 2000): DRR = max 1 rank(c)? (	MRR	Mean Reciprocal$Mean Reciprocal Rank$mean reciprocal rank$	2
Experimental results show that EMR can select seed sets that pro- vide significantly higher MRR on realistic data than existing naive selection methods or random seed sets.	MRR	Mean Reciprocal$Mean Reciprocal Rank$mean reciprocal rank$	2
Consid- ering L2, accuracy, average probability, equal er- ror rate, log probability and MRR across all components of the the dialog state, these give a total of 318 metrics.	MRR	Mean Reciprocal$Mean Reciprocal Rank$mean reciprocal rank$	2
These accuracies correspond to MRRs of 0.47?0.51, and the random baseline on WORDSPLEND and MAC-CONF in terms of this mea- sure is 0.03?0.07.	MRR	Mean Reciprocal$Mean Reciprocal Rank$mean reciprocal rank$	2
5 Similar results are obtained with MRR.	MRR	Mean Reciprocal$Mean Reciprocal Rank$mean reciprocal rank$	2
One of them consists in using pre-processing components that can detect the LV at stake.	LV	language variety$light verb$	0
These genres vary considerably in terms of their functions and the LV employed.	LV	language variety$light verb$	0
Starting with the vocabulary extracted from a corpus of European Spanish newspaper texts, we expect our bilingual dictionary to be biased with respect to the LV, register and genre.	LV	language variety$light verb$	0
By a vocabulary profile, it is meant that the fre- quency of each vocabulary item is calculated from a reference corpus covering the LV of the target situation.	LV	language variety$light verb$	0
Such lexicons may not be available for a given LV.	LV	language variety$light verb$	0
4.4 Dialectal Arabic Features Dialect: We apply the two gold LV features, {MSA, DA}, on the Twitter data set to rep- resent whether the tweet is in MSA or in a dialect.	LV	language variety$light verb$	0
These include verbs of becoming or seeming (e.g., trans- form, appear), LVs, auxiliaries, and aspec- tual verbs.	LV	language variety$light verb$	1
at Boulder, Boulder CO 80309  2Department of Computer Science, Brandeis University, Waltham MA 02453  3Department of Linguistics, University of Illinois at Urbana-Champaign, Urbana IL 61801  {hwangd,claire.bonial,aous.mansouri,ashwini.vaidya,martha.palmer}  @colorado.edu, bhatia@illinois.edu, xuen@brandeis.edu         Abstract  In this paper, we have addressed the task  of PropBank annotation of LV  constructions, which like multi-word  expressions pose special problems.	LV	language variety$light verb$	1
ha.palmer}  @colorado.edu, bhatia@illinois.edu, xuen@brandeis.edu         Abstract  In this paper, we have addressed the task  of PropBank annotation of LV  constructions, which like multi-word  expressions pose special problems.	LV	language variety$light verb$	1
andeis.edu         Abstract  In this paper, we have addressed the task  of PropBank annotation of LV  constructions, which like multi-word  expressions pose special problems.	LV	language variety$light verb$	1
We also discuss how in  various languages the LV  constructions are identified and can be  distinguished from the non-LV  word groupings.	LV	language variety$light verb$	1
The final method involves three passes:  (1) manual identification of a LV  construction, (2) annotation based on the  LV construction?s Frame File, and  (3) a deterministic merging of the first  two passes.	LV	language variety$light verb$	1
Figure 2: The LDA Model (left) and our SBT Document Model (right).	LDA	latent Dirichlet al location$Latent Dirichlet Allocation$latent Dirichlet alocatio$let alocation$latent Dirichlet alocation$let Allocation$	1
LDA.	LDA	latent Dirichlet al location$Latent Dirichlet Allocation$latent Dirichlet alocatio$let alocation$latent Dirichlet alocation$let Allocation$	1
night day cold long sleep dream days ... 9 mind world lose nothing left gonna shake ... 10 time life long live day end die ... 11 god lord man hell king heaven jesus ... 12 hear play call people good talk heard ... 13 sky eyes fire fly blue high sea ... 14 heart inside pain soul break broken deep ... 15 go back home again time gonna coming ... Table 2: Example words from a topic model induced by LDA on the lyrics corpus.	LDA	latent Dirichlet al location$Latent Dirichlet Allocation$latent Dirichlet alocatio$let alocation$latent Dirichlet alocation$let Allocation$	1
Our expe- riments with LDA corrobo- rate a recent proposal that appending training  data with perceptible feature or property infor- mation for a subset of concrete nouns can signif- icantly improve the quality of the model?s lexical  representations.	LDA	latent Dirichlet al location$Latent Dirichlet Allocation$latent Dirichlet alocatio$let alocation$latent Dirichlet alocation$let Allocation$	1
Gibbs sampling in the generative  model of LDA.	LDA	latent Dirichlet al location$Latent Dirichlet Allocation$latent Dirichlet alocatio$let alocation$latent Dirichlet alocation$let Allocation$	1
As a point of comparison, the original approach for LyriCloud was to employ a topic model such as LDA (Blei et al, 2003).	LDA	latent Dirichlet al location$Latent Dirichlet Allocation$latent Dirichlet alocatio$let alocation$latent Dirichlet alocation$let Allocation$	1
Word features for LDAn.	LDA	latent Dirichlet al location$Latent Dirichlet Allocation$latent Dirichlet alocatio$let alocation$latent Dirichlet alocation$let Allocation$	2
Online inference of topics with LDAn.	LDA	latent Dirichlet al location$Latent Dirichlet Allocation$latent Dirichlet alocatio$let alocation$latent Dirichlet alocation$let Allocation$	2
Holistic sentiment analysis across languages: Multilingual supervised LDAn.	LDA	latent Dirichlet al location$Latent Dirichlet Allocation$latent Dirichlet alocatio$let alocation$latent Dirichlet alocation$let Allocation$	2
A spectral algorithm for LDAn.	LDA	latent Dirichlet al location$Latent Dirichlet Allocation$latent Dirichlet alocatio$let alocation$latent Dirichlet alocation$let Allocation$	2
Online learning for LDAn.	LDA	latent Dirichlet al location$Latent Dirichlet Allocation$latent Dirichlet alocatio$let alocation$latent Dirichlet alocation$let Allocation$	2
Holistic sentiment analysis across languages: Multilingual su- pervised LDAn.	LDA	latent Dirichlet al location$Latent Dirichlet Allocation$latent Dirichlet alocatio$let alocation$latent Dirichlet alocation$let Allocation$	2
Word features for latent DirichLDA.	LDA	latent Dirichlet al location$Latent Dirichlet Allocation$latent Dirichlet alocatio$let alocation$latent Dirichlet alocation$let Allocation$	3
Latent dirichLDA.	LDA	latent Dirichlet al location$Latent Dirichlet Allocation$latent Dirichlet alocatio$let alocation$latent Dirichlet alocation$let Allocation$	3
Latent Dirich- LDA.	LDA	latent Dirichlet al location$Latent Dirichlet Allocation$latent Dirichlet alocatio$let alocation$latent Dirichlet alocation$let Allocation$	3
Latent DirichLDA.	LDA	latent Dirichlet al location$Latent Dirichlet Allocation$latent Dirichlet alocatio$let alocation$latent Dirichlet alocation$let Allocation$	3
Word features for LDA.	LDA	latent Dirichlet al location$Latent Dirichlet Allocation$latent Dirichlet alocatio$let alocation$latent Dirichlet alocation$let Allocation$	4
Online inference of topics with LDA.	LDA	latent Dirichlet al location$Latent Dirichlet Allocation$latent Dirichlet alocatio$let alocation$latent Dirichlet alocation$let Allocation$	4
Holistic sentiment analysis across languages: Multilingual supervised LDA.	LDA	latent Dirichlet al location$Latent Dirichlet Allocation$latent Dirichlet alocatio$let alocation$latent Dirichlet alocation$let Allocation$	4
A spectral algorithm for LDA.	LDA	latent Dirichlet al location$Latent Dirichlet Allocation$latent Dirichlet alocatio$let alocation$latent Dirichlet alocation$let Allocation$	4
Online learning for LDA.	LDA	latent Dirichlet al location$Latent Dirichlet Allocation$latent Dirichlet alocatio$let alocation$latent Dirichlet alocation$let Allocation$	4
Holistic sentiment analysis across languages: Multilingual su- pervised LDA.	LDA	latent Dirichlet al location$Latent Dirichlet Allocation$latent Dirichlet alocatio$let alocation$latent Dirichlet alocation$let Allocation$	4
Our expe- riments with Latent DirichLDA corrobo- rate a recent proposal that appending training  data with perceptible feature or property infor- mation for a subset of concrete nouns can signif- icantly improve the quality of the model?s lexical  representations.	LDA	latent Dirichlet al location$Latent Dirichlet Allocation$latent Dirichlet alocatio$let alocation$latent Dirichlet alocation$let Allocation$	5
Gibbs sampling in the generative  model of Latent DirichLDA.	LDA	latent Dirichlet al location$Latent Dirichlet Allocation$latent Dirichlet alocatio$let alocation$latent Dirichlet alocation$let Allocation$	5
As a point of comparison, the original approach for LyriCloud was to employ a topic model such as Latent DirichLDA (Blei et al, 2003).	LDA	latent Dirichlet al location$Latent Dirichlet Allocation$latent Dirichlet alocatio$let alocation$latent Dirichlet alocation$let Allocation$	5
ld long sleep dream days ... 9 mind world lose nothing left gonna shake ... 10 time life long live day end die ... 11 god lord man hell king heaven jesus ... 12 hear play call people good talk heard ... 13 sky eyes fire fly blue high sea ... 14 heart inside pain soul break broken deep ... 15 go back home again time gonna coming ... Table 2: Example words from a topic model induced by Latent DirichLDA on the lyrics corpus.	LDA	latent Dirichlet al location$Latent Dirichlet Allocation$latent Dirichlet alocatio$let alocation$latent Dirichlet alocation$let Allocation$	5
Latent Di- richLDA.	LDA	latent Dirichlet al location$Latent Dirichlet Allocation$latent Dirichlet alocatio$let alocation$latent Dirichlet alocation$let Allocation$	5
Dur- ing learning, TC is calculated online according to the noise model, penalising all slots which are filled but not confirmed.	TC	Task Completion$Text Categorization$transitive closure$TurnCos$token classification approach$	0
31  TC The first column of Table 4 shows  that the subjects were able to provide an answer in allthe  scenarios when the system was in robust mode, whereas  only 83% of the scenarios were completed in non-robust  mode.	TC	Task Completion$Text Categorization$transitive closure$TurnCos$token classification approach$	0
TC Time The task completion time is  summarized in the third column of Table 4.	TC	Task Completion$Text Categorization$transitive closure$TurnCos$token classification approach$	0
Actual TC?	TC	Task Completion$Text Categorization$transitive closure$TurnCos$token classification approach$	0
Partial TC statistics System Succ. (	TC	Task Completion$Text Categorization$transitive closure$TurnCos$token classification approach$	0
Full TC statistics System Succ. (	TC	Task Completion$Text Categorization$transitive closure$TurnCos$token classification approach$	0
Perceived TC?	TC	Task Completion$Text Categorization$transitive closure$TurnCos$token classification approach$	0
Fea- ture Generation for TC Using World Knowledge.	TC	Task Completion$Text Categorization$transitive closure$TurnCos$token classification approach$	1
TC with  Support Vector Machines: Learning with Many  Relevant Features.	TC	Task Completion$Text Categorization$transitive closure$TurnCos$token classification approach$	1
In Proceedings of AAAI-98 Workshop on Learning for TC.	TC	Task Completion$Text Categorization$transitive closure$TurnCos$token classification approach$	1
Overcoming the Brittleness Bottleneck using Wikipedia: Enhancing TC with Encyclopedic Knowledge.	TC	Task Completion$Text Categorization$transitive closure$TurnCos$token classification approach$	1
1 Introduction The subjective analysis of a text is becoming impor- tant for many Natural Language Processing (NLP) applications such as Question Answering, Informa- tion Extraction, TC among others (Shanahan et al, 2006).	TC	Task Completion$Text Categorization$transitive closure$TurnCos$token classification approach$	1
RCV1: A New Benchmark Col- lection for TC Research.	TC	Task Completion$Text Categorization$transitive closure$TurnCos$token classification approach$	1
Some properties of surface syntactic structure can be ex- pressed only in terms of both dependency (or its TC called dominance) and prece- dence.	TC	Task Completion$Text Categorization$transitive closure$TurnCos$token classification approach$	2
To compensate for this, Shaw and Hatzivassiloglou (1999) propose to compute the TC of the ordering relation: if a ?	TC	Task Completion$Text Categorization$transitive closure$TurnCos$token classification approach$	2
A single LIG derivation step  will be notated with ~ and its reflexive TC with 3* .	TC	Task Completion$Text Categorization$transitive closure$TurnCos$token classification approach$	2
The reflex-  ive and TC of \[-c, is denoted with  H~. The state projection of Hc is the binary  relation   (Ho) = {(r, r')l  /3x: (p,/3) (p',/3x)}.	TC	Task Completion$Text Categorization$transitive closure$TurnCos$token classification approach$	2
After all the mentions have been processed, the links are used to generate a TC that corresponds to the recognized entities in the document.	TC	Task Completion$Text Categorization$transitive closure$TurnCos$token classification approach$	2
(b) 73 is the reflexive TC of  79, and 73 is antisymmetric.	TC	Task Completion$Text Categorization$transitive closure$TurnCos$token classification approach$	2
The set of rela-  tions used in TGs come from three  sources.	TG	TransGraph$temporary graph$template generation$Template Generation$	1
syntactic pattern  s:np\[A\],vp\[B\]  John eats  vp:vp\[A\],inf_vp\[B\]  eat to grow  TG  \[B\]- >(instrument )- > \[A\]  \[A\]-> (part-of)-> \[B\]  \[Bl->(loc)->\[A\]  TG  \[B\]-> (agent)-> \[h\]  \[eat\]- > (agent)- >\[John\]  \[A\]-> (goal)-> \[B\]  \[e at\]- > ( goal)- > \[grow\]  Table 1: Examples of relations found in sentences  and their corresponding TGs  3 Knowledge integration  This section describes how given a trigger word,  we per	TG	TransGraph$temporary graph$template generation$Template Generation$	1
syntactic pattern  s:np\[A\],vp\[B\]  John eats  vp:vp\[A\],inf_vp\[B\]  eat to grow  TG  \[B\]- >(instrument )- > \[A\]  \[A\]-> (part-of)-> \[B\]  \[Bl->(loc)->\[A\]  TG  \[B\]-> (agent)-> \[h\]  \[eat\]- > (agent)- >\[John\]  \[A\]-> (goal)-> \[B\]  \[e at\]- > ( goal)- > \[grow\]  Table 1: Examples of relations found in sentences  and their corresponding TGs  3 Knowledge integration  This section describes how given a trigger word,  we perform a series of forward and backward  searches in the dictionary to build a CCKG con-  taining	TG	TransGraph$temporary graph$template generation$Template Generation$	1
By keeping  the preposition itself within the TG,  we delay the ambiguity resolution process until  we have gathered more information and we even  hopefully avoid the decision process as the ambi-  guity might later be resolved by the integration  process itself.	TG	TransGraph$temporary graph$template generation$Template Generation$	1
Our processing of the definitions results in the  construction of a special type of conceptual graph  which we call a TG.	TG	TransGraph$temporary graph$template generation$Template Generation$	1
syntactic pattern  s:np\[A\],vp\[B\]  John eats  vp:vp\[A\],inf_vp\[B\]  eat to grow  TG  \[B\]- >(instrument )- > \[A\]  \[A\]-> (part-of)-> \[B\]  \[Bl->(loc)->\[A\]  TG  \[B\]-> (agent)-> \[h\]  \[eat\]- > (agent)- >\[John\]  \[A\]-> (goal)-> \[B\]  \[e at\]- > ( goal)- > \[grow\]  Table 1: Examples of relations found in sentences  and their corresponding TGs  3 Knowledge integration  This section describes how given a trigger word,  we perform a series of forward and backward  searches in the dictionary to build a CCKG con-  taining useful information pertaining to the trig-  ger word and to closely related words.	TG	TransGraph$temporary graph$template generation$Template Generation$	1
Precision Overgeneratio n TST3 8 90 T5T4 10 87 Table 3 : Overall precision scores (all templates row) The effects of this TG strategy on our precision scores were fairly dramatic .	TG	TransGraph$temporary graph$template generation$Template Generation$	2
There are three major steps in LSI TG : 1) the first pass, in which templates for specifie d events and the related entity templates are generated ; 2) the second pass, in which entity templates are generated for MUC-5 relevant words that are not treated in the first pass ; and 3) template linking, in which co-reference relations are determined .	TG	TransGraph$temporary graph$template generation$Template Generation$	2
In the LSI TG for the walkthrough text, during the first pass two critical EME domai n verbs are identified : "use" and "sell", Event templates are generated for them with a set of theta role slot s (agent, patient, etc .)	TG	TransGraph$temporary graph$template generation$Template Generation$	2
AIML is an XML language that provides a pat- tern/TG model mainly used for chatbot systems.	TG	TransGraph$temporary graph$template generation$Template Generation$	2
Less than 10 percent of the current code is dedicated to MUC4-like processing, 90 percent of whic h consists of TG and merging as dictated by MUC4 template guidelines.	TG	TransGraph$temporary graph$template generation$Template Generation$	2
ISSUES WITH PRECISIO N Our precision error rate is largely accounted for by overly eager TG .	TG	TransGraph$temporary graph$template generation$Template Generation$	2
TG As mentioned, we rely on the inference mechanism to perform the bulk of the work of data extraction.	TG	TransGraph$temporary graph$template generation$Template Generation$	3
3.7 TG  The template generator takes the DDOs produced by  discourse processing and fills out the application-specific  templates.	TG	TransGraph$temporary graph$template generation$Template Generation$	3
--------------------------------------------------------------------- - IN-AND-OUT-PERSON-OF: "Dooner" (score= 0 ) "McCann" (score= 4 ) "official" (score= 4 ) "James" (score= 4) <=> 	 "Robert L. James " "One" (score= 6) <=> 	 "Kevin Goldman " IN-AND-OUT-NEW-STATUS-OF: POSITION-STATUS-GEN-IN (score= 0) POSITION-STATUS-GEN-OUT (score= 4) TG The template generator takes the DDOs produced by discourse processing and fills out the application-specifi c templates .	TG	TransGraph$temporary graph$template generation$Template Generation$	3
3.1.4 TG.	TG	TransGraph$temporary graph$template generation$Template Generation$	3
TG The task of the template generation component is to take the results of the abductive proofs in pragmatics , and put them into a template form according to the specifications of the task .	TG	TransGraph$temporary graph$template generation$Template Generation$	3
TG The template generator takes the DDOs produced by discourse processing and fills out the application-specifi c templates .	TG	TransGraph$temporary graph$template generation$Template Generation$	3
46 but the first two WN senses were considered.	WN	WordNet$wN$	0
a subset of the STORIES dataset that included 10 stories, the following schemes were used for filter- ing of candidate speakers: (i) Scheme 1: all speak- ers linked with speech verbs, (ii) Scheme 2: speak- ers, who are persons or animals or spiritual entities according to their first WN sense, linked with speech verbs , and (iii) Scheme 3: as Scheme 2, 5SU+/?	WN	WordNet$wN$	0
A character was retained if any of its hypernyms was found to fall into certain types of WN concepts: person, animal, plant, artifact, spiritual being, physical entity.	WN	WordNet$wN$	0
Our approach demonstrates the  use of two human built knowledge bases  (WN and FrameNet) for the task of  semantic extraction.	WN	WordNet$wN$	0
In this paper we report results obtained from  combining IE and graphical modeling techniques,  with semantic resources (WN and FrameNet)  for automatic Semantic Extraction.	WN	WordNet$wN$	0
ORIES dataset that included 10 stories, the following schemes were used for filter- ing of candidate speakers: (i) Scheme 1: all speak- ers linked with speech verbs, (ii) Scheme 2: speak- ers, who are persons or animals or spiritual entities according to their first WN sense, linked with speech verbs , and (iii) Scheme 3: as Scheme 2, 5SU+/?	WN	WordNet$wN$	0
The ongoing development of public  knowledge bases such as WN, FrameNet, CYC,  etc.	WN	WordNet$wN$	0
l , 951 the following holds: max w1,w2,...,WN ,w0 [ N?	WN	WordNet$wN$	1
WN ) = N?	WN	WordNet$wN$	1
i||wi||2 We first prove the following relation: Lemma 2.1 Assume (w?0, ...,w?N ) = arg maxw1,...,WN ,w0 [ N?	WN	WordNet$wN$	1
HoWNet is an on-line common- sense knowledge base providing a universal lexi- cal concept representation mechanism.	WN	WordNet$wN$	1
i||wi||2 )] = max w1,w2,...,WN [ N?	WN	WordNet$wN$	1
The semantic  information can be extracted from a Chinese- English bilingual semantic resource: HoWNet  (Dong, 2000).	WN	WordNet$wN$	1
Even though Wambaya is in some respects very different from the well-studied lan- guages on which the Matrix is based, the existing machinery otherwise worked quite well, providing a significant JUMP-start to the grammar development process.	JUMP	jump$jumping$	0
The input consists of a rich set of fea- 1559 The        fox JUMPs     .stack buffer det (a) Parser configuration c. Figure 4: A schematic for the PARSER stack-propagation update.	JUMP	jump$jumping$	0
w0=JUMPs, w-1=fox, ?	JUMP	jump$jumping$	0
Despite large typological differ- ences between Wambaya and the languages on which the development of the resource was based, the Grammar Matrix is found to pro- vide a significant JUMP-start in the creation of the grammar for Wambaya: With less than 5.5 person-weeks of development, the Wambaya grammar was able to assign correct seman- tic representations to 76% of the sentences in a naturally occurring text.	JUMP	jump$jumping$	0
This paper aims to evaluate both the utility of the Grammar Matrix in JUMP-starting precision gram- mar development and the current state of its cross- linguistic hypotheses through a case study of a 977 language typologically very different from any of the languages above: the non-Pama-Nyungan Aus- tralian language Wambaya (Nordlinger, 1998).	JUMP	jump$jumping$	0
JUMP?	JUMP	jump$jumping$	0
In or- der to improve the transition model in HMM, we  extend the transition probabilities to be word de- pendent so that the probability of JUMP from  state aj_to aj not only depends on aj_, but also de- pends on the English word at position aj_.	JUMP	jump$jumping$	1
For each source word e, we not only model  its self-transition probability, but also the probabil- ity of JUMP from word e to a different word	JUMP	jump$jumping$	1
Important knowledge of  JUMP from e to another position, e.g., JUMP  80 forward (monotonic alignment) or JUMP back- ward (non-monotonic alignment), is not modeled.	JUMP	jump$jumping$	1
%     state i=0 denotes the state of a null word at the  English side, and p0 is the probability of JUMP  to state 0, which is estimated from hold-out data.	JUMP	jump$jumping$	1
For each source word e, we not only model  its self-transition probability, but also the probabil- ity of JUMP from word e to a different word.	JUMP	jump$jumping$	1
In this paper, we initially present a method for collecting time-series data from students (e.g. marks, LA) and use ex- ample feedback from lecturers in a data- driven approach to content selection.	LA	lectures attended$low attachment$label accuracy$lip aperture$Levantine Arabic$left adjoining$Latin$Lexical Acquisition$	0
For in- stance, a summary that talks about the student?s performance, the number of lectures that he/she attended, potential health problems and revision done can be translated into the following ngram: start, marks, LA, health issues, re- vision, end.	LA	lectures attended$low attachment$label accuracy$lip aperture$Levantine Arabic$left adjoining$Latin$Lexical Acquisition$	0
It was found that the most frequent or- dering is: start, marks, hours studied, understand- ability, difficulty, deadlines, health issues, per- sonal issues, LA, revision, end.	LA	lectures attended$low attachment$label accuracy$lip aperture$Levantine Arabic$left adjoining$Latin$Lexical Acquisition$	0
138 Raw Data factors week 2 week 3 ... week 10 marks 5 4 ... 5 hours studied 1 2 ... 3 ... ... ... ... ... Trends from Data factors trend (1) marks trend other (2) hours studied trend increasing (3) understandability trend decreasing (4) difficulty trend decreasing (5) deadlines trend increasing (6) health issues trend other (7) personal issues trend decreasing (8) LA trend other (9) revision trend decreasing Summary Your overall performance was excellent during the semester.	LA	lectures attended$low attachment$label accuracy$lip aperture$Levantine Arabic$left adjoining$Latin$Lexical Acquisition$	0
factor trend way it is mentioned (1) marks stable average (2) hours studied decreasing trend (3) health issues decreasing weeks (4) LA stable average (5) personal issues increasing trend Table 2: The top 5 features out of the 18 selected through PCR analysis.	LA	lectures attended$low attachment$label accuracy$lip aperture$Levantine Arabic$left adjoining$Latin$Lexical Acquisition$	0
An example of the initial state of a student can be: <marks increased, LA stable, hours studied increased, understandability stable, difficulty increased, health issues stable, per- sonal issues stable, revision increased, 0> The agent explores the state space by selecting a factor and then by deciding whether to talk about it or not.	LA	lectures attended$low attachment$label accuracy$lip aperture$Levantine Arabic$left adjoining$Latin$Lexical Acquisition$	0
This value reflects the bias for LA: the ma- jority of relat	LA	lectures attended$low attachment$label accuracy$lip aperture$Levantine Arabic$left adjoining$Latin$Lexical Acquisition$	1
subj?link 0 System?subj?link 0 +system?subj?link +currency?obj?link 0 system?subj?link 1.2 +company?subj?link +currency?obj?link 0 company?subj?link -1.1 empty 3 Table 3: Queries for computing high attach- ment (above) and LA (below) for Example 1.	LA	lectures attended$low attachment$label accuracy$lip aperture$Levantine Arabic$left adjoining$Latin$Lexical Acquisition$	1
Parser X tends to produce LA for PPs while the language tends to have high attachment is a claim about structural bias, which is related to parser errors.	LA	lectures attended$low attachment$label accuracy$lip aperture$Levantine Arabic$left adjoining$Latin$Lexical Acquisition$	1
The highest values are 12.2 for high attachment (mechanism) and 3 for LA (System).	LA	lectures attended$low attachment$label accuracy$lip aperture$Levantine Arabic$left adjoining$Latin$Lexical Acquisition$	1
Other high-frequency word classes with relatively LA accuracy are preposi- tions (80%), adverbs (82%) and subordinating con- junctions (80%), for a total of another 23% of the test corpus.	LA	lectures attended$low attachment$label accuracy$lip aperture$Levantine Arabic$left adjoining$Latin$Lexical Acquisition$	1
The value 3 for LA is the de- fault value for the empty context.	LA	lectures attended$low attachment$label accuracy$lip aperture$Levantine Arabic$left adjoining$Latin$Lexical Acquisition$	1
The arc above the NP can go either left (for high attachment (a) of the coordinated phrase) or right (for LA (b) of the coordinated phrase).	LA	lectures attended$low attachment$label accuracy$lip aperture$Levantine Arabic$left adjoining$Latin$Lexical Acquisition$	1
bj?link +currency?obj?link 10.2 mechanism?subj?link 3.4 +European Monetary System?subj?link 0 +currency?obj?link +System?subj?link +currency?obj?link 0 European Monetary System?subj?link 0 System?subj?link 0 +system?subj?link +currency?obj?link 0 system?subj?link 1.2 +company?subj?link +currency?obj?link 0 company?subj?link -1.1 empty 3 Table 3: Queries for computing high attach- ment (above) and LA (below) for Example 1.	LA	lectures attended$low attachment$label accuracy$lip aperture$Levantine Arabic$left adjoining$Latin$Lexical Acquisition$	1
By contrast, Turkish (Oflazer et al, 2003; Atalay et al, 2003) exhibits high root accu- racy but consistently LA scores (about 88% for length 1 and 68% for length 2).	LA	lectures attended$low attachment$label accuracy$lip aperture$Levantine Arabic$left adjoining$Latin$Lexical Acquisition$	1
This value reflects the bias for LA: the ma- jority of relative clauses are attached low.	LA	lectures attended$low attachment$label accuracy$lip aperture$Levantine Arabic$left adjoining$Latin$Lexical Acquisition$	1
887 CONFLICT LOW MED HIGH 0.4 0.6 0.8 1.0  0  5 10 15  0  5 10 15  0  5 10 15  0  5 10 15 Number of annotated instances x 1,000 Ac cu ra cy algorithm MomResp LogResp Majority 20 Newsgroups CONFLICT LOW MED HIGH 0.4 0.6 0.8 1.0  0  5 10 15  0  5 10 15  0  5 10 15  0  5 10 15 Number of annotated instances x 1,000 Te st  Ac cu ra cy algorithm MomResp LogResp 20 Newsgroups Figure 3: Top row: Inferred LA on three-deep annotations.	LA	lectures attended$low attachment$label accuracy$lip aperture$Levantine Arabic$left adjoining$Latin$Lexical Acquisition$	2
Improving dependency LA using statistical post-editing: A cross-framework study.	LA	lectures attended$low attachment$label accuracy$lip aperture$Levantine Arabic$left adjoining$Latin$Lexical Acquisition$	2
Inferred LA on items that have been annotated is the primary task of crowdsourc- ing; we track this measure accordingly.	LA	lectures attended$low attachment$label accuracy$lip aperture$Levantine Arabic$left adjoining$Latin$Lexical Acquisition$	2
The SRL systems compared in (Ruppenhofer et al, 2010) all achieved precision in the mid 60% range, but SEMAFOR achieved substantially higher re- call, F 1 , and LA on this subtask. (	LA	lectures attended$low attachment$label accuracy$lip aperture$Levantine Arabic$left adjoining$Latin$Lexical Acquisition$	2
Table 2 reports the percent of the dataset that must be annotated three-deep before LOGRESP?s in- ferred LA surpasses that of MOMRESP.	LA	lectures attended$low attachment$label accuracy$lip aperture$Levantine Arabic$left adjoining$Latin$Lexical Acquisition$	2
LA?,	LA	lectures attended$low attachment$label accuracy$lip aperture$Levantine Arabic$left adjoining$Latin$Lexical Acquisition$	2
Figure 2 shows the degree of LA (i.e., the distance between UL and LL) over time for a control and a dysarthric speaker repeating the se- quence /ah p iy/. Here, the dysarthric speech is no- tably slower and has more excessive movement.	LA	lectures attended$low attachment$label accuracy$lip aperture$Levantine Arabic$left adjoining$Latin$Lexical Acquisition$	3
Figure 1: Two roughly equivalent Arabic sentences, one in MSA and one in LA, translated by the same MT system into English.	LA	lectures attended$low attachment$label accuracy$lip aperture$Levantine Arabic$left adjoining$Latin$Lexical Acquisition$	4
The variations are a miniature version of the variations in LA in general.	LA	lectures attended$low attachment$label accuracy$lip aperture$Levantine Arabic$left adjoining$Latin$Lexical Acquisition$	4
Darwish (2013)'s data is more focused on  Egyptian and LA and code switch- ing with English.	LA	lectures attended$low attachment$label accuracy$lip aperture$Levantine Arabic$left adjoining$Latin$Lexical Acquisition$	4
For more information on PAL and LA in general, see (Rice and Sa?id, 1960; Cowell, 1964; Bateson, 1967; Brustad, 2000; Halloun, 2000; Holes, 2004; Elihai, 2004).	LA	lectures attended$low attachment$label accuracy$lip aperture$Levantine Arabic$left adjoining$Latin$Lexical Acquisition$	4
Sawaf (2010), Salloum and Habash (2011) and Salloum and Habash (2013) used morpholog- ical analysis and morphosyntactic transformation rules for processing EGY and LA.	LA	lectures attended$low attachment$label accuracy$lip aperture$Levantine Arabic$left adjoining$Latin$Lexical Acquisition$	4
PAL is part of the South LA dialect subgroup (of which Jordanian Arabic is another dialect).	LA	lectures attended$low attachment$label accuracy$lip aperture$Levantine Arabic$left adjoining$Latin$Lexical Acquisition$	4
For example, LR means the source side site only allows LA trees and the target side site only allows right adjoining trees.	LA	lectures attended$low attachment$label accuracy$lip aperture$Levantine Arabic$left adjoining$Latin$Lexical Acquisition$	5
if node is substitution site or word then append site or word to y else append LA sites to y in outside-to-inside order foreach child c of node do append result of get-yield(c) to y append right adjoining sites to y in inside-to-outside order return y end function add-states(ruld-id, node-list) foreach substitution or adjunction site s i and in node-list do if s i is substitution site then state = concat(?q?,	LA	lectures attended$low attachment$label accuracy$lip aperture$Levantine Arabic$left adjoining$Latin$Lexical Acquisition$	5
On each side of the rule, we traverse the tree in a top- down, left-to-right order, recording words, substi- tution sites, and adjoining sites in the order en- countered (LAs before the node?s chil- dren and right adjoinings after).	LA	lectures attended$low attachment$label accuracy$lip aperture$Levantine Arabic$left adjoining$Latin$Lexical Acquisition$	5
We attach all foreign-side adjoining sites to be LA, except on the right side of the right-hand child.	LA	lectures attended$low attachment$label accuracy$lip aperture$Levantine Arabic$left adjoining$Latin$Lexical Acquisition$	5
For example, in the alignment of  LA factum with Spanish hecho, the affricate \[if\]  should be linked with both \[k\] and \[t\] rather than  with just one of them, because it originates from the  merger of the two consonants.	LA	lectures attended$low attachment$label accuracy$lip aperture$Levantine Arabic$left adjoining$Latin$Lexical Acquisition$	6
For  example, at morphological level, foreign suffixes,  mostly originating from LA and Greek, are of- ten ?	LA	lectures attended$low attachment$label accuracy$lip aperture$Levantine Arabic$left adjoining$Latin$Lexical Acquisition$	6
occurrence of bi-grams or tri-grams typical  for other languages (especially LA and  English), and  ?	LA	lectures attended$low attachment$label accuracy$lip aperture$Levantine Arabic$left adjoining$Latin$Lexical Acquisition$	6
Note that taking a se-  g r ~e s  g r ~ m e n  II g r s II  II g r a m II en  II g r II s  II g r a II men  Table 5: Three alignments of English grass and  LA gramen obtained with global, semiglobal, and  local comparison.	LA	lectures attended$low attachment$label accuracy$lip aperture$Levantine Arabic$left adjoining$Latin$Lexical Acquisition$	6
glish grass with LA gramen, it is importa~nt to  match only the first three segments in each word;  the remaining segments are unrelated.	LA	lectures attended$low attachment$label accuracy$lip aperture$Levantine Arabic$left adjoining$Latin$Lexical Acquisition$	6
Note that taking a se-  g r ~e s  g r ~ m e n  II g r s II  II g r a m II en  II g r II s  II g r a II men  Table 5: Three alignments of English grass and  LA gramen obtained with global, semiglobal, a	LA	lectures attended$low attachment$label accuracy$lip aperture$Levantine Arabic$left adjoining$Latin$Lexical Acquisition$	6
The remaining average  feature distances were calculated using a set of most  frequent phonemes represented by 25 letters of the  LA alphabet (all but q).	LA	lectures attended$low attachment$label accuracy$lip aperture$Levantine Arabic$left adjoining$Latin$Lexical Acquisition$	6
Zernik, U. LA.	LA	lectures attended$low attachment$label accuracy$lip aperture$Levantine Arabic$left adjoining$Latin$Lexical Acquisition$	7
A Graph Model for Unsupervised LA.	LA	lectures attended$low attachment$label accuracy$lip aperture$Levantine Arabic$left adjoining$Latin$Lexical Acquisition$	7
LA: Ex-  ploiting On-Line Resou~ves to Build a Lex-  icon.	LA	lectures attended$low attachment$label accuracy$lip aperture$Levantine Arabic$left adjoining$Latin$Lexical Acquisition$	7
Carter, D.M. (1989) "LA in  the Core Language Engine", Proceedings  of the Fourth Conference of the European  Chapter of the Association for Computa-  tional Linguistics, pp 137-144.	LA	lectures attended$low attachment$label accuracy$lip aperture$Levantine Arabic$left adjoining$Latin$Lexical Acquisition$	7
Miller A. G., C. Fellbaum and D. Gross 1989,  WORDNET a Lexical Database Organised on  Psycholinguistic Principles, Proceedings IJCAI,  First International LA Workshop,  Detroit.	LA	lectures attended$low attachment$label accuracy$lip aperture$Levantine Arabic$left adjoining$Latin$Lexical Acquisition$	7
We only use EP along with two sections of the Gigaword corpus: Agence France Press English Service(afe) and The New York Times NW Service (nyt).	NW	Newswire$newswire$	0
The tail documents typically have lower phrase coverage, thus incor- rect phrase translation pairs derived from incorrect 937 # phrase pairs Average Tail TER BLEU (TER-BLEU)/2 TER BLEU (TER-BLEU)/2 Baseline 934206 60.74 28.05 16.35 69.02 17.83 25.60 ALF 797685 60.33 28.52 15.91 68.31 19.27 24.52 Table 5: Improved Chinese-English NW Translation with Alignment Link Filtering # phrase pairs Average Tail TER BLEU (TER-BLEU)/2 TER BLEU (TER-BLEU)/2 Baseline 934206 62.87 25.08 18.89 66.55 18.80 23.88 ALF 797685 62.30 24.89 18.70 65.97 19.25 23.36 Table 6: Improved Chinese-English Web-Blog Translation with Alignment Link Filtering alignment links are more likely to be selected.	NW	Newswire$newswire$	0
The first is the sentence alignment confidence measure, based on which the best whole sentence alignment is se- 938 # phrase pairs Average Tail TER BLEU (TER-BLEU)/2 TER BLEU (TER-BLEU)/2 Baseline 939911 43.53 50.51 -3.49 53.14 40.60 6.27 ALF 618179 43.11 50.24 -3.56 51.75 42.05 4.85 Table 7: Improved Arabic-English NW Translation with Alignment Link Filtering # phrase pairs Average Tail TER BLEU (TER-BLEU)/2 TER BLEU (TER-BLEU)/2 Baseline 598721 49.91 39.90 5.00 57.30 30.98 13.16 ALF 383561 48.94 40.00 4.42 55.99 31.92 12.04 Table 8: Improved Arabic-English Web-Blog Translation with Alignment Link Filtering lected among multiple alignments and it obtained 0.8 F-measure improvement over the single best	NW	Newswire$newswire$	0
Associated Press NW, June 1988.	NW	Newswire$newswire$	0
Mani, I., et al (1993) "Identifying Unknown Proper  Names in NW Text."	NW	Newswire$newswire$	0
The system F-score performance is 81.73%, 75.67%, 58.11% on ACE2005 Broadcast News, NW, and Web blogs respectively.	NW	Newswire$newswire$	0
To estimate the timeliness and semantic cred- ibility indicators, we use AQUAINT-2, a set of NW articles (2.5 GB, about 907K documents) that are roughly contemporaneous with the TREC Blog06 collection (AQUAINT-2, 2007).	NW	Newswire$newswire$	1
On the whole test set the difference is smaller, 0.07 for the NW translation and 0.58 for the web-blog translation.	NW	Newswire$newswire$	1
For NW, the translation quality is improved by 0.44 on the whole test set and 1.1 on the tail documents, as measured by (TER-BLEU)/2.	NW	Newswire$newswire$	1
Ta- bles 5 and 6 show the NW and web-blog translation scores as well as the number of phrase translation pairs obtained from each alignment.	NW	Newswire$newswire$	1
These filters  were used for ATR from NW corpora and in biomedi- cine (Frantzi et al, 2000; Nenadi?	NW	Newswire$newswire$	1
The domains are broadcast conversations (bc), broadcast news (bn), conversa- tional telephone speech (cts), NW (nw), usenet (un) and weblog (wl).	NW	Newswire$newswire$	1
1 Introduction  An overwhelming amount of textual information  presented in NW, scientific literature, legal  texts, etc.,	NW	Newswire$newswire$	1
We tested 6 metrics: Re?nyi, Vari- ational (L1), Euclidean, Cosine, Kullback-Leibler, and the BC.	BC	Bhattacharyya coefficient$Broadcast Conversations$broadcasting conversation$BE CONCRETE$Brown Corpus$broadcast conversations$broadcast conversation$Base clauses$	0
248 2.2 BC When the context profiles are probability distribu- tions, we usually utilize the measures on probabil- ity distributions such as the Jensen-Shannon (JS) divergence to calculate similarities (Dagan et al, 1994; Dagan et al, 1997).	BC	Bhattacharyya coefficient$Broadcast Conversations$broadcasting conversation$BE CONCRETE$Brown Corpus$broadcast conversations$broadcast conversation$Base clauses$	0
When the context pro- files are multinomial distributions, the pri- ors are Dirichlet, and the base measure is the BC, we can de- rive an analytical form that allows efficient calculation.	BC	Bhattacharyya coefficient$Broadcast Conversations$broadcasting conversation$BE CONCRETE$Brown Corpus$broadcast conversations$broadcast conversation$Base clauses$	0
LIPN: Introducing a new geographical con- text similarity measure and a statistical similarity measure based on the BC.	BC	Bhattacharyya coefficient$Broadcast Conversations$broadcasting conversation$BE CONCRETE$Brown Corpus$broadcast conversations$broadcast conversation$Base clauses$	0
In Section 2, we briefly introduce the Bayesian esti- mation and the BC.	BC	Bhattacharyya coefficient$Broadcast Conversations$broadcasting conversation$BE CONCRETE$Brown Corpus$broadcast conversations$broadcast conversation$Base clauses$	0
3.2 Similarity metrics To measure the difference between two corpora we implemented six similarity metrics: Re?nyi2 (Re?nyi, 1961), Variational (L1) (Lee, 2001), Euclidean (Lee, 2001), Cosine (Lee, 2001), Kullback-Leibler (Kullback and Leibler, 1951) and BC (Comaniciu et al, 2003; Bhattacharyya, 1943).	BC	Bhattacharyya coefficient$Broadcast Conversations$broadcasting conversation$BE CONCRETE$Brown Corpus$broadcast conversations$broadcast conversation$Base clauses$	0
ME Adapted ME English Broadcast News 147K 290 255 243 230 27.2 26.3 25.9 292K 286 250 236 223 26.7 25.8 25.6 591K 280 243 228 215 26.6 25.9 25.6 1119K 272 232 217 204 26.2 25.6 24.9 Estonian BC 104K 237 197 200 169 40.5 38.9 37.4 17.7 17.3 16.6 Table 2: Perplexity, WER and LER results comparing pooled and interpolated N -gram models and interpolated and adapted ME models, with changing amount of available in-domain data.	BC	Bhattacharyya coefficient$Broadcast Conversations$broadcasting conversation$BE CONCRETE$Brown Corpus$broadcast conversations$broadcast conversation$Base clauses$	1
The 2005 ACE data comes from 5 domains: Broad- cast News (bn), BC (bc), Newswire (nw), Weblog (wl), Usenet (un) and Converstaional Telephone Speech (cts).	BC	Bhattacharyya coefficient$Broadcast Conversations$broadcasting conversation$BE CONCRETE$Brown Corpus$broadcast conversations$broadcast conversation$Base clauses$	1
2 ID English Broadcast News 147K 2e8 3e5 5e7 2e7 2e6 292K 2e8 5e5 5e7 2e7 2e6 591K 2e8 1e6 5e7 2e7 2e6 1119K 2e8 2e6 5e7 2e7 5e6 Estonian BC 104K 5e8 3e5 5e7 1e7 2e6 Table 1: The unnormalized values of Gaus- sian prior variances for interpolated out-of-domain (OD) and in-domain (ID) ME models, and hierar- chically adapted global (*), out-of-odomain (OD) and in-domain (ID) models that were used in the experiments.	BC	Bhattacharyya coefficient$Broadcast Conversations$broadcasting conversation$BE CONCRETE$Brown Corpus$broadcast conversations$broadcast conversation$Base clauses$	1
2006): 1996 CSR Hub4 Language Model data, EARS BN03 closed captions, GALE Phase 2 Distillation GNG Evaluation Supplemental Mul- tilingual data, Hub4 acoustic model training tran- scripts, TDT4 closed captions, TDT4 newswire, and GALE BC and GALE Broad- cast News.	BC	Bhattacharyya coefficient$Broadcast Conversations$broadcasting conversation$BE CONCRETE$Brown Corpus$broadcast conversations$broadcast conversation$Base clauses$	1
Task 2: Estonian BC.	BC	Bhattacharyya coefficient$Broadcast Conversations$broadcasting conversation$BE CONCRETE$Brown Corpus$broadcast conversations$broadcast conversation$Base clauses$	1
c?2011 Association for Computational Linguistics Detection of Agreement and Disagreement in BC Wen Wang1 Sibel Yaman2y Kristin Precoda1 Colleen Richey1 Geoffrey Raymond3 1SRI International, 333 Ravenswood Avenue, Menlo Park, CA 94025, USA 2IBM T. J. Watson Research Center P.O.Box 218, Yorktown Heights, NY 10598, USA 3University of California, Santa Barbara, CA, USA fwwang,precoda,colleeng@speech.sri.com, syaman@us.ibm.com, graymond@soc.ucsb.edu Abstract We present C	BC	Bhattacharyya coefficient$Broadcast Conversations$broadcasting conversation$BE CONCRETE$Brown Corpus$broadcast conversations$broadcast conversation$Base clauses$	1
Using the technique described above and the  lexicon derived frora the BC we ex-  tracted prefix morphological rules (no alter-  ations), suffix morphological rules without alter-  ations and ending guessing rules, exactly as it was  done in (Mikheev, 1996).	BC	Bhattacharyya coefficient$Broadcast Conversations$broadcasting conversation$BE CONCRETE$Brown Corpus$broadcast conversations$broadcast conversation$Base clauses$	4
We performed a rule-induction experiment us-  ing the lexicon and word-frequencies derived  from the BC (Prancis&Kucera, 1982).	BC	Bhattacharyya coefficient$Broadcast Conversations$broadcasting conversation$BE CONCRETE$Brown Corpus$broadcast conversations$broadcast conversation$Base clauses$	4
BC which is useflll for  comparison and eval	BC	Bhattacharyya coefficient$Broadcast Conversations$broadcasting conversation$BE CONCRETE$Brown Corpus$broadcast conversations$broadcast conversation$Base clauses$	4
BC which is useflll for  comparison and evaluation.	BC	Bhattacharyya coefficient$Broadcast Conversations$broadcasting conversation$BE CONCRETE$Brown Corpus$broadcast conversations$broadcast conversation$Base clauses$	4
sky  teacher  plant  school  the BC, texts from the Wall Street Your-  hal, Grolier's Electronic Encyclopedia nd scientific  abstracts from different fields.	BC	Bhattacharyya coefficient$Broadcast Conversations$broadcasting conversation$BE CONCRETE$Brown Corpus$broadcast conversations$broadcast conversation$Base clauses$	4
ng the lexicon and word-frequencies derived  from the BC (Prancis&Kucera, 1982).	BC	Bhattacharyya coefficient$Broadcast Conversations$broadcasting conversation$BE CONCRETE$Brown Corpus$broadcast conversations$broadcast conversation$Base clauses$	4
The most im-  portant ones are that the BC provides  a model of general multi-domain language use,  so general language regularities carl be induced  h'om it;, and second, many taggers come with data  trained on the.	BC	Bhattacharyya coefficient$Broadcast Conversations$broadcasting conversation$BE CONCRETE$Brown Corpus$broadcast conversations$broadcast conversation$Base clauses$	4
There are a number of reasons tbr choosing tile  BC data for training.	BC	Bhattacharyya coefficient$Broadcast Conversations$broadcasting conversation$BE CONCRETE$Brown Corpus$broadcast conversations$broadcast conversation$Base clauses$	4
The aspect of social communication most ex- plored so far is the detection of participant role, particularly in spoken genres such as broadcast news, BC, and meetings.	BC	Bhattacharyya coefficient$Broadcast Conversations$broadcasting conversation$BE CONCRETE$Brown Corpus$broadcast conversations$broadcast conversation$Base clauses$	5
Wang et al (2011) presented a condi- tional random field based approach for detecting agreement/disagreement between speakers in En- glish BC.	BC	Bhattacharyya coefficient$Broadcast Conversations$broadcasting conversation$BE CONCRETE$Brown Corpus$broadcast conversations$broadcast conversation$Base clauses$	5
The English lan- guage portion comprises roughly 1.7M words and Chinese language portion comprises roughly 1M words of newswire, magazine articles, broadcast news, BC, web data and con- versational speech data3.	BC	Bhattacharyya coefficient$Broadcast Conversations$broadcasting conversation$BE CONCRETE$Brown Corpus$broadcast conversations$broadcast conversation$Base clauses$	5
Both the dev and the test set are composed of a mixture of broadcast news and BC crawled from the web and have two references.	BC	Bhattacharyya coefficient$Broadcast Conversations$broadcasting conversation$BE CONCRETE$Brown Corpus$broadcast conversations$broadcast conversation$Base clauses$	5
The domains are BC (bc), broadcast news (bn), conversa- tional telephone speech (cts), newswire (nw), usenet (un) and weblog (wl).	BC	Bhattacharyya coefficient$Broadcast Conversations$broadcasting conversation$BE CONCRETE$Brown Corpus$broadcast conversations$broadcast conversation$Base clauses$	5
195 from newswire, broadcast news, weblogs, usenet  newsgroups/discussion forum, conversational  telephone speech and BC.	BC	Bhattacharyya coefficient$Broadcast Conversations$broadcasting conversation$BE CONCRETE$Brown Corpus$broadcast conversations$broadcast conversation$Base clauses$	5
More recently, the OntoNotes project (Pradhan et al, 2007) released a one mil- lion word English corpus of newswire, broadcast news, and BC that is annotated for Penn Treebank syntax, PropBank predicate ar- gument structures, coreference, and named enti- ties.	BC	Bhattacharyya coefficient$Broadcast Conversations$broadcasting conversation$BE CONCRETE$Brown Corpus$broadcast conversations$broadcast conversation$Base clauses$	6
especially the ones in the BC, web data, 5doc/propbank/english-propbank.pdf 6As we will see later these are not used during the task.	BC	Bhattacharyya coefficient$Broadcast Conversations$broadcasting conversation$BE CONCRETE$Brown Corpus$broadcast conversations$broadcast conversation$Base clauses$	6
The English lan- guage portion comprises roughly 1.7M words and Chinese language portion comprises roughly 1M words of newswire, magazine articles, broadcast news, BCs, web data and con- versational speech data3.	BC	Bhattacharyya coefficient$Broadcast Conversations$broadcasting conversation$BE CONCRETE$Brown Corpus$broadcast conversations$broadcast conversation$Base clauses$	6
More recently under DARPA GALE funding it has been expanded to include broadcast news, BC, news groups and web log data.	BC	Bhattacharyya coefficient$Broadcast Conversations$broadcasting conversation$BE CONCRETE$Brown Corpus$broadcast conversations$broadcast conversation$Base clauses$	6
The domains are BCs (bc), broadcast news (bn), conversa- tional telephone speech (cts), newswire (nw), usenet (un) and weblog (wl).	BC	Bhattacharyya coefficient$Broadcast Conversations$broadcasting conversation$BE CONCRETE$Brown Corpus$broadcast conversations$broadcast conversation$Base clauses$	6
The training corpora in- clude a mixed genre of newswire, weblog, broad- cast news, BC, discussion fo- rums and comes from various sources such as LDC, HK Law, HK Hansard and UN data.	BC	Bhattacharyya coefficient$Broadcast Conversations$broadcasting conversation$BE CONCRETE$Brown Corpus$broadcast conversations$broadcast conversation$Base clauses$	6
Based on  this resource, we also intend to analyze  the syntactic correlations of prosodic  phrase in BN speech, and  compare the phonetic and prosodic fea- tures in movie dialogues among several  same-name movies in different histori- cal eras.	BN	broadcasting news$British National$broadcast news$Bayesian network$Bengali$Broadcast News$	0
One reason for this may be due to OntoNotes  corpus is from BN domain.	BN	broadcasting news$British National$broadcast news$Bayesian network$Bengali$Broadcast News$	0
1 Introduction In the recent years, the microblogging service Twit- ter has become a very popular tool for express- ing opinions, BN, and simply com- municating with friends.	BN	broadcasting news$British National$broadcast news$Bayesian network$Bengali$Broadcast News$	0
However, in BN, these sentences  maybe simply joined by conjunction word ?	BN	broadcasting news$British National$broadcast news$Bayesian network$Bengali$Broadcast News$	0
In particular, statistical machine translation systems (Koehn et al, 2007; Bach et al, 2007; Shen et al, 2008) have advanced to a state that the transla- tion quality for certain language pairs (e.g. Spanish- English, French-English, Iraqi-English) in certain do- mains (e.g. BN, force-protection, travel) is acceptable to users.	BN	broadcasting news$British National$broadcast news$Bayesian network$Bengali$Broadcast News$	0
1 Introduction In the recent years, the microblogging service Twit- ter has become a popular tool for expressing opin- ions, BN, and simply communicating with friends.	BN	broadcasting news$British National$broadcast news$Bayesian network$Bengali$Broadcast News$	0
BN Corpus Consortium, Ox- ford University Computing Service.	BN	broadcasting news$British National$broadcast news$Bayesian network$Bengali$Broadcast News$	1
We use the data set from our previous work, which contain sentences from the first half of the BN Corpus, with near- synonyms from the eleven groups from Table 4.	BN	broadcasting news$British National$broadcast news$Bayesian network$Bengali$Broadcast News$	1
100 million words of English: the BN Corpus.	BN	broadcasting news$British National$broadcast news$Bayesian network$Bengali$Broadcast News$	1
2013) trained on a corpus consisting of the 400M word Text8 corpus of Wikipedia text 2 together with the 100M word BN Corpus (Leech et al.,	BN	broadcasting news$British National$broadcast news$Bayesian network$Bengali$Broadcast News$	1
3.2.2 Language Model For our language model, we use a Kneser-Ney smoothed trigram model learned from a version of the BN Corpus modified to use Americanized spellings (Chen and Goodman, 1996; Burnard, 1995).	BN	broadcasting news$British National$broadcast news$Bayesian network$Bengali$Broadcast News$	1
Claws4: the tagging of the BN Corpus.	BN	broadcasting news$British National$broadcast news$Bayesian network$Bengali$Broadcast News$	1
News substantiate this relevance that is also sup- ported by the spoken language scenario, where most speech summarization systems concentrate on BN (McKeown et al, 2005).	BN	broadcasting news$British National$broadcast news$Bayesian network$Bengali$Broadcast News$	2
We use BN as a case study and news stories from online newspapers provide the background information.	BN	broadcasting news$British National$broadcast news$Bayesian network$Bengali$Broadcast News$	2
Feature-based methods combine several types of features: current work uses lexical, acoustic/prosodic, structural, and dis- course features to summarize documents from do- mains like BN or meetings (Maskey and Hirschberg, 2005; Murray et al, 2006; Ribeiro and de Matos, 2007).	BN	broadcasting news$British National$broadcast news$Bayesian network$Bengali$Broadcast News$	2
In fact, the previously enumerated problems that make speech summarization such a difficult task constrain the applicability of text summarization techniques to speech summariza- tion (although in the presence of planned speech, as it partly happens in the BN domain, that portability is more feasible (Christensen et al, 2003)).	BN	broadcasting news$British National$broadcast news$Bayesian network$Bengali$Broadcast News$	2
The domains are broadcast conversations (bc), BN (bn), conversa- tional telephone speech (cts), newswire (nw), usenet (un) and weblog (wl).	BN	broadcasting news$British National$broadcast news$Bayesian network$Bengali$Broadcast News$	2
Even though this heuristic may appear naif, we believe it is adequate as a rough approach, considering the target material (BN).	BN	broadcasting news$British National$broadcast news$Bayesian network$Bengali$Broadcast News$	2
5 A Case Study Using Broadcast News 5.1 Media Monitoring System SSNT (Amaral et al, 2007) is a system for selec- tive dissemination of multimedia contents, work- ing primarily with Portuguese BN ser- vices.	BN	broadcasting news$British National$broadcast news$Bayesian network$Bengali$Broadcast News$	2
We use graph Gs as a representation of the BN with random variables Es and rs.	BN	broadcasting news$British National$broadcast news$Bayesian network$Bengali$Broadcast News$	3
The contrast classifier is also competitive with the best case result in (Galley et al, 2004) (last entry), which adds speaker change, segment duration, and adjacency pair sequence dependency features using a dynamic BN.	BN	broadcasting news$British National$broadcast news$Bayesian network$Bengali$Broadcast News$	3
Identifying agreement and  disagreement in conversational speech: Use of  BNs to model pragmatic  dependencies.	BN	broadcasting news$British National$broadcast news$Bayesian network$Bengali$Broadcast News$	3
Identifying agreement and disagreement in con- versational speech: use of BNs to modeldependencies.	BN	broadcasting news$British National$broadcast news$Bayesian network$Bengali$Broadcast News$	3
More recently, Abend et al (2009) propose an unsupervised algorithm for argument identifica- tion that relies only on part-of-speech annotations, whereas Grenager and Manning (2006) focus on role induction which they formalize as probabilis- tic inference in a BN.	BN	broadcasting news$British National$broadcast news$Bayesian network$Bengali$Broadcast News$	3
rh(; ,,s(, of  BNs for WSI) has I)een l)rop()sed by  others such as Wiebe eL al (1.998), but a different  fl)rmulation is used in this mod('l.	BN	broadcasting news$British National$broadcast news$Bayesian network$Bengali$Broadcast News$	3
We could not find any publica- tion on BN stemmer following rule-based ap- proach.	BN	broadcasting news$British National$broadcast news$Bayesian network$Bengali$Broadcast News$	4
c?2008 Asian Federation of Natural Language Processing Design of a Rule-based Stemmer for Natural Language Text in BN Sandipan Sarkar  IBM India  sandipan.sarkar@in.ibm.com,  sandipansarkar@gmail.com  Sivaji Bandyopadhyay  Computer Science and Engineering Department  Jadavpur University, Kolkata  sbandyopadhyay@cse.jdvu.ac.in      Abstract  This paper presents a rule-based approach  for finding out the stems from text in Ben- gali, a resource-poor language.	BN	broadcasting news$British National$broadcast news$Bayesian network$Bengali$Broadcast News$	4
Our approach in this work is to identify and  formalize rules in BN to build a stemming sys- tem with acceptable accuracy.	BN	broadcasting news$British National$broadcast news$Bayesian network$Bengali$Broadcast News$	4
2006)  accepted the absence of rule-based stemmer in  BN and proposed a statistical clustering-based  approach to discover equivalence classes of root  words from electronic texts in different languages  including BN.	BN	broadcasting news$British National$broadcast news$Bayesian network$Bengali$Broadcast News$	4
This paper deals  with design of such a system to stem BN  words tokens tagged with their respective parts of  speech (POS).	BN	broadcasting news$British National$broadcast news$Bayesian network$Bengali$Broadcast News$	4
2005) proposed a  clustering-based approach to identify stem from  BN image documents.	BN	broadcasting news$British National$broadcast news$Bayesian network$Bengali$Broadcast News$	4
It starts by  introducing the concept of orthographic  syllable, the basic orthographic unit of  BN.	BN	broadcasting news$British National$broadcast news$Bayesian network$Bengali$Broadcast News$	4
c  syllable, the basic orthographic unit of  BN.	BN	broadcasting news$British National$broadcast news$Bayesian network$Bengali$Broadcast News$	4
These concepts are  applied in the design and implementation  of an extensible architecture of a stemmer  system for BN text.	BN	broadcasting news$British National$broadcast news$Bayesian network$Bengali$Broadcast News$	4
In  Proceedings of the DARPA BN Tran-  scription and Understanding Workshop.	BN	broadcasting news$British National$broadcast news$Bayesian network$Bengali$Broadcast News$	5
Extractive Sum- marization of BN: Comparing Strate- gies for European Portuguese.	BN	broadcasting news$British National$broadcast news$Bayesian network$Bengali$Broadcast News$	5
Are Extractive Text Summarisation Tech- niques Portable To BN?	BN	broadcasting news$British National$broadcast news$Bayesian network$Bengali$Broadcast News$	5
Automatic vs. Manual Topic Seg- mentation and Indexation in BN.	BN	broadcasting news$British National$broadcast news$Bayesian network$Bengali$Broadcast News$	5
A Prototype System for Selective Dissemination of BN in European Por- tuguese.	BN	broadcasting news$British National$broadcast news$Bayesian network$Bengali$Broadcast News$	5
5 A Case Study Using BN 5.1 Media Monitoring System SSNT (Amaral et al, 2007) is a system for selec- tive dissemination of multimedia contents, work- ing primarily with Portuguese broadcast news ser- vices.	BN	broadcasting news$British National$broadcast news$Bayesian network$Bengali$Broadcast News$	5
UN (15%) and Conversational Telephone Speech (15%).	UN	Usenet Newsgroups$Usenet$	0
The 20-Newsgroups (20NG) corpus is a collection of newsgroup postings gathered from twenty different categories from the UN hierarchy4.	UN	Usenet Newsgroups$Usenet$	0
Four of the clusters are from  UN newsgroups.	UN	Usenet Newsgroups$Usenet$	1
Our data is a corpus of articles from 20 different UN newsgroups released by Mitchell (1999).	UN	Usenet Newsgroups$Usenet$	1
Subjects were recruited via postings to local UN newsgroups.	UN	Usenet Newsgroups$Usenet$	1
The most similar work to our own is that of Wang & Rose (2010) who analyzed UN fo- rum quote/response structures.	UN	Usenet Newsgroups$Usenet$	1
Hassan et al use a supervised Markov model, part of speech, and dependency patterns to identify attitudinal polarities in threads posted to UN discussion posts (Hassan et al, 2010).	UN	Usenet Newsgroups$Usenet$	1
uracy produced for the two feature sets The results show that there is no signicant increase by either algorithm by the addition of semantic features The original motivation behind our work on question terminology was to improve the re trieval accuracy of our system called FAQFinder 	Burke et al    Lytinen and Tomuro   FAQFinder is a webbased natural language QA system which uses UN Frequently Asked Questions 	FAQ  les to answer users questions Figures  and  show an example ses sion with FAQFinder First the user enters a question in natural language The system then searches the FAQ les for questions that are similar to the users Based on the results of the search FAQFinder displays a maximum of  FAQ questions which are ranked the highest by the systems s	UN	Usenet Newsgroups$Usenet$	1
In International Conference for WLs and Social Media.	WL	Webblog$Word level$word lexicon$Weblogs$	0
5.1 Newswire Data vs. WL Data Considering the big gap in accuracy between newswire and webblog data in our baseline results, we delve deeper into the data and found several major distinctions between these two domains that might have contributed to the rather significant dif- ference in performance on tense inference.	WL	Webblog$Word level$word lexicon$Weblogs$	0
We use the ACE 2005 coreference cor- pus as released by the LDC, which consists of the 599 training documents used in the official ACE evaluation.3 To ensure diversity, the corpus was created by selecting documents from six different sources: Broadcast News (bn), Broadcast Con- versations (bc), Newswire (nw), WL (wb), Usenet (un), and conversational telephone speech (cts).	WL	Webblog$Word level$word lexicon$Weblogs$	0
In Conference for WLs and Social Media.	WL	Webblog$Word level$word lexicon$Weblogs$	0
The ACE05 corpus is drawn from six sources (Newswire, Broadcast News, Broadcast Conversations, Conversational Telephone Speech, WLs, and Usenet).	WL	Webblog$Word level$word lexicon$Weblogs$	0
Sentence level and word level  annotation    TypeCraft provides a set of glosses for  syntactic and semantic coding and a set of  parameters along which sentences may be  classified that allow for standardized  annotation and cross linguistic comparison as  illustrated in figure1:        figure1: Word and sentence level annotation     3.1 WL    WL annotation allows for analysis of  predicates in terms of syntactic and semantic  properties including information about the  subcategorization properties and argument  structure of predicates.	WL	Webblog$Word level$word lexicon$Weblogs$	1
WL  precision and recall with F1-measure is evaluated.	WL	Webblog$Word level$word lexicon$Weblogs$	1
WL precision and recall with F1-measure  are employed as evaluation standard.	WL	Webblog$Word level$word lexicon$Weblogs$	1
WL agreement between the annotators was calculated as the per- cent of the decisions on which they agreed, and found to be 97.1%.	WL	Webblog$Word level$word lexicon$Weblogs$	1
2.3 Sentence Vector Representations WL representation often cannot properly capture more complex linguistic phenomena in a sentence or multi-word phrase.	WL	Webblog$Word level$word lexicon$Weblogs$	1
In the algorithm, the opinion WL O and review data R about a product are provided as the input.	WL	Webblog$Word level$word lexicon$Weblogs$	2
For the English?French task (Section 3.1), we train translation mod- els on different training data sets and augment the phrase-based system with a hierarchical re- ordering model, a word class language model, a discriminative WL and a insertion and deletion model.	WL	Webblog$Word level$word lexicon$Weblogs$	2
Further, we applied the hierar- chical reordering model, the word class language model, the discriminative WL, and the insertion and deletion model.	WL	Webblog$Word level$word lexicon$Weblogs$	2
The 2~g0-WL for the  Michael Reese stroke database takes up  about a third of a megabyte, so this  approach would work on a mainframe or a  large minicomputer such as our Vax 75g,  but could not readily be ported to a  smaller machine; nor could we handle a  much larger vocabulary such as we plan to  create with the automatic lexicon builder.	WL	Webblog$Word level$word lexicon$Weblogs$	2
Nu- merous approaches emerged over the years that try to induce bilingual WLs on the basis of distributional information.	WL	Webblog$Word level$word lexicon$Weblogs$	2
classified as a positive word by Gen- eral Inquirer5 , a sentiment WL, as a  positive opinion indicator.	WL	Webblog$Word level$word lexicon$Weblogs$	2
In Emre Kiciman, Nicole B. Ellison, Bernie Hogan, Paul Resnick, and Ian Soboroff, editors, ICWSM?13: Proceedings of the 7th International AAAI Conference on WL and Social Media.	WL	Webblog$Word level$word lexicon$Weblogs$	3
In John G. Breslin, Nicole B. Ellison, James G. Shana- han, and Zeynep Tufekci, editors, ICWSM?12: Pro- ceedings of the 6th International AAAI Conference on WL and Social Media.	WL	Webblog$Word level$word lexicon$Weblogs$	3
of AAAI Spring Symposium: Compu- tational Approaches to Analyzing WL.	WL	Webblog$Word level$word lexicon$Weblogs$	3
WL: Credibility and collaboration in an online world.	WL	Webblog$Word level$word lexicon$Weblogs$	3
In AAAI Conference on WL and Social Media.	WL	Webblog$Word level$word lexicon$Weblogs$	3
In John G. Breslin, Nicole B. Ellison, James G. Shanahan, and Zeynep Tufekci, editors, ICWSM?12: Proceedings of the 6th International AAAI Conference on WL and So- cial Media.	WL	Webblog$Word level$word lexicon$Weblogs$	3
3.2 Extracting IAs We define implicit attitudes as the semantic sim- ilarity between texts comprising discussant utter- ances or posts in a thread.	IA	Implicit Attitude$Incremental Algorithm$INSTRUMENTAGENCY$Input Analyzer$	0
c?2012 Association for Computational Linguistics Genre Independent Subgroup Detection in Online Discussion Threads: A Pilot Study of IA using Latent Textual Semantics Pradeep Dasigi pd2359@columbia.edu Weiwei Guo weiwei@cs.columbia.edu Center for Computational Learning Systems, Columbia University Mona Diab mdiab@ccls.columbia.edu Abstract We describe an unsupervised approach to the problem of automatically detecting sub- groups of people holding similar opinions in a discussion thread.	IA	Implicit Attitude$Incremental Algorithm$INSTRUMENTAGENCY$Input Analyzer$	0
Alfred was good in suspense and all, but his work is not as deep as Kubrick?s Table 1: Example of Agreement based on IA WIKI CD Median No.	IA	Implicit Attitude$Incremental Algorithm$INSTRUMENTAGENCY$Input Analyzer$	0
Algorithm 1 The Basic IA Require: T = target object; D = set of distractor objects.	IA	Implicit Attitude$Incremental Algorithm$INSTRUMENTAGENCY$Input Analyzer$	1
The IA.	IA	Implicit Attitude$Incremental Algorithm$INSTRUMENTAGENCY$Input Analyzer$	1
Algorithm 2 The Locative IA DESC = Basic-Incremental-Algorithm(T,D) if DESC !	IA	Implicit Attitude$Incremental Algorithm$INSTRUMENTAGENCY$Input Analyzer$	1
For the IA (Dale and Re- iter, 1995), this could be achieved by augmenting the list of preferred attributes with a list of ?	IA	Implicit Attitude$Incremental Algorithm$INSTRUMENTAGENCY$Input Analyzer$	1
the IA (Dale and Reiter, 1995), one of the most widely used REG algorithms, assumes that certain at- tributes are preferred over others, partly based on evidence provided by Pechmann (1989); a chair would first be described in terms of its color, and only if this does not result in a unique charac- terization, other, less preferred attributes such as orientation are tried.	IA	Implicit Attitude$Incremental Algorithm$INSTRUMENTAGENCY$Input Analyzer$	1
For example, the IA (Dale and Reiter, 1995), one of the most widely used REG algorithms, assumes that certain at- tributes are preferred over others, partly based on evidence provided by Pechmann (1989); a chair would first be described in terms of its color, and only if this does not result in a unique charac- terization, other, less preferred attributes such as orientation are tried.	IA	Implicit Attitude$Incremental Algorithm$INSTRUMENTAGENCY$Input Analyzer$	1
The IA is arguably unique in assuming a complete pref- erence order of attributes, but other REG algo- rithms rely on similar distinctions.	IA	Implicit Attitude$Incremental Algorithm$INSTRUMENTAGENCY$Input Analyzer$	1
14 2.3 IA and Input Interpreter The IA is designed as both domain and dialogue context independent.	IA	Implicit Attitude$Incremental Algorithm$INSTRUMENTAGENCY$Input Analyzer$	3
erpreter The IA is designed as both domain and dialogue context independent.	IA	Implicit Attitude$Incremental Algorithm$INSTRUMENTAGENCY$Input Analyzer$	3
First of all, the user's inputs are interpreted  through the IA.	IA	Implicit Attitude$Incremental Algorithm$INSTRUMENTAGENCY$Input Analyzer$	3
In contrast to the IA, the Input In- terpreter analyzes the input with respect to the context of the dialogue.	IA	Implicit Attitude$Incremental Algorithm$INSTRUMENTAGENCY$Input Analyzer$	3
6.4 Ranking Models RR (see Section 4.1) is per- formed using the implementation of the extremely randomized trees algorithm (Geurts et al, 2006) provided by the Scikit-learn package (Pedregosa et al, 2011).	RR	RelationalRealizational$Ranking by regression$reranking$Resource  Repository$Reduced Relative$reciprocal rank$Relational-Realizational$Reference resolution$	1
Coarse-to-fine n-best parsing and maxent dis- criminative RR.	RR	RelationalRealizational$Ranking by regression$reranking$Resource  Repository$Reduced Relative$reciprocal rank$Relational-Realizational$Reference resolution$	2
Then we learn a RR model using these candidate trees.	RR	RelationalRealizational$Ranking by regression$reranking$Resource  Repository$Reduced Relative$reciprocal rank$Relational-Realizational$Reference resolution$	2
We use two parses per sentence: the outputs of a self-trained RR parser Charniak and Johnson (2005); McClosky and Charniak (2008) and a CCG parser (Clark and Curran, 2007), provided as part of the shared task dataset.	RR	RelationalRealizational$Ranking by regression$reranking$Resource  Repository$Reduced Relative$reciprocal rank$Relational-Realizational$Reference resolution$	2
For comparison, we implement the following RR models: ?	RR	RelationalRealizational$Ranking by regression$reranking$Resource  Repository$Reduced Relative$reciprocal rank$Relational-Realizational$Reference resolution$	2
Word lattice RR for Chinese word segmentation and part-of-speech tagging.	RR	RelationalRealizational$Ranking by regression$reranking$Resource  Repository$Reduced Relative$reciprocal rank$Relational-Realizational$Reference resolution$	2
score O3 (T ) and score rerank (T ) are the outputs of the third order parser and the RR classifier respectively.	RR	RelationalRealizational$Ranking by regression$reranking$Resource  Repository$Reduced Relative$reciprocal rank$Relational-Realizational$Reference resolution$	2
While a sim- ilar effect may be possible with RR archi- tectures, we believe that in terms of implemen- tation efforts our approach is at least as simple.	RR	RelationalRealizational$Ranking by regression$reranking$Resource  Repository$Reduced Relative$reciprocal rank$Relational-Realizational$Reference resolution$	2
In  Type  Total noun phrases  Articles  Left Modifiers of Nouns  Navy  339  27  72  4  \[ Medical  532  38  Adjectival Modifiers:  Adj  Adj + Adj  Possessive N 138  34  4 0  Noun Modifiers:  Noun 99 76  N+N 25 4  Verb 7 0  Table I: Left Modifier Statistics  Right Modifiers of Nouns  Type \[ Navy \[ Medical  Prepositional Phrases 95 107  Relative Clauses 1 5  Adverb 4 0  RR Clauses 7 9  Table 2: Right Modifier Statistics  506  the sublanguage of Navy messages, unmarked verb  modifiers of nouns also occur.	RR	RelationalRealizational$Ranking by regression$reranking$Resource  Repository$Reduced Relative$reciprocal rank$Relational-Realizational$Reference resolution$	4
Seen that RR Clauses are headed by the past participle verb form, and that Participial Adjuncts may be attached to any NP head nouns quite consistently; and seen also that is very hard to apply strict subcategorization tests for participial SUBJect - or deep OBJect in case of passives - with good enough confidence we assume that such tests will only be performed in case the parser is at the co	RR	RelationalRealizational$Ranking by regression$reranking$Resource  Repository$Reduced Relative$reciprocal rank$Relational-Realizational$Reference resolution$	4
1.59946 5.87759 Figure 4: Mean 10.5 the banker told about the buy-back resigned 1 2 3 4 5 6 Log[ previous prefix current prefix ] RR Clause 0.798547 1.59946 0.622262 1.3212 0.	RR	RelationalRealizational$Ranking by regression$reranking$Resource  Repository$Reduced Relative$reciprocal rank$Relational-Realizational$Reference resolution$	4
35  PP > RR Clause > Relative Clause  A method frequently proposed to account for this  distinction is to use, as a measure of the cost of  raising, a count of the number of nodes in the syn-  tactic structure over which the quantifier is raised.	RR	RelationalRealizational$Ranking by regression$reranking$Resource  Repository$Reduced Relative$reciprocal rank$Relational-Realizational$Reference resolution$	4
f 9 , rank inform(s, i, v) = the sum of all the RR of the scores assigned by the SLU to the user informing the value of slot s is v, or 0 if informing v cannot be found in the SLU n-best list.	RR	RelationalRealizational$Ranking by regression$reranking$Resource  Repository$Reduced Relative$reciprocal rank$Relational-Realizational$Reference resolution$	5
Experimental results show that EMR can select seed sets that pro- vide significantly higher mean RR on realistic data than existing naive selection methods or random seed sets.	RR	RelationalRealizational$Ranking by regression$reranking$Resource  Repository$Reduced Relative$reciprocal rank$Relational-Realizational$Reference resolution$	5
Consid- ering L2, accuracy, average probability, equal er- ror rate, log probability and mean RR across all components of the the dialog state, these give a total of 318 metrics.	RR	RelationalRealizational$Ranking by regression$reranking$Resource  Repository$Reduced Relative$reciprocal rank$Relational-Realizational$Reference resolution$	5
f 12 , rank deny(s, i, v) = the sum of all the RR of the scores assigned by the SLU to the user denying the value of slot s is v, or 0 if denying v cannot	RR	RelationalRealizational$Ranking by regression$reranking$Resource  Repository$Reduced Relative$reciprocal rank$Relational-Realizational$Reference resolution$	5
Finally, we also have two meta features: (1) is the person answering the question the one who asked it; (2) RR of the comment in the thread.	RR	RelationalRealizational$Ranking by regression$reranking$Resource  Repository$Reduced Relative$reciprocal rank$Relational-Realizational$Reference resolution$	5
f 10 , rank affirm(s, i, v) = the sum of all the RR of the scores assigned by the SLU to the user affirming the value of slot s is v, or 0 if affirming v cannot be found in the SLU n-best list.	RR	RelationalRealizational$Ranking by regression$reranking$Resource  Repository$Reduced Relative$reciprocal rank$Relational-Realizational$Reference resolution$	5
Not surprisingly, the most important turn out to be the TASK FEATURES (contributing over five MAP points) as they handle important informa- tion sources that are not available to the system from other feature groups, e.g., the RR alone contributes about two points.	RR	RelationalRealizational$Ranking by regression$reranking$Resource  Repository$Reduced Relative$reciprocal rank$Relational-Realizational$Reference resolution$	5
We develop the RR approach to pars- ing in which we untangle the projection of grammatical functions and their means of realization to allow for phrase-structure variability and morphological-syntactic in- teraction.	RR	RelationalRealizational$Ranking by regression$reranking$Resource  Repository$Reduced Relative$reciprocal rank$Relational-Realizational$Reference resolution$	6
Through a quantitative and quali- tative analysis we illustrate the advantages of the RR approach and its poten- tial promise for parsing other ?	RR	RelationalRealizational$Ranking by regression$reranking$Resource  Repository$Reduced Relative$reciprocal rank$Relational-Realizational$Reference resolution$	6
Our RR parsing pro- posal, strongly inspired by Relational Grammar (Perlmutter, 1982), takes grammatical relations such as ?	RR	RelationalRealizational$Ranking by regression$reranking$Resource  Repository$Reduced Relative$reciprocal rank$Relational-Realizational$Reference resolution$	6
179  Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 889?896 Manchester, August 2008 RR Parsing Reut Tsarfaty and Khalil Sima?an Institute for Logic, Language and Computation, University of Amsterdam Plantage Muidergracht 24, 1018TV, Amsterdam, The Netherlands {rtsarfat,simaan}@science.uva.nl Abstract State-of-the-art statistical parsing models applied to free word-order languages tend to underperform compared to, e.g., pars- ing English.	RR	RelationalRealizational$Ranking by regression$reranking$Resource  Repository$Reduced Relative$reciprocal rank$Relational-Realizational$Reference resolution$	6
RR Parsing.	RR	RelationalRealizational$Ranking by regression$reranking$Resource  Repository$Reduced Relative$reciprocal rank$Relational-Realizational$Reference resolution$	6
The awareness of the model- ing challenges gave rise to new lines of work on top- ics such as joint morpho-syntactic processing (Gold- berg and Tsarfaty, 2008), RR Parsing (Tsarfaty, 2010), EasyFirst Parsing (Gold- berg, 2011), PLCFRS parsing (Kallmeyer and Maier, 2013), the use of factored lexica (Green et al 2013), the use of bilingual data (Fraser et al 2013), and more developments that are currently under way.	RR	RelationalRealizational$Ranking by regression$reranking$Resource  Repository$Reduced Relative$reciprocal rank$Relational-Realizational$Reference resolution$	6
D. A. DaM and C. N. Ball, "RR in PUN-  DIT," in P. Saint-Dizier and S. Szpakowicz, eds., (	RR	RelationalRealizational$Ranking by regression$reranking$Resource  Repository$Reduced Relative$reciprocal rank$Relational-Realizational$Reference resolution$	7
2.3 Reference Resolution RR identifies the most prob- able antecedent for each anaphor within a text (Hirschman and Chinchor, 1997).	RR	RelationalRealizational$Ranking by regression$reranking$Resource  Repository$Reduced Relative$reciprocal rank$Relational-Realizational$Reference resolution$	7
RR, which in-  volves carrying out a proof that a retbrring ex-  pression denotes, is implemented as part of the  update step.	RR	RelationalRealizational$Ranking by regression$reranking$Resource  Repository$Reduced Relative$reciprocal rank$Relational-Realizational$Reference resolution$	7
RR is now performed using a discourse focus list .	RR	RelationalRealizational$Ranking by regression$reranking$Resource  Repository$Reduced Relative$reciprocal rank$Relational-Realizational$Reference resolution$	7
RR within the framework of cog- nitive grammar.	RR	RelationalRealizational$Ranking by regression$reranking$Resource  Repository$Reduced Relative$reciprocal rank$Relational-Realizational$Reference resolution$	7
RR within IE, however,  cannot operate only on those parts that describe  target information because anaphoric expressions  within target linguistic patterns may have an-  tecedents outside of the target, and those that oc-  cur in an apparently irrelevant pattern may ac-  tually resolve to target entities.	RR	RelationalRealizational$Ranking by regression$reranking$Resource  Repository$Reduced Relative$reciprocal rank$Relational-Realizational$Reference resolution$	7
solution for SVM training and  classification - LIBSVM6 (Fan et al, 2005) with  6 http://www.csie.ntu.edu.tw/~cjlin/libsvm/ the default parameters (C-SVC classification and RBF).	RBF	radial basis kernel function$radial basis functions$Radial Basis kernel function$radial basis function$	0
6.3 Learning Parameters  For SVR, the RBF is em- ployed and the optimal values for parameters C,  v and g (for the kernel) are found using the gri- dregression.py tool provided by LibSVM  (Chang and Lin, 2001) with a 5-fold cross vali- dation on the training set.	RBF	radial basis kernel function$radial basis functions$Radial Basis kernel function$radial basis function$	0
We have thus far presented results using  mixture Gaussian models, but are now experimenting with  other types of models and discriminators including multi-  layer perceptrons and RBF.	RBF	radial basis kernel function$radial basis functions$Radial Basis kernel function$radial basis function$	1
Ney \[11\] has found strong  similarities between RBF and mixture  densities using Gaussians with diagonal covariances.	RBF	radial basis kernel function$radial basis functions$Radial Basis kernel function$radial basis function$	1
5.1 Method We adopted the method of Georgescul (2010): Sup- port Vector Machine classification based on a Gaus- sian RBF (Vapnik, 1998; Fan et al, 2005), employing n-grams from annotated cue phrases as features, as described in more detail be- low.	RBF	radial basis kernel function$radial basis functions$Radial Basis kernel function$radial basis function$	2
The gain is the same for 3https://github.com/lspecia/QualityEstimation/blob/master/ test set.tar.gz 4available at https://github.com/lspecia/QualityEstimation/ blob/master/test set.likert 117 the optimisation of the RBF param- eters.	RBF	radial basis kernel function$radial basis functions$Radial Basis kernel function$radial basis function$	3
SVM (Chang and Lin, 2001): cache size = 40; cost = 1; degree = 3; eps = 0.001; loss = 0.1; kernel type = RBF; nu = 0.5; seed = 1.	RBF	radial basis kernel function$radial basis functions$Radial Basis kernel function$radial basis function$	3
RBF or Gaussian), k(x,x?)	RBF	radial basis kernel function$radial basis functions$Radial Basis kernel function$radial basis function$	3
Baselines: The baselines use the SVM regres- sion algorithm with RBF kernel and parameters ?,	RBF	radial basis kernel function$radial basis functions$Radial Basis kernel function$radial basis function$	3
We experimented two ker- nels: linear, and RBF.	RBF	radial basis kernel function$radial basis functions$Radial Basis kernel function$radial basis function$	3
We experiment with two kernels: linear and RBF.	RBF	radial basis kernel function$radial basis functions$Radial Basis kernel function$radial basis function$	3
The recognition of ORG was much better in this article: R 64%, P 80%.	ORG	organisations$Organization$organizatio$organization$	0
The best performing subtask was the entity slot, particularly where the task required the extraction of ORG and persons.	ORG	organisations$Organization$organizatio$organization$	0
The training corpora are processed with a part- of-speech tagger and a module for Named Entity Recognition and Classification (NERC) that anno- tates people, ORG, locations, dates, rela- tive temporal expressions and numbers (Alfonseca et al, 2006b), so this information can be included in the patterns.	ORG	organisations$Organization$organizatio$organization$	0
Views expressed in this publication cannot be ascribed to any of these funding ORG.	ORG	organisations$Organization$organizatio$organization$	0
The additions include names of major USA institutions and ORG (e.g., government departments), names of newspapers, names of major geographical locations in the USA, US states abbreviations and names of countries and nationalities of the world.	ORG	organisations$Organization$organizatio$organization$	0
The UN and Europe were pan national ORG CHUNK the honeymoon is over VARIANT the honey moon period is over The shortest post-election honeymoon is over.	ORG	organisations$Organization$organizatio$organization$	0
N Table 3: Syllable ORG for the       Logistic Regression Model 4.1 Logistic Regression Model for Combining Syllables The model to combine syllables is built upon Binary Logistic Regression whose answers are either combine or not combine.	ORG	organisations$Organization$organizatio$organization$	1
I would like a higher optical zoom, the W200 does a great digital zoom translation... Table 3: Opinion ORG Result for Sony Cybershot DSC-W200 Camera listed in Freebase, but we can find it in people?s online discussion using mutual information.	ORG	organisations$Organization$organizatio$organization$	1
Database Center for Life Science, Research ORG of Information and System, Japan ?	ORG	organisations$Organization$organizatio$organization$	1
Opinion ORG: Table 2 and Table 3 present sample results for President Ronald Rea- gan and Sony Cybershot DSC-W200 camera re- spectively5.	ORG	organisations$Organization$organizatio$organization$	1
Table 2: Opinion ORG Result for President Ronald Reagan FreeBase Aspects Supt Representative Opinion Sentences Format: 13 Quality pictures in a compact package.	ORG	organisations$Organization$organizatio$organization$	1
Topic Detection and Tracking: Event- based Information ORG, Kluwer Academic  Publishers, 2002.	ORG	organisations$Organization$organizatio$organization$	1
in news or  transcripts (BBN 2001), or for other information  ORGn tasks that might benefit from precise  knowledge of how names occur, such as Topic  Detection and Tracking (Allan 2002).	ORG	organisations$Organization$organizatio$organization$	2
Help desk systems in large ORGns: Help desk staff in large ORGns need to quickly satisfy the customer?s need for information.	ORG	organisations$Organization$organizatio$organization$	2
Evaluation       Given a collection of named entities from  documents, the coreferencing task is to put them into  equivalence classes, where every mention in the same  class refers to the same entity (person, location,  ORGn, and so on).	ORG	organisations$Organization$organizatio$organization$	2
A third category contains texts with uncertain provenance, e.g. the texts from various European Union ORGns or from Wikipedia.	ORG	organisations$Organization$organizatio$organization$	2
In providing the fundamental ORGn of the gram- 983 mar, to the extent that that ORGn is consistent with the language modeled, these types significantly ease the path to creating a working grammar.	ORG	organisations$Organization$organizatio$organization$	2
in news or  transcripts (BBN 2001), or for other information  ORG tasks that might benefit from precise  knowledge of how names occur, such as Topic  Detection and Tracking (Allan 2002).	ORG	organisations$Organization$organizatio$organization$	3
Help desk systems in large ORGs: Help desk staff in large ORGs need to quickly satisfy the customer?s need for information.	ORG	organisations$Organization$organizatio$organization$	3
Evaluation       Given a collection of named entities from  documents, the coreferencing task is to put them into  equivalence classes, where every mention in the same  class refers to the same entity (person, location,  ORG, and so on).	ORG	organisations$Organization$organizatio$organization$	3
A third category contains texts with uncertain provenance, e.g. the texts from various European Union ORGs or from Wikipedia.	ORG	organisations$Organization$organizatio$organization$	3
In providing the fundamental ORG of the gram- 983 mar, to the extent that that ORG is consistent with the language modeled, these types significantly ease the path to creating a working grammar.	ORG	organisations$Organization$organizatio$organization$	3
Specifically, we show how Discourse Representation Structures can be combined with distribu- tional models for word meaning inside a MLN and used to successfully perform inferences that take advantage of logical concepts such as factivity as well as probabilistic informa- tion on word meaning in context.	MLN	Markov Logic Network$Markov Logic Networks$Markov logic networks$	0
2 MLNs and Related Work Markov logic [14, 4] is a recently developed theoretically sound framework for combining first-order logic and probabilistic graphical models.	MLN	Markov Logic Network$Markov Logic Networks$Markov logic networks$	0
Coref (x , y) Table 2: Rules used in the MLN noted that, because we are assuming an order on the arguments of Coref (x , y), we need three formulae to capture transivity relationships.	MLN	Markov Logic Network$Markov Logic Networks$Markov logic networks$	0
Subse- quently, we will describe Markov Logic (section 4) and our MLN for event ex- !"# !"	MLN	Markov Logic Network$Markov Logic Networks$Markov logic networks$	0
Compared to other structured prediction frameworks such as MLNs (Poon and Vanderwende, 2010), SEARN provides high modeling flexibility but it does not requiring task- dependent approximate inference.	MLN	Markov Logic Network$Markov Logic Networks$Markov logic networks$	0
Distributional similarity between pairs of words is converted into weighted inference rules that are added to the logical repre- sentation, and MLNs are used to perform probabilistic logical inference.	MLN	Markov Logic Network$Markov Logic Networks$Markov logic networks$	0
The systems use a combination of deep semantic parsing, MLN and Conditional Random Field classifiers.	MLN	Markov Logic Network$Markov Logic Networks$Markov logic networks$	1
2 MLN and Related Work Markov logic [14, 4] is a recently developed theoretically sound framework for combining first-order logic and probabilistic graphical models.	MLN	Markov Logic Network$Markov Logic Networks$Markov logic networks$	1
To model the reciprocal relationship be- tween scope assignment and disambiguation, we propose a latent variables based approach using MLN that allows us to learn the parameters for the scope assignment and the disambiguation tasks jointly and enables us to per- form joint inference.	MLN	Markov Logic Network$Markov Logic Networks$Markov logic networks$	1
There has also been a revival in using weighted logical forms in structured relational learning, such as MLN (Domingos and Kok, 2005), and this is related to the scoring of facts used by the current system in merging texts.	MLN	Markov Logic Network$Markov Logic Networks$Markov logic networks$	1
Compared to other structured prediction frameworks such as MLN (Poon and Vanderwende, 2010), SEARN provides high modeling flexibility but it does not requiring task- dependent approximate inference.	MLN	Markov Logic Network$Markov Logic Networks$Markov logic networks$	1
Distributional similarity between pairs of words is converted into weighted inference rules that are added to the logical repre- sentation, and MLN are used to perform probabilistic logical inference.	MLN	Markov Logic Network$Markov Logic Networks$Markov logic networks$	1
MLN.	MLN	Markov Logic Network$Markov Logic Networks$Markov logic networks$	2
Systems have also tried to take advantage of more global in- formation to ensure that the pair-wise classifications satisfy temporal logic transitivity constraints, using frameworks such as integer linear programming and MLN (Bramsen et al, 2006; Cham- bers and Jurafsky, 2008; Yoshikawa et al, 2009; Uz- Zaman and Allen, 2010).	MLN	Markov Logic Network$Markov Logic Networks$Markov logic networks$	2
The bilingual parallel corpus that we used was distributed as part of the 2008 NIST OpenMT Workshop.4 The training set contained 88,108 Urdu?English sentence pairs, and a bilingual dictionary with 113,911 entries.	OpenMT	Open Machine Translation Evaluation$Open Machine Translation$open machine translation$	0
In NIST OpenMT Meeting.	OpenMT	Open Machine Translation Evaluation$Open Machine Translation$open machine translation$	0
52008 NIST OpenMT: Chi- nese to English Task.	OpenMT	Open Machine Translation Evaluation$Open Machine Translation$open machine translation$	0
The bilingual parallel corpus that we used was distributed as part of the 2008 NIST OpenMT Evaluation Workshop.4 The training set contained 88,108 Urdu?English sentence pairs, and a bilingual dictionary with 113,911 entries.	OpenMT	Open Machine Translation Evaluation$Open Machine Translation$open machine translation$	1
The 2008 NIST OpenMT  Evaluation.	OpenMT	Open Machine Translation Evaluation$Open Machine Translation$open machine translation$	1
In NIST OpenMT Evaluation Meeting.	OpenMT	Open Machine Translation Evaluation$Open Machine Translation$open machine translation$	1
Pro- ceedings of the 2008 NIST OpenMT Evaluation Workshop.	OpenMT	Open Machine Translation Evaluation$Open Machine Translation$open machine translation$	1
NIST OpenMT 2009 Evaluation (MT09).	OpenMT	Open Machine Translation Evaluation$Open Machine Translation$open machine translation$	1
OpenMT 2008 Evalua- tion.	OpenMT	Open Machine Translation Evaluation$Open Machine Translation$open machine translation$	1
For the work that we are reporting, we have adopted the InterlinguaPlus approach using the Carabao OpenMT framework (Berman, 2012).	OpenMT	Open Machine Translation Evaluation$Open Machine Translation$open machine translation$	2
The next section discusses the implementation of the system using Carabao?s OpenMT framework and the results obtained.	OpenMT	Open Machine Translation Evaluation$Open Machine Translation$open machine translation$	2
FL Function which describes how  an utterance constraints the future beliefs and  actions of the participants, and affects the dis-  course, and  ?	FL	Forward Looking$Flagging$	0
The Main Topic may be regarded the FL Center in the centering terminology or the Current Focus.	FL	Forward Looking$Flagging$	0
Backward Looking/FL Fea- tures.	FL	Forward Looking$Flagging$	0
FL such semantic mismatches between verbs and arguments is the task of preference vio- lation detection.	FL	Forward Looking$Flagging$	1
FL individual elements Previous work scored only entire rules, but some dependencies are problematic and others are not.	FL	Forward Looking$Flagging$	1
The Dev-5k and Dev columns report labeled AS on the development sets.	AS	attachment score$Automatic Summarization$Application server$Academia Sinica$Academia Sinica Corpus$Automatic Summarisation$	0
5 75.73 75.73 77.67 77.67 Table 1: Labeled AS per phase compared to default settings for all training sets from the Shared Task on PMRLs in the gold scenario on the held-out test set for optimization.	AS	attachment score$Automatic Summarization$Application server$Academia Sinica$Academia Sinica Corpus$Automatic Summarisation$	0
The unlabeled ASs of the converted dependencies are shown as the accuracies in Table 5, since most bunsetsu-based dependency parsers out- put only unlabeled structure.	AS	attachment score$Automatic Summarization$Application server$Academia Sinica$Academia Sinica Corpus$Automatic Summarisation$	0
79.98 83.59 86.96 Hebrew 76.78 76.80 79.37 80.17 3.39 79.83 79.83 76.61 76.61 80.03 80.03 Hungarian 70.37 71.11 71.98 81.91 11.54 80.69 80.74 71.27 72.34 82.37 83.14 Korean 87.22 87.22 87.22 88.94 1.72 86.52 90.20 81.69 88.43 83.74 89.39 Polish 75.52 75.58 79.28 80.27 4.75 81.58 81.91 76.64 77.70 79.79 80.49 Swedish 76.75 76.75 78.91 79.76 3.01 74.85 74.85 75.73 75.73 77.67 77.67 Table 1: Labeled AS per phase compared to default settings for all training sets from the Shared Task on PMRLs in the gold scenario on the held-out test set for optimization.	AS	attachment score$Automatic Summarization$Application server$Academia Sinica$Academia Sinica Corpus$Automatic Summarisation$	0
Benefiting from the rich features selected in the tree kernel space, our model achieved the best reported unlabeled AS of 93.72 without using any additional resource.	AS	attachment score$Automatic Summarization$Application server$Academia Sinica$Academia Sinica Corpus$Automatic Summarisation$	0
77.81 79.22 82.75 Hebrew 76.29 76.31 79.01 79.67 3.38 73.40 73.40 69.97 69.97 73.01 73.01 Hungarian 68.26 69.12 69.96 78.71 10.45 76.82 77.62 69.08 70.15 79.00 79.63 Korean 80.08 80.08 80.08 81.63 1.55 77.96 83.02 74.87 82.06 75.90 82.65 Polish 74.43 74.49 76.93 78.41 3.98 80.61 80.83 75.29 75.63 79.50 80.49 Swedish 74.53 74.53 76.51 77.66 3.13 72.90 72.90 73.21 73.21 75.82 75.82 Table 2: Labeled AS per phase compared to default settings for all training sets from the Shared Task on PMRLs in the predicted scenario on the held-out test set for optimization.	AS	attachment score$Automatic Summarization$Application server$Academia Sinica$Academia Sinica Corpus$Automatic Summarisation$	0
The result is promising: these features significantly improved a state-of-the-art third order dependency parser, yielding the best reported unlabeled AS of 93.72 without using any additional resource.	AS	attachment score$Automatic Summarization$Application server$Academia Sinica$Academia Sinica Corpus$Automatic Summarisation$	0
4  Proceedings of the Workshop on AS for Different Genres, Media, and Languages, pages 1?7, Portland, Oregon, June 23, 2011.	AS	attachment score$Automatic Summarization$Application server$Academia Sinica$Academia Sinica Corpus$Automatic Summarisation$	1
Of the Workshop on AS (including DUC 2002), pp 27- 36.	AS	attachment score$Automatic Summarization$Application server$Academia Sinica$Academia Sinica Corpus$Automatic Summarisation$	1
AS.	AS	attachment score$Automatic Summarization$Application server$Academia Sinica$Academia Sinica Corpus$Automatic Summarisation$	1
In Proceedings of the NAACL Workshop on AS, Pitts- burgh, PA.	AS	attachment score$Automatic Summarization$Application server$Academia Sinica$Academia Sinica Corpus$Automatic Summarisation$	1
Of the Workshop on AS (including DUC 2002), pp 1-8.	AS	attachment score$Automatic Summarization$Application server$Academia Sinica$Academia Sinica Corpus$Automatic Summarisation$	1
Chinese NLP resources     Part 2: Text Processing  2.1 Lexical processing   a. Segmentation   b. Disambiguation   c. Unknown word detection   d. Named Entity Recognition  2.2 Syntactic processing   a. Issues in PoS tagging   b. Hidden Markov Models  2.3 NLP Applications  References   AS Balance Corpus of Mandarin Chi- nese.	AS	attachment score$Automatic Summarization$Application server$Academia Sinica$Academia Sinica Corpus$Automatic Summarisation$	3
from the AS Balanced Corpus.	AS	attachment score$Automatic Summarization$Application server$Academia Sinica$Academia Sinica Corpus$Automatic Summarisation$	3
AS?),	AS	attachment score$Automatic Summarization$Application server$Academia Sinica$Academia Sinica Corpus$Automatic Summarisation$	3
Technical Report, Taiwan,  Taipei, AS.	AS	attachment score$Automatic Summarization$Application server$Academia Sinica$Academia Sinica Corpus$Automatic Summarisation$	3
279  Conceptual Metaphors: Ontology-based representation and corpora driven Mapping Principles Kathleen Ahrens National Taiwan University kathleenahrens@yahoo.com Siaw Fong Chung National Taiwan University claricefong6376@hotmail.com Chu-Ren Huang AS churen@sinica.edu.tw Abstract The goal of this paper is to integrate the Conceptual Mapping Model with an on- tology-based knowledge representation (i.e. Suggested Upper Merged Ontology (SUMO)) in order to demonstrate that conceptual metaphor analysis can be re- stricted and eventually, automated.	AS	attachment score$Automatic Summarization$Application server$Academia Sinica$Academia Sinica Corpus$Automatic Summarisation$	3
Institute of Information Science,  AS.	AS	attachment score$Automatic Summarization$Application server$Academia Sinica$Academia Sinica Corpus$Automatic Summarisation$	3
Some Distributional Properties  of Mandarin Chinese-A Study Based on the  AS.	AS	attachment score$Automatic Summarization$Application server$Academia Sinica$Academia Sinica Corpus$Automatic Summarisation$	4
2 Related Work 2.1 AS Work on automatic summarisation dates back more than 50 years, with a focus on the English language (Luhn, 1958).	AS	attachment score$Automatic Summarization$Application server$Academia Sinica$Academia Sinica Corpus$Automatic Summarisation$	5
In Proceedings of the NAACL Workshop on AS, Association for Computational Linguistics, pages 41?49.	AS	attachment score$Automatic Summarization$Application server$Academia Sinica$Academia Sinica Corpus$Automatic Summarisation$	5
in AS: ANLP/NAACL  2000 Workshop, New Brunswick, New Jersey.	AS	attachment score$Automatic Summarization$Application server$Academia Sinica$Academia Sinica Corpus$Automatic Summarisation$	5
2 AS 2.1 Background Much of the previous NLP work in the legal domain con- cerns Information Retrieval (IR) and the computation of simple features such as word frequency.	AS	attachment score$Automatic Summarization$Application server$Academia Sinica$Academia Sinica Corpus$Automatic Summarisation$	5
In Proceedings of the NAAACL Workshop on AS, Pittsburgh, USA, June.	AS	attachment score$Automatic Summarization$Application server$Academia Sinica$Academia Sinica Corpus$Automatic Summarisation$	5
How can we use the discourse information for the evaluation of AS and Readability Assessment?	AS	attachment score$Automatic Summarization$Application server$Academia Sinica$Academia Sinica Corpus$Automatic Summarisation$	5
and ACC object such as ?	ACC	accusative$accusat$ACCOMPANIMENT$Average Clustering Coefficient$	0
Sense id Pattern Synonyms 1 kare(he) ga(nominative) soba(noodles) wo(ACC) kuu (eat) 2 kare (he) ga(nominative) fukugyo(a part-time job) de(ACC) kurasu (live) Table 2: Examples of test verbs and their polysemic gold standard senses Id Sense Verb Classes Id Sense Verb Classes 1 treat {ashirau, atsukau} 11 tell {oshieru, shimesu, shiraseru} 2 prey {negau, inoru} 12 persuade {oshieru, satosu} 3 wish {negau, nozomu} 13 congratulate {iwau, syukufukusuru} 4 ask {negau, tanomu} 14 accept {uketoru, ukeru, morau, osameru} 5 leave {saru,	ACC	accusative$accusat$ACCOMPANIMENT$Average Clustering Coefficient$	0
refers to the co- occurrence pattern between a verb and a noun 33 [Sentence pattern] <word1> ga <word2> wo taberu (eat) [Sense relation] agent object [Case particle] ga (nominative) wo (ACC) [Sense identifier] 30f6b0 (human);30f6bf (animal) 30f6bf(animal);30f6ca(plants); 30f6e5(parts of plants); 3f9639(food and drink); 3f963a(feed) Figure 1: An example of a verb ?	ACC	accusative$accusat$ACCOMPANIMENT$Average Clustering Coefficient$	0
Sense id Pattern Synonyms 1 kare(he) ga(nominative) soba(noodles) wo(ACC) kuu (eat) 2 kare (he) ga(nominative) fukugyo(a part-time job) de(ACC) kurasu (live) Table 2: Examples of test verbs and their polysemic gold standard senses Id Sense Verb Classes Id Sense Verb Classes 1 treat {ashirau, atsukau} 11 tell {oshieru, shimesu, shiraseru} 2 prey {negau, inoru} 12 persuade {oshieru, satosu} 3 wish {negau, nozomu} 13 congratulate {iwau, syukufukusuru} 4	ACC	accusative$accusat$ACCOMPANIMENT$Average Clustering Coefficient$	0
Merlo and Stevenson (2001) report inter-judge  values of 0.53 to 0.66 for a task we consider to be comparable to ours, that of classifying verbs into unergative, unACC and object-drop, and argue that Car- letta?s ?	ACC	accusative$accusat$ACCOMPANIMENT$Average Clustering Coefficient$	0
One is to use several methods 38 [Sentence pattern] <word1> ga <word2> wo ukeireru / yurusu (forgive) [Concept relation] agent object [Case particle] ga (nominative) wo (ACC) [Sense identifier] 0ee0de; 0f58b4; 0f98ee 0f0157; 30f6b0 0ee0de: the part of a something written that makes reference to a particular matter 0f58b4: a generally-held opinion 0f98ee: the people who citizens of a nation 0f0157: a human being 30f6b0: human Figure 4: Extracted Verb frames of ?	ACC	accusative$accusat$ACCOMPANIMENT$Average Clustering Coefficient$	0
and ACCive object such as ?	ACC	accusative$accusat$ACCOMPANIMENT$Average Clustering Coefficient$	1
Sense id Pattern Synonyms 1 kare(he) ga(nominative) soba(noodles) wo(ACCive) kuu (eat) 2 kare (he) ga(nominative) fukugyo(a part-time job) de(ACCive) kurasu (live) Table 2: Examples of test verbs and their polysemic gold standard senses Id Sense Verb Classes Id Sense Verb Classes 1 treat {ashirau, atsukau} 11 tell {oshieru, shimesu, shiraseru} 2 prey {negau, inoru} 12 persuade {oshieru, satosu} 3 wish {negau, nozomu} 13 congratulate {iwau, syukufukusuru} 4 ask {negau, tanomu} 14 accept {uketoru, ukeru, morau, osameru} 5 leave {saru,	ACC	accusative$accusat$ACCOMPANIMENT$Average Clustering Coefficient$	1
refers to the co- occurrence pattern between a verb and a noun 33 [Sentence pattern] <word1> ga <word2> wo taberu (eat) [Sense relation] agent object [Case particle] ga (nominative) wo (ACCive) [Sense identifier] 30f6b0 (human);30f6bf (animal) 30f6bf(animal);30f6ca(plants); 30f6e5(parts of plants); 3f9639(food and drink); 3f963a(feed) Figure 1: An example of a verb ?	ACC	accusative$accusat$ACCOMPANIMENT$Average Clustering Coefficient$	1
Sense id Pattern Synonyms 1 kare(he) ga(nominative) soba(noodles) wo(ACCive) kuu (eat) 2 kare (he) ga(nominative) fukugyo(a part-time job) de(ACCive) kurasu (live) Table 2: Examples of test verbs and their polysemic gold standard senses Id Sense Verb Classes Id Sense Verb Classes 1 treat {ashirau, atsukau} 11 tell {oshieru, shimesu, shiraseru} 2 prey {negau, inoru} 12 persuade {oshieru, satosu} 3 wish {negau, nozomu} 13 congratulate {iwau, syukufukusuru} 4	ACC	accusative$accusat$ACCOMPANIMENT$Average Clustering Coefficient$	1
Merlo and Stevenson (2001) report inter-judge  values of 0.53 to 0.66 for a task we consider to be comparable to ours, that of classifying verbs into unergative, unACCive and object-drop, and argue that Car- letta?s ?	ACC	accusative$accusat$ACCOMPANIMENT$Average Clustering Coefficient$	1
One is to use several methods 38 [Sentence pattern] <word1> ga <word2> wo ukeireru / yurusu (forgive) [Concept relation] agent object [Case particle] ga (nominative) wo (ACCive) [Sense identifier] 0ee0de; 0f58b4; 0f98ee 0f0157; 30f6b0 0ee0de: the part of a something written that makes reference to a particular matter 0f58b4: a generally-held opinion 0f98ee: the people who citizens of a nation 0f0157: a human being 30f6b0: human Figure 4: Extracted Verb frames of ?	ACC	accusative$accusat$ACCOMPANIMENT$Average Clustering Coefficient$	1
in the GEN case  (g), while the case (z1) in the first pair is ?	GEN	genitive$gender$Genitive$genetics$	0
Namely,  as indicated previously (see Table 2), the major- ity of nested terms in Serbian are in GEN case,  which means that the termhood for a term candi- date in GEN case would differ significantly  from its counterparts in other cases.	GEN	genitive$gender$Genitive$genetics$	0
preposition + noun  113  e.g. /bar/ + /as?s-e/   on   +   basis  /e/ an obligatory GEN ending,  2.	GEN	genitive$gender$Genitive$genetics$	0
Some Polish numerals have forms that agree in case with noun (marked congr), as well as forms that require a noun in GEN case (marked rec): (2) Przyszli came dwaj two-nom.congr ch?opcy.	GEN	genitive$gender$Genitive$genetics$	0
As a rule, the fro- zen part is in GEN.	GEN	genitive$gender$Genitive$genetics$	0
We incorpo-  rate multiple anaphora resolution factors into  a statistical framework - -  specifically the dis-  tance between the pronoun and the proposed  antecedent, GEN/number/animaticity of the  proposed antecedent, governing head informa-  tion and noun phrase repetition.	GEN	genitive$gender$Genitive$genetics$	1
In particular, the scheme infers the GEN of a  referent from the GEN of the pronouns that  161  refer to it and selects referents using the pro-  noun anaphora program.	GEN	genitive$gender$Genitive$genetics$	1
The second half of the paper describes a  method for using (portions of) t~e aforemen-  tioned program to learn automatically the typi-  cal GEN of English words, information that is  itself used in the pronoun resolution program.	GEN	genitive$gender$Genitive$genetics$	1
The second  experiment investigates a method for unsuper-  vised learning of GEN/number/animaticity  information.	GEN	genitive$gender$Genitive$genetics$	1
Next, the actual words in a proposed noun-  phrase antecedent give us information regarding  the GEN, number, and animaticity of the pro-  posed referent.	GEN	genitive$gender$Genitive$genetics$	1
The first set (ANERGaz) pro- posed by (Benajiba and Rosso, 2008), which 80 Feature Feature Values Aspect Verb aspect: Command, Imperfective, Perfective, Not applicable Case Grammatical case: Nominative, Accusative, GEN, Not applicable, Undefined Gender Nominal Gender: Feminine, Masculine, Not applicable Mood Grammatical mood: Indicative, Jussive, Subjunctive, Not applicable, Undefined Number Grammatical number: Singular, Plural, Dual, Not applicable, Undefined Person Person Information: 1st, 2nd, 3rd, Not applicable State Grammatical state: Indefinite, Definite, Construct/Poss/Idafa, Not applicable, Und	GEN	genitive$gender$Genitive$genetics$	2
(i) Nuori pitk~ vieh~tt~v~ tytt6  Young tall charming girl  GEN attributes, themselves nominals~ im~dify  head nominals recursively, as in the phrase (2).	GEN	genitive$gender$Genitive$genetics$	2
On the other hand, for such  a noun as professor 'professor' GEN con.	GEN	genitive$gender$Genitive$genetics$	2
Example  2 makes use of the domain f)red  icate: it is the predicate implied by a lexico-  gr~l)hic definition of a noun th;Lt deterinine, ill very inany eases, the exact interpretation f the  GEN construction with a concrete noun ~  a heard.	GEN	genitive$gender$Genitive$genetics$	2
 Z  (someone) ` Z Z (which)   Adverbial kaf pro  (AKP)  }$ (where) | Z  (when) ? Z (how)    GEN reflexive  (GR)  >? (my)    GENs (G)   (my) 	  Z (your)  Z 	 (our)   Z (your)   Verb (VB)  >?" (	GEN	genitive$gender$Genitive$genetics$	2
The forms of the GEN case of the two words are g6da and g6ta:  syllabically go-da, go-ta, morphologically god-a, got-a.	GEN	genitive$gender$Genitive$genetics$	2
Unlike in phyloGEN and graphical models, where a single latent tree is constructed for all the data, in our case, each part of speech sequence is associated with its own parse tree.	GEN	genitive$gender$Genitive$genetics$	3
In particular we leverage the con- cept of additive tree metrics (Buneman, 1971; Buneman, 1974) in phyloGEN and machine learning that can create a special distance met- ric among the observed variables as a function of the underlying spectral dependencies (Choi et al.,	GEN	genitive$gender$Genitive$genetics$	3
in phyloGEN).	GEN	genitive$gender$Genitive$genetics$	3
These ten  were chosen as recent documents as of early March  2012 and which contained the text word maize and  discussed GEN.	GEN	genitive$gender$Genitive$genetics$	3
It is an adaptive heuristic search algorithm based on the evolutionary ideas of natural selec- tion and GEN.	GEN	genitive$gender$Genitive$genetics$	3
Unlike in phyloGEN and graphical models, where a single latent tree is constructed for all the data, in our case, each part of speech sequence is associated with its own par	GEN	genitive$gender$Genitive$genetics$	3
This implicit reporting of results expressed in terms of negative experimental outcomes is very common in molecular biology and GEN.	GEN	genitive$gender$Genitive$genetics$	3
PCFG with latent annotations.	PCFG	Probabilistic  Context-Free Grammar$Probabilistic CFG$probabilistic CFG$probabilistic context free grammar$	1
Simple PCFGs dictate that, given this information,  "determiner noun" should be the most likely interpretation of a  noun phrase.	PCFG	Probabilistic  Context-Free Grammar$Probabilistic CFG$probabilistic CFG$probabilistic context free grammar$	2
Supervised training for PCFGs requires parsed cor-  pora.,	PCFG	Probabilistic  Context-Free Grammar$Probabilistic CFG$probabilistic CFG$probabilistic context free grammar$	2
of Scores 657 60%  Figure 3: Search Space Reduction and Accuracy for 1,bur Probabilistic  Models  a simple PCFG model, the parser produced a much  lower accuracy rate (35%).	PCFG	Probabilistic  Context-Free Grammar$Probabilistic CFG$probabilistic CFG$probabilistic context free grammar$	2
They parse raw text into LFG f-structures by first parsing with a PCFG parser to choose the most proba- ble c-structure.	PCFG	Probabilistic  Context-Free Grammar$Probabilistic CFG$probabilistic CFG$probabilistic context free grammar$	2
For example, probabilities were de- fined over grammar rules in PCFG (Collins, 1999; Klein and Manning, 2003; Char- niak and Johnson, 2005) or over complex phrase structures of head-driven phrase structure gram- mar (HPSG) or combinatory categorial grammar (CCG) (Clark and Curran, 2004b; Malouf and van Noord, 2004; Miyao and Tsujii, 2005).	PCFG	Probabilistic  Context-Free Grammar$Probabilistic CFG$probabilistic CFG$probabilistic context free grammar$	2
The search space for CFG with CSP was 4 to 5  times lower than the simple PCFG.	PCFG	Probabilistic  Context-Free Grammar$Probabilistic CFG$probabilistic CFG$probabilistic context free grammar$	2
Simple PCFGs provide generalinformation about  how likely a construct is going to appear anywhere in a sam-  ple of a language.	PCFG	Probabilistic  Context-Free Grammar$Probabilistic CFG$probabilistic CFG$probabilistic context free grammar$	2
ONYX contains a number of innovative ideas including a novel adaptation of Kay's (1980) parse algorithm; a symbolic language extended to include probabilis-tic and procedural elements; an integration of syn-tax and semantics that includes a semantically weighted PCFG and interpretation based both on a semantic network and a semantic grammar.	PCFG	Probabilistic  Context-Free Grammar$Probabilistic CFG$probabilistic CFG$probabilistic context free grammar$	3
In our implementation, we can use either a PCFG, or a lexicalized context free grammar which condi- tions rules on parent category and parent lexical head, and conditions the heads of non-head chil- dren on child category, parent category, and par- ent head (Eisner, 1997; Charniak, 1995; Carroll and Rooth, 1998).	PCFG	Probabilistic  Context-Free Grammar$Probabilistic CFG$probabilistic CFG$probabilistic context free grammar$	3
While these models are well suited for the effective handling of highly divergent sentential word orders, the above frameworks have a lim- itation shared with PCFGs that the preferred ordering of subtrees is insufficiently constrained by their embedding context, which is espe- cially problematic for very deep syntactic parses.	PCFG	Probabilistic  Context-Free Grammar$Probabilistic CFG$probabilistic CFG$probabilistic context free grammar$	3
To generate such a  semantic frame, words in the utterance are usually  aligned to a semantic tree by a parsing algorithm such  as a PCFG or a recursive  network whose nodes represent semantic symbols of  the words and arcs consist of transition probabilities.	PCFG	Probabilistic  Context-Free Grammar$Probabilistic CFG$probabilistic CFG$probabilistic context free grammar$	3
Basic methods  of PCFGs."	PCFG	Probabilistic  Context-Free Grammar$Probabilistic CFG$probabilistic CFG$probabilistic context free grammar$	3
Ba- sic methods of PCFGs.	PCFG	Probabilistic  Context-Free Grammar$Probabilistic CFG$probabilistic CFG$probabilistic context free grammar$	3
Coreference as the  Foundations for Link Analysis Over Free Text  DBs.	DB	Database$data storage$database$DialogBot$	0
In Christiane Fellbaum, editor, WordNet: An Electronic Lexical DB, Language, Speech, and Communication.	DB	Database$data storage$database$DialogBot$	0
DB Center for Life Science, Research Organization of Information and System, Japan ?	DB	Database$data storage$database$DialogBot$	0
Responses from a Portable Natural  Language DB Qury System, in  Brady,M.,Berwick,R.C.(eds): Computational Models of  Discourse, MIT Press(1983).	DB	Database$data storage$database$DialogBot$	0
Fellbaum C., WordNet: an Electionic Lexical  DB, Cambridge, MA, The MIT Press.	DB	Database$data storage$database$DialogBot$	0
Euro WordNet: A Multilingual DB with Lexical Semantic Networks.	DB	Database$data storage$database$DialogBot$	0
This introduces an addi-  tional data type with its own management principles for  DB, retrieval and update.	DB	Database$data storage$database$DialogBot$	1
As regards DB, all recorded information is permanently stored in a MySQL database.	DB	Database$data storage$database$DialogBot$	1
We have modularized the feature gen- eration, instance representation, DB for- mats, and algorithm implementations; this allows users to make seamless transitions along any of these dimensions with minimal effort.	DB	Database$data storage$database$DialogBot$	1
The system is designed to separate cleanly low-level tasks such as DB, data visualisation, location and loading of com- ponents and execution of processes from the data structures and algorithms that actu- ally process human language.	DB	Database$data storage$database$DialogBot$	1
When considering time-critical BKB  applications, uch as the BKB within a machine trans-  lation system, it is clear that efficient DB  techniques arc needed.	DB	Database$data storage$database$DialogBot$	1
--9--  b. The DB for each entry of the glossary  is of variable length, since the lists of dependents,  governing probabilities, hypernyms and semantic classes  associated with the entries are of variable length.	DB	Database$data storage$database$DialogBot$	1
For com- puting the counts of positive and negative words (Feature 15 and 16) we used the General Inquirer DB (Stone et al.,	DB	Database$data storage$database$DialogBot$	2
The story-independent resources that we used are: (a) the U.S. Social Security Administration baby name DB (Security, 2014), in which person names are linked with gender and (b) a large name-gender association list developed us- ing a corpus-based bootstrapping approach, which even included the estimated gender for non-person entities (Bergsma and Lin, 2006).	DB	Database$data storage$database$DialogBot$	2
U.S. social security adminis- tration baby name DB.	DB	Database$data storage$database$DialogBot$	2
This DB is  complemented by an intelligent search procedure  able to handle multimedia representations of  knowledge.	DB	Database$data storage$database$DialogBot$	2
In order to accomplish this goal, we suggest a  multimedia DB with powerful indexing and  classification functions. (	DB	Database$data storage$database$DialogBot$	2
The result is a DB that contains a set  of frames (related through hierarchy and  composition), a set of frame elements for each frame,  and a set of frame annotated sentences that covers the  different patterns of usage for lexical units in the  frame.	DB	Database$data storage$database$DialogBot$	2
ning of spatial language, we next discuss two agents that play the game: ListenerBot (Section 4) makes decisions us- ing a single-agent POMDP that does not take into account the beliefs or actions of its partner, whereas DB (Section 5) maintains a model of its part- ner?s beliefs.	DB	Database$data storage$database$DialogBot$	3
At each time step, DB marginal- izes out the possible observations o?	DB	Database$data storage$database$DialogBot$	3
To decide when and how to speak, DB maintains a dis- tribution over its partner?s beliefs and reasons about the effects his utterances will have on those beliefs.	DB	Database$data storage$database$DialogBot$	3
5 DB We now introduce DB, a Cards agent which is capable of producing linguistic advice.	DB	Database$data storage$database$DialogBot$	3
(c) DB POMDP Figure 3: The decision diagram for the ListenerBot POMDP, the full Dec-POMDP, and the DB ap- proximation POMDP.	DB	Database$data storage$database$DialogBot$	3
Us- ing this task and a model of the meaning of spatial language, we next discuss two agents that play the game: ListenerBot (Section 4) makes decisions us- ing a single-agent POMDP that does not take into account the beliefs or actions of its partner, whereas DB (Section 5) maintains a model of its part- ner?s beliefs.	DB	Database$data storage$database$DialogBot$	3
To handle these complexities, DB models the world as a Decentralized Partially Observable Markov Decision Process (Dec-POM	DB	Database$data storage$database$DialogBot$	3
As a result of the cooperative structure of the underlying model and the effects of commu- nication within it, DB?s contributions are rel- evant, truthful, and informative, which leads to sig- nificantly improved task performance.	DB	Database$data storage$database$DialogBot$	3
ML estimate of the probability distributions, i.e. the relative frequency in the training data.	ML	Mean logprob$Maximum Likelihood$maximum likelihoo$Merged languages$machine learning$minimum length$Machine Learning$	1
2.2.2 ML Used in earlier models (Och and Ney, 2002), the likelihood criterion is defined as the likelihood of an oracle hypothesis e(t)k? ,	ML	Mean logprob$Maximum Likelihood$maximum likelihoo$Merged languages$machine learning$minimum length$Machine Learning$	1
In training, the CRF model is built with labeled  data and by means of an iterative algorithm based  on ML Estimation.	ML	Mean logprob$Maximum Likelihood$maximum likelihoo$Merged languages$machine learning$minimum length$Machine Learning$	1
ML from Incomplete Data via  the EM Algorithm?,	ML	Mean logprob$Maximum Likelihood$maximum likelihoo$Merged languages$machine learning$minimum length$Machine Learning$	1
Bahl, L. R., Jelinek, E, and Mercer, R. A ML  Approach to Continuous Speech Recognition.	ML	Mean logprob$Maximum Likelihood$maximum likelihoo$Merged languages$machine learning$minimum length$Machine Learning$	1
VM model applied to RRR dataset (RRR-basic experiment) and the same experiment applied to TB2 dataset (TB2- 278 Description Accuracy Data Extra Supervision Always noun 55.0 RRR Most likely for each P 72.19 RRR Most likely for each P 72.30 TB2 Most likely for each P 81.73 FN Average human, headwords (Ratnaparkhi et al, 1994) 88.2 RRR Average human, whole sentence (Ratnaparkhi et al, 1994) 93.2 RRR ML-based (Hindle and Rooth, 1993) 79.7 AP Maximum entropy, words (Ratnaparkhi et al, 1994) 77.7 RRR Maximum entropy, words & classes (Ratnaparkhi et al, 1994) 81.6 RRR Decision trees (Ratnaparkhi et al, 1994) 77.7 RRR Transformation-Based Learning (Brill and Resnik, 1994) 81.8 WordNet Maximum-Likelihood based (Collins and Brooks, 1995) 84.5 RRR Maximum-Likelihood based (Collins and	ML	Mean logprob$Maximum Likelihood$maximum likelihoo$Merged languages$machine learning$minimum length$Machine Learning$	1
In each case 7 the final estimate is  e----Ale1 + (1 - &l)(A2e2 + (1 - &2)ea)  where ex, e2 and e3 are MLd esti-  mates with the context at levels 1, 2 and 3 in the  table, and ,kl, ,k2 and )~3 are smoothing parameters  where 0 _< ,ki _< 1.	ML	Mean logprob$Maximum Likelihood$maximum likelihoo$Merged languages$machine learning$minimum length$Machine Learning$	2
15) We use the expectation maximization algorithm (Dempster et al, 1977) for the MLd estimate of ?	ML	Mean logprob$Maximum Likelihood$maximum likelihoo$Merged languages$machine learning$minimum length$Machine Learning$	2
This is effectively a comparison of the MLd estimates of/)(pll ,  nl ) and P(PI(}, v), a  different measure from the backed-off estimate which gives i5(lIv,p , nl).	ML	Mean logprob$Maximum Likelihood$maximum likelihoo$Merged languages$machine learning$minimum length$Machine Learning$	2
Spectral conversion based on MLd es- timation considering global variance of converted pa- rameter.	ML	Mean logprob$Maximum Likelihood$maximum likelihoo$Merged languages$machine learning$minimum length$Machine Learning$	2
Most probability models for DOP use the relative frequency estimator to estimate fragment probabilities, although Bod (2000b) trains fragment probabilities by a MLd reestimation procedure belonging to the class of expectation-maximization algorithms.	ML	Mean logprob$Maximum Likelihood$maximum likelihoo$Merged languages$machine learning$minimum length$Machine Learning$	2
Counts of lower order tuples can also  be made-  for example f(1, P = from) is the number of times (P = from) is seen with noun attach-  ment in training data, f (V  = is, N2 = research) is the number of times (V = is, N2 = research)  is seen with either attachment and any value of N1 and P.  29  3.2 Max imum L ike l ihood  Es t imat ion   A MLd method would use the training data to give the following estimation for the  conditional probability:  l~(l\[v, nl,p, n2)= f(1,v, nl,p, n2)  f(v, nl, p, n2)  Unfortunately sparse data problems make this estimate useless.	ML	Mean logprob$Maximum Likelihood$maximum likelihoo$Merged languages$machine learning$minimum length$Machine Learning$	2
He has developed several ML based natural language  processing systems that are widely used in the computational linguistics community and  in industry and has presented invited talks and tutorials in several major conferences.	ML	Mean logprob$Maximum Likelihood$maximum likelihoo$Merged languages$machine learning$minimum length$Machine Learning$	4
2013) use a su- pervised ML approach to address the same problem, though many of their preliminary steps and input features are similar to those used in (Elson and McKeown, 2010).	ML	Mean logprob$Maximum Likelihood$maximum likelihoo$Merged languages$machine learning$minimum length$Machine Learning$	4
Roth has published broadly in ML, natural language processing,  knowledge representation and reasoning and received several paper, teaching and  research awards.	ML	Mean logprob$Maximum Likelihood$maximum likelihoo$Merged languages$machine learning$minimum length$Machine Learning$	4
As these expressions are sparsely scattered throughout the texts, it is not easy to gen- eralize results of ML from a training set to a test set.	ML	Mean logprob$Maximum Likelihood$maximum likelihoo$Merged languages$machine learning$minimum length$Machine Learning$	4
The proposed methodology has a strong hybrid char- acter in that it employs different approaches that range from pattern-based to ML- based to the incorporation of external knowledge resources.	ML	Mean logprob$Maximum Likelihood$maximum likelihoo$Merged languages$machine learning$minimum length$Machine Learning$	4
He published several papers in natural language  processing, ML and semantic interpretation.	ML	Mean logprob$Maximum Likelihood$maximum likelihoo$Merged languages$machine learning$minimum length$Machine Learning$	4
In this task, we use a dataset consisting of a subset of the Wall Street Journal (WSJ) corpus, in which the ML of a text is 20 sentences, and the average length is 41 sentences.	ML	Mean logprob$Maximum Likelihood$maximum likelihoo$Merged languages$machine learning$minimum length$Machine Learning$	5
Minimum length of component: Specifies the ML of potential compound elements;  shorter substrings are excluded to avoid accidental matching of very short words.	ML	Mean logprob$Maximum Likelihood$maximum likelihoo$Merged languages$machine learning$minimum length$Machine Learning$	5
The risk of false positives is managed by making incorporating a fingerprint of the n-gram, and by making bit vectors longer than the ML required to store values.	ML	Mean logprob$Maximum Likelihood$maximum likelihoo$Merged languages$machine learning$minimum length$Machine Learning$	5
During processing we filtered out documents whose top domain scores were below a previously set minimum threshold and those whose document length was below a ML.	ML	Mean logprob$Maximum Likelihood$maximum likelihoo$Merged languages$machine learning$minimum length$Machine Learning$	5
This is because the ML of a Chi- nese common string is 2 characters and each  has 16 binary bits.	ML	Mean logprob$Maximum Likelihood$maximum likelihoo$Merged languages$machine learning$minimum length$Machine Learning$	5
When  the ML path between two concepts is  discovered the propositions standing on i t  are  returned as being relevant to the current input.	ML	Mean logprob$Maximum Likelihood$maximum likelihoo$Merged languages$machine learning$minimum length$Machine Learning$	5
Minimum length of compound: The ML of string that should be subject to  decompounding; short strings are unlikely to be compounds, so for efficiency reasons, they are not  decompounded.	ML	Mean logprob$Maximum Likelihood$maximum likelihoo$Merged languages$machine learning$minimum length$Machine Learning$	5
Genetic Algorithms in Search, Optimization, and ML.	ML	Mean logprob$Maximum Likelihood$maximum likelihoo$Merged languages$machine learning$minimum length$Machine Learning$	6
He  has worked on ML in the context of Natural Language Processing and  has published papers in several conferences.	ML	Mean logprob$Maximum Likelihood$maximum likelihoo$Merged languages$machine learning$minimum length$Machine Learning$	6
Journal of ML Re- search 3:951991.	ML	Mean logprob$Maximum Likelihood$maximum likelihoo$Merged languages$machine learning$minimum length$Machine Learning$	6
In ML and Cybernetics, 2007 Interna- tional Conference on, volume 7, pages 3997?4002.	ML	Mean logprob$Maximum Likelihood$maximum likelihoo$Merged languages$machine learning$minimum length$Machine Learning$	6
In Proceedings of AAAI Spring Symposium  on Applying ML to Discourse  ?	ML	Mean logprob$Maximum Likelihood$maximum likelihoo$Merged languages$machine learning$minimum length$Machine Learning$	6
Disambiguating Proteins, Genes, and RNA in Text:  A ML Approach.	ML	Mean logprob$Maximum Likelihood$maximum likelihoo$Merged languages$machine learning$minimum length$Machine Learning$	6
Following their  lead, the research presented here uses Priority  MaxiMLE modified from  the backoff combination as follows:  P? (	MLE	mum Likelihood Estimation$maximum likelihood estimator$Maximum Likelihood Estimation$maximum likelihood estimation$maximum likelihood estimates$	0
In training, the CRF model is built with labeled  data and by means of an iterative algorithm based  on MaxiMLE.	MLE	mum Likelihood Estimation$maximum likelihood estimator$Maximum Likelihood Estimation$maximum likelihood estimation$maximum likelihood estimates$	0
The n-gram LMs are word-based backoff  models, where the n-gram probabilities are esti- mated using MaxiMLE  with smoothing.	MLE	mum Likelihood Estimation$maximum likelihood estimator$Maximum Likelihood Estimation$maximum likelihood estimation$maximum likelihood estimates$	0
Language model (LM)  A character-based trigram language model with  Katz back-off is constructed from the training  data to estimate the language model p(S) using  MaxiMLE.	MLE	mum Likelihood Estimation$maximum likelihood estimator$Maximum Likelihood Estimation$maximum likelihood estimation$maximum likelihood estimates$	0
Intercept -0.136  Imposter -2.180**  Time (Question) -0.134*  *p<.05; **p<.01; Fit by MaxiMLE.	MLE	mum Likelihood Estimation$maximum likelihood estimator$Maximum Likelihood Estimation$maximum likelihood estimation$maximum likelihood estimates$	0
1 1 1 1 11 )()( ),(log),()( n i n i n ij ji jin ij ji spsp sspssISMI  (6)  The parameters p(si,sj), p(si) and p(sj) are esti- mated using MaxiMLE on  the same training data as for training PTM.	MLE	mum Likelihood Estimation$maximum likelihood estimator$Maximum Likelihood Estimation$maximum likelihood estimation$maximum likelihood estimates$	0
Due to the power-law nature of language (Zipf, 1949), the MLE mas- sively overestimates the probability of rare events and assigns zero probability to legitimate word se- quences that happen not to have been observed in the training data (Manning and Sch?utze, 1999).	MLE	mum Likelihood Estimation$maximum likelihood estimator$Maximum Likelihood Estimation$maximum likelihood estimation$maximum likelihood estimates$	1
P. The MLE for n-grams is derived from frequency counts for sequence X and symbol c, PML(c|X) = count(Xc)/extCount(X), where count(X) is the number of times the sequence X was observed in the training data and extCount(X) is the number of single-symbol extensions of X observed: extCount(X) = ?	MLE	mum Likelihood Estimation$maximum likelihood estimator$Maximum Likelihood Estimation$maximum likelihood estimation$maximum likelihood estimates$	1
Then the MLE for this model can be obtained by maximizing the log likelihood function: LL(?)	MLE	mum Likelihood Estimation$maximum likelihood estimator$Maximum Likelihood Estimation$maximum likelihood estimation$maximum likelihood estimates$	1
Among the estimators, the MLE  provides the best results for the training set, but it is the worst on the test set.	MLE	mum Likelihood Estimation$maximum likelihood estimator$Maximum Likelihood Estimation$maximum likelihood estimation$maximum likelihood estimates$	1
Then we calculated five different association measures for each candidate: MLE (mle), pointwise mutual information (pmi), Student's t-test (t), Dice's coefficient (dice), and log-likelihood (ll).	MLE	mum Likelihood Estimation$maximum likelihood estimator$Maximum Likelihood Estimation$maximum likelihood estimation$maximum likelihood estimates$	1
Expectation maximization is an iterative procedure for computing the MLE of a parameter set when only a subset of data is available.	MLE	mum Likelihood Estimation$maximum likelihood estimator$Maximum Likelihood Estimation$maximum likelihood estimation$maximum likelihood estimates$	1
Following their  lead, the research presented here uses Priority  MLE modified from  the backoff combination as follows:  P? (	MLE	mum Likelihood Estimation$maximum likelihood estimator$Maximum Likelihood Estimation$maximum likelihood estimation$maximum likelihood estimates$	2
In training, the CRF model is built with labeled  data and by means of an iterative algorithm based  on MLE.	MLE	mum Likelihood Estimation$maximum likelihood estimator$Maximum Likelihood Estimation$maximum likelihood estimation$maximum likelihood estimates$	2
The n-gram LMs are word-based backoff  models, where the n-gram probabilities are esti- mated using MLE  with smoothing.	MLE	mum Likelihood Estimation$maximum likelihood estimator$Maximum Likelihood Estimation$maximum likelihood estimation$maximum likelihood estimates$	2
Language model (LM)  A character-based trigram language model with  Katz back-off is constructed from the training  data to estimate the language model p(S) using  MLE.	MLE	mum Likelihood Estimation$maximum likelihood estimator$Maximum Likelihood Estimation$maximum likelihood estimation$maximum likelihood estimates$	2
Intercept -0.136  Imposter -2.180**  Time (Question) -0.134*  *p<.05; **p<.01; Fit by MLE.	MLE	mum Likelihood Estimation$maximum likelihood estimator$Maximum Likelihood Estimation$maximum likelihood estimation$maximum likelihood estimates$	2
1 1 1 1 11 )()( ),(log),()( n i n i n ij ji jin ij ji spsp sspssISMI  (6)  The parameters p(si,sj), p(si) and p(sj) are esti- mated using MLE on  the same training data as for training PTM.	MLE	mum Likelihood Estimation$maximum likelihood estimator$Maximum Likelihood Estimation$maximum likelihood estimation$maximum likelihood estimates$	2
The empirical probability for each sentence pair is estimated by MLE over the training data (Brown et al, 1993).	MLE	mum Likelihood Estimation$maximum likelihood estimator$Maximum Likelihood Estimation$maximum likelihood estimation$maximum likelihood estimates$	3
Secondly, MLE is  usually used, which is only loosely related to minimum sen-  tence error.	MLE	mum Likelihood Estimation$maximum likelihood estimator$Maximum Likelihood Estimation$maximum likelihood estimation$maximum likelihood estimates$	3
The structure of our Bayesian classifiers were derived from the K2 al- gorithm6, and their parameters were derived from MLE.	MLE	mum Likelihood Estimation$maximum likelihood estimator$Maximum Likelihood Estimation$maximum likelihood estimation$maximum likelihood estimates$	3
k that the logistic regression model estimates (from train- ing data, using MLE) 30 Coeff.	MLE	mum Likelihood Estimation$maximum likelihood estimator$Maximum Likelihood Estimation$maximum likelihood estimation$maximum likelihood estimates$	3
So the main problem is to estimate Pe(t), the  probability of a term t appearing in the context of  the entity e.  Using the annotated name mention data set M,  we can get the MLE of  Pe(t) as follows:  _ ( )( ) ( ) e e ML e t Count tP t Count t??	MLE	mum Likelihood Estimation$maximum likelihood estimator$Maximum Likelihood Estimation$maximum likelihood estimation$maximum likelihood estimates$	3
In spite of the simplifying n-gram assump- tion, MLE remains un- reliable and tends to underestimate the proba- bility of very rare n-grams.	MLE	mum Likelihood Estimation$maximum likelihood estimator$Maximum Likelihood Estimation$maximum likelihood estimation$maximum likelihood estimates$	3
is the probability distribution function whose parameters are MLE from the training set.	MLE	mum Likelihood Estimation$maximum likelihood estimator$Maximum Likelihood Estimation$maximum likelihood estimation$maximum likelihood estimates$	4
P (ti|ti?1) HMMs can be trained directly from labeled data by calculating MLE or from incomplete data using Expectation Maximization (EM) (Dempster et al1977).	MLE	mum Likelihood Estimation$maximum likelihood estimator$Maximum Likelihood Estimation$maximum likelihood estimation$maximum likelihood estimates$	4
, z,n) (2) 2 The word frequency distribution does not impact the in- ferred topics (because words are always observed), and in our experiments we simply use MLE for ?	MLE	mum Likelihood Estimation$maximum likelihood estimator$Maximum Likelihood Estimation$maximum likelihood estimation$maximum likelihood estimates$	4
i ) can be easily calculated us-ing the MLE of the prob- abilities (i.e., the estimate of each probability is the corresponding relative frequency).	MLE	mum Likelihood Estimation$maximum likelihood estimator$Maximum Likelihood Estimation$maximum likelihood estimation$maximum likelihood estimates$	4
Unfortunately, using multiple paraphrased ver- sions of the same sentence changes the word fre- quencies in the training bi-text, thus causing worse MLE, which results in bad system performance.	MLE	mum Likelihood Estimation$maximum likelihood estimator$Maximum Likelihood Estimation$maximum likelihood estimation$maximum likelihood estimates$	4
This is effectively a comparison of the MLE of/)(pll ,  nl ) and P(PI(}, v), a  different measure from the backed-off estimate which gives i5(lIv,p , nl).	MLE	mum Likelihood Estimation$maximum likelihood estimator$Maximum Likelihood Estimation$maximum likelihood estimation$maximum likelihood estimates$	4
Discriminative training meth- ods for hidden MMs: Theory and exper- iments with perceptron algorithms.	MM	Markov model$media$	0
2 Sequential Labeling We discriminatively train a MM us- ing Bayes Point Machines (BPM).	MM	Markov model$media$	0
As with hidden MMs (Rabiner, 1989), yw(x) can be com- puted efficiently for suitable feature functions using dynamic programming.	MM	Markov model$media$	0
We then prepared features, and fed the training data to a sequential labeling system, a discriminative MM much like Conditional Random Fields (CRF), with the difference being that the model parameters are tuned using Bayes Point Machines (BPM), and then compared our model against an equivalent CRF model.	MM	Markov model$media$	0
We focus on the simple and tractable hidden MM, and present an efficient learning algorithm for incorporating approximate bijectivity and symmetry constraints.	MM	Markov model$media$	0
It is not imMMtely obvious that the only paths which  can be formed by the remaining arcs are optimal.	MM	Markov model$media$	1
2006) for gender identification, applied to the identification of users in social MM.	MM	Markov model$media$	1
Associations are almost  always multiMMl, i.e., we associate texts,  sounds, pictures, movement e c.  The structure of an interactive, multiMM  encyclopedia that is based on associations includes  1) a knowledge space/domain,  2) an associative search procedure,  3) a function for storing associative traces.	MM	Markov model$media$	1
Interactive MultiMM Navigation  Prof. Dr. Dr. Mihai NADIN  Computational Design, University of Wuppertal  Hofaue 35-39  D-42103 Wuppertal, Germany  nadin @ code.uni-wuppertal.de  Dipl.	MM	Markov model$media$	1
Interactive MultiMM Navigation  Until now, problems of navigation have in the main  been reduced to the research of i	MM	Markov model$media$	1
Clitic pronouns were tagged as verbs, for they always imMMtely precede or follow a verb.	MM	Markov model$media$	1
When M is the initial state I, the result follows imMMtely  from line 3 which sets hartnony(I) to zero.	MM	Markov model$media$	1
our approach 2 , SG model is picked as the baseline.	SG	Skip Gram$Slot Grammars$	0
For instance, the SG likelihood aims to maximize the follow- ing conditional: L(?,?,	SG	Skip Gram$Slot Grammars$	0
Finally, we also compare to shallow representation learning net- works such as SG and Continuous Bag of Words (CBoW) (Mikolov et al, 2013a), competitive state of the art window based baselines.	SG	Skip Gram$Slot Grammars$	0
In the regression the input x is trans- formed to directly predict y. The SG model, however, transforms both the context x and the tar- get y, and can therefore be seen as a generalization of the MLR.	SG	Skip Gram$Slot Grammars$	0
3.2 Sensitivity Analysis In order to test the sensitivity of our model and base- line SG to variations in the training set, we perform two sensitivity analyses.	SG	Skip Gram$Slot Grammars$	0
6.2 SG Vectors In the RNN model (?	SG	Skip Gram$Slot Grammars$	0
M. C. Mc('ord, "SG," American Journal of  Computational Linguistics, Vol.	SG	Skip Gram$Slot Grammars$	1
SG.	SG	Skip Gram$Slot Grammars$	1
McCord, M. C. (1980) "SG," Com-  putational Linguistics, vol.	SG	Skip Gram$Slot Grammars$	1
McCord, M.C., "SG", Computationa/  Linguistics, vol 6, 31-43 (1980)  13.	SG	Skip Gram$Slot Grammars$	1
SG" Com-  putational Linguistlcs, Vol 6:31-43.	SG	Skip Gram$Slot Grammars$	1
2.1 Document-Level Analysis  Document-level analysis is performed by component  PE's named Text Analysis Engines  (TAEs).	PE's	processing elements$Phrasal Elements$	0
2 A Plug and Play Scenario In this section we present example dialogues from our current demonstrator and brie y outline the main PE's.	PE's	processing elements$Phrasal Elements$	0
The PE's include:  Document tokenisation  Sentence splitting  Parts-of-speech tagging  Named Entity Recognition using a gazetteer lookup module and regular expressions  Named entity coreference using an ortho- graphic name matcher Named entities of type person, organization, ad- dress, date, and location are considered relevant document terms and stored in a special named en- tity	PE's	processing elements$Phrasal Elements$	0
However, instead of  swapping the memories, we activate a sec-  ond set of PE's that are con-  nected to the memories in the right way.	PE's	processing elements$Phrasal Elements$	0
1 In the rest of this paper, we begin by detail- ing our concrete Plug and Play scenario - device control in the home - with an example dialogue from our demonstrator and an outline of the main dialogue PE's.	PE's	processing elements$Phrasal Elements$	0
Additionally, Reconcile can be eas- ily reconfigured to use different algorithms, fea- tures, prePE's, evaluation settings and metrics.	PE's	processing elements$Phrasal Elements$	0
Table 3 Rules for Assigning Syntactic Roles to PE's  Pattern to be Scanned New Pattern to be Generated  TOGOV~ + OBJ  *: focus, - - :  not mentioned, ~: empty, \[...\]: optional  Table 4 Rules for Constructing Clausal Elements  Pattern to be Scanned New Element to be Generated  I*  \[ SENT |  162  He saw a bird with a ribbon.	PE's	processing elements$Phrasal Elements$	1
Unit completion for a CAT typing system.	CAT	computer-aided translation$Computer Aided T$	0
INTRODUCTION  Automatic language processing makes a current use of multilingual  databases as a component of the computer environment of machine  translation projects or CAT~ and as a means  of coordinating terminological standardization across several  lar~cages.	CAT	computer-aided translation$Computer Aided T$	0
Transtype: a CAT typing system.	CAT	computer-aided translation$Computer Aided T$	0
These linguistic resources  are integrated in a CAT  environment used by technical writers.	CAT	computer-aided translation$Computer Aided T$	0
FUNCTIONS A~ND USERS OF THE SYSTEM TERMSERVICE  The system TEk~VICE is designed to be used in the following  environments:  - in the CAT environment the terminological  database can be used by human translators and specialists in  scientific and technical fields as a computer-aided multilingual  dictionary;  - as far as the terminological environment is concerned, the data-  base provides sufficient linguistic information to conduct research  on terminology and to standardize terms, abbreviations, acronym	CAT	computer-aided translation$Computer Aided T$	0
Unit com- pletion for a CAT typing system.	CAT	computer-aided translation$Computer Aided T$	0
In this section, we define a standard automatic evaluation protocol, akin to the ones used in Computer-Aided Trans- lation and CAText Recognition.	CAT	computer-aided translation$Computer Aided T$	1
"An Approacn t o  CATranslation," i n  IEEE Transactions  on Engineering W r i t i n g  and Speech, Vol.	CAT	computer-aided translation$Computer Aided T$	1
CATranslation: Theory and Practice.	CAT	computer-aided translation$Computer Aided T$	1
c?2014 Association for Computational Linguistics Human Effort and Machine Learnability in CATranslation Spence Green, Sida Wang, Jason Chuang, * Jeffrey Heer, * Sebastian Schuster, and Christopher D. Manning Computer Science Department, Stanford University {spenceg,sidaw,sebschu,manning}@stanford.edu * Computer Science Department, University of Washington {jcchuang,jheer}@uw.edu Abstract Analyses of computer aided translation typi- cally focus on either frontend interfaces	CAT	computer-aided translation$Computer Aided T$	1
Lippman, Erhard  O., and Plath,  W.J.  1970 "Time Sharing and CATranslation," i n  The F i n i t e   Str ing,  .Vole 7 ,  No.	CAT	computer-aided translation$Computer Aided T$	1
c?2009 ACL and AFNLP A Web-Based Interactive CATranslation Tool Philipp Koehn School of Informatics University of Edinburgh pkoehn@inf.ed.ac.uk Abstract We developed caitra, a novel tool that aids human translators by (a) making sugges- tions for sentence completion in an inter- active machine translation setting, (b) pro- viding alternative word and phrase trans- lations, and (c) allowing them to post- edit machine translation	CAT	computer-aided translation$Computer Aided T$	1
Approacn t o  CATranslation," i n  IEEE Transactions  on Engineering W r i t i n g  and Speech, Vol.	CAT	computer-aided translation$Computer Aided T$	1
The first part of the following theorem follows from the existence of a GNF for ECFG (Albert et al, 1999).	GNF	Greibach Normal Form$Greibach normal form$	0
Watanabe et al (2006) introduce an Early- style top-down parser based on binary-branching GNF.	GNF	Greibach Normal Form$Greibach normal form$	0
It is known that a context free grammar can be con- verted to GNF, where each pro- duction will have the form:  *aVA ? ,	GNF	Greibach Normal Form$Greibach normal form$	0
Since GcFc is in GNF,  it is easy to make one-to-one correspondence between  a word in a sentence and a rule application in a  phrase-structure t e. The details of the proof are  given in Maruyama (1990).	GNF	Greibach Normal Form$Greibach normal form$	0
This can  be proved by constructing a constraint dependency  grammar GCDG from an arbitrary context-free gram-  mar GCFG in GNF, and by show-  ing that the two grammars generate exactly the same  language.	GNF	Greibach Normal Form$Greibach normal form$	0
as a result, their framework requires gram- mar transformation into the binary-branching GNF (which is not always possible) so that the resulting grammar always contain at least one Chinese word in each rule in order for a prediction step to always make progress.	GNF	Greibach Normal Form$Greibach normal form$	0
Watanabe et al (2006) further reduces the grammar?s size by enforcing all rules to comply with GNF.	GNF	Greibach Normal Form$Greibach normal form$	0
We have also presented a descrip-  tion method of such predictions as grammar ules which is based  on GNF, mad recognition mechanisms that are  specified by these rules, realized by using three stacks: the predic-  tion stack, the and stack, and the NP stack.	GNF	Greibach Normal Form$Greibach normal form$	1
We make use of  three kinds of stacks whose behavior is specified by grammar rules  in an extended version of GNF.	GNF	Greibach Normal Form$Greibach normal form$	1
GNF pro-  vides a well-known example of such a lexical-  ized context-free formalism.	GNF	Greibach Normal Form$Greibach normal form$	1
A  predictive a n a l y z e r  i s  a top-down parser  for c o n t e s t - f r e e   g rammars  written in GNF; t h i s  formulation of  t h e  grammar w a s  adopted from e a r l i e r  work by Ida Rhodes for  h e r  R u s s i a n - E n g l i s h  t r a n s l a t i o n  p r o j e c t .	GNF	Greibach Normal Form$Greibach normal form$	1
For example, a spec ia l  v e r s i o n   o f  t h i s  algorithm (for GNF grammars) was u s e d  i n   t h e  original Harvard p r e d i c t i v e  Analyzer [ICuno 19621.	GNF	Greibach Normal Form$Greibach normal form$	1
The analyzer makes use of the sim-  ple stack mechanism whose behavior is specified by rules described  in GNF.	GNF	Greibach Normal Form$Greibach normal form$	1
In Proceedings of 9th AMTA.	AMTA	Conference of the Association for Machine Translation in the Americas$Association for Machine Translation in the Americas$	0
In AMTA.	AMTA	Conference of the Association for Machine Translation in the Americas$Association for Machine Translation in the Americas$	0
of the 3rd AMTA, pp.	AMTA	Conference of the Association for Machine Translation in the Americas$Association for Machine Translation in the Americas$	0
In Machine Translation: From Research to Real Users (Proceedings, 5th AMTA, Tiburon, California), Springer-Verlag, Heidelberg, Germany, 135-244  Simard, M., Foster, G., and Isabelle, P. 1992.	AMTA	Conference of the Association for Machine Translation in the Americas$Association for Machine Translation in the Americas$	0
of the AMTA.	AMTA	Conference of the Association for Machine Translation in the Americas$Association for Machine Translation in the Americas$	1
In Proceedings of the AMTA, pages 223?231.	AMTA	Conference of the Association for Machine Translation in the Americas$Association for Machine Translation in the Americas$	1
In Proceedings of 9th Conference of the AMTA.	AMTA	Conference of the Association for Machine Translation in the Americas$Association for Machine Translation in the Americas$	1
In Proceedings of  AMTA.	AMTA	Conference of the Association for Machine Translation in the Americas$Association for Machine Translation in the Americas$	1
In Proceedings of the 7th Conference of the AMTA, pages 223?231, Cambridge, Massachusetts, USA, Au- gust.	AMTA	Conference of the Association for Machine Translation in the Americas$Association for Machine Translation in the Americas$	1
In The Ninth Conference of the AMTA, Denver,Colorado.	AMTA	Conference of the Association for Machine Translation in the Americas$Association for Machine Translation in the Americas$	1
For the shared task, we have CTs for six lan- guage pairs including English, German, French and Spanish.	CT	computed translation$Character Type$cost tim$Chunking Tree$Competition Title$constituent tree$contrast$certain$citation texts$counterfactual$Collins Thesaurus$	0
So far, we always CTs to single source words.	CT	computed translation$Character Type$cost tim$Chunking Tree$Competition Title$constituent tree$contrast$certain$citation texts$counterfactual$Collins Thesaurus$	0
For this purpose, we ran- domly selected 100 of the German multiword  expressions with an occurrence frequency above  nine, and verified their CTs  (i.e. the top ranked item for each) manually.	CT	computed translation$Character Type$cost tim$Chunking Tree$Competition Title$constituent tree$contrast$certain$citation texts$counterfactual$Collins Thesaurus$	0
2014) used continuous vector to represent the source language or target language of each phrase, and then CT probability using vector distance.	CT	computed translation$Character Type$cost tim$Chunking Tree$Competition Title$constituent tree$contrast$certain$citation texts$counterfactual$Collins Thesaurus$	0
For accessing them during decoding, we simply store them in the decoder?s data struc- ture, rather than storing pre-CT model features.	CT	computed translation$Character Type$cost tim$Chunking Tree$Competition Title$constituent tree$contrast$certain$citation texts$counterfactual$Collins Thesaurus$	0
System ID Correlation  E09 0.385  E11 0.299  E12 0.278  E14 0.307  E15 0.306  E17 0.385  E22 0.355  Average 0.331    Table 3: Correlation between METEOR Scores and  Human Assessments for the Chinese Dataset  3.5 Comparison with Other Metrics  We CT by translation correla- tions between human assessments and other met- rics besides the METEOR score, namely precision,  recall and Fmean.	CT	computed translation$Character Type$cost tim$Chunking Tree$Competition Title$constituent tree$contrast$certain$citation texts$counterfactual$Collins Thesaurus$	0
Corpus CT Setting 1 Setting 2 Setting 3 Setting 4 CITYU test 67,689 N1 1,428,763 609,921 463,756 406,312 N2 1,308,964 491,328 363,120 312,151 PKU test 172,733 N1 4,431,621 1,640,688 1,248,317 1,093,046 N2 3,953,008 1,214,480 896,741 769,674 MSR test 184,355 N1 3,910,003 1,665,511 1,313,351 1,269,858 N2 3,462,762 1,227,640 946,220 907,226 AS test 197,681 N1 1,840,266 1,353,924 1,305,937 1,210,	CT	computed translation$Character Type$cost tim$Chunking Tree$Competition Title$constituent tree$contrast$certain$citation texts$counterfactual$Collins Thesaurus$	1
Table 3:CT  2.2 Post-Processing  Two methods are used in post-processing to opti- mize the results obtained from basic segmenter.	CT	computed translation$Character Type$cost tim$Chunking Tree$Competition Title$constituent tree$contrast$certain$citation texts$counterfactual$Collins Thesaurus$	1
System Overview  NE recognized text (local code)  Language X Plain text (local code)  Lexical Analysis Rule  Character Code Converter (local code to Unicode) Character Code Converter (Unicode to local code) Lexical Analyzer  Word Candidates JP    CN   Statistical   Language Model (Dictionaries)  KR   EN NE   Recognizer  Morph  AnalyzerN-best  Word  Sequence  Search  Analytical Engine 2.2.1 CT and Word Length  Table 1 shows the varieties of character  types in each language.	CT	computed translation$Character Type$cost tim$Chunking Tree$Competition Title$constituent tree$contrast$certain$citation texts$counterfactual$Collins Thesaurus$	1
Table 3:CT  2.2 Post-Processing  Two methods are used in	CT	computed translation$Character Type$cost tim$Chunking Tree$Competition Title$constituent tree$contrast$certain$citation texts$counterfactual$Collins Thesaurus$	1
e 1:6-tag Set    Table 2: Feature Templates in Close Test    CT Example  Chinese Character ? ?	CT	computed translation$Character Type$cost tim$Chunking Tree$Competition Title$constituent tree$contrast$certain$citation texts$counterfactual$Collins Thesaurus$	1
CTs                2.2.2 Orthography and Spacing   There is an obvious difference in  orthography between each language, that is,  European languages put a space between  words while Japanese and Chinese do not.	CT	computed translation$Character Type$cost tim$Chunking Tree$Competition Title$constituent tree$contrast$certain$citation texts$counterfactual$Collins Thesaurus$	1
It is apparent that the  efficiency of word candidate generation  improves dramatically compared to the case  of generating all character strings as  CT  kanji  hiragana  katakana  alphabet  number  symbol  kanji ?	CT	computed translation$Character Type$cost tim$Chunking Tree$Competition Title$constituent tree$contrast$certain$citation texts$counterfactual$Collins Thesaurus$	1
Figure 1: Flow Chat    Status Tag  begin B  2nd B2  3rd B3  middle M  end E  single S  Table 1:6-tag Set    Table 2: Feature Templates in Close Test    CT Example  Chinese Character ? ?	CT	computed translation$Character Type$cost tim$Chunking Tree$Competition Title$constituent tree$contrast$certain$citation texts$counterfactual$Collins Thesaurus$	1
The four compo-  nents (Kirkpatrick et al, 1983) of a simulated anneal-  ing algorithm are (1) a specification of conf igurat ion,   (2) a random move generator  for rearrangements  of the elements in a configuration, (3) a CT(:-  l i on  for evaluating a configuration, (4) an annea l ing   s( 'hedule that specifies time and duration to decrease  the control parameter (or temperature).	CT	computed translation$Character Type$cost tim$Chunking Tree$Competition Title$constituent tree$contrast$certain$citation texts$counterfactual$Collins Thesaurus$	2
Under a time-sharing environment, which is the only prac-  tical environment for on-line systems of this kind, every inter-  ruption and interaction will CTe, and the total effect will make  the system so slow and cumbersome to make it impractical.	CT	computed translation$Character Type$cost tim$Chunking Tree$Competition Title$constituent tree$contrast$certain$citation texts$counterfactual$Collins Thesaurus$	2
In Fig- ure 1, solid lines indicate parent-child relationships in the CT, and dotted lines represent the linkage.	CT	computed translation$Character Type$cost tim$Chunking Tree$Competition Title$constituent tree$contrast$certain$citation texts$counterfactual$Collins Thesaurus$	5
A dependency tree is then extracted from the resulting lexicalized CFG-style tree, as is commonly done for converting CTs into dependency trees after the application of a head- percolation table (Collins, 1999).	CT	computed translation$Character Type$cost tim$Chunking Tree$Competition Title$constituent tree$contrast$certain$citation texts$counterfactual$Collins Thesaurus$	5
The output of the parser is twofold: it produces a CT as well as a linkage that shows the dependencies between words.	CT	computed translation$Character Type$cost tim$Chunking Tree$Competition Title$constituent tree$contrast$certain$citation texts$counterfactual$Collins Thesaurus$	5
Along with negation focus annotation,  this corpus also contains other annotations, such  as POS tag, named entity, chunk, CT,  dependency tree, and semantic role.	CT	computed translation$Character Type$cost tim$Chunking Tree$Competition Title$constituent tree$contrast$certain$citation texts$counterfactual$Collins Thesaurus$	5
In contrast, Lexical-Functional Grammar (Kaplan &  Bresnan 1982; Kaplan 1989), which assigns  representations consisting of a surface CT  enriched with a corresponding functional structure, is  known to be beyond context-free.	CT	computed translation$Character Type$cost tim$Chunking Tree$Competition Title$constituent tree$contrast$certain$citation texts$counterfactual$Collins Thesaurus$	5
Essentially, a PTQL query is a hierarchical pattern that is matched against a set 87 of CTs together with additional require- ments on linkages between matches.	CT	computed translation$Character Type$cost tim$Chunking Tree$Competition Title$constituent tree$contrast$certain$citation texts$counterfactual$Collins Thesaurus$	5
Conclusion and Future Work         We were able to compare and CT our results  directly with previous work of Bagga and Baldwin by  using the same corpus and evaluation technique.	CT	computed translation$Character Type$cost tim$Chunking Tree$Competition Title$constituent tree$contrast$certain$citation texts$counterfactual$Collins Thesaurus$	6
In CT, OT  assigns a ranking to all of the candidale realisations of a  word, calling the scale a tlleasta'e o1' harmony.	CT	computed translation$Character Type$cost tim$Chunking Tree$Competition Title$constituent tree$contrast$certain$citation texts$counterfactual$Collins Thesaurus$	6
The sentences  extracted form a summary that represents the entity (in  CT to our 55-word snippets).	CT	computed translation$Character Type$cost tim$Chunking Tree$Competition Title$constituent tree$contrast$certain$citation texts$counterfactual$Collins Thesaurus$	6
2 Anaphora,  by CT, is a nonsymmetrical and nontransi-  tive relation: if NP1 is anaphoric to NP2 then,  usually, NP2 is not anaphoric to NP1, for ex-  ample.	CT	computed translation$Character Type$cost tim$Chunking Tree$Competition Title$constituent tree$contrast$certain$citation texts$counterfactual$Collins Thesaurus$	6
In CT, the project described here has explored a non-cancellation analysis for Wambaya: even after a head combines with one of its argu- ments, that argument remains on the appropriate va- lence list of the mother, so that it is visible for further combination with modifiers.	CT	computed translation$Character Type$cost tim$Chunking Tree$Competition Title$constituent tree$contrast$certain$citation texts$counterfactual$Collins Thesaurus$	6
In CT, German verbs with pre- fixes usually differ markedly in their preferences from the base verb.	CT	computed translation$Character Type$cost tim$Chunking Tree$Competition Title$constituent tree$contrast$certain$citation texts$counterfactual$Collins Thesaurus$	6
Here we are concerned with the probability that  a proposed antecedent is correct given that it  has been repeated a CT number of times.	CT	computed translation$Character Type$cost tim$Chunking Tree$Competition Title$constituent tree$contrast$certain$citation texts$counterfactual$Collins Thesaurus$	7
This strategy is  CTly simple-minded but, as noted earlier, it  achieves an accuracy of 43%.	CT	computed translation$Character Type$cost tim$Chunking Tree$Competition Title$constituent tree$contrast$certain$citation texts$counterfactual$Collins Thesaurus$	7
A character was retained if any of its hypernyms was found to fall into CT types of WordNet concepts: person, animal, plant, artifact, spiritual being, physical entity.	CT	computed translation$Character Type$cost tim$Chunking Tree$Competition Title$constituent tree$contrast$certain$citation texts$counterfactual$Collins Thesaurus$	7
We  also transform our trees under CT condi-  tions to meet Hobbs' assumptions as much as  possible.	CT	computed translation$Character Type$cost tim$Chunking Tree$Competition Title$constituent tree$contrast$certain$citation texts$counterfactual$Collins Thesaurus$	7
The decomposition  makes use of Bayes' theorem and is based on  CT independence assumptions discussed be-  low.	CT	computed translation$Character Type$cost tim$Chunking Tree$Competition Title$constituent tree$contrast$certain$citation texts$counterfactual$Collins Thesaurus$	7
However, although there is indeed a CT corre- lation between morphological class and semantic class, we claim that morphology is not sufficient for a reliable classification because it is by no means a one-to-one relationship.	CT	computed translation$Character Type$cost tim$Chunking Tree$Competition Title$constituent tree$contrast$certain$citation texts$counterfactual$Collins Thesaurus$	7
For each DBLP record we searched on the web for matching CT using the first author?s last name and words in the title.	CT	computed translation$Character Type$cost tim$Chunking Tree$Competition Title$constituent tree$contrast$certain$citation texts$counterfactual$Collins Thesaurus$	8
We reduced the num- ber of schema labels by: (1) mapping the labels address, booktitle, journal and school to venue, (2) mapping month and year to date, and (3) dropping the fields url, ee, cdrom, note, isbn and chapter, since they never appeared in CT.	CT	computed translation$Character Type$cost tim$Chunking Tree$Competition Title$constituent tree$contrast$certain$citation texts$counterfactual$Collins Thesaurus$	8
As is common practice (Haghighi and Klein, 2006; Mann and McCallum, 2008), we simulate user-specified expectation criteria through statis- tics on manually labeled CT.	CT	computed translation$Character Type$cost tim$Chunking Tree$Competition Title$constituent tree$contrast$certain$citation texts$counterfactual$Collins Thesaurus$	8
More recently, Qazvinian and Radev (2008) argue that CT are useful in creating a summary of the important contributions of a research paper.	CT	computed translation$Character Type$cost tim$Chunking Tree$Competition Title$constituent tree$contrast$certain$citation texts$counterfactual$Collins Thesaurus$	8
and year to date, and (3) dropping the fields url, ee, cdrom, note, isbn and chapter, since they never appeared in CT.	CT	computed translation$Character Type$cost tim$Chunking Tree$Competition Title$constituent tree$contrast$certain$citation texts$counterfactual$Collins Thesaurus$	8
We eval- uate our method on a citation extraction task in which alignments between DBLP database records and CT are used to train an extractor.	CT	computed translation$Character Type$cost tim$Chunking Tree$Competition Title$constituent tree$contrast$certain$citation texts$counterfactual$Collins Thesaurus$	8
GS-CRF: CRF trained on human annotated CT.	CT	computed translation$Character Type$cost tim$Chunking Tree$Competition Title$constituent tree$contrast$certain$citation texts$counterfactual$Collins Thesaurus$	8
To support the use of such an in- trinsic definition, we examine the theoret- ical problems that the CT def- inition faces, show how the intrinsic defi- nition solves those problems, and explain how the intrinsic definition adheres to psy- chological reality, at least for our annota- tion purposes, better than the counterfac- tual definition.	CT	computed translation$Character Type$cost tim$Chunking Tree$Competition Title$constituent tree$contrast$certain$citation texts$counterfactual$Collins Thesaurus$	9
Finally, No-emb requires the detection of a CT context in the Premise.	CT	computed translation$Character Type$cost tim$Chunking Tree$Competition Title$constituent tree$contrast$certain$citation texts$counterfactual$Collins Thesaurus$	9
Suppose your adversary  accepts the invitation to discuss, and takes the  antecedent of the CT s a temporary  additi	CT	computed translation$Character Type$cost tim$Chunking Tree$Competition Title$constituent tree$contrast$certain$citation texts$counterfactual$Collins Thesaurus$	9
We present the challenges faced using the definition of causation in terms of CT dependence and propose new guidelines for cause-effect annotation using an alternative definition which treats causation as an intrinsic relation between events.	CT	computed translation$Character Type$cost tim$Chunking Tree$Competition Title$constituent tree$contrast$certain$citation texts$counterfactual$Collins Thesaurus$	9
Suppose your adversary  accepts the invitation to discuss, and takes the  antecedent of the CT s a temporary  additional concession.	CT	computed translation$Character Type$cost tim$Chunking Tree$Competition Title$constituent tree$contrast$certain$citation texts$counterfactual$Collins Thesaurus$	9
We take seriously Tichy's suggestion to  look for the use of counteffactuals and formulate  a semantics for counteffactuals not in terms of  "troth" and "falsehood", but rather in terms of what  is done and not done in a dialogue in which a  CT ppears.	CT	computed translation$Character Type$cost tim$Chunking Tree$Competition Title$constituent tree$contrast$certain$citation texts$counterfactual$Collins Thesaurus$	9
To support the use of such an in- trinsic definition, we examine the theoret- ical problems that the CT def- inition faces, show how the intrinsic defi- nition solves those problems, and explain how the intrinsi	CT	computed translation$Character Type$cost tim$Chunking Tree$Competition Title$constituent tree$contrast$certain$citation texts$counterfactual$Collins Thesaurus$	9
Apart from the results  on CTs mentioned above, the prover  works well on a range of cases of default reasoning,  including "double diamonds", hierarchies of predi-  cates and relevant implication.	CT	computed translation$Character Type$cost tim$Chunking Tree$Competition Title$constituent tree$contrast$certain$citation texts$counterfactual$Collins Thesaurus$	9
not done in a dialogue in which a  CT ppears.	CT	computed translation$Character Type$cost tim$Chunking Tree$Competition Title$constituent tree$contrast$certain$citation texts$counterfactual$Collins Thesaurus$	9
The New CT.	CT	computed translation$Character Type$cost tim$Chunking Tree$Competition Title$constituent tree$contrast$certain$citation texts$counterfactual$Collins Thesaurus$	10
\[1\] "IBM SAA bnagePlus Object Distribution  Manager MVS/ESA High-Speed Capture Sub-  system Guide Version 2 Release 1.1," IBM Cor-  1, (1991)  \[2\] "IBM Application System/400 New User's  Guide Version 2," IBM Corp. (1992)  \[3\] "The New CT," Collins Publish-  ers, Glasgow (1.984)  1163   Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1370?1380, Baltimore, Maryland, USA, June 23-25 2014.	CT	computed translation$Character Type$cost tim$Chunking Tree$Competition Title$constituent tree$contrast$certain$citation texts$counterfactual$Collins Thesaurus$	10
3.1 Acquisition of Conjunctive Rela-  tionships from Corpora  The New CT, which is used in SENA  as a source of synonym or is-a relationships, gives  the following synonyms of "store":  store:  accumulate, deposit, garner, hoard, keep, etc.	CT	computed translation$Character Type$cost tim$Chunking Tree$Competition Title$constituent tree$contrast$certain$citation texts$counterfactual$Collins Thesaurus$	10
Our examples describe analyses of data from Webster's Seventh  Collegiate Dictionary, the Longman Dictionary of Contemporary English, the Collins bilingual  dictionaries, the CT, and the Zingarelli Italian dictionary.	CT	computed translation$Character Type$cost tim$Chunking Tree$Competition Title$constituent tree$contrast$certain$citation texts$counterfactual$Collins Thesaurus$	10
Collins (1984) The New CT,  Collins Publishers, Glasgow.	CT	computed translation$Character Type$cost tim$Chunking Tree$Competition Title$constituent tree$contrast$certain$citation texts$counterfactual$Collins Thesaurus$	10
1984 The New CT.	CT	computed translation$Character Type$cost tim$Chunking Tree$Competition Title$constituent tree$contrast$certain$citation texts$counterfactual$Collins Thesaurus$	10
The AB algorithm (Freund and Schapire, 1997) was the first practical boost- ing algorithm, and remains one of the most widely used and studied, with applications in numerous fields.	AB	A?{B$audiobased$abstraction%1:03:00$AdaBoost$abstracts$	3
We use a modified version of AB (Freund and Schapire, 1995) that opti- mizes for recall.	AB	A?{B$audiobased$abstraction%1:03:00$AdaBoost$abstracts$	3
We compare our methods against baselines including a majority baseline, a baseline logistic regression classifier with L2 regularized features, and two common en- semble methods, AB (Freund and Schapire, 1996) and bagging (Breiman, 1996) with logistic regression base classifiers5.	AB	A?{B$audiobased$abstraction%1:03:00$AdaBoost$abstracts$	3
638 0% 10% 20% 30% 40% 50% 0 5 10 15 20 25 30 35 40 45 50 T e s t   s e t   r e c a l l - a t - 1 2 2 0 Number of languages (k) Combined with AB Individual Bilingual Statistics looking forward to mirando adelante a looking forward to deseando looking mirando looking buscando  a 3 1 5 3 Aligned Phrase Pair N(e, f) ?	AB	A?{B$audiobased$abstraction%1:03:00$AdaBoost$abstracts$	3
We can eval- uate the quality of the ranker by outputting the top N ranked candidates and measuring recall relative Algorithm 1 Recall-Oriented Ranking AB 1: for i = 1 : |X| do 2: w[i]?	AB	A?{B$audiobased$abstraction%1:03:00$AdaBoost$abstracts$	3
The three final lines in Table 3 evaluate our 642 0% 10% 20% 30% 40% 50% 0 5 10 15 20 25 30 35 40 45 50 T e s t   s e t   r e c a l l - a t - 1 2 2 0 Number of languages (k) Combined with AB Individual Bilingual Statistics Figure 2: The solid line shows recall-at-1220 when com- bining the k best-performing bilingual statistics and three monolingual statistics.	AB	A?{B$audiobased$abstraction%1:03:00$AdaBoost$abstracts$	3
2 Preprocessing The original data format provided by the shared task organizers consists of (a) a collection biomedical AB, and (b) standoff anno- tation that describes the proteins, events and sites mentioned in these AB.	AB	A?{B$audiobased$abstraction%1:03:00$AdaBoost$abstracts$	4
Note that these parses are based on a different tokenisation of the text in the AB.	AB	A?{B$audiobased$abstraction%1:03:00$AdaBoost$abstracts$	4
The data source used by the system is the set of MEDLINE AB, a large bibliographic database that is accessed on-line via PubMed.	AB	A?{B$audiobased$abstraction%1:03:00$AdaBoost$abstracts$	4
The organiz- ers also provided a set of dependency and con- stituent parses for the AB.	AB	A?{B$audiobased$abstraction%1:03:00$AdaBoost$abstracts$	4
In the course of the shared task the organizers provided a training/development set of AB for biomedical papers, annotated with the mentioned events.	AB	A?{B$audiobased$abstraction%1:03:00$AdaBoost$abstracts$	4
A central task of the system is the automatic identification of PICO elements in the MEDLINE AB and their matching with the input query frame.	AB	A?{B$audiobased$abstraction%1:03:00$AdaBoost$abstracts$	4
Demner-Fushman and Lin (2005) operationalize knowledge extraction for populat- ing a database with PICO (Population, Intervention, Comparison, and Outcome) ele- ments from medical AB obtained from MEDLINE.	AB	A?{B$audiobased$abstraction%1:03:00$AdaBoost$abstracts$	4
1504   Proceedings of the Eighth Meeting of the ACL SIGPHON at HLT-NAACL 2006, pages 60?68, New York City, USA, June 2006.	SIGPHON	Special Interest Group on Computational Phonology$Special Interest Group in Computational Phonology$	0
611  Proceedings of the Eighth Meeting of the ACL SIGPHON at HLT-NAACL 2006, page 31, New York City, USA, June 2006.	SIGPHON	Special Interest Group on Computational Phonology$Special Interest Group in Computational Phonology$	0
260   Proceedings of the Eighth Meeting of the ACL SIGPHON at HLT-NAACL 2006, pages 69?78, New York City, USA, June 2006.	SIGPHON	Special Interest Group on Computational Phonology$Special Interest Group in Computational Phonology$	0
Proceedings of the Eighth Meeting of the ACL SIGPHON at HLT-NAACL 2006, pages 32?40, New York City, USA, June 2006.	SIGPHON	Special Interest Group on Computational Phonology$Special Interest Group in Computational Phonology$	0
Proceedings of the Eighth Meeting of the ACL SIGPHON at HLT-NAACL 2006, pages 79?88, New York City, USA, June 2006.	SIGPHON	Special Interest Group on Computational Phonology$Special Interest Group in Computational Phonology$	0
1996  47    Proceedings of the Eighth Meeting of the ACL SIGPHON at HLT-NAACL 2006, pages 41?49, New York City, USA, June 2006.	SIGPHON	Special Interest Group on Computational Phonology$Special Interest Group in Computational Phonology$	0
In Proceedings of the Sixth Meeting of  the Association for Computational Linguistics  SIGPHON  in Philadelphia, July 2002, ed.	SIGPHON	Special Interest Group on Computational Phonology$Special Interest Group in Computational Phonology$	1
In Workshop of the ACL SIGPHON, pages 21?30, 2002.	SIGPHON	Special Interest Group on Computational Phonology$Special Interest Group in Computational Phonology$	1
In Proceedings of the Seventh Meeting of the ACL SIGPHON, pages 78?85, Barcelona, Spain, July.	SIGPHON	Special Interest Group on Computational Phonology$Special Interest Group in Computational Phonology$	1
Other work in this area is to be found in the Proceedings  of the First Meeting of the ACL SIGPHON, published  by the ACL in 1994, and in two edited collections (Bird 1991; Ellison and Scobbie  1993).	SIGPHON	Special Interest Group on Computational Phonology$Special Interest Group in Computational Phonology$	1
In Proceedings of the Seventh Meeting of the ACL SIGPHON, pages 43?51, Barcelona, Spain, July.	SIGPHON	Special Interest Group on Computational Phonology$Special Interest Group in Computational Phonology$	1
of the ACL SIGPHON, pages 11?18, Madrid, Spain.	SIGPHON	Special Interest Group on Computational Phonology$Special Interest Group in Computational Phonology$	1
David Vilar, Jan-T. Peter and Hermann Ney Lehrstuhl fu?r Informatik 6 RWTH Aachen University D-52056 Aachen, Germany {vilar,peter,ney}@cs.rwth-aachen.de Abstract Current statistical machine translation sys- tems handle the TP as the transformation of a string of symbols into another string of symbols.	TP	translation process$positive$True Positives$true positive$True Positive$transitional probability$	0
1In this query TP, we compared an MT software with a bilingual lexicon.	TP	translation process$positive$True Positives$true positive$True Positive$transitional probability$	0
Posterior Regularization Word alignment models in general and the HMM in particular are very gross over- simplifications of the TP and the optimal likelihood parameters learned often do not correspond to sensible alignments.	TP	translation process$positive$True Positives$true positive$True Positive$transitional probability$	0
One solution to this problem is to add more complexity to the model to better reflect the TP.	TP	translation process$positive$True Positives$true positive$True Positive$transitional probability$	0
Since a lot of the idioms in Japanese are originally from China, the conversion of kanji/hanzi will make the TP faster and more ac- curate.	TP	translation process$positive$True Positives$true positive$True Positive$transitional probability$	0
For ex- ample, Vintar (2000) presented two methods for  extraction of terminological collocations in order  to assist the TP in Slovene.	TP	translation process$positive$True Positives$true positive$True Positive$transitional probability$	0
For com- puting the counts of TP and negative words (Feature 15 and 16) we used the General Inquirer database (Stone et al.,	TP	translation process$positive$True Positives$true positive$True Positive$transitional probability$	1
has been shown to be TPly correlated with children?s linguis- tic and intellectual development (Natsiopoulou et al.,	TP	translation process$positive$True Positives$true positive$True Positive$transitional probability$	1
TP surprise? (	TP	translation process$positive$True Positives$true positive$True Positive$transitional probability$	1
In order to align the existing annotations to our three-class scheme the following mapping5 was adopted: (i) AN, DI, FE, SA were mapped to negative affect, (ii) NE was mapped to neutral affect, and (iii) HA was mapped to TP affect.	TP	translation process$positive$True Positives$true positive$True Positive$transitional probability$	1
pronouns 12 count of quote tokens 13 count of 1st person plural pronouns 14 count of 2nd person singular pronouns 15 count of quote TP words 16 count of quote negative words 17 count of nouns 18 count of verbs 19 count of adjectives 20 count of adverbs 21 up to 3-grams extracted from quote Table 1: Common feature set.	TP	translation process$positive$True Positives$true positive$True Positive$transitional probability$	1
TP?.	TP	translation process$positive$True Positives$true positive$True Positive$transitional probability$	1
These authors can be 129 Collaboration Type TP False Positives Accuracy New Blood, Catalyst or High Synergy Papers 43 23 65.15% Apprentice or Low Synergy Papers 32 22 59.25% Overall 75 45 62.50% Table 3: Evaluation based on annotation by one expert considered the hedgehogs, as they have highly sta- ble signatures that their new papers resemble.	TP	translation process$positive$True Positives$true positive$True Positive$transitional probability$	2
14  TP 191 246  False Positives 82 133  False Negatives 1587 1874  Correct Labels 189 237  Precision 0.700 0.649  Recall 0.107 0.116  F-Score 0.186 0.197  Label Accuracy 0.106 0.112    As can be seen, for entries with patterns (albeit  a low recall), a substantial number of frame ele- ments could be recognized with high precision  from a very small number of constituent match- ing function	TP	translation process$positive$True Positives$true positive$True Positive$transitional probability$	2
The number of TP, True Negatives, and  False Positives are listed.	TP	translation process$positive$True Positives$true positive$True Positive$transitional probability$	2
The Expected TP and Expected True Negatives for Cohen Kappa, as well as Chi-squared significance, are estimated as the product of Bias and Prevalence, and the product of Inverse Bias and Inverse Prevalence, resp.,	TP	translation process$positive$True Positives$true positive$True Positive$transitional probability$	2
Figure 4: Algorithm for calculating the F-measure confusion matrix of TP (T.P.), False Positives (F.P.), True Negatives (T.N.), and False Negatives (F.N.).	TP	translation process$positive$True Positives$true positive$True Positive$transitional probability$	2
There were 62  decisions involving Subphrase Relations (with 44  TP and 18 False Negatives), and 10  decisions involving WordNet (with 12 True  Positives).	TP	translation process$positive$True Positives$true positive$True Positive$transitional probability$	2
We say that s?k is a TP if s?k ?	TP	translation process$positive$True Positives$true positive$True Positive$transitional probability$	3
For NEs and zone titles, IAA was calculated using P, R and F1, defining two mentions as equal if they had the same left and right 8P, R and F1 are calculated in standard fashion from the number of TPs, false positives and false negatives.	TP	translation process$positive$True Positives$true positive$True Positive$transitional probability$	3
Secondly, a sentence is correct if all decoded segments are TPs.	TP	translation process$positive$True Positives$true positive$True Positive$transitional probability$	3
s. The precision and recall, then, are measured as the total num- ber of TPs divided by the total num- ber of decoded and true segments respectively.	TP	translation process$positive$True Positives$true positive$True Positive$transitional probability$	3
For each query, Table 3 shows total, the number of documents returned; good, the number of TPs; and top 5, the number of TPs in the top five returned documents.	TP	translation process$positive$True Positives$true positive$True Positive$transitional probability$	3
The number of TPs (correctly detected alternate linkings) is 27,606, the number of false positives (incorrectly marked al- ternations) is 32,031, the number of true negatives (cases where the model correctly did not detect an alternate linking) is 132,556, and the number of false negatives (alternate linkings that the model should have detected but did not) is 32,516.4.	TP	translation process$positive$True Positives$true positive$True Positive$transitional probability$	3
14  TPs 191 246  False Positives 82 133  False Negatives 1587 1874  Correct Labels 189 237  Precision 0.700 0.649  Recall 0.107 0.116  F-Score 0.186 0.197  Label Accuracy 0.106 0.112    As can be seen, for entries with patterns (albeit  a low recall), a substantial number of frame ele- ments could be recognized with high precision  from a very small number of constituent match- ing function	TP	translation process$positive$True Positives$true positive$True Positive$transitional probability$	4
Table 3 rep- resents the values for such commonly used met- rics as: Accuracy, TP Rate, False Posi- tive Rate, Precision, Recall and F-Measure respec- tively for each one of the tested methods.	TP	translation process$positive$True Positives$true positive$True Positive$transitional probability$	4
The (1,0) point represents perfect performance with 100% TP Rate and 0% False Negative Rate.	TP	translation process$positive$True Positives$true positive$True Positive$transitional probability$	4
We see that striving loses by a slight amount to stacking early in the curve but still  0.5  0.6  0.7  0.8  0.9  1  0  0.2  0.4  0.6  0.8  1 TP Rate False Positive R ate M BN B (sent ,ng ram) SV M (sent ,ng ram) Stacking STR IV E Figure 4: ROC curves (rotated).	TP	translation process$positive$True Positives$true positive$True Positive$transitional probability$	4
The number of TPs, True Negatives, and  False Positives are listed.	TP	translation process$positive$True Positives$true positive$True Positive$transitional probability$	4
The Expected TPs and Expected True Negatives for Cohen Kappa, as well as Chi-squared significance, are estimated as the product of Bias and Prevalence, and the product of Inverse Bias and Inverse Prevalence, resp.,	TP	translation process$positive$True Positives$true positive$True Positive$transitional probability$	4
Natalie M. Schrimpf Department of Linguistics Yale University natalie.schrimpf@yale.edu  Gaja Jarosz Department of Linguistics Yale University gaja.jarosz@yale.edu  19 While the experimental work emphasizes syl-lable-level TP, recent com-putational modeling work and corpus analyses have primarily focused on the utility of pho-neme-level statistics.	TP	translation process$positive$True Positives$true positive$True Positive$transitional probability$	5
Mc- 197 Donald and Shillcock (2003) show that forward and backward transitional probabilities are pre- dictive of first fixation and first pass durations: the higher the TP, the shorter the fixation time.	TP	translation process$positive$True Positives$true positive$True Positive$transitional probability$	5
The syllable-based TP model achieves a word token f-score of nearly 80%, the high-est reported performance for a phonotactic segmentation model with no lexicon.	TP	translation process$positive$True Positives$true positive$True Positive$transitional probability$	5
One statistical cue that has received a great deal of support in experimental work on infant speech segmentation is TP calculated over syllables.	TP	translation process$positive$True Positives$true positive$True Positive$transitional probability$	5
34 Computational Linguistics, Volume 14, Number 1, Winter 1988  Steven J. DeRose Grammatical Category Disambiguation by Statistical Optimization  2 THE LINEAR-TIME ALGORITHM (VOLSUNGA)  The algorithm described here depends on a similar  empirically-derived TP matrix to  that of CLAWS, and has a similar definition of "optimal  path".	TP	translation process$positive$True Positives$true positive$True Positive$transitional probability$	5
sensitivity to TP in an artificial language learning set-ting, the utility of these statistical cues in a natu-ral language learning context is disputed.	TP	translation process$positive$True Positives$true positive$True Positive$transitional probability$	5
The reason for the  significant drop in the precision in the second and  third intervals is mainly the same: apart from few  TN11, the majority of false term candi- dates contained common ?	TN	true negatives$Tense$True Negatives$Team Name$true negative$	0
The proposed method uses tokens as the unit of evaluation (instead of phrase-level edits), which pro- vides a stable unit of comparison and facilitates the computation of TN.	TN	true negatives$Tense$True Negatives$Team Name$true negative$	0
The overall accuracy of the  classifier (% of true positives + TN) is 82%.	TN	true negatives$Tense$True Negatives$Team Name$true negative$	0
Alternatively, we can compute precision (P ), re- call (R) and F -score by comparing system edits to gold-standard edits and thus circumvent the problem of counting TN.	TN	true negatives$Tense$True Negatives$Team Name$true negative$	0
The number of true positives (correctly detected alternate linkings) is 27,606, the number of false positives (incorrectly marked al- ternations) is 32,031, the number of TN (cases where the model correctly did not detect an alternate linking) is 132,556, and the number of false negatives (alternate linkings that the model should have detected but did not) is 32,516.4.	TN	true negatives$Tense$True Negatives$Team Name$true negative$	0
In addition, Leacock et al (2014) discuss key is- sues concerning system evaluation, such as the es- timation of TN and good practices for re- porting results, which are currently not addressed by the M 2 scorer.	TN	true negatives$Tense$True Negatives$Team Name$true negative$	0
van Eynde, Frank 1988: The Analysis of TN and  Aspect in Eurotra.	TN	true negatives$Tense$True Negatives$Team Name$true negative$	1
Sing POS=AUX Mood=Ind Number=Sing Person=3 TN=Pres VerbForm=Fin POS=VERB TN=Past VerbForm=Part Voice=Pass POS=ADP POS=NOUN Number=Sing POS=NOUN Number=Sing POS=PUNCT Our independence is guaranteed by law today .	TN	true negatives$Tense$True Negatives$Team Name$true negative$	1
POS=PRON Number=Plur Person=1 Poss=Yes PronType=Prs POS=NOUN Number=Sing POS=AUX Aspect=Imp Mood=Ind Number=Sing Person=3 TN=Pres VerbForm=Fin POS=ADV Degree=Pos POS=ADP AdpType=Prep POS=NOUN Num	TN	true negatives$Tense$True Negatives$Team Name$true negative$	1
TN  If none of the aforesaid criteria applies ome German  tenses determine the aspect choice:  Past perfect is translated to perfective aspect form,  in the case of the present tense (pracsens futuri ex-  cluded) the imperfective aspect is preferred.	TN	true negatives$Tense$True Negatives$Team Name$true negative$	1
To 1955 POS=PRON Number=Plur Person=1 Poss=Yes PronType=Prs POS=NOUN Number=Sing POS=AUX Mood=Ind Number=Sing Person=3 TN=Pres VerbForm=Fin POS=VERB TN=Past VerbForm=Part Voice=Pass POS=ADP POS=NOUN Number=Sing POS=NOUN Number=Sing POS=PUNCT Our independence is guaranteed by law today .	TN	true negatives$Tense$True Negatives$Team Name$true negative$	1
POS=PRON Number=Plur Person=1 Poss=Yes PronType=Prs POS=NOUN Number=Sing POS=AUX Aspect=Imp Mood=Ind Number=Sing Person=3 TN=Pres VerbForm=Fin POS=ADV Degree=Pos	TN	true negatives$Tense$True Negatives$Team Name$true negative$	1
POS=PRON Number=Plur Person=1 Poss=Yes PronType=Prs POS=NOUN Number=Sing POS=AUX Aspect=Imp Mood=Ind Number=Sing Person=3 TN=Pres VerbForm=Fin POS=ADV Degree=Pos POS=ADP AdpType=Prep POS=NOUN Number=Sing POS=VERB TN=Past VerbForm=Part SubCat=Tran POS=PUNCT Figure 1: A parallel sentence in English and Dutch annotated with universal morphological tags, showing high-confidence automatic word-alignments.	TN	true negatives$Tense$True Negatives$Team Name$true negative$	1
For each CS and for each ei E CS DO:  e2_.ez/.t N IF -prior > 1 + r THE  (F  ei is correct,  i.e. manually validated, THEN  ++TruePositives;  OTHERWISE  ++ FalsePositives;  OTHERWISE IF ~ < 1 - r THEN  IF e~ is correct pp~,Or~HEN  ++ FalseNegatives;  OTHERWISE  ++TN;  ++Ncases  precision =  TruePositives-~ TrueNe~atives  TruePositives+ TrueNegatives+ FalsePositives?	TN	true negatives$Tense$True Negatives$Team Name$true negative$	2
The number of True Positives, TN, and  False Positives are listed.	TN	true negatives$Tense$True Negatives$Team Name$true negative$	2
The Expected True Positives and Expected TN for Cohen Kappa, as well as Chi-squared significance, are estimated as the product of Bias and Prevalence, and the product of Inverse Bias and Inverse Prevalence, resp.,	TN	true negatives$Tense$True Negatives$Team Name$true negative$	2
since for us TN have same importance as True Positives.	TN	true negatives$Tense$True Negatives$Team Name$true negative$	2
Figure 4: Algorithm for calculating the F-measure confusion matrix of True Positives (T.P.), False Positives (F.P.), TN (T.N.), and False Negatives (F.N.).	TN	true negatives$Tense$True Negatives$Team Name$true negative$	2
For each sub-task, we have tables listing the 51 TN Abbreviation Bobicev BOB Chonger CHO CMU-Haifa HAI Cologne-Nijmegen CN CoRAL Lab @ UAB COR CUNI (Charles University) CUN cywu CYW dartmouth DAR eurac EUR HAUTCS HAU ItaliaNLP ITA Jarvis JAR kyle, crossley, dai, mcnamara KYL LIMSI LIM LTRC IIIT Hyderabad HYD Michigan MIC MITRE ?	TN	true negatives$Tense$True Negatives$Team Name$true negative$	3
2 0.3558 0.3607 0.4087 0.3470 36 UNED-run22 p np 0.1043 0.3148 0.0374 0.3243 0.5086 0.4898 0.3097 37 LIPN-run2 0.0843 - - - - - 0.0101 38 Our difference against the average 9% 7% 10% 2% -2% 17% 7% - Table 3: Results obtained at the Task 10 of the Semeval competition for the Spanish language (NOTE: The * symbol denotes a system that used Wikipedia to build its model for the Wikipedia test dataset) TN System type Wikipedia News Weighted correlation Rank UMCC DLSI-run2 supervised 0.7802 0.8254 0.8072 1 Meerkat Mafia-run2 unsupervised 0.7431 0.8454 0.8042 2 UNAL-NLP-run1 weakly supervised 0.7804 0.8154 0.8013 3 BUAP-run2 unsupervised 0.6396 0.7637 0.7137 14 Overall average - 0.6193 0.7504 0.6976 14-15 BUAP-run1 supervised 0.5504 0.6785 0.6269 17 RTM-DCU-run2 supervised 0.3689 0.6253 0.5	TN	true negatives$Tense$True Negatives$Team Name$true negative$	3
TN Affiliation CLMB (Rozovskaya et al.,	TN	true negatives$Tense$True Negatives$Team Name$true negative$	3
The reason for the  significant drop in the precision in the second and  third intervals is mainly the same: apart from few  TNs11, the majority of false term candi- dates contained common ?	TN	true negatives$Tense$True Negatives$Team Name$true negative$	4
The overall accuracy of the  classifier (% of true positives + TNs) is 82%.	TN	true negatives$Tense$True Negatives$Team Name$true negative$	4
Alternatively, we can compute precision (P ), re- call (R) and F -score by comparing system edits to gold-standard edits and thus circumvent the problem of counting TNs.	TN	true negatives$Tense$True Negatives$Team Name$true negative$	4
The number of true positives (correctly detected alternate linkings) is 27,606, the number of false positives (incorrectly marked al- ternations) is 32,031, the number of TNs (cases where the model correctly did not detect an alternate linking) is 132,556, and the number of false negatives (alternate linkings that the model should have detected but did not) is 32,516.4.	TN	true negatives$Tense$True Negatives$Team Name$true negative$	4
(g) The lack of a TN count (i.e. the num- ber of non-errors) precludes the computation of accuracy, which is useful for discriminating be- tween systems with F = 0.	TN	true negatives$Tense$True Negatives$Team Name$true negative$	4
In addition, Leacock et al (2014) discuss key is- sues concerning system evaluation, such as the es- timation of TNs and good practices for re- porting results, which are currently not addressed by the M 2 scorer.	TN	true negatives$Tense$True Negatives$Team Name$true negative$	4
We use the BP as the prior to construct a Bayesian framework for summary sentence selec- tion.	BP	beta process$Base phrase$Bin Packing$Belief Propagation$Bigram Predictability$Base Phrase$log (Prob(N2 | N1)) (2)$belief propagation$BLEU points$	0
Nonparametric factor analysis with BP pri- ors.	BP	beta process$Base phrase$Bin Packing$Belief Propagation$Bigram Predictability$Base Phrase$log (Prob(N2 | N1)) (2)$belief propagation$BLEU points$	0
For the sentence selection step, we use the varia- tional inference described in Section 4, where the parameters in the BP (5) are set as ?	BP	beta process$Base phrase$Bin Packing$Belief Propagation$Bigram Predictability$Base Phrase$log (Prob(N2 | N1)) (2)$belief propagation$BLEU points$	0
Integrating the linear reconstruction (4) and the BP3 (1), we get the complete process of summary sentence selection as follows.	BP	beta process$Base phrase$Bin Packing$Belief Propagation$Bigram Predictability$Base Phrase$log (Prob(N2 | N1)) (2)$belief propagation$BLEU points$	0
Nonparametric factor analysis with BP priors.	BP	beta process$Base phrase$Bin Packing$Belief Propagation$Bigram Predictability$Base Phrase$log (Prob(N2 | N1)) (2)$belief propagation$BLEU points$	0
Using a BP as a prior for the binary vector zi, we can automatically infer the number of active component associated with zi.	BP	beta process$Base phrase$Bin Packing$Belief Propagation$Bigram Predictability$Base Phrase$log (Prob(N2 | N1)) (2)$belief propagation$BLEU points$	0
Integrating the linear reconstruction (4) and the BP3 (1), we ge	BP	beta process$Base phrase$Bin Packing$Belief Propagation$Bigram Predictability$Base Phrase$log (Prob(N2 | N1)) (2)$belief propagation$BLEU points$	0
BPs the former generates are smaller than that transfor- mation patterns treat.	BP	beta process$Base phrase$Bin Packing$Belief Propagation$Bigram Predictability$Base Phrase$log (Prob(N2 | N1)) (2)$belief propagation$BLEU points$	1
BP chunk We add a boolean feature to detect whether there is any base phrase chunk in the text span between the two mentions.	BP	beta process$Base phrase$Bin Packing$Belief Propagation$Bigram Predictability$Base Phrase$log (Prob(N2 | N1)) (2)$belief propagation$BLEU points$	1
(5) BP chunking: The base phrase  chunking is proved to play an important role in  semantic relation extraction.	BP	beta process$Base phrase$Bin Packing$Belief Propagation$Bigram Predictability$Base Phrase$log (Prob(N2 | N1)) (2)$belief propagation$BLEU points$	1
SP-NIST(i)p-n BP chunks ?	BP	beta process$Base phrase$Bin Packing$Belief Propagation$Bigram Predictability$Base Phrase$log (Prob(N2 | N1)) (2)$belief propagation$BLEU points$	1
Input Type and bracket tagging  model   Obtain feature vectors   Predict the phrase boundary   Brackets matching Grammar rules                            Correct the types of base  phrases  Lexical  information Output BPs acquisition model Figure 1: system overview  4 Predicting the phrase boundaries  with MBL  Memory-Based Learning (MBL) is a classification  based, supervised learning approach: a  memory-based learning algorithm constructs a  classifier for a task by storing a set of examples.	BP	beta process$Base phrase$Bin Packing$Belief Propagation$Bigram Predictability$Base Phrase$log (Prob(N2 | N1)) (2)$belief propagation$BLEU points$	1
SP-NISTc BP chunks.	BP	beta process$Base phrase$Bin Packing$Belief Propagation$Bigram Predictability$Base Phrase$log (Prob(N2 | N1)) (2)$belief propagation$BLEU points$	1
Dependency Parsing by BP.	BP	beta process$Base phrase$Bin Packing$Belief Propagation$Bigram Predictability$Base Phrase$log (Prob(N2 | N1)) (2)$belief propagation$BLEU points$	3
Understand- ing BP and its Generalizations.	BP	beta process$Base phrase$Bin Packing$Belief Propagation$Bigram Predictability$Base Phrase$log (Prob(N2 | N1)) (2)$belief propagation$BLEU points$	3
Understanding BP and its Gen- eralizations, Exploring Artificial Intelligence in the New Millennium, chapter 8, pages 236?239.	BP	beta process$Base phrase$Bin Packing$Belief Propagation$Bigram Predictability$Base Phrase$log (Prob(N2 | N1)) (2)$belief propagation$BLEU points$	3
Fast Inference in Phrase Extraction Models with BP.	BP	beta process$Base phrase$Bin Packing$Belief Propagation$Bigram Predictability$Base Phrase$log (Prob(N2 | N1)) (2)$belief propagation$BLEU points$	3
c?2012 Association for Computational Linguistics Fast Inference in Phrase Extraction Models with BP David Burkett and Dan Klein Computer Science Division University of California, Berkeley {dburkett,klein}@cs.berkeley.edu Abstract Modeling overlapping phrases in an align- ment model can improve alignment quality but comes with a high inference cost.	BP	beta process$Base phrase$Bin Packing$Belief Propagation$Bigram Predictability$Base Phrase$log (Prob(N2 | N1)) (2)$belief propagation$BLEU points$	3
We apply Loopy BP to propagate sentiments among entities.	BP	beta process$Base phrase$Bin Packing$Belief Propagation$Bigram Predictability$Base Phrase$log (Prob(N2 | N1)) (2)$belief propagation$BLEU points$	3
In or- der to achieve this, we used machine learning Corpus Read Spontaneous r p-value r p-value Baseline (Unigram) r =  0:166 p = 0:0002 r =  0:02 p = 0:39 BP r =  0:236 p < 0:0001 r =  0:36 p < 0:0001 Pointwise Mutual Information r =  0:185 p < 0:0001 r =  0:177 p < 0:0001 Dice Coe?cient r =  0:079 p = 0:066 r =  0:094 p < 0:0001 Table 2: Correlation of Dierent Collocation Measures with Accent Decision techniques to automatically build accent pre- diction models using bigram word predictabil- ity scores.	BP	beta process$Base phrase$Bin Packing$Belief Propagation$Bigram Predictability$Base Phrase$log (Prob(N2 | N1)) (2)$belief propagation$BLEU points$	4
Of these, PMI and Dice Coefficient are symmetric mea- sures while BP and PKL are non- symmetric (unidirectional) measures.	BP	beta process$Base phrase$Bin Packing$Belief Propagation$Bigram Predictability$Base Phrase$log (Prob(N2 | N1)) (2)$belief propagation$BLEU points$	4
Automatic Tagging of Arabic Text: From  Raw Text to BP Chunks, In Proc.	BP	beta process$Base phrase$Bin Packing$Belief Propagation$Bigram Predictability$Base Phrase$log (Prob(N2 | N1)) (2)$belief propagation$BLEU points$	5
2.1 BPs Following Ratnaparkhi (1999), we define a base phrase as any parse node with only preterminal chil- dren.	BP	beta process$Base phrase$Bin Packing$Belief Propagation$Bigram Predictability$Base Phrase$log (Prob(N2 | N1)) (2)$belief propagation$BLEU points$	5
Automatic Tagging of Arabic Text: From Raw Text to BP Chunks.	BP	beta process$Base phrase$Bin Packing$Belief Propagation$Bigram Predictability$Base Phrase$log (Prob(N2 | N1)) (2)$belief propagation$BLEU points$	5
Second Generation Tools (AMIRA 2.0): Fast and Robust Tokenization, POS tagging, and BP Chunking.	BP	beta process$Base phrase$Bin Packing$Belief Propagation$Bigram Predictability$Base Phrase$log (Prob(N2 | N1)) (2)$belief propagation$BLEU points$	5
Statistics  Based Hybrid Approach to Chinese BP  Identification?,	BP	beta process$Base phrase$Bin Packing$Belief Propagation$Bigram Predictability$Base Phrase$log (Prob(N2 | N1)) (2)$belief propagation$BLEU points$	5
Moreover, using loopy BP means that the in- ference method is not closely coupled to the task structure, and need not change when applying this method to other types of graphs.	BP	beta process$Base phrase$Bin Packing$Belief Propagation$Bigram Predictability$Base Phrase$log (Prob(N2 | N1)) (2)$belief propagation$BLEU points$	7
Computing the gradient requires computation of the marginals which can be performed efficiently using BP (Yedidia et al, 2003).	BP	beta process$Base phrase$Bin Packing$Belief Propagation$Bigram Predictability$Base Phrase$log (Prob(N2 | N1)) (2)$belief propagation$BLEU points$	7
More specifically, we imple- ment the model as a factor graph, a bipartite graph composed of factors and variables in which we can efficiently compute the marginal beliefs of any vari- able set with the sum-product algorithm for cyclic graphs, loopy BP,.	BP	beta process$Base phrase$Bin Packing$Belief Propagation$Bigram Predictability$Base Phrase$log (Prob(N2 | N1)) (2)$belief propagation$BLEU points$	7
Infer- ence is done via loopy BP, making this framework trivially extensible to most graph structures.	BP	beta process$Base phrase$Bin Packing$Belief Propagation$Bigram Predictability$Base Phrase$log (Prob(N2 | N1)) (2)$belief propagation$BLEU points$	7
First, we perform sum- product BP on the full factor graph.	BP	beta process$Base phrase$Bin Packing$Belief Propagation$Bigram Predictability$Base Phrase$log (Prob(N2 | N1)) (2)$belief propagation$BLEU points$	7
Finally, we use sum-product variant of BP inference, but more specialized inference schemes may show additional benefits.	BP	beta process$Base phrase$Bin Packing$Belief Propagation$Bigram Predictability$Base Phrase$log (Prob(N2 | N1)) (2)$belief propagation$BLEU points$	7
For instance, the top system outperforms the bottom system by 15 BP!	BP	beta process$Base phrase$Bin Packing$Belief Propagation$Bigram Predictability$Base Phrase$log (Prob(N2 | N1)) (2)$belief propagation$BLEU points$	8
We show that it is possible to use our data selection methods to subse- lect less than 1% (or discard 99%) of a large general training corpus and still increase translation perfor- mance by nearly 2 BP.	BP	beta process$Base phrase$Bin Packing$Belief Propagation$Bigram Predictability$Base Phrase$log (Prob(N2 | N1)) (2)$belief propagation$BLEU points$	8
Aug- menting same models with same paraphrases filtered for antonyms resulted in further gains of 1.6 and 1 BP for both subset and full models, respec- tively, relative to the respective baselines.	BP	beta process$Base phrase$Bin Packing$Belief Propagation$Bigram Predictability$Base Phrase$log (Prob(N2 | N1)) (2)$belief propagation$BLEU points$	8
4http://mecab.googlecode.com/svn/trunk/mecab/doc/index.html 5http://nlp.stanford.edu/software/segmenter.shtml 6http://nlp.cs.berkeley.edu/Software.shtml 7http://triplet.cc/software/corbit As it can be observed, our DPC method obtains around 0.7 BP of improvement when compared to the second best system in both cor- pora.	BP	beta process$Base phrase$Bin Packing$Belief Propagation$Bigram Predictability$Base Phrase$log (Prob(N2 | N1)) (2)$belief propagation$BLEU points$	8
However, a closer examination of the length difference evident through the BLEU brevity penalty and the reference:system-output length ra- tio (columns 4-5 of Table 2), reveals that the dif- ferences are small and inconsistent; on average, the brevity penalty difference accounts for roughly 0.1 absolute BP and 0.2 absolute lemmatized BP of the respective differences.7 Last, Modern Standard Arabic is a morphologi- cally rich language: It has many inflected forms for most verbs, and several inflected forms for nouns, adjectives and other parts of speech ?	BP	beta process$Base phrase$Bin Packing$Belief Propagation$Bigram Predictability$Base Phrase$log (Prob(N2 | N1)) (2)$belief propagation$BLEU points$	8
In these chains of utterances, the SP is not explicitly mentioned because the author relies on the shared understanding with the reader that adja- cent pieces of quoted speech are not independent (Zhang et al.,	SP	speaker$shortest path$Surgical Pathology$simple past$Sparse projection$Stanford Parser$splits$Shallow Syntactic Similarity$Spelling$Sentiment Polarity$perspective$Split$Sparse Projection$Supplement$	0
First is the problem of iden- tifying anaphoric SPs, i.e., in the utterance ?	SP	speaker$shortest path$Surgical Pathology$simple past$Sparse projection$Stanford Parser$splits$Shallow Syntactic Similarity$Spelling$Sentiment Polarity$perspective$Split$Sparse Projection$Supplement$	0
In this work, we address both sub- problems, namely, anaphoric SP and implicit SP identification.	SP	speaker$shortest path$Surgical Pathology$simple past$Sparse projection$Stanford Parser$splits$Shallow Syntactic Similarity$Spelling$Sentiment Polarity$perspective$Split$Sparse Projection$Supplement$	0
2003), the quote-identification module detects whether a piece of quoted speech is a new quote (NEW), spoken by a SP dif- ferent from the previous SP, or a continuation quote (CONT) spoken by the same SP as that of the previous quote.	SP	speaker$shortest path$Surgical Pathology$simple past$Sparse projection$Stanford Parser$splits$Shallow Syntactic Similarity$Spelling$Sentiment Polarity$perspective$Split$Sparse Projection$Supplement$	0
The second problem is resolving utterance chains with implicit SPs.	SP	speaker$shortest path$Surgical Pathology$simple past$Sparse projection$Stanford Parser$splits$Shallow Syntactic Similarity$Spelling$Sentiment Polarity$perspective$Split$Sparse Projection$Supplement$	0
2 Related Work Elson and McKeown (2010) used rule-based and statistical learning approaches to identify candi- date characters and attribute each quote to the most likely SP.	SP	speaker$shortest path$Surgical Pathology$simple past$Sparse projection$Stanford Parser$splits$Shallow Syntactic Similarity$Spelling$Sentiment Polarity$perspective$Split$Sparse Projection$Supplement$	0
Then, the language, noise, and sentence models (sans  substitutions) are composed together, and the SP is computed.	SP	speaker$shortest path$Surgical Pathology$simple past$Sparse projection$Stanford Parser$splits$Shallow Syntactic Similarity$Spelling$Sentiment Polarity$perspective$Split$Sparse Projection$Supplement$	1
A SP dependency kernel for relation extrac- tion.	SP	speaker$shortest path$Surgical Pathology$simple past$Sparse projection$Stanford Parser$splits$Shallow Syntactic Similarity$Spelling$Sentiment Polarity$perspective$Split$Sparse Projection$Supplement$	1
inference process is performed in a similar fashion, the main difference being that we use the negative log Viterbi semiring for com- puting SPs instead of the V-expectation semiring.	SP	speaker$shortest path$Surgical Pathology$simple past$Sparse projection$Stanford Parser$splits$Shallow Syntactic Similarity$Spelling$Sentiment Polarity$perspective$Split$Sparse Projection$Supplement$	1
We plan to replace our shortest-path extraction  algorithm with one of the recently developed k-  SP algorithms (Eppstein, 1994).	SP	speaker$shortest path$Surgical Pathology$simple past$Sparse projection$Stanford Parser$splits$Shallow Syntactic Similarity$Spelling$Sentiment Polarity$perspective$Split$Sparse Projection$Supplement$	1
Methods based on manually built lexical knowledge bases, such as WordNet, compute the SP be- tween two concepts in the knowledge base and/or look at word overlap in the glosses (see Budan- itsky and Hirst (2006) for an overview).	SP	speaker$shortest path$Surgical Pathology$simple past$Sparse projection$Stanford Parser$splits$Shallow Syntactic Similarity$Spelling$Sentiment Polarity$perspective$Split$Sparse Projection$Supplement$	1
Finding the k SPs.	SP	speaker$shortest path$Surgical Pathology$simple past$Sparse projection$Stanford Parser$splits$Shallow Syntactic Similarity$Spelling$Sentiment Polarity$perspective$Split$Sparse Projection$Supplement$	1
The decoding or inference process is performed in a similar fashion, the main difference being that we use the negative log Viterbi semiring for com- puting SPs instead of the V-expectation semiring.	SP	speaker$shortest path$Surgical Pathology$simple past$Sparse projection$Stanford Parser$splits$Shallow Syntactic Similarity$Spelling$Sentiment Polarity$perspective$Split$Sparse Projection$Supplement$	1
8 Limitations  The small sample size of reports and few condi- tions found in three report genres (Operative Ga- strointestinal, Radiology, and SP)  is a limitation in this study.	SP	speaker$shortest path$Surgical Pathology$simple past$Sparse projection$Stanford Parser$splits$Shallow Syntactic Similarity$Spelling$Sentiment Polarity$perspective$Split$Sparse Projection$Supplement$	2
SP reports may be the most  temporally distinct report in our study, showing the  highest proportion of historical conditions.	SP	speaker$shortest path$Surgical Pathology$simple past$Sparse projection$Stanford Parser$splits$Shallow Syntactic Similarity$Spelling$Sentiment Polarity$perspective$Split$Sparse Projection$Supplement$	2
For instance, one may want to reward clauses in SP or present tenses, reflecting the fact that such clauses are more likely to be descriptive than those in perfect or progressive tenses.	SP	speaker$shortest path$Surgical Pathology$simple past$Sparse projection$Stanford Parser$splits$Shallow Syntactic Similarity$Spelling$Sentiment Polarity$perspective$Split$Sparse Projection$Supplement$	3
For instance, a fine-grained feature vector has three different features with seven possi- ble values to carry tense-related information: tense, is progressive, and is perfect, whereas a coarse-grained vector carries only one binary feature, is SP or present.6 Finally, the system selects salient descriptive sentences.	SP	speaker$shortest path$Surgical Pathology$simple past$Sparse projection$Stanford Parser$splits$Shallow Syntactic Similarity$Spelling$Sentiment Polarity$perspective$Split$Sparse Projection$Supplement$	3
for a male speaker (here suis is used to form the SP of the verb aller, ?	SP	speaker$shortest path$Surgical Pathology$simple past$Sparse projection$Stanford Parser$splits$Shallow Syntactic Similarity$Spelling$Sentiment Polarity$perspective$Split$Sparse Projection$Supplement$	3
and Olsen 1997) n/a has modal A yes, no yes if the clause contains a modal verb no past perfect A yes, no yes if the clause is realized in past perfect tense no politeness with be A yes, no yes if the clause contains one of the following expressions: to be sorry, to be delighted, to be glad, to be sad; the feature is designed to help capture politeness expressions (e.g., I am glad to see you) no SP present A yes, no yes if the clause is realized in simple present or past tense no tmp exp long duration A no, long, short long if the clause contains a temporal expression denoting a long period of time, short if it contains an expression denoting a short period of time and no otherwise no is assertive clause O yes, no no if the clause is not an assertion yes is assertive sent O yes,	SP	speaker$shortest path$Surgical Pathology$simple past$Sparse projection$Stanford Parser$splits$Shallow Syntactic Similarity$Spelling$Sentiment Polarity$perspective$Split$Sparse Projection$Supplement$	3
This information is expressed in the coarse-grained data set using one binary feature SP present and fine-tuning the score is trivial.	SP	speaker$shortest path$Surgical Pathology$simple past$Sparse projection$Stanford Parser$splits$Shallow Syntactic Similarity$Spelling$Sentiment Polarity$perspective$Split$Sparse Projection$Supplement$	3
English SP is written using a single word, so join together French passe?	SP	speaker$shortest path$Surgical Pathology$simple past$Sparse projection$Stanford Parser$splits$Shallow Syntactic Similarity$Spelling$Sentiment Polarity$perspective$Split$Sparse Projection$Supplement$	3
SPs onto the simplex.	SP	speaker$shortest path$Surgical Pathology$simple past$Sparse projection$Stanford Parser$splits$Shallow Syntactic Similarity$Spelling$Sentiment Polarity$perspective$Split$Sparse Projection$Supplement$	4
if the two words to be compared are antonyms, then the returned score is 0; 1 http://www.d.umn.edu/?tpederse/similarity.html 2 https://github.com/CNGLdlab/LORG-Release 3We used the default built-in converter provided with the SP (2012-11-12 revision).	SP	speaker$shortest path$Surgical Pathology$simple past$Sparse projection$Stanford Parser$splits$Shallow Syntactic Similarity$Spelling$Sentiment Polarity$perspective$Split$Sparse Projection$Supplement$	5
We use Semgrex atop the typed dependen- cies from the SP (de Marneffe et al, 2006b), as aligned in the alignment phase, to iden- tify both semantic patterns in a single text and over two aligned pieces of text.	SP	speaker$shortest path$Surgical Pathology$simple past$Sparse projection$Stanford Parser$splits$Shallow Syntactic Similarity$Spelling$Sentiment Polarity$perspective$Split$Sparse Projection$Supplement$	5
4.1 Learning Reordering Rules We tag both language sides of the bilingual corpus with POS information using the SP3 and extract POS based reordering patterns from word alignment information.	SP	speaker$shortest path$Surgical Pathology$simple past$Sparse projection$Stanford Parser$splits$Shallow Syntactic Similarity$Spelling$Sentiment Polarity$perspective$Split$Sparse Projection$Supplement$	5
Documents are tokenized and parsed with the SP.	SP	speaker$shortest path$Surgical Pathology$simple past$Sparse projection$Stanford Parser$splits$Shallow Syntactic Similarity$Spelling$Sentiment Polarity$perspective$Split$Sparse Projection$Supplement$	5
We use the freely available SP and NER system1 to generate the syntactic interpretation for these features.	SP	speaker$shortest path$Surgical Pathology$simple past$Sparse projection$Stanford Parser$splits$Shallow Syntactic Similarity$Spelling$Sentiment Polarity$perspective$Split$Sparse Projection$Supplement$	5
In the future, we plan to add syntactic parse information for each span such as that generated using the SP (De Marneffe et al, 2006).	SP	speaker$shortest path$Surgical Pathology$simple past$Sparse projection$Stanford Parser$splits$Shallow Syntactic Similarity$Spelling$Sentiment Polarity$perspective$Split$Sparse Projection$Supplement$	5
ted this  process,  Basque texts  with this  tagged  clause SP were not available.	SP	speaker$shortest path$Surgical Pathology$simple past$Sparse projection$Stanford Parser$splits$Shallow Syntactic Similarity$Spelling$Sentiment Polarity$perspective$Split$Sparse Projection$Supplement$	6
We used the same training/test set SP as in the previous experiments and used both ungeneralized and generalized fragments up to depth 4 together with the discounted RF estimator.	SP	speaker$shortest path$Surgical Pathology$simple past$Sparse projection$Stanford Parser$splits$Shallow Syntactic Similarity$Spelling$Sentiment Polarity$perspective$Split$Sparse Projection$Supplement$	6
gain ratio(X) = gain(X)/split info~')  where:  split info(X) = - ~ IT, \[ x " \[ Ti I  I T \[ rag2 I T I  *-=l  The  ratio decreases with an increase in the number  of SP.	SP	speaker$shortest path$Surgical Pathology$simple past$Sparse projection$Stanford Parser$splits$Shallow Syntactic Similarity$Spelling$Sentiment Polarity$perspective$Split$Sparse Projection$Supplement$	6
We used the same training/test set SP as in the previous experiments.	SP	speaker$shortest path$Surgical Pathology$simple past$Sparse projection$Stanford Parser$splits$Shallow Syntactic Similarity$Spelling$Sentiment Polarity$perspective$Split$Sparse Projection$Supplement$	6
Because of the small size of the corpora we averaged our results on 10 different training/test set SP.	SP	speaker$shortest path$Surgical Pathology$simple past$Sparse projection$Stanford Parser$splits$Shallow Syntactic Similarity$Spelling$Sentiment Polarity$perspective$Split$Sparse Projection$Supplement$	6
We used the same training/test set SP as in the previous experiments and used both ungeneralized and generalized fragments together with the discounted RF estimator.	SP	speaker$shortest path$Surgical Pathology$simple past$Sparse projection$Stanford Parser$splits$Shallow Syntactic Similarity$Spelling$Sentiment Polarity$perspective$Split$Sparse Projection$Supplement$	6
SP The spelling prior is estimated as pspelling(d) = 1?	SP	speaker$shortest path$Surgical Pathology$simple past$Sparse projection$Stanford Parser$splits$Shallow Syntactic Similarity$Spelling$Sentiment Polarity$perspective$Split$Sparse Projection$Supplement$	8
SP error correction can be formalized as a  classification problem.	SP	speaker$shortest path$Surgical Pathology$simple past$Sparse projection$Stanford Parser$splits$Shallow Syntactic Similarity$Spelling$Sentiment Polarity$perspective$Split$Sparse Projection$Supplement$	8
Type Frequency Informal phonological variation 804 (92.9%) SP error 27 (3.1%) Twitter-specific abbreviation 34 (3.9%) Total 865 (100%) Table 2 illustrates that phonological variations constitute a vast majority of ill-spelled words in Japanese microblog texts.	SP	speaker$shortest path$Surgical Pathology$simple past$Sparse projection$Stanford Parser$splits$Shallow Syntactic Similarity$Spelling$Sentiment Polarity$perspective$Split$Sparse Projection$Supplement$	8
SP variation is clearest in cases where an En-  glish word like swiIeh shows up transliterated vari-  ously (:~ ~" :, ?-, :	SP	speaker$shortest path$Surgical Pathology$simple past$Sparse projection$Stanford Parser$splits$Shallow Syntactic Similarity$Spelling$Sentiment Polarity$perspective$Split$Sparse Projection$Supplement$	8
Prescher et al (2000): verb-object 49.40 68.20 Table 4: Performance comparison with the literature for candidate selection for MT 4 Context-sensitive SP Correction Context-sensitive spelling correction is the task of cor- recting spelling errors that result in valid words.	SP	speaker$shortest path$Surgical Pathology$simple past$Sparse projection$Stanford Parser$splits$Shallow Syntactic Similarity$Spelling$Sentiment Polarity$perspective$Split$Sparse Projection$Supplement$	8
1,2 V, N Sem Generation SP correction 1,2,3 Any Syn/Sem Generation Adjective ordering 1,2 Adj Sem Generation Compound bracketing 1,2 N Syn Analysis Compound interpret.	SP	speaker$shortest path$Surgical Pathology$simple past$Sparse projection$Stanford Parser$splits$Shallow Syntactic Similarity$Spelling$Sentiment Polarity$perspective$Split$Sparse Projection$Supplement$	8
3.2 SP Overlay By exploiting existing lexical resources for senti- ment analysis, an explicit affective dimension can be overlaid on this basic text model.	SP	speaker$shortest path$Surgical Pathology$simple past$Sparse projection$Stanford Parser$splits$Shallow Syntactic Similarity$Spelling$Sentiment Polarity$perspective$Split$Sparse Projection$Supplement$	9
2.2 Classification of SP Let us consider how to infer the sentiment polarity p ? {	SP	speaker$shortest path$Surgical Pathology$simple past$Sparse projection$Stanford Parser$splits$Shallow Syntactic Similarity$Spelling$Sentiment Polarity$perspective$Split$Sparse Projection$Supplement$	9
3.3 SP Classification Given the large collection of emotion-labelled ex- amples, it may seem straightforward to develop a trainable model for emotion classification.	SP	speaker$shortest path$Surgical Pathology$simple past$Sparse projection$Stanford Parser$splits$Shallow Syntactic Similarity$Spelling$Sentiment Polarity$perspective$Split$Sparse Projection$Supplement$	9
As Table 3 shows, the SP is cor- rect in 57.0% of cases and partially correct (Cor- rect + Context-dep.)	SP	speaker$shortest path$Surgical Pathology$simple past$Sparse projection$Stanford Parser$splits$Shallow Syntactic Similarity$Spelling$Sentiment Polarity$perspective$Split$Sparse Projection$Supplement$	9
then filter out this pair if w, ant is an antonymous pair, and ant is in cand, and there is no negator up to two words before w and ant, or there is such a negator before both then filter out this pair 3 Antonyms, Trends, SP Native speakers of a language are good at deter- mining whether two words are antonyms (hot?cold, ascend?descend, friend?foe) or not (penguin?clown, cold?chilly, boat?rudder) (Cruse, 1986; Lehrer and Lehrer, 1982; Deese, 1965).	SP	speaker$shortest path$Surgical Pathology$simple past$Sparse projection$Stanford Parser$splits$Shallow Syntactic Similarity$Spelling$Sentiment Polarity$perspective$Split$Sparse Projection$Supplement$	9
c?2007 Association for Computational Linguistics SP Identification in Financial News: A Cohesion-based Approach Ann Devitt School of Computer Science & Statistics, Trinity College Dublin, Ireland Ann.Devitt@cs.tcd.ie Khurshid Ahmad School of Computer Science & Statistics, Trinity College Dublin, Ireland Khurshid.Ahmad@cs.tcd.ie Abstract Text is not unadulterated fact.	SP	speaker$shortest path$Surgical Pathology$simple past$Sparse projection$Stanford Parser$splits$Shallow Syntactic Similarity$Spelling$Sentiment Polarity$perspective$Split$Sparse Projection$Supplement$	9
The tutorial will be useful for senior and junior researchers who are  interested in structured prediction and global decision problems in NLP, providing a  concise overview of recent SPs and research results.	SP	speaker$shortest path$Surgical Pathology$simple past$Sparse projection$Stanford Parser$splits$Shallow Syntactic Similarity$Spelling$Sentiment Polarity$perspective$Split$Sparse Projection$Supplement$	10
2007 Association for Computational Linguistics Computational Linguistics Volume 33, Number 1 Research in QA has been developed from two different scientific SPs, artificial intelligence (AI) and information retrieval (IR).	SP	speaker$shortest path$Surgical Pathology$simple past$Sparse projection$Stanford Parser$splits$Shallow Syntactic Similarity$Spelling$Sentiment Polarity$perspective$Split$Sparse Projection$Supplement$	10
From this SP, question answering focuses on finding text excerpts that contain the answer within large collections of documents.	SP	speaker$shortest path$Surgical Pathology$simple past$Sparse projection$Stanford Parser$splits$Shallow Syntactic Similarity$Spelling$Sentiment Polarity$perspective$Split$Sparse Projection$Supplement$	10
onslraint violation and con\[licl  from the SP ofdeclarative phonology.	SP	speaker$shortest path$Surgical Pathology$simple past$Sparse projection$Stanford Parser$splits$Shallow Syntactic Similarity$Spelling$Sentiment Polarity$perspective$Split$Sparse Projection$Supplement$	10
The article contains a historical SP on question answering over restricted domains and an overview of the current methods and applications used in restricted domains.	SP	speaker$shortest path$Surgical Pathology$simple past$Sparse projection$Stanford Parser$splits$Shallow Syntactic Similarity$Spelling$Sentiment Polarity$perspective$Split$Sparse Projection$Supplement$	10
Currently we are witnessing a surge of activity in the area from the SP of IR, initiated by the Question Answering track of TREC1 in 1999 (Voorhees 2001).	SP	speaker$shortest path$Surgical Pathology$simple past$Sparse projection$Stanford Parser$splits$Shallow Syntactic Similarity$Spelling$Sentiment Polarity$perspective$Split$Sparse Projection$Supplement$	10
dprel Dependent label h Head lm Leftmost child rm Rightmost child rn Right nearest child form Word form lemma Word lemma pos Predicted PoS tag sp Y SP Y , which may be form, lemma or pos.	SP	speaker$shortest path$Surgical Pathology$simple past$Sparse projection$Stanford Parser$splits$Shallow Syntactic Similarity$Spelling$Sentiment Polarity$perspective$Split$Sparse Projection$Supplement$	11
SP the cluster with largest intra-cluster distance into two by finding the pair of vectors with largest distance in the cluster.	SP	speaker$shortest path$Surgical Pathology$simple past$Sparse projection$Stanford Parser$splits$Shallow Syntactic Similarity$Spelling$Sentiment Polarity$perspective$Split$Sparse Projection$Supplement$	11
SPting of Compound Terms in non-Prototypical Compounding Languages.	SP	speaker$shortest path$Surgical Pathology$simple past$Sparse projection$Stanford Parser$splits$Shallow Syntactic Similarity$Spelling$Sentiment Polarity$perspective$Split$Sparse Projection$Supplement$	11
SPting a natu- ral group into subgroups reduces the criterion  less than when well-separated clusters are dis- covered.	SP	speaker$shortest path$Surgical Pathology$simple past$Sparse projection$Stanford Parser$splits$Shallow Syntactic Similarity$Spelling$Sentiment Polarity$perspective$Split$Sparse Projection$Supplement$	11
SP- ting noun compounds via monolingual and bilingual paraphrasing: A study on Japanese Katakana words.	SP	speaker$shortest path$Surgical Pathology$simple past$Sparse projection$Stanford Parser$splits$Shallow Syntactic Similarity$Spelling$Sentiment Polarity$perspective$Split$Sparse Projection$Supplement$	11
excises verbs from existing chunks; SPRule(?<NN>?, ?	SP	speaker$shortest path$Surgical Pathology$simple past$Sparse projection$Stanford Parser$splits$Shallow Syntactic Similarity$Spelling$Sentiment Polarity$perspective$Split$Sparse Projection$Supplement$	11
In: Proceedings SP,  Second International Conference on Theoretical  and Methodological Issues in Machine Translation  of Natural Languages, Carnegie Mellon University,  Pittsburgh, 12-14 June 1988.	SP	speaker$shortest path$Surgical Pathology$simple past$Sparse projection$Stanford Parser$splits$Shallow Syntactic Similarity$Spelling$Sentiment Polarity$perspective$Split$Sparse Projection$Supplement$	13
CT-6, Special SP (May  1959), pp.	SP	speaker$shortest path$Surgical Pathology$simple past$Sparse projection$Stanford Parser$splits$Shallow Syntactic Similarity$Spelling$Sentiment Polarity$perspective$Split$Sparse Projection$Supplement$	13
Journal of Experimental Psychology Mo- nograph SP, 76(1, Pt.	SP	speaker$shortest path$Surgical Pathology$simple past$Sparse projection$Stanford Parser$splits$Shallow Syntactic Similarity$Spelling$Sentiment Polarity$perspective$Split$Sparse Projection$Supplement$	13
4.1 SPary Lexicon The tagger may use an external lexicon which sup- plies entries for additional words which are not found in the training corpus, and additional tags for words which did occur in the training data.	SP	speaker$shortest path$Surgical Pathology$simple past$Sparse projection$Stanford Parser$splits$Shallow Syntactic Similarity$Spelling$Sentiment Polarity$perspective$Split$Sparse Projection$Supplement$	13
A SPal Material Our Wikipedia dump from which the training, de- velopment, and test sets are constructed is from Jan 2, 2014.	SP	speaker$shortest path$Surgical Pathology$simple past$Sparse projection$Stanford Parser$splits$Shallow Syntactic Similarity$Spelling$Sentiment Polarity$perspective$Split$Sparse Projection$Supplement$	13
Tables in SPary Material show the fea- tures for the classifier built over each language?s en- tire dataset.	SP	speaker$shortest path$Surgical Pathology$simple past$Sparse projection$Stanford Parser$splits$Shallow Syntactic Similarity$Spelling$Sentiment Polarity$perspective$Split$Sparse Projection$Supplement$	13
4.4 ES In Experiment IV, we use the manually annotated data collected by Dodds et al (2015).	ES	Encyclopedia Specialist$Emotion Score$Exploratory Search$	1
04[24.98] 61.32[87.66] 20.12[39.10]  sadness 37.78[69.86] 15.60[25.31] 22.08[37.22] 13.33[23.07] 12.12[22.57] 12.70[18.71]  surprise 17.72[20.34] 8.14[18.56] 11.16[20.35] 3.80[8.50] 7.50[12.50] 5.04[10.11]  Table 6: Precision, Recall and F-Scores (in %) of the system per emotion class on the translated Japanese  SemEval 2007 test corpus before and after including morphology on different ranges of ESs.	ES	Encyclopedia Specialist$Emotion Score$Exploratory Search$	1
In SIGCHI 2007 Workshop on ES and HCI Workshop.	ES	Encyclopedia Specialist$Emotion Score$Exploratory Search$	2
ES Using Timelines.	ES	Encyclopedia Specialist$Emotion Score$Exploratory Search$	2
1.2 ES for Relations To address these limitations, we propose a process of exploration for relations of interest in available data.	ES	Encyclopedia Specialist$Emotion Score$Exploratory Search$	2
ES: from Finding to Understanding.	ES	Encyclopedia Specialist$Emotion Score$Exploratory Search$	2
2 Background 2.1 ES Exploratory search addresses the need of users to quickly identify the important pieces of information in a target set of documents.	ES	Encyclopedia Specialist$Emotion Score$Exploratory Search$	2
In Proceedings of the ACM SIGCHI Workshop on ES and HCI.	ES	Encyclopedia Specialist$Emotion Score$Exploratory Search$	2
After splitting the TS into three partitions, given the first partition as the TS, the fitness is measured by the score of predicting the second partition.	TS	training set$translation spotting$TrueSkill$Telecommunication Services$Task Set$	0
To fit the weight vector w using the TS {(xi,yi)}ni=1, we use a standard gradient-descent method to find the weight vector that maximizes the log likelihood?n i logP (yi|xi) (Sha and Pereira, 2003).	TS	training set$translation spotting$TrueSkill$Telecommunication Services$Task Set$	0
The actual feature vector is created by instan- tiating all the combinations in the table using the TS.	TS	training set$translation spotting$TrueSkill$Telecommunication Services$Task Set$	0
In order to evaluate the effects of feature templates, in Section 5, we re- move each feature template and find that several feature templates overfit the TS.	TS	training set$translation spotting$TrueSkill$Telecommunication Services$Task Set$	0
The ith candidate is regarded as oc-  curring at "Hobbs distance" dH = i. Then the  probability P(dH = ila) is simply:  P(du -= ila)  164  I correct antecedent at Hobbs distance i i  \[ correct antecedents 1  We use \[ z \[ to denote the number of times z is  observed in our TS.	TS	training set$translation spotting$TrueSkill$Telecommunication Services$Task Set$	0
However, word  alignment-based TS has some  drawbacks.	TS	training set$translation spotting$TrueSkill$Telecommunication Services$Task Set$	1
The  performances of the TS  module and the ranking module are  evaluated in terms of precision-recall  measures and coverage rate respectively.	TS	training set$translation spotting$TrueSkill$Telecommunication Services$Task Set$	1
Identifying the translation equivalents,  TS, is the most challenging part of  a bilingual concordancer.	TS	training set$translation spotting$TrueSkill$Telecommunication Services$Task Set$	1
A multi-view approach for term TS.	TS	training set$translation spotting$TrueSkill$Telecommunication Services$Task Set$	1
However, it has other drawbacks in  TS task.	TS	training set$translation spotting$TrueSkill$Telecommunication Services$Task Set$	1
Re- visiting context-based projection methods for term- TS in comparable corpora.	TS	training set$translation spotting$TrueSkill$Telecommunication Services$Task Set$	1
When the outcome of a match is observed, TS uses the relative status of the two systems to update these estimates.	TS	training set$translation spotting$TrueSkill$Telecommunication Services$Task Set$	2
S j , and a per- system measure of TS?s uncertainty of those estimates, ?	TS	training set$translation spotting$TrueSkill$Telecommunication Services$Task Set$	2
The TS approach was best overall, so we used it to produce the official rankings for all lan- 6 It is a total ordering when r = 0, or when all the system scores are outside the decision radius.	TS	training set$translation spotting$TrueSkill$Telecommunication Services$Task Set$	2
We sample from the set of pairwise rankings an equal sized set of pairwise rankings (allowing for multi- ple drawings of the same pairwise ranking), com- pute a TS model score for each system based on this sample, and then rank the systems from 1..|{S j }|.	TS	training set$translation spotting$TrueSkill$Telecommunication Services$Task Set$	2
3.3.2), and another based on TS (Sakaguchi et al.,	TS	training set$translation spotting$TrueSkill$Telecommunication Services$Task Set$	2
Each player S j is modeled by two parameters: TS?s current estimate of each system?s relative ability, ?	TS	training set$translation spotting$TrueSkill$Telecommunication Services$Task Set$	2
On average, there is a small improvement in accuracy moving from Expected Wins to the H&M model, and then again to the TS model; however, there is no pat- tern to the best model for each class.	TS	training set$translation spotting$TrueSkill$Telecommunication Services$Task Set$	2
It has been sug- 878 Change test years BOW sLDA FWD SemTreeFWD Consumer Staples 2008-2010 0.1015 0.0774 0.1079 0.1426 2011-2012 0.1663 0.1203 0.1664 0.1736 5 years 0.1274 0.0945 0.1313 0.1550 Information Technology 2008-2010 0.0580 0.0585 0.0701 0.0846 2011-2012 0.0894 0.0681 0.1076 0.1273 5 years 0.0705 0.0623 0.0851 0.1017 TS 2008-2010 0.1501 0.1615 0.1497 0.2409 2011-2012 0.2256 0.2084 0.2191 0.4009 5 years 0.1803 0.1803 0.1774 0.3049 Polarity Consumer Staples 2008-2010 0.0359 0.0383 0.0956 0.1054 2011-2012 0.0938 0.0270 0.1131 0.1285 5 years 0.0590 0.0338 0.1026 0.1147 p-value >>0.1000 0.0918 0.0489 Information Technology 2008-2010 0.0551 0.0332 0.0697 0.0763 2011-2012 0.0591 0.0516 0.0764	TS	training set$translation spotting$TrueSkill$Telecommunication Services$Task Set$	3
12 0.2256 0.2084 0.2191 0.4009 5 years 0.1803 0.1803 0.1774 0.3049 Polarity Consumer Staples 2008-2010 0.0359 0.0383 0.0956 0.1054 2011-2012 0.0938 0.0270 0.1131 0.1285 5 years 0.0590 0.0338 0.1026 0.1147 p-value >>0.1000 0.0918 0.0489 Information Technology 2008-2010 0.0551 0.0332 0.0697 0.0763 2011-2012 0.0591 0.0516 0.0764 0.0857 5 years 0.0567 0.0405 0.0723 0.0801 p-value 0.0626 0.0948 0.0103 TS 2008-2010 0.0402 0.0464 0.0821 0.0745 2011-2012 0.0366 0.0781 0.0611 0.0809 5 years 0.0388 0.0591 0.0737 0.0770 p-value >>0.1000 0.0950 0.0222 Table 4: Average MCC for the change and polarity tasks by feature representation, for 2008-2010; for 2011-2012; for all 5 years and associated p-values of ANOVAs for comparison to BOW.	TS	training set$translation spotting$TrueSkill$Telecommunication Services$Task Set$	3
Evaluating Spoken Dialogue Systems for TS.	TS	training set$translation spotting$TrueSkill$Telecommunication Services$Task Set$	3
26 2 TSting The ID task broadly follows the task definition and event types of the BioNLP ST?09, extending it with new entity categories, correspondingly broadening the scope of events, and introducing a new class of events, high-level biological processes.	TS	training set$translation spotting$TrueSkill$Telecommunication Services$Task Set$	4
2 TSup The SemEval 2016 Stance Detection for Twitter shared task (Mohammad et al, 2016) consists of two subtasks, Task A and Task B. In Task A the goal is to detect the stance of tweets towards tar- gets given labelled training data for all test targets (Climate Change is a Real Concern, Feminist Move- ment, Atheism, Legalization of Abortion and Hillary Clinton).	TS	training set$translation spotting$TrueSkill$Telecommunication Services$Task Set$	4
2 TSting In the design of the REL task, we followed the gen- eral policy of the shared task in assuming named entity recognition (NER) as a given starting point: participants were provided with manually annotated gold standard annotations identifying gene/protein names in all of the training, development, and final test data.	TS	training set$translation spotting$TrueSkill$Telecommunication Services$Task Set$	4
2 TSting The EPI task is an event extraction task in the sense popularized by a number of recent domain resources and challenges (e.g. (Pyysalo et al, 2007; Kim et al, 2008; Thompson et al, 2009; Kim et al, 2009; Ana- niadou et al, 2010)).	TS	training set$translation spotting$TrueSkill$Telecommunication Services$Task Set$	4
3 TSting In the task, the training, development and test data sets are provided in three types of files: the text, the protein annotation, and the coreference annotation files.	TS	training set$translation spotting$TrueSkill$Telecommunication Services$Task Set$	4
3 Shared TSup The following section describes the system setup using the Spanish?English and French?English Eu- roParl, and Czech?English CzEng training data.	TS	training set$translation spotting$TrueSkill$Telecommunication Services$Task Set$	4
Figure 4 shows F1  scores on VS using LAF.	VS	validating set$vector space$vowel  sequences$	0
We divide the  training portion of the Tsinghua Chinese  Treebank provided by CLP2010 into three parts  as follows: 500 trees are randomly extracted as  development set, another 500 as VS  and the rest trees are taken as training set.	VS	validating set$vector space$vowel  sequences$	0
F1 scores of VS  varying with D in equation (4) are shown in  Figure 2.	VS	validating set$vector space$vowel  sequences$	0
We apply SSPTC to the test set of  Task 2 in CLP2010, and get 1.275 percentage  points improvement over baseline parser using  the parameters tuned on VS.	VS	validating set$vector space$vowel  sequences$	0
Figure 3 shows F1 scores of VS  using UAF to select higher quality parses.	VS	validating set$vector space$vowel  sequences$	0
The parsing  results on VS show SSPTC is  effective.	VS	validating set$vector space$vowel  sequences$	0
The F1 score of  VS parsed by baseline parser is  85.72%.	VS	validating set$vector space$vowel  sequences$	0
In all the experiments, we clustered the whole set of 2283 adjectives, as the set of objects alters the VS and thus the classification results.	VS	validating set$vector space$vowel  sequences$	1
Much of the work in this study is based on that by  Bagga and Baldwin (1998), where they presented a  successful cross-document coreference resolution  algorithm to resolve ambiguities between people having  the same name using the VS model.	VS	validating set$vector space$vowel  sequences$	1
Incremental VS          Our intent with the incremental VS model  is to approximate the work reported by Bagga and  Baldwin (1998).	VS	validating set$vector space$vowel  sequences$	1
We show that the previous  approach is effective but that a variation on it,  agglomerative VS, provides improved and  much more stable results.	VS	validating set$vector space$vowel  sequences$	1
In the  remainder of this section, we describe the three  methods: incremental VS, KL divergence, and  agglomerative VS.	VS	validating set$vector space$vowel  sequences$	1
EP Network member institutions were contacted who had access to language learners and who had previously participated in data collec- tion for the EP Programme2.	EP	English Profile$Entity Profile$Europarl$end position$expletive$	0
EP Journal, 2:e1.	EP	English Profile$Entity Profile$Europarl$end position$expletive$	0
EP Journal, vol2:e1.	EP	English Profile$Entity Profile$Europarl$end position$expletive$	0
EP Journal, 1(1):1?23.	EP	English Profile$Entity Profile$Europarl$end position$expletive$	0
EP Journal, 2.	EP	English Profile$Entity Profile$Europarl$end position$expletive$	0
Table 3:  Sample EP  Name Mohamed Atta  Aliases Atta; Mohamed   Position apparent mastermind;   ring leader; engineer; leader   Age 33; 29; 33-year-old;  34-year-old   Where-from United Arab Emirates;  Spain; Hamburg; Egyptian;  ??	EP	English Profile$Entity Profile$Europarl$end position$expletive$	1
Our results in the Reuters RCV1/RCV2 task, obtained using EP v7 as parallel data, show that our method has no trouble handling different levels of representations simutaneously (document, sen- tence and word).	EP	English Profile$Entity Profile$Europarl$end position$expletive$	2
EP: A parallel corpus 2027 for statistical machine translation.	EP	English Profile$Entity Profile$Europarl$end position$expletive$	2
used the full 1.8M parallel sentences in EP.	EP	English Profile$Entity Profile$Europarl$end position$expletive$	2
In our experiments we learn taggers for a set of 11 European lan- guages that have both UD training data with mor- phological features, and parallel data in EP: Bulgarian, Czech, Danish, Dutch, Finnish, Ital- ian, Polish, Portuguese, Slovene, Spanish and Swedish.	EP	English Profile$Entity Profile$Europarl$end position$expletive$	2
EP: A parallel corpus for statistical machine translation.	EP	English Profile$Entity Profile$Europarl$end position$expletive$	2
Following prior work, our dataset D u consists of 500,000 parallel sen- tences from the EP v7 English-German cor- pus (Koehn, 2005); and our labeled dataset D l consists of English and German documents from the RCV1/RCV2 corpora (Lewis et al, 2004), each categorized with one out of L = 4 labels.	EP	English Profile$Entity Profile$Europarl$end position$expletive$	2
We extend  the notion of adjacent indices to be any two non-  overlapping indices where one has a start position  that equals an EP of the other.	EP	English Profile$Entity Profile$Europarl$end position$expletive$	3
Redo step 1 from those EPs, rather than  from the beginning, if there is any new EP  equal to or exceeding previous greatest one, a type I or  type I1 ambiguity, respectively, is found.	EP	English Profile$Entity Profile$Europarl$end position$expletive$	3
Due to discontinuous nuclei, each edge spans not a  single pair of string positions, indicating its start  and EP, \])tit a set of such string-position  pairs, and we call this set an index.	EP	English Profile$Entity Profile$Europarl$end position$expletive$	3
1) The exact-word-match measure considers annotations to match if their start and EPs are exactly the same. (	EP	English Profile$Entity Profile$Europarl$end position$expletive$	3
Find all possible words from the beginning of the  string and record their EPs;  2.	EP	English Profile$Entity Profile$Europarl$end position$expletive$	3
es Fails Fails/Trees CopulaBe 60 1 1% ilV 2 0 0% n0V 10 0 0% n0ClV 9 0 0% n0ClVn1 45 2 4% n0ClVden1 36 3 8% n0ClVpn1 29 3 10% n0Vn1 84 3 3% n0Vn1Adj2 24 6 25% n0Van1 87 3 3% n0Vden1 38 3 7% n0Vpn1 30 3 10% ilVcs1 2 0 0% n0Vcs1 30 23 74% n0Vas1 15 10 66% n0Vn1Adj2 24 0 0% s0Vn1 72 9 12% n0Vs1int 15 12 80% n0Vn1n2 24 0 0% n0Vn1an2 681 54 7% Table 1: Checking for Gaps in the Grammar (impersonal with EP subject, ?	EP	English Profile$Entity Profile$Europarl$end position$expletive$	4
What I mean by semantic filtering my be illus-  trated by reference to the analysis of EP  NP's like there in Sag (1982).	EP	English Profile$Entity Profile$Europarl$end position$expletive$	4
If they are bound, they are associated to an antecedent?s index; else they might also be interpreted as EPs, i.e. they receive a label that prevents the following submodule to con-sider them for further computation.	EP	English Profile$Entity Profile$Europarl$end position$expletive$	4
For example, i f   someone uses an EP, is this a sign of intense  anger or is i t  her/his usual way of talking?	EP	English Profile$Entity Profile$Europarl$end position$expletive$	4
A similar example is the use of EPs (e.g., There is a unicorn in the garden.)	EP	English Profile$Entity Profile$Europarl$end position$expletive$	4
We extract HLDS- based quasi logical form graphs from the CCG- bank and semantically empty function words such as complementizers, infinitival-to, EP subjects, and case-marking prepositions are adjusted to reflect their purely syntactic status.	EP	English Profile$Entity Profile$Europarl$end position$expletive$	4
5 Learning Representations with CE In recent years, many NLP practitioners have be- gun using discriminative models, and especially maximum-entropy-based models like CRFs, be- cause they allow the modeler to incorporate ar- bitrary, interacting features of the observation se- quence while still providing tractable inference.	CE	Chinese Encyclopedia$Contrastive Estimation$Confidence Estimation$Correlated Entity$	1
Noise- CE: A New Estimation Principle for Unnormalized Statistical Models.	CE	Chinese Encyclopedia$Contrastive Estimation$Confidence Estimation$Correlated Entity$	1
Labeled Pseudo-Projective Dependency ... 2006 27 5 84 P05-1044 Smith & Eisner CE: Training Log-Linear ... 2005 30 13 262 P05-1073 Toutanova et, al.	CE	Chinese Encyclopedia$Contrastive Estimation$Confidence Estimation$Correlated Entity$	1
Sparse priors have 819 45 tag set 17 tag set All train 973k train All train 973k train Observational initialization (this work) 92.1 92.8 93.9 94.8 CE (Smith and Eisner, 2005) ? ?	CE	Chinese Encyclopedia$Contrastive Estimation$Confidence Estimation$Correlated Entity$	1
CE max- imizes the conditional probability of the observed sentences given a neighborhood of similar unseen sequences.	CE	Chinese Encyclopedia$Contrastive Estimation$Confidence Estimation$Correlated Entity$	1
5 Training Although unsupervised training technique such as CE as in (Smith and Eisner, 2005), (Dyer et al, 2011) can be adapted to train 1In practice, the number of non-zero parameters in clas- sic HMM model would be much smaller, as many words do not co-occur in bilingual sentence pairs.	CE	Chinese Encyclopedia$Contrastive Estimation$Confidence Estimation$Correlated Entity$	1
4.5 CE for Extracted Triples The automatically extracted triples inevitably con- tain errors and are often considered as with high recall but low precision.	CE	Chinese Encyclopedia$Contrastive Estimation$Confidence Estimation$Correlated Entity$	2
5.5 Results with CEs Now, we will investigate the results from another perspective with the help of confidence estima- tions.	CE	Chinese Encyclopedia$Contrastive Estimation$Confidence Estimation$Correlated Entity$	2
2 Related Work Reference-free MT quality assessment was ini- tially approached as a CE task, strongly biased towards exploiting data from a Sta- tistical MT (SMT) system and the translation pro- cess to model the confidence of the system in the produced translation.	CE	Chinese Encyclopedia$Contrastive Estimation$Confidence Estimation$Correlated Entity$	2
TREC 2002 QA at BBN: Answer Selection  and CE.	CE	Chinese Encyclopedia$Contrastive Estimation$Confidence Estimation$Correlated Entity$	2
3.3 CE by Random Walking with Restart We believe that considering confidence of patterns can potentially improve the extraction accuracy.	CE	Chinese Encyclopedia$Contrastive Estimation$Confidence Estimation$Correlated Entity$	2
5 CE In addition to its merits for computing the entropy gradient, subsequence constrained entropy has other uses, including confidence estimation.	CE	Chinese Encyclopedia$Contrastive Estimation$Confidence Estimation$Correlated Entity$	2
DR RR SS  Central Venous Pressure  Inotropic State 0  Stroke Volume  HR +  Cardiac Output +  Total Peripheral Resistance +  Mean Arterial Pressure +  T> Can you tell me what controls TPR?	HR	Heart Rate$hierarchical rule$	0
Subsequent work has addressed improvements and extensions to the search proce- dure itself, the extraction of the HRs needed for translation, and has also reported con- trastive experiments with other SMT architectures.	HR	Heart Rate$hierarchical rule$	1
Marton and Resnik (2008) exploit shal- low correspondences of HRs with source syntactic constituents extracted from par- allel text, an approach also investigated by Chiang (2005).	HR	Heart Rate$hierarchical rule$	1
We then describe techniques to analyze and reduce the set of HRs.	HR	Heart Rate$hierarchical rule$	1
search through HRs which greatly speeds translation without any effect on quality.	HR	Heart Rate$hierarchical rule$	1
The number of HRs extracted far exceeds the number of phrase translations typ- ically found in aligned text.	HR	Heart Rate$hierarchical rule$	1
Hierarchical rule extraction Zhang et al (2008) describe a linear algorithm, a modified version of shift-reduce, to extract phrase pairs organized into a tree from which HRs can be directly extracted.	HR	Heart Rate$hierarchical rule$	1
Zhang and Gildea (2006) propose bina- rization for synchronous grammars as a means to control search complexity arising from more com- plex, syntactic, HRs sets.	HR	Heart Rate$hierarchical rule$	1
In addition, we manually collected English diminishers (e.g. less or approximately), in- tensifiers (e.g. very or indeed) and INV (e.g. not or barely).	INV	invertors$inverted$	0
The negation expressions (don?t, can?t...) are rep- resented by the list of INV from Steinberger?s lexicon (Steinberger et al.,	INV	invertors$inverted$	0
In ALL diminishers, intensifiers and INV are included as well.	INV	invertors$inverted$	0
They were followed by features derived from the lexicons of Steinberger, which includes INV, intensifiers and four polarity levels of words.	INV	invertors$inverted$	0
The authors of the best approach in this task re- port that their knowledge-based system has a con- siderable vocabulary including 15 thousand nega- tive expressions, 7 thousand positive expressions, around 120 so-called operators (intensifiers and INV) and around 200 neutral stop expressions including sentiment words as their components.	INV	invertors$inverted$	0
Optimization of  INV vector searches.	INV	invertors$inverted$	1
o mea- sured the number of INV alignments over all pairs of English word positions.	INV	invertors$inverted$	1
IDF: the INV document frequency.	INV	invertors$inverted$	1
We redesign our system  in two aspects: 1) on-demand network creation for  retrieval and learning - this eliminates full 'INV  file' creation saving space and reducing 'dead time'  between a collection is acquired and made searchable,  and provides for fast learning capability; 2)  'subeollecfions within a master' file design - this  enables us to handle very large collections in an  incremental nd robust fashion, yet retaining retrieval  ranking flexibility as if all items are in one single  large file.	INV	invertors$inverted$	1
Since the relaxed score takes into ac- count renaming relations even if the arguments are INV, it will necessarily be greater or equal than the strict score.	INV	invertors$inverted$	1
Dividing this sum by the obvious denominator (replacing (1 if i1 > i2) with (1) in the sum) yielded avalue of 1.6% INV alignments.	INV	invertors$inverted$	1
Since this method of counting retrograde alignments would assign a low count to mass movements of large contiguous chunks, we also mea- sured the number of INV alignments over all pairs of English word positions.	INV	invertors$inverted$	1
Jar  Completing Synset  Finding Semantic Relations  Creating GUI   Creating Linguistics Interface  Drafting Reports  Writing Final Report  Publishing The Kurdnet  Figure 5: Management Plan References Purya Aliabadi, Mohammad Sina Ahmadi, Shahin Salavati, and Kyumars Sheykh Esmaili.	GUI	Graphical User Interface$graphical user interface$graphical user inteffac$	0
c?2012 Association for Computational Linguistics A GUI for Feature-Based Opinion Mining  Pedro Balage Filho   University of Wolverhampton  pedrobalage@gmail.com  Caroline Brun  Xerox Research Centre Europe  Caroline.Brun@xrce.xerox.com  Gilbert Rondeau   Xerox Research Centre Europe  Gilbert.Rondeau@xrce.xerox.com      Abstract  In this paper, we present XOpin, a graphical  user interface that have been developed to  provide a	GUI	Graphical User Interface$graphical user interface$graphical user inteffac$	0
95 Workshop on Nicht-visuelle graphische Benutzungsoberfla?chen (Non-visual GUIs), Darmstadt, Germany, February.	GUI	Graphical User Interface$graphical user interface$graphical user inteffac$	0
In practice the interaction with the natural language system is via a GUI.	GUI	Graphical User Interface$graphical user interface$graphical user inteffac$	0
Use of perceptron learning to create a routing  query given a set of relevance judgments  6) GUI  ?	GUI	Graphical User Interface$graphical user interface$graphical user inteffac$	0
4 GUI We developed a graphical user interface to interac- tively experiment with the software for computing semantic relatedness.	GUI	Graphical User Interface$graphical user interface$graphical user inteffac$	0
This tool, which runs on a Sun workstation and uses the Xwindows GUI , provided an interface that allowed analysts to easily visualize the relationships among objects and thus avoid errors i n linking objects together.	GUI	Graphical User Interface$graphical user interface$graphical user inteffac$	1
schmid/tools/TreeTagger/ 6 https://code.google.com/p/mate-tools/ 7 https://github.com/termsuite/ 8 Maven group id is fr.univ-nantes.termsuite 17 Figure 2: TermSuite GUI and Technical Information.	GUI	Graphical User Interface$graphical user interface$graphical user inteffac$	1
Each participant is seated at a personal computer with a simple GUI with a button which plays or replays the audio (up to 5 times), a text box in which to write responses, and a second button to submit those responses.	GUI	Graphical User Interface$graphical user interface$graphical user inteffac$	1
It can be used in three ways: the Java API, the command line API, or the GUI as shown on Figure 2.	GUI	Graphical User Interface$graphical user interface$graphical user inteffac$	1
Finally, the language must have an easy-to-use graphics library to support the development of GUIs.	GUI	Graphical User Interface$graphical user interface$graphical user inteffac$	1
Each of the different ypes of user interacts  with a GUI, which allows the users to  add components from a component s ore to the develop-  ing design.	GUI	Graphical User Interface$graphical user interface$graphical user inteffac$	1
Concise ILP formu- lations for dependency parsing.	ILP	integer linear programming$integer linear progra$integer linear program$Integer Linear Programming$Inductive Logic Programming$integer linear programs$Integer Linear Programmin$	0
Multi-lingual dependency parsing with incremental ILP.	ILP	integer linear programming$integer linear progra$integer linear program$Integer Linear Programming$Inductive Logic Programming$integer linear programs$Integer Linear Programmin$	0
Grammatical error correction using ILP.	ILP	integer linear programming$integer linear progra$integer linear program$Integer Linear Programming$Inductive Logic Programming$integer linear programs$Integer Linear Programmin$	0
Thadani and McKeown (2011) substituted MANLI?s simulated annealing-based decoding with ILP, and achieved a consider- able speed-up.	ILP	integer linear programming$integer linear progra$integer linear program$Integer Linear Programming$Inductive Logic Programming$integer linear programs$Integer Linear Programmin$	0
Several recent works have tried to improve this model using Bayesian estimation (Goldwater and Griffiths, 2007; Johnson, 2007; Gao and Johnson, 2008), sophisticated initialization (Goldberg et al, 2008), induction of an initial clustering used to train an HMM (Freitag, 2004; Biemann, 2006), infinite HMM models (Van Gael et al, 2009), in- tegration of ILP into the parameter estimation process (Ravi and Knight, 2009), and biasing the model such that the num- ber of possible tags that each word can get is small (Grac?a et al, 2009).	ILP	integer linear programming$integer linear progra$integer linear program$Integer Linear Programming$Inductive Logic Programming$integer linear programs$Integer Linear Programmin$	0
Systems have also tried to take advantage of more global in- formation to ensure that the pair-wise classifications satisfy temporal logic transitivity constraints, using frameworks such as ILP and Markov logic networks (Bramsen et al, 2006; Cham- bers and Jurafsky, 2008; Yoshikawa et al, 2009; Uz- Zaman and Allen, 2010).	ILP	integer linear programming$integer linear progra$integer linear program$Integer Linear Programming$Inductive Logic Programming$integer linear programs$Integer Linear Programmin$	0
Concise ILPmming formu- lations for dependency parsing.	ILP	integer linear programming$integer linear progra$integer linear program$Integer Linear Programming$Inductive Logic Programming$integer linear programs$Integer Linear Programmin$	1
2010) also improved upon the work by Baldridge (2008) by using ILPm- ming to find a minimal model of supertag transi- tions, thereby generating a better starting point for EM than the grammatical constraints alone could provide.	ILP	integer linear programming$integer linear progra$integer linear program$Integer Linear Programming$Inductive Logic Programming$integer linear programs$Integer Linear Programmin$	1
Multi-lingual dependency parsing with incremental ILPmming.	ILP	integer linear programming$integer linear progra$integer linear program$Integer Linear Programming$Inductive Logic Programming$integer linear programs$Integer Linear Programmin$	1
Grammatical error correction using ILPmming.	ILP	integer linear programming$integer linear progra$integer linear program$Integer Linear Programming$Inductive Logic Programming$integer linear programs$Integer Linear Programmin$	1
Thadani and McKeown (2011) substituted MANLI?s simulated annealing-based decoding with ILPmming, and achieved a consider- able speed-up.	ILP	integer linear programming$integer linear progra$integer linear program$Integer Linear Programming$Inductive Logic Programming$integer linear programs$Integer Linear Programmin$	1
Several recent works have tried to improve this model using Bayesian estimation (Goldwater and Griffiths, 2007; Johnson, 2007; Gao and Johnson, 2008), sophisticated initialization (Goldberg et al, 2008), induction of an initial clustering used to train an HMM (Freitag, 2004; Biemann, 2006), infinite HMM models (Van Gael et al, 2009), in- tegration of ILPmming into the parameter estimation process (Ravi and Knight, 2009), and biasing the model such that the num- ber of possible tags that each word can get is small (Grac?a et al, 2009).	ILP	integer linear programming$integer linear progra$integer linear program$Integer Linear Programming$Inductive Logic Programming$integer linear programs$Integer Linear Programmin$	1
Concise ILPming formu- lations for dependency parsing.	ILP	integer linear programming$integer linear progra$integer linear program$Integer Linear Programming$Inductive Logic Programming$integer linear programs$Integer Linear Programmin$	2
2010) also improved upon the work by Baldridge (2008) by using ILP- ming to find a minimal model of supertag transi- tions, thereby generating a better starting point for EM than the grammatical constraints alone could provide.	ILP	integer linear programming$integer linear progra$integer linear program$Integer Linear Programming$Inductive Logic Programming$integer linear programs$Integer Linear Programmin$	2
Multi-lingual dependency parsing with incremental ILPming.	ILP	integer linear programming$integer linear progra$integer linear program$Integer Linear Programming$Inductive Logic Programming$integer linear programs$Integer Linear Programmin$	2
Grammatical error correction using ILPming.	ILP	integer linear programming$integer linear progra$integer linear program$Integer Linear Programming$Inductive Logic Programming$integer linear programs$Integer Linear Programmin$	2
Thadani and McKeown (2011) substituted MANLI?s simulated annealing-based decoding with ILPming, and achieved a consider- able speed-up.	ILP	integer linear programming$integer linear progra$integer linear program$Integer Linear Programming$Inductive Logic Programming$integer linear programs$Integer Linear Programmin$	2
Several recent works have tried to improve this model using Bayesian estimation (Goldwater and Griffiths, 2007; Johnson, 2007; Gao and Johnson, 2008), sophisticated initialization (Goldberg et al, 2008), induction of an initial clustering used to train an HMM (Freitag, 2004; Biemann, 2006), infinite HMM models (Van Gael et al, 2009), in- tegration of ILPming into the parameter estimation process (Ravi and Knight, 2009), and biasing the model such that the num- ber of possible tags that each word can get is small (Grac?a et al, 2009).	ILP	integer linear programming$integer linear progra$integer linear program$Integer Linear Programming$Inductive Logic Programming$integer linear programs$Integer Linear Programmin$	2
In Proceedings of the NAACL Workshop on ILP for Natural Langauge Processing.	ILP	integer linear programming$integer linear progra$integer linear program$Integer Linear Programming$Inductive Logic Programming$integer linear programs$Integer Linear Programmin$	3
Constrained Conditional Models (CCM) formulation of NLP problems (also known as:  ILP for NLP) is a learning and inference framework that  augments the learning of conditional (probabilistic or discriminative) models with  declarative constraints (written, for example, using a first-order representation).	ILP	integer linear programming$integer linear progra$integer linear program$Integer Linear Programming$Inductive Logic Programming$integer linear programs$Integer Linear Programmin$	3
Bergsma and Kondrak (2007b) present a method for identifying sets of cognates across groups of languages using the global inference framework of ILP.	ILP	integer linear programming$integer linear progra$integer linear program$Integer Linear Programming$Inductive Logic Programming$integer linear programs$Integer Linear Programmin$	3
In Proceedings of the Workshop on ILP for Natural Lan- guage Processing, pages 1?9, Boulder, Colorado, June.	ILP	integer linear programming$integer linear progra$integer linear program$Integer Linear Programming$Inductive Logic Programming$integer linear programs$Integer Linear Programmin$	3
We will also mention various possibilities for  performing the inference, from commercial ILP  packages to search techniques to Lagrangian relaxation approximation methods.	ILP	integer linear programming$integer linear progra$integer linear program$Integer Linear Programming$Inductive Logic Programming$integer linear programs$Integer Linear Programmin$	3
InProceedings of the  Workshop on ILP for  Natural Langauge Processing, pp.	ILP	integer linear programming$integer linear progra$integer linear program$Integer Linear Programming$Inductive Logic Programming$integer linear programs$Integer Linear Programmin$	3
9//6   Learning Constraint Grammar-style disambiguation rules using  ILP  Nikolaj  L indberg   Centre for Speech Technology  Royal Institute of Technology  SE-100 44 Stockholm, Sweden  nikolaj ~speech.	ILP	integer linear programming$integer linear progra$integer linear program$Integer Linear Programming$Inductive Logic Programming$integer linear programs$Integer Linear Programmin$	4
J. M. Zelle and R. J. Mooney, J. B. Konvisser,  Combining Top-down and Bottom-up Meth-  ods in ILP, Proc  of The 11th Tntcrnational Conference on Ma-  chine Learning (ML-94), pp.343-351, 1994.	ILP	integer linear programming$integer linear progra$integer linear program$Integer Linear Programming$Inductive Logic Programming$integer linear programs$Integer Linear Programmin$	4
of the 21st International Conference on ILP, pages 347?357.	ILP	integer linear programming$integer linear progra$integer linear program$Integer Linear Programming$Inductive Logic Programming$integer linear programs$Integer Linear Programmin$	4
Moreover, results were compared to a base line set of rules produced without learning and the difference reaches a maximum improve- ment using ILP of 22%.	ILP	integer linear programming$integer linear progra$integer linear program$Integer Linear Programming$Inductive Logic Programming$integer linear programs$Integer Linear Programmin$	4
Probabilistic ILP - Theory and Appli- cations, volume 4911 of Lecture Notes in Computer Science.	ILP	integer linear programming$integer linear progra$integer linear program$Integer Linear Programming$Inductive Logic Programming$integer linear programs$Integer Linear Programmin$	4
ILP: derivations, successes and shortcomings.	ILP	integer linear programming$integer linear progra$integer linear program$Integer Linear Programming$Inductive Logic Programming$integer linear programs$Integer Linear Programmin$	4
By treating inference problems as instances of ILP, we proposed three exact theorems which identify exam- ples for which the inference procedure need not be called at all and previous solutions can be re-used with the guarantee of optimality.	ILP	integer linear programming$integer linear progra$integer linear program$Integer Linear Programming$Inductive Logic Programming$integer linear programs$Integer Linear Programmin$	5
While in the past first-order logic has been translated to NP-hard ILP, we use polynomial-time-solvable linear programs, al- 877 lowing us to readily scale to large problems with extensive prior knowledge, as demonstrated by our experiments.	ILP	integer linear programming$integer linear progra$integer linear program$Integer Linear Programming$Inductive Logic Programming$integer linear programs$Integer Linear Programmin$	5
We describe two formulations of ILP that learn the edges: one maximizing a global score function, and another maximizing a global probability function.	ILP	integer linear programming$integer linear progra$integer linear program$Integer Linear Programming$Inductive Logic Programming$integer linear programs$Integer Linear Programmin$	5
On this data, the unaltered baseline system, processes 5127 ILP and achieves an F1 of 75.85%.	ILP	integer linear programming$integer linear progra$integer linear program$Integer Linear Programming$Inductive Logic Programming$integer linear programs$Integer Linear Programmin$	5
We present an approach which solves the problem in- crementally, thus we avoid creating in- tractable ILP.	ILP	integer linear programming$integer linear progra$integer linear program$Integer Linear Programming$Inductive Logic Programming$integer linear programs$Integer Linear Programmin$	5
We use ILOG CPLEX to solve all of the ILP in our experiments.	ILP	integer linear programming$integer linear progra$integer linear program$Integer Linear Programming$Inductive Logic Programming$integer linear programs$Integer Linear Programmin$	5
In Proceedings of the NAACL Workshop on ILPg for Natural Langauge Processing.	ILP	integer linear programming$integer linear progra$integer linear program$Integer Linear Programming$Inductive Logic Programming$integer linear programs$Integer Linear Programmin$	6
Constrained Conditional Models (CCM) formulation of NLP problems (also known as:  ILPg for NLP) is a learning and inference framework that  augments the learning of conditional (probabilistic or discriminative) models with  declarative constraints (written, for example, using a first-order representation).	ILP	integer linear programming$integer linear progra$integer linear program$Integer Linear Programming$Inductive Logic Programming$integer linear programs$Integer Linear Programmin$	6
Bergsma and Kondrak (2007b) present a method for identifying sets of cognates across groups of languages using the global inference framework of ILPg.	ILP	integer linear programming$integer linear progra$integer linear program$Integer Linear Programming$Inductive Logic Programming$integer linear programs$Integer Linear Programmin$	6
In Proceedings of the Workshop on ILPg for Natural Lan- guage Processing, pages 1?9, Boulder, Colorado, June.	ILP	integer linear programming$integer linear progra$integer linear program$Integer Linear Programming$Inductive Logic Programming$integer linear programs$Integer Linear Programmin$	6
We will also mention various possibilities for  performing the inference, from commercial ILPg  packages to search techniques to Lagrangian relaxation approximation methods.	ILP	integer linear programming$integer linear progra$integer linear program$Integer Linear Programming$Inductive Logic Programming$integer linear programs$Integer Linear Programmin$	6
InProceedings of the  Workshop on ILPg for  Natural Langauge Processing, pp.	ILP	integer linear programming$integer linear progra$integer linear program$Integer Linear Programming$Inductive Logic Programming$integer linear programs$Integer Linear Programmin$	6
This FAC is  especially useful when CLARE is being tai-  lored for use in a particular domain, since  it allows people not expert in linguistics or  the CLARE grammar to extend grammati-  cal coverage in simple and approximate, but  often practically important, ways.	FAC	facility$Facility$	0
land (1998) is one of the broad- est studies about hedging functions in scientific articles, and which makes use of categories that have strong relationship, at face value, to the like- lihood that the reader of hedged material will find the material sufficiently useful or sufficiently well expressed to prompt the reader to rate highly the message containing the material, whether with an explicit FAC to record kudos or otherwise.	FAC	facility$Facility$	0
Moreover, the visualization system can be used  for interfaces which include a FAC for pro-  gramming by demonstration (with macro defi-  nitions) and can offer textual support for inter-  action through other media.	FAC	facility$Facility$	0
As is usl~.lly  the case in automata-based approaches, the system  treats analysis and generation symmetrically, and  tile same description can be run with equal FAC  in either direction.	FAC	facility$Facility$	0
The version of the system used in our own  FAC has been augmented with list-processing routines  and other specialized programming which greatly increased  its efficiency and data-capacity.	FAC	facility$Facility$	0
labasc FAC.	FAC	facility$Facility$	0
However, based on preliminary investigation results, we decided to exclude the following semantic classes from tar- gets of the annotation: Timex (Temporal Expres- sion, 12 classes), Numex (Numerical Expression, 34 classes), Address (e.g., postal address and urls, 1 class), Title Other (e.g., Mr., Mrs., 1 class), FAC Part (e.g, 9th floor, sec- ond basement, 1 class).	FAC	facility$Facility$	1
Study of a l,lultiiingual FAC fo_ir  videote?t information networks , Utrecht,  The Netherlands; BSO  434   Proceedings of the Eighth Workshop on Innovative Use of NLP for Building Educational Applications, pages 32?41, Atlanta, Georgia, June 13 2013.	FAC	facility$Facility$	1
Hendrix, G.G. 1977 LIFER: a Natural Language Interface FAC.	FAC	facility$Facility$	1
The project made use of the resources provided by the Edinburgh Compute and Data FAC (http://www.ecdf.ed.ac.uk/).	FAC	facility$Facility$	1
IBM (1983, 1984): Query Management FAC, General Information  GC26-4071, International Business Machines Corporation, San Jose,  California.	FAC	facility$Facility$	1
IBM (1982): Interactive System Productivity FAC, General Informa-  tion, GC34-2181, International Business Machines Corporation, Cary,  North Carohna.	FAC	facility$Facility$	1
First, Stoyanov 1The ACE-2004/05 semantic types are person, organiza- tion, GPE, location, facility, vehicle, weapon.	GPE	geo-political entity$Geo-Political$geopolitical entity$	0
ACE restricts CEs to entities that belong to one of seven semantic classes: per- son, organization, GPE, location, fa- cility, vehicle, and weapon.	GPE	geo-political entity$Geo-Political$geopolitical entity$	0
Similarly, one can express a GPE, e.g. United States, as his country or another person entity, e.g. Hillary Clinton, as his wife, and their relation to the entity Bill Clinton as ?	GPE	geo-political entity$Geo-Political$geopolitical entity$	0
The data is anno- tated with five types of entities: person, organization, GPE, location, facility; each mention can be either named, nominal or pronominal, and can be ei- ther generic (not referring to a clearly described entity) or specific.	GPE	geo-political entity$Geo-Political$geopolitical entity$	0
GPE mentions (?	GPE	geo-political entity$Geo-Political$geopolitical entity$	0
The  ACE 2005 task can detect seven types of named  entities: person, organization, GPE,  location, facility, vehicle, and weapon; each type  of named entity can occur in a document with any  of three distinct formats: name, nominal construc- tion, and pronoun.	GPE	geo-political entity$Geo-Political$geopolitical entity$	0
They are of three  types: GPE entities are composite entities  comprised of a physical location, a population, a  government, and a nation (or province, state,  county, city, etc.).	GPE	geo-political entity$Geo-Political$geopolitical entity$	1
Type Inventory: ACE and ERE share the Per- son, Organization, GPE Entity, and Location Types.	GPE	geo-political entity$Geo-Political$geopolitical entity$	1
This feature is a single binary fea- ture to guarantee that the type of entity in docu- ment (i.e. Person, GPE Entity and Or- ganization) is consistent with the type of entity  in KB.	GPE	geo-political entity$Geo-Political$geopolitical entity$	1
The test data has  3904 queries across three named entity types:  Person, GPE Entity and Organization.	GPE	geo-political entity$Geo-Political$geopolitical entity$	1
Therefore, there were eight coarse-grained cate- gories in total: Facility, GPE, Location, Organisation, Person, Vehicle, Weapon, and Prod- uct.	GPE	geo-political entity$Geo-Political$geopolitical entity$	1
Then we link the document to  the entity in KB if the document contains a  named entity whose name exactly matches with  the unambiguous mention and type (i.e. Person,  Organization and GPE Entity) exactly  matches with the type of entity in KB.	GPE	geo-political entity$Geo-Political$geopolitical entity$	1
The ACE 2005 named entity recognition dataset includes 7 named entity class labels (person, organization, location, GPE, facility, vehicle, weapon) for 5 text genres (newswire, broad- cast news, broadcast conversations, conversational telephone speech, weblogs).	GPE	geo-political entity$Geo-Political$geopolitical entity$	2
it has as its antecedent a GPE?,	GPE	geo-political entity$Geo-Political$geopolitical entity$	2
Finin et al (2010) obtained named entity an- notations (person, organization, GPE) for several hundred Twitter messages.	GPE	geo-political entity$Geo-Political$geopolitical entity$	2
254 O NATURAL OBJECT natural feature or nonliving object in nature barrier reef nest neutron star planet sky fishpond metamorphic rock Mediterranean cave stepping stone boulder Orion ember universe A ARTIFACT man-made structures and objects bridge restaurant bedroom stage cabinet toaster antidote aspirin L LOCATION any name of a GPE, as well as other nouns functioning as locations or regions Cote d?Ivoire New York City downtown stage left India Newark interior airspace P PERSON humans or personified beings; names of social groups (ethnic, political, etc.)	GPE	geo-political entity$Geo-Political$geopolitical entity$	2
GPE) mention.	GPE	geo-political entity$Geo-Political$geopolitical entity$	2
Australia and country are mentions of type named and nominal, respectively, of a single GPE.	GPE	geo-political entity$Geo-Political$geopolitical entity$	2
The procedure is not based on the deno-  tative meaning of a word, but only on the connota-  tive EMs attached to the word; it is difficult to  choose the relevant dimensions, i.e. the dimensions  required for the sufficient semantic space.	EM	emotion$ExpectationMaximization$Entropy$exact match$Emma$Expectation-Maximization$expectation maximization$expectation maximisation$Expectation Maximization$	0
2012), and express quotes demonstrating EMs such as sadness, fear, happiness, anger and surprise (Alm, 2008) with realistic expression (Murray and Arnott, 2008).	EM	emotion$ExpectationMaximization$Entropy$exact match$Emma$Expectation-Maximization$expectation maximization$expectation maximisation$Expectation Maximization$	0
Overall, two anno- tators were employed, while each annotator pro- vided two annotations: one for EM and one for mood.	EM	emotion$ExpectationMaximization$Entropy$exact match$Emma$Expectation-Maximization$expectation maximization$expectation maximisation$Expectation Maximization$	0
Each story sentence (regard- less if quotes were included or not) was anno- tated regarding primary EMs and mood us- ing the following labels: ?	EM	emotion$ExpectationMaximization$Entropy$exact match$Emma$Expectation-Maximization$expectation maximization$expectation maximisation$Expectation Maximization$	0
This method can capture Mmost all types of se-  mantic relations (except EMal and situational  relation), such as paraphrasing by superordinate ( x.  cat/pet) ,  systematic relation (ex.	EM	emotion$ExpectationMaximization$Entropy$exact match$Emma$Expectation-Maximization$expectation maximization$expectation maximisation$Expectation Maximization$	0
Emotions from text: Machine learning for text-based EM prediction.	EM	emotion$ExpectationMaximization$Entropy$exact match$Emma$Expectation-Maximization$expectation maximization$expectation maximisation$Expectation Maximization$	0
Conditional EM-based: Aspects from a struc- tured ontology are generally quite meaningful, but they are not designed specifically for organizing the opinions in our data set.	EM	emotion$ExpectationMaximization$Entropy$exact match$Emma$Expectation-Maximization$expectation maximization$expectation maximisation$Expectation Maximization$	2
On the Robustness of EM-based Similarity Measures in Evaluation of Subcategorization Acquisiton Sys- tems.	EM	emotion$ExpectationMaximization$Entropy$exact match$Emma$Expectation-Maximization$expectation maximization$expectation maximisation$Expectation Maximization$	2
This Conditional EM measures the uncer- tainty about the cluster label of a sentence given the knowledge of its aspect.	EM	emotion$ExpectationMaximization$Entropy$exact match$Emma$Expectation-Maximization$expectation maximization$expectation maximisation$Expectation Maximization$	2
Intu- itively, the conditional entropy-based method es- sentially selects the most appropriate aspects from 736 Algorithm 1 Greedy Algorithm for Conditional EM Based Aspect Selection Input: A = {A1, ..., Am} Output: k-sized A? ?	EM	emotion$ExpectationMaximization$Entropy$exact match$Emma$Expectation-Maximization$expectation maximization$expectation maximisation$Expectation Maximization$	2
A Maximum EM Model for Prepositional Phrase Attachment.	EM	emotion$ExpectationMaximization$Entropy$exact match$Emma$Expectation-Maximization$expectation maximization$expectation maximisation$Expectation Maximization$	2
IDF as features, and then choose the sub- set of aspects that minimize Conditional EM of the cluster label given the aspect: A?	EM	emotion$ExpectationMaximization$Entropy$exact match$Emma$Expectation-Maximization$expectation maximization$expectation maximisation$Expectation Maximization$	2
All queries (other than the ones using the NEAR oper- ator) were performed as EMes (using quotation marks in Altavista).	EM	emotion$ExpectationMaximization$Entropy$exact match$Emma$Expectation-Maximization$expectation maximization$expectation maximisation$Expectation Maximization$	3
EM?:	EM	emotion$ExpectationMaximization$Entropy$exact match$Emma$Expectation-Maximization$expectation maximization$expectation maximisation$Expectation Maximization$	3
Besides an EM accuracy metric, we also used a more fine-grained score based on the well-known PARSEVAL metrics that evaluate phrase-structure trees (Black et al 1991).	EM	emotion$ExpectationMaximization$Entropy$exact match$Emma$Expectation-Maximization$expectation maximization$expectation maximisation$Expectation Maximization$	3
For the EM scheme, the obtained performance is higher7 than the baseline (random guess) that equals to 0.250.	EM	emotion$ExpectationMaximization$Entropy$exact match$Emma$Expectation-Maximization$expectation maximization$expectation maximisation$Expectation Maximization$	3
The search is for word forms, which may be phrases (n-grams), in which case EMes are sought, i.e. respecting the actual sequence of words.	EM	emotion$ExpectationMaximization$Entropy$exact match$Emma$Expectation-Maximization$expectation maximization$expectation maximisation$Expectation Maximization$	3
Experimental results on the Homecentre The tables show that the simple RF estimator scores extremely bad if all fragments are used: the EM is only 1.1% on the Verbmobil corpus and 2.7% on the Homecentre corpus, whereas the discounted RF estimator scores respectively 35.9% and 38.4% on these corpora.	EM	emotion$ExpectationMaximization$Entropy$exact match$Emma$Expectation-Maximization$expectation maximization$expectation maximisation$Expectation Maximization$	3
EMnuel Navarro, Franck Sajous, Bruno Gaume, Laurent Pre?vot, ShuKai Hsieh, Tzu Y. Kuo, Pierre Magistry, and Chu R. Huang.	EM	emotion$ExpectationMaximization$Entropy$exact match$Emma$Expectation-Maximization$expectation maximization$expectation maximisation$Expectation Maximization$	4
Ahmet Aker, Monica Paramita, EM Barker, and Robert Gaizauskas.	EM	emotion$ExpectationMaximization$Entropy$exact match$Emma$Expectation-Maximization$expectation maximization$expectation maximisation$Expectation Maximization$	4
License details: http://creativecommons.org/licenses/by/4.0/ Assigning Terms to Domains by Document Classification Robert Gaizauskas, EM Barker, Monica Lestari Paramita and Ahmet Aker Department of Computer Science, University of Sheffield, United Kingdom {r.gaizauskas,e.barker,m.paramita,ahmet.aker}@sheffield.ac.uk Abstract In this paper we investigate a number of questions relating to the identification of the domain of a term by domain classification of the document in which the term occurs.	EM	emotion$ExpectationMaximization$Entropy$exact match$Emma$Expectation-Maximization$expectation maximization$expectation maximisation$Expectation Maximization$	4
Assigning Terms to Domains by Document Classification Robert Gaizauskas, EM Barker, Monica Lestari Paramita and Ahmet Aker Department of Computer Science, University of Sheffield, United Kingdom {r.gaizauskas,e.barker,m.paramita,ahmet.aker}@sheffield.ac.uk Abstract In this paper we investigate a number of questions relating to the identification of the domain of a term by domain classification of the document in which the term occurs.	EM	emotion$ExpectationMaximization$Entropy$exact match$Emma$Expectation-Maximization$expectation maximization$expectation maximisation$Expectation Maximization$	4
EMnuel Roche and Yves Schabes.	EM	emotion$ExpectationMaximization$Entropy$exact match$Emma$Expectation-Maximization$expectation maximization$expectation maximisation$Expectation Maximization$	4
In EMnuel Roche and Yves Sch-  abes, edito	EM	emotion$ExpectationMaximization$Entropy$exact match$Emma$Expectation-Maximization$expectation maximization$expectation maximisation$Expectation Maximization$	4
May. EMnuel Morin and B?atrice Daille.	EM	emotion$ExpectationMaximization$Entropy$exact match$Emma$Expectation-Maximization$expectation maximization$expectation maximisation$Expectation Maximization$	4
EMnuel Roche and Yves Schabes, editors.	EM	emotion$ExpectationMaximization$Entropy$exact match$Emma$Expectation-Maximization$expectation maximization$expectation maximisation$Expectation Maximization$	4
Moon, T. K. (1996) The EM Algorithm, IEEE Signal Processing Magazine, No- vember, 1996, pp.	EM	emotion$ExpectationMaximization$Entropy$exact match$Emma$Expectation-Maximization$expectation maximization$expectation maximisation$Expectation Maximization$	5
Viterbi EM is performed over each sentence in the corpus to infer the pa- rameters of the model.	EM	emotion$ExpectationMaximization$Entropy$exact match$Emma$Expectation-Maximization$expectation maximization$expectation maximisation$Expectation Maximization$	5
(3)    and the efficient EM al- gorithm can be used to optimize ?	EM	emotion$ExpectationMaximization$Entropy$exact match$Emma$Expectation-Maximization$expectation maximization$expectation maximisation$Expectation Maximization$	5
Our method selects the translation based on the context in an EM style training algo- rithm which explicitly handles polysemy through in- corporating multiple dictionary translations (word sense and translation are closely linked (Resnik and Yarowsky, 1999)).	EM	emotion$ExpectationMaximization$Entropy$exact match$Emma$Expectation-Maximization$expectation maximization$expectation maximisation$Expectation Maximization$	5
This model is trained by using hard discriminative EM.	EM	emotion$ExpectationMaximization$Entropy$exact match$Emma$Expectation-Maximization$expectation maximization$expectation maximisation$Expectation Maximization$	5
Section 4 intro- duces a basic EM model and two extensions for the alignment task.	EM	emotion$ExpectationMaximization$Entropy$exact match$Emma$Expectation-Maximization$expectation maximization$expectation maximisation$Expectation Maximization$	5
3.2.5 Training and Decoding For training, we hold language model parameters constant and use EM (Demp- ster et al, 1977) to learn noise model parameters as follows.	EM	emotion$ExpectationMaximization$Entropy$exact match$Emma$Expectation-Maximization$expectation maximization$expectation maximisation$Expectation Maximization$	6
15) We use the EM algorithm (Dempster et al, 1977) for the maximum likelihood estimate of ?	EM	emotion$ExpectationMaximization$Entropy$exact match$Emma$Expectation-Maximization$expectation maximization$expectation maximisation$Expectation Maximization$	6
We use the CCG grammatical framework and train a non-parametric Bayesian model of parse structure with online variational Bayesian EM.	EM	emotion$ExpectationMaximization$Entropy$exact match$Emma$Expectation-Maximization$expectation maximization$expectation maximisation$Expectation Maximization$	6
The semantic fit between a verb and its argument is modeled using a class-based lexicon that is derived from unlabeled data using the EM algorithm (verb-argument model).	EM	emotion$ExpectationMaximization$Entropy$exact match$Emma$Expectation-Maximization$expectation maximization$expectation maximisation$Expectation Maximization$	6
Self-training and EM are perhaps the best known semi-supervised learn- ing algorithms (Abney, 2008).	EM	emotion$ExpectationMaximization$Entropy$exact match$Emma$Expectation-Maximization$expectation maximization$expectation maximisation$Expectation Maximization$	6
2010) train a probabilistic model of a variety of sentence simplification rules using EM with a parallel corpus of aligned sentences from Wikipedia and Simple Wikipedia.	EM	emotion$ExpectationMaximization$Entropy$exact match$Emma$Expectation-Maximization$expectation maximization$expectation maximisation$Expectation Maximization$	6
These probabilities can be estimated by us- ing the Baum-Welch EM al- gorithm (Baum et al, 1970).	EM	emotion$ExpectationMaximization$Entropy$exact match$Emma$Expectation-Maximization$expectation maximization$expectation maximisation$Expectation Maximization$	7
2008), which use EM to simultaneously cluster verbs into verb classes and nominal arguments into noun classes; these ap- proaches are not compatible with the evaluation framework we have used here.	EM	emotion$ExpectationMaximization$Entropy$exact match$Emma$Expectation-Maximization$expectation maximization$expectation maximisation$Expectation Maximization$	7
Marcu and Wong (2002) proposed a phrase-based alignment model which suffered from a massive parameter space and intractable inference using EM.	EM	emotion$ExpectationMaximization$Entropy$exact match$Emma$Expectation-Maximization$expectation maximization$expectation maximisation$Expectation Maximization$	7
Kupiec et al, 95) and (Aone et al, 97)  employ the EM algorithm to derive  coefficients for their systems.	EM	emotion$ExpectationMaximization$Entropy$exact match$Emma$Expectation-Maximization$expectation maximization$expectation maximisation$Expectation Maximization$	8
2) We use the EM algo- rithm to solve this maximization problem.	EM	emotion$ExpectationMaximization$Entropy$exact match$Emma$Expectation-Maximization$expectation maximization$expectation maximisation$Expectation Maximization$	8
The algorithm 10 combines EM and Naive Bayes algo- rithms, and we used randomly extracted 50,000 unlabeled synsets in WordNet as the necessary un- labeled data.	EM	emotion$ExpectationMaximization$Entropy$exact match$Emma$Expectation-Maximization$expectation maximization$expectation maximisation$Expectation Maximization$	8
Work on type-supervision goes back to (Merialdo, 1994), who introduced the still standard procedure of using a bigram Hidden Markov Model (HMM) trained via EM.	EM	emotion$ExpectationMaximization$Entropy$exact match$Emma$Expectation-Maximization$expectation maximization$expectation maximisation$Expectation Maximization$	8
But these translations can be ambiguous, hence we can use EM approach similar to (Khapra et al, 2011) as follows: E-Step: P (SL1 |u, a) = ?	EM	emotion$ExpectationMaximization$Entropy$exact match$Emma$Expectation-Maximization$expectation maximization$expectation maximisation$Expectation Maximization$	8
We learn these parame- ters via EM (Dempster et al, 1977), iterating between computing expected counts and adjusting parameters to maximize the posterior probability of the parameters.	EM	emotion$ExpectationMaximization$Entropy$exact match$Emma$Expectation-Maximization$expectation maximization$expectation maximisation$Expectation Maximization$	8
Experimental results demonstrate an ER of 35% over a previous state-of-the-art method that uses heuristic alignments.	ER	error reduction$Error rate$	0
Empirical results demonstrate a 20.6% ER in token labeling accuracy com- pared to a strong baseline method that employs a set of high-precision alignments.	ER	error reduction$Error rate$	0
Furthermore, we provide a 63.8% ER compared to IBM Model 4 (Brown et al, 1993).	ER	error reduction$Error rate$	0
2000), with about 3% relative improve- ment (ER from 18.6% to 18%, trained on small data) over the best original system.	ER	error reduction$Error rate$	0
We obtain an ER of 35.1% over a previous state-of-the-art extraction method that us	ER	error reduction$Error rate$	0
ExtrCRF also pro- duces an ER of 21.7% compared to M+R-CRF without the use of matching records.	ER	error reduction$Error rate$	0
We obtain an ER of 35.1% over a previous state-of-the-art extraction method that uses heuristically generated alignments.	ER	error reduction$Error rate$	0
In comparison to the previous state- of-the-artM-CRF, theExtrCRFmethod provides an ER of 35.1%.	ER	error reduction$Error rate$	0
ERs for standard recognizers are 5-10 times higher than for dictation tasks.	ER	error reduction$Error rate$	1
5 98.3% 98.7% 99.1% 99.5% 99.9% 0% 5% 10 % 15 % 20 % 25 % 30 % 35 % 40 % 45 % 50 % p err  (ping error rate) Ta sk  c o pm le tio n   ra te   (% ) 5 5.4 5.8 6.2 6.6 7 Av e ra ge   di a lo g  le n gt h  (tu rn s) Task completion rate Average dialog length  Figure 5: ER of the ping action vs. success- ful task completion rate and average dialog length.	ER	error reduction$Error rate$	1
Approach ER TwitterSA(maxconf) 18.7 TwitterSA(weights) 19.4 TwitterSA(single) 20 TwitterSA(voting) 22.6 Unigrams 20.9 ReviewSA 21.7 Unigrams-TS 24.3 Table 5: Results for polarity detection.	ER	error reduction$Error rate$	1
All chunks be- gin with a B symbol, regardless of whether the previous word is tagged O or I. NP Chunking Results Method F-Measure Numits Perc, avg, cc=0 93.53 13 Perc, noavg, cc=0 93.04 35 Perc, avg, cc=5 93.33 9 Perc, noavg, cc=5 91.88 39 ME, cc=0 92.34 900 ME, cc=5 92.65 200 POS Tagging Results Method ER/% Numits Perc, avg, cc=0 2.93 10 Perc, noavg, cc=0 3.68 20 Perc, avg, cc=5 3.03 6 Perc, noavg, cc=5 4.04 17 ME, cc=0 3.4 100 ME, cc=5 3.28 200 Figure 4: Results for various methods on the part-of- speech tagging and chunking tasks on development data.	ER	error reduction$Error rate$	1
Approach ER TwitterSA(cleaning) 18.1 TwitterSA(no-cleaning) 19.9 Unigrams 27.6 ReviewSA 32 Table 4: Results for subjectivity detection.	ER	error reduction$Error rate$	1
As perr increases, the policy decreasingly employs the ping diagnostic action in favor of the ask-working-ok communicative action until perr = 20%, at which point the ping action is 84 85 86 87 88 89 90 91 92 93 94 0% 5% 10 % 15 % 20 % 25 % 30 % 35 % 40 % 45 % 50 % p err  (ping error rate) Av e ra ge   re tu rn Figure 4: ER of the ping action vs. reward gained per dialog.	ER	error reduction$Error rate$	1
NUM?;	NUM	numeral$numerals$National University of Mongolia$Numbe$number$Number$	0
It may seem surprising, at first, to see 9 gender values in an Indo-European language (as opposed to, say, a Bantu language), but this position is well argued for by (Saloni, 1976), who distinguishes those genders on the basis of agreement with ad- jectives and NUMs;4 we will not attempt to fur- ther justify this position here.	NUM	numeral$numerals$National University of Mongolia$Numbe$number$Number$	0
Lexical adjectives, including demonstratives ad- verbs, NUMs, and possessive adjectives, as well as ordinary intersective adjectives ?	NUM	numeral$numerals$National University of Mongolia$Numbe$number$Number$	0
Some Polish NUMs have forms that agree in case with noun (marked congr), as well as forms that require a noun in genitive case (marked rec): (2) Przyszli came dwaj two-nom.congr ch?opcy.	NUM	numeral$numerals$National University of Mongolia$Numbe$number$Number$	0
The category of accomodability is important for the description of Polish NUM-nominal phrase.	NUM	numeral$numerals$National University of Mongolia$Numbe$number$Number$	0
This c lass  covers  most nouns, verbs ,  verb ia l  ad~ect ives   and NUMs .	NUM	numeral$numerals$National University of Mongolia$Numbe$number$Number$	0
Suushi below is a  grammar term of a class of NUM.	NUM	numeral$numerals$National University of Mongolia$Numbe$number$Number$	1
It may seem surprising, at first, to see 9 gender values in an Indo-European language (as opposed to, say, a Bantu language), but this position is well argued for by (Saloni, 1976), who distinguishes those genders on the basis of agreement with ad- jectives and NUM;4 we will not attempt to fur- ther justify this position here.	NUM	numeral$numerals$National University of Mongolia$Numbe$number$Number$	1
It is important to realize, however, that these classes are defined mainly on the basis of the inflectional properties of their members; e.g., the class numeral is much nar- rower here than traditionally, as it does not include so-called ordinal NUM (which, morphosyntac- tically, are adjectives).	NUM	numeral$numerals$National University of Mongolia$Numbe$number$Number$	1
l emma exp lanat ion   o  suushi*  wa  suru   J  f  mo  (  )  nado  nai  aru   kara  koto  dewa  nen  hi  no  comma  period  NUM  topic marker  'do'  right angular parenthesis  left angular parenthesis  topic marker  left parenthes	NUM	numeral$numerals$National University of Mongolia$Numbe$number$Number$	1
Lexical adjectives, including demonstratives ad- verbs, NUM, and possessive adjectives, as well as ordinary intersective adjectives ?	NUM	numeral$numerals$National University of Mongolia$Numbe$number$Number$	1
Some Polish NUM have forms that agree in case with noun (marked congr), as well as forms that require a noun in genitive case (marked rec): (2) Przyszli came dwaj two-nom.congr ch?opcy.	NUM	numeral$numerals$National University of Mongolia$Numbe$number$Number$	1
This c lass  covers  most nouns, verbs ,  verb ia l  ad~ect ives   and NUM .	NUM	numeral$numerals$National University of Mongolia$Numbe$number$Number$	1
582  Corpus Building for Mongolian Language  Purev Jaimai  Center for Research on Language Processing,  NUM, Mongolia  purev@num.edu.mn  Odbayar Chimeddorj  Center for Research on Language Processing,  NUM, Mongolia  odbayar@num.edu.mn      Abstract  This paper presents an ongoing research  aimed to build the first corpus, 5 million  words, for Mongolian language by focus- ing on annotating and tagging corpus texts  according to TEI XML (McQuee	NUM	numeral$numerals$National University of Mongolia$Numbe$number$Number$	2
Two years ago, a research project to build  a tagged corpus for Mongolian began at the  Center for Research on Language Processing,  NUM.	NUM	numeral$numerals$National University of Mongolia$Numbe$number$Number$	2
c?2009 ACL and AFNLP Part of Speech Tagging for Mongolian Corpus       Purev Jaimai and Odbayar Chimeddorj  Center for Research on Language Processing  NUM  {purev, odbayar}@num.edu.mn         Abstract    This paper introduces the current result of a  research work which aims to build a 5 million  tagged word corpus for Mongolian.	NUM	numeral$numerals$National University of Mongolia$Numbe$number$Number$	2
582  Corpus Building for Mongolian Language  Purev Jaimai  Center for Research on Language Processing,  NUM, Mongolia  purev@num.edu.mn  Odbayar Chimeddorj  Center for Research on Language Processing,  NUM, Mongolia  odbayar@num.edu.mn      Abstract  This paper presents an ongoing research  aimed to build the first corpus, 5 million  words, for Mongolian language by focus- ing on annotating and tagging corpus texts  according to TEI XML (McQueen, 2004)  format.	NUM	numeral$numerals$National University of Mongolia$Numbe$number$Number$	2
1  " " " ' ' ' " |   10 100  NUMr of references  O ?	NUM	numeral$numerals$National University of Mongolia$Numbe$number$Number$	3
Thus,  Genre  NUMr of  Documents   NUMr of person- x entities  Art      3346       1455  Business        315         182  Education      6177       2351  Government      3374         945  Healthcare        914         405  Movies        677       2292  Music        976         366  Politics      4298         949  Religion      2699       1030  Science      7211       2783  Sports      4417       2009    Table 2: Breakdown of docu	NUM	numeral$numerals$National University of Mongolia$Numbe$number$Number$	3
2007 Association for Computational Linguistics Computational Linguistics Volume 33, NUMr 1 Research in QA has been developed from two different scientific perspectives, artificial intelligence (AI) and information retrieval (IR).	NUM	numeral$numerals$National University of Mongolia$Numbe$number$Number$	3
Thus,  Genre  NUMr of  Documents   NUMr of person- x entities  Art      3346       1455  Business        315         182  Education      6177       2351  Government      3374         945  Healthcare        914         405  Movies        677       2292  Music        976         366  Politics      4298         949  Religion      2699       1030  Science      7211       2783  Sports      4417       2009    Tab	NUM	numeral$numerals$National University of Mongolia$Numbe$number$Number$	3
We first obtained from 10,000 to 50,000 unique  documents from the TREC 1, 2 and 3 volumes using the  Inquery search engine from UMass Amherst for each of  the following subjects: art, business, education,  government, healthcare, movies, music, politics,  NUMr of  occurrences  Percentage  of entities  1 46.66  2 18.78  3 9.03  4 4.55  5 1.86  6 1.16  7 0.83  8 0.46  9 or more 16.67    Table 1: Breakdown of distribution by number of  occurrences within the Person X corpus.	NUM	numeral$numerals$National University of Mongolia$Numbe$number$Number$	3
Still, 43 Computational Linguistics Volume 33, NUMr 1 many of these early systems (including LUNAR and BASEBALL) were no more than ?	NUM	numeral$numerals$National University of Mongolia$Numbe$number$Number$	3
We incorpo-  rate multiple anaphora resolution factors into  a statistical framework - -  specifically the dis-  tance between the pronoun and the proposed  antecedent, gender/NUM/animaticity of the  proposed antecedent, governing head informa-  tion and noun phrase repetition.	NUM	numeral$numerals$National University of Mongolia$Numbe$number$Number$	4
The training corpus is  marked with the NUM of times a referent has  been mentioned up to that point in the story.	NUM	numeral$numerals$National University of Mongolia$Numbe$number$Number$	4
Here we are concerned with the probability that  a proposed antecedent is correct given that it  has been repeated a certain NUM of times.	NUM	numeral$numerals$National University of Mongolia$Numbe$number$Number$	4
The second  experiment investigates a method for unsuper-  vised learning of gender/NUM/animaticity  information.	NUM	numeral$numerals$National University of Mongolia$Numbe$number$Number$	4
Next, the actual words in a proposed noun-  phrase antecedent give us information regarding  the gender, NUM, and animaticity of the pro-  posed referent.	NUM	numeral$numerals$National University of Mongolia$Numbe$number$Number$	4
The words in the antecedent sometimes  also let us test for NUM agreement.	NUM	numeral$numerals$National University of Mongolia$Numbe$number$Number$	4
1  " " " ' ' ' " |   10 100  NUM of references  O ?	NUM	numeral$numerals$National University of Mongolia$Numbe$number$Number$	5
Thus,  Genre  NUM of  Documents   NUM of person- x entities  Art      3346       1455  Business        315         182  Education      6177       2351  Government      3374         945  Healthcare        914         405  Movies        677       2292  Music        976         366  Politics      4298         949  Religion      2699       1030  Science      7211       2783  Sports      4417       2009    Table 2: Breakdown of docu	NUM	numeral$numerals$National University of Mongolia$Numbe$number$Number$	5
2007 Association for Computational Linguistics Computational Linguistics Volume 33, NUM 1 Research in QA has been developed from two different scientific perspectives, artificial intelligence (AI) and information retrieval (IR).	NUM	numeral$numerals$National University of Mongolia$Numbe$number$Number$	5
Thus,  Genre  NUM of  Documents   NUM of person- x entities  Art      3346       1455  Business        315         182  Education      6177       2351  Government      3374         945  Healthcare        914         405  Movies        677       2292  Music        976         366  Politics      4298         949  Religion      2699       1030  Science      7211       2783  Sports      4417       2009    Tab	NUM	numeral$numerals$National University of Mongolia$Numbe$number$Number$	5
We first obtained from 10,000 to 50,000 unique  documents from the TREC 1, 2 and 3 volumes using the  Inquery search engine from UMass Amherst for each of  the following subjects: art, business, education,  government, healthcare, movies, music, politics,  NUM of  occurrences  Percentage  of entities  1 46.66  2 18.78  3 9.03  4 4.55  5 1.86  6 1.16  7 0.83  8 0.46  9 or more 16.67    Table 1: Breakdown of distribution by number of  occurrences within the Person X corpus.	NUM	numeral$numerals$National University of Mongolia$Numbe$number$Number$	5
Still, 43 Computational Linguistics Volume 33, NUM 1 many of these early systems (including LUNAR and BASEBALL) were no more than ?	NUM	numeral$numerals$National University of Mongolia$Numbe$number$Number$	5
Efficient sampling with RB While caching the sampling equation as described in the previous section improved the efficiency, the smooth- ing only bucket s is small, but computing the asso- ciated mass is costly because it requires us to con- sider all topics and paths.	RB	refined bucket$expressing adverbials$	0
This program differs  from earlier work in its almost complete lack of  hAD-crafting, relying instead on a very small  corpus of Penn Wall Street Journal Tree-bank  text (Marcus et al, 1993) that has been mark	AD	and$Automatic differentiation$	0
We incorpo-  rate multiple anaphora resolution factors into  a statistical framework - -  specifically the dis-  tance between the pronoun AD the proposed  antecedent, gender/number/animaticity of the  proposed antecedent, governing head informa-  tion AD noun phrase repetition.	AD	and$Automatic differentiation$	0
We present some experiments il-  lustrating the accuracy of the method AD note  that with this information added, our pronoun  resolution method achieves 84.2% accuracy.	AD	and$Automatic differentiation$	0
A Statistical Approach to Anaphora Resolution  Niyu  Ge, John  Hale AD Eugene Charn iak   Dept.	AD	and$Automatic differentiation$	0
brown, edu  Abst ract   This paper presents an algorithm for identi-  fying pronominal anaphora AD two experi-  ments based upon this algorithm.	AD	and$Automatic differentiation$	0
yu  Ge, John  Hale AD Eugene Charn iak   Dept.	AD	and$Automatic differentiation$	0
We incorpo-  rate multiple anaphora resolution factors into  a statistical framework - -  specifically the dis-  tance between the pronoun AD the proposed  a	AD	and$Automatic differentiation$	0
s into  a statistical framework - -  specifically the dis-  tance between the pronoun AD the proposed  antecedent, gender/number/animaticity of the  proposed antecedent, governing head informa-  tion AD noun phrase repetition.	AD	and$Automatic differentiation$	0
Our first experiment  shows the relative contribution of each source  Of information AD demonstrates a uccess rate  of 82.9% for all sources combined.	AD	and$Automatic differentiation$	0
AD as nonar- chimedean analysis.	AD	and$Automatic differentiation$	1
AD may be used on any function (e.g., a neural net), but for our simple sum-of-products function Z, it happens that ?	AD	and$Automatic differentiation$	1
It started off with the following declaration (Karttunen 1971b): It is evident that logical relations between main sentences and their complements are of great significance in any system of ADP that depends on natural language.	ADP	automatic data processing$prepositions or postpositions$adposition$	0
cture that  uses full custom integrated circuits to per-  form hidden Markov-moddel-based speech  87   Computing relative polarity for textual inference Rowan Nairn, Cleo Condoravdi, Lauri Karttunen Palo Alto Research Center rnairn@gmail.com , condorav@parc.com , Lauri.Karttunen@parc.com Abstract Semantic relations between main and complement sentences are of great signifi- cance in any system of ADP that depends on natural lan- guage.	ADP	automatic data processing$prepositions or postpositions$adposition$	0
9] Lauri Karttunen, 29, wrote: It is evident that logical relations between main sentences and their comple- ments are of great significance in any system of ADP that depends on natural language.	ADP	automatic data processing$prepositions or postpositions$adposition$	0
To quote again the opening paragraph of my 1970 ACL presentation (Karttunen 1971b): It is evident that logical relations between main sentences and their complements are of great significance in any system of ADP that depends on natural language.	ADP	automatic data processing$prepositions or postpositions$adposition$	0
A heuristic to deal with this is to specify for each of the two  languages whether ADP are more common, where "preposi-  tion" here is meant not in the usual part-of-speech sense, but rather in a broad sense  of the tendency of function words to attach left or right.	ADP	automatic data processing$prepositions or postpositions$adposition$	1
We generally learn that adpositions (ADP) take nouns as argu- ments.	ADP	automatic data processing$prepositions or postpositions$adposition$	1
A heuristic to deal with this is to specify for each of the  two languages whether ADP  more common, where "preposition" here is meant not  in the usual part-of-speech sense, but rather in a broad  sense of the tendency of function words to attach left  or right.	ADP	automatic data processing$prepositions or postpositions$adposition$	1
Case-marking adpositions must be specified as either ADP.	ADP	automatic data processing$prepositions or postpositions$adposition$	1
Note that the possibility of different means for realizing conjunction, e.g., using morphemes, punctuation or multiple ADPs, falls out naturally from this setup.)	ADP	automatic data processing$prepositions or postpositions$adposition$	2
= "possessive"]; matcher P: [category=="ADP" & subCategory=="preposition"]; term "an": A+ N ; term "npn": N P D?	ADP	automatic data processing$prepositions or postpositions$adposition$	2
The tag set selected for the annotation of the  COME and GO data frames consists of morpho- syntactic tags that characterize verb usage as well as  semantic tags that aim to highlight the semantic com- ponent of, for instance, adverbial and ADPal  phrases that accompany the verb.	ADP	automatic data processing$prepositions or postpositions$adposition$	2
between the ordered elements (marked gr i : gr j ), signalling periphrastic ADPs and/or modifi- cation.	ADP	automatic data processing$prepositions or postpositions$adposition$	2
and gender (M1), the fields included in M1 along with additional information (lemmas) for conjunc- tions, particles and ADPs (M2), and finally the information included in M2 enriched with per- son for pronouns and person, tense and aspect for verbs (M3).	ADP	automatic data processing$prepositions or postpositions$adposition$	2
Major word classes include  thirteen tags: noun, verb, adjective, pro- noun/determiner, article, adverb, ADP, con- junction, numeral, interjection, unassigned, resi- dual and punctuation.	ADP	automatic data processing$prepositions or postpositions$adposition$	2
2003; Mnih and Hinton, 2007; CW, 2008) have demonstrated impressive performance at the task of language modeling.	CW	Collobert and Weston$Confidence Weighted$characters per word$	0
In previous studies, these vectors themselves can capture distributionally de- rived similarities, by directly comparing the word vectors themselves using simple measures such as 155 Euclidean distance (CW, 2008).	CW	Collobert and Weston$Confidence Weighted$characters per word$	0
As shown in (CW, 2008) and then (Huang et al 2012), simple distance measures us- ing the representations derived from this process are both useful for assessing word similarity and relat- edness.	CW	Collobert and Weston$Confidence Weighted$characters per word$	0
Inspired by these successes and recent work us- ing neural networks to learn phrase- or sentence- 1621 level embeddings (CW, 2008; Huang et al, 2013; Le and Mikolov, 2014; Sutskever et al, 2014; Kiros et al, 2015), we propose a novel deep architecture for text under- standing, which we call a deep reinforcement rele- vance network (DRRN).	CW	Collobert and Weston$Confidence Weighted$characters per word$	0
Inspired by recent advances of neural network mod- els for NLP applications (CW, 2008; Mikolov et al.,	CW	Collobert and Weston$Confidence Weighted$characters per word$	0
1 Introduction Distributed representations of text (embeddings) have been the target of much research in natural language processing (CW, 2008; Mikolov et al, 2013; Pennington et al, 2014; Levy et al, 2015).	CW	Collobert and Weston$Confidence Weighted$characters per word$	0
4 Multi-Class CW Learning As in the binary case, we maintain a distribution over weight vectors w ?	CW	Collobert and Weston$Confidence Weighted$characters per word$	1
6 CW Margins In the previous section, we showed that margin val- ues could be used to detect domain shifts.	CW	Collobert and Weston$Confidence Weighted$characters per word$	1
Additionally, we showed improved detection results using a probabilistic margin based on CW learning.	CW	Collobert and Weston$Confidence Weighted$characters per word$	1
Furthermore, we show that the pre- viously proposed CW learning al- gorithm (Dredze et al, 2008) can provide a more informative measure than a simple margin for this task.	CW	Collobert and Weston$Confidence Weighted$characters per word$	1
Average number of CW.	CW	Collobert and Weston$Confidence Weighted$characters per word$	2
cpw: desired (mean) number of CW ?	CW	Collobert and Weston$Confidence Weighted$characters per word$	2
It is generally considered that frequently occurring words are usually short, so the average number of CW was broadly used for mea- suring readability in a robust manner.	CW	Collobert and Weston$Confidence Weighted$characters per word$	2
F2 - average number of CW ?	CW	Collobert and Weston$Confidence Weighted$characters per word$	2
CW 5.22 words per sentence 21.17 words per text 8,476 simple words 75.52% sentences per text 400.34 passive voice 15.11% total sentences 13,091 simplified sentences 16,71% Table 5: Statistics from the balanced text sample Figure 2: Clauses per sentence in the sample 4.2 Simplification analysis We manually analysed and annotated all sentences  in  our  samples.	CW	Collobert and Weston$Confidence Weighted$characters per word$	2
We put a threshold on the number of CW to decide whether it will be considered for edit distance 1 or 2 errors.	CW	Collobert and Weston$Confidence Weighted$characters per word$	2
Given the undirected graph G, we can turn it into a MRF by defining unary poten- tials over nodes and binary potentials over edges.	MRF	Markov Random Field$Markov random field$	0
Latent Concept Expansion Using MRFs.	MRF	Markov Random Field$Markov random field$	0
To do this, we define a MRF over the latent topic layer.	MRF	Markov Random Field$Markov random field$	0
Pat-  tern recognition techniques such as Hidden Markov Models,  MRFs, Multi Layer Perceptrons, Boltz-  mann Machines are used in speech, language and vision.	MRF	Markov Random Field$Markov random field$	0
Exam- ples of such techniques are MRFs (Rat- naparkhi et al, 1994; Abney, 1997; Della Pietra et al, 1997; Johnson et al, 1999), and boosting or perceptron approaches to reranking (Freund et al, 1998; Collins, 2000; Collins and Duffy, 2002).	MRF	Markov Random Field$Markov random field$	0
Discriminative methods such as Conditional Random Fields (CRFs) (Lafferty et al, 2001), Semi-MRFs (Sarawagi and Cohen, 2004), and perceptrons (Collins, 2002a) have been popular approaches for sequence label- ing because of their excellent performance, which is mainly due to their ability to incorporate many kinds of overlapping and non-independent features.	MRF	Markov Random Field$Markov random field$	0
Hinge-loss MRFs: Convex inference for structured prediction.	MRF	Markov Random Field$Markov random field$	1
Haghighi and Klein (2006) assign each label in the model certain prototypical features and train a MRF for sequence tagging from these labeled features.	MRF	Markov Random Field$Markov random field$	1
Their potential func-     Figure 2: The structure of the MRF for  representing the term dependency among the query    and the expansion terms        .	MRF	Markov Random Field$Markov random field$	1
6), this exponentiated form becomes a product of local multiplicative factors, and hence our model forms an undirected graphical model, or MRF.	MRF	Markov Random Field$Markov random field$	1
Other approaches include transformation based learning (Brill, 1995), contrastive estimation for conditional random fields (Smith and Eisner, 2005), MRFs (Haghighi and Klein, 2006), a multilingual approach (Snyder et al.,	MRF	Markov Random Field$Markov random field$	1
2 Latent Pseudo-Syntactic Structure The models presented in this paper are phrased in terms of variables in an undirected graphical model, MRF.	MRF	Markov Random Field$Markov random field$	1
This work was supported in  part by the Center for Intelligent IRl,  in part by SPAWARSYSCEN-SD grant number  N66001-02-1-8903 and in part by Advanced Research  and Development Activity under contract number  MDA904-01-C-0984.	IR	Information Retrieva$information retrieva$information retrieval$Information retrieval$inside region$iterative re$Information Retrieval$Informtion Retrieval$informal register$	0
In Proceedings of the Span- ish Conference in IRl.	IR	Information Retrieva$information retrieva$information retrieval$Information retrieval$inside region$iterative re$Information Retrieval$Informtion Retrieval$informal register$	0
For each entity mention e in the evaluation set, we  first locate the truth chain TC that contains that mention  (it can be in only one truth chain) and the system?s  hypothesized chain HC that contains it (again, there can  Chung Heong Gooi and James Allan  Center for Intelligent IRl  Department of Computer Science  University of Massachusetts  Amherst, MA 01003  {cgooi,allan}@cs.umass.edu  be only one hypothesis chain).	IR	Information Retrieva$information retrieva$information retrieval$Information retrieval$inside region$iterative re$Information Retrieval$Informtion Retrieval$informal register$	0
Readings in IRl.	IR	Information Retrieva$information retrieva$information retrieval$Information retrieval$inside region$iterative re$Information Retrieval$Informtion Retrieval$informal register$	0
83  Learning the Space of Word Meanings  for IRl Systems  Ko ich i  HORI ,  Se inosuke  TODA and  H isash i  YASUNAGA  Nat iona l  Ins t i tu te  of  Japanese  L i te ra ture   1 -16-10  Yutakacho  Sh inagawaku Tokyo  142 Japan   Abstract :  Several methods  to represent  mean ings   of words have been proposed.	IR	Information Retrieva$information retrieva$information retrieval$Information retrieval$inside region$iterative re$Information Retrieval$Informtion Retrieval$informal register$	0
Center for Intelligent  IRl, Department of Computer  Science, University of Massachusetts, 2002.	IR	Information Retrieva$information retrieva$information retrieval$Information retrieval$inside region$iterative re$Information Retrieval$Informtion Retrieval$informal register$	0
The SoNaR-500 corpus 3 requires IRl tools for efficient searching, as trivial solutions are simply too slow or cumbersome to use.	IR	Information Retrieva$information retrieva$information retrieval$Information retrieval$inside region$iterative re$Information Retrieval$Informtion Retrieval$informal register$	1
These resources are typically developed for the domain users to help them categorize the domain knowledge and agree on notational standards, and to help them retrieve information using conventional IRl applications.	IR	Information Retrieva$information retrieva$information retrieval$Information retrieval$inside region$iterative re$Information Retrieval$Informtion Retrieval$informal register$	1
In  order to perform a careful excursion into the limited  work on cross document coreferencing, we deployed  different IRl techniques for entity  disambiguation and clustering.	IR	Information Retrieva$information retrieva$information retrieval$Information retrieval$inside region$iterative re$Information Retrieval$Informtion Retrieval$informal register$	1
In the present paper, 1 By modeling lexical cohesion as a graph structure, we follow earlier approaches in IRl, notably by Salton and colleagues (Salton et al, 1994).	IR	Information Retrieva$information retrieva$information retrieval$Information retrieval$inside region$iterative re$Information Retrieval$Informtion Retrieval$informal register$	1
While not as obviously important to natural lan- guage IRl, this property of al- ternative phrases can be used to improve future queries (see Section 4.1).	IR	Information Retrieva$information retrieva$information retrieval$Information retrieval$inside region$iterative re$Information Retrieval$Informtion Retrieval$informal register$	1
Recogniz- ing MWEs has been shown to be useful for a num- ber of applications such as IRl (Lewis and Croft, 1990; Rila Mandala and Tanaka, 2000; Wacholder and Song, 2003) and POS tag- ging (Piao et al, 2003).	IR	Information Retrieva$information retrieva$information retrieval$Information retrieval$inside region$iterative re$Information Retrieval$Informtion Retrieval$informal register$	1
The SoNaR-500 corpus 3 requires IR tools for efficient searching, as trivial solutions are simply too slow or cumbersome to use.	IR	Information Retrieva$information retrieva$information retrieval$Information retrieval$inside region$iterative re$Information Retrieval$Informtion Retrieval$informal register$	2
These resources are typically developed for the domain users to help them categorize the domain knowledge and agree on notational standards, and to help them retrieve information using conventional IR applications.	IR	Information Retrieva$information retrieva$information retrieval$Information retrieval$inside region$iterative re$Information Retrieval$Informtion Retrieval$informal register$	2
In  order to perform a careful excursion into the limited  work on cross document coreferencing, we deployed  different IR techniques for entity  disambiguation and clustering.	IR	Information Retrieva$information retrieva$information retrieval$Information retrieval$inside region$iterative re$Information Retrieval$Informtion Retrieval$informal register$	2
In the present paper, 1 By modeling lexical cohesion as a graph structure, we follow earlier approaches in IR, notably by Salton and colleagues (Salton et al, 1994).	IR	Information Retrieva$information retrieva$information retrieval$Information retrieval$inside region$iterative re$Information Retrieval$Informtion Retrieval$informal register$	2
While not as obviously important to natural lan- guage IR, this property of al- ternative phrases can be used to improve future queries (see Section 4.1).	IR	Information Retrieva$information retrieva$information retrieval$Information retrieval$inside region$iterative re$Information Retrieval$Informtion Retrieval$informal register$	2
Recogniz- ing MWEs has been shown to be useful for a num- ber of applications such as IR (Lewis and Croft, 1990; Rila Mandala and Tanaka, 2000; Wacholder and Song, 2003) and POS tag- ging (Piao et al, 2003).	IR	Information Retrieva$information retrieva$information retrieval$Information retrieval$inside region$iterative re$Information Retrieval$Informtion Retrieval$informal register$	2
5 Application of Domain Model IR from spoken dialog data is an important requirement for call centers.	IR	Information Retrieva$information retrieva$information retrieval$Information retrieval$inside region$iterative re$Information Retrieval$Informtion Retrieval$informal register$	3
202  Noun-Phrase Analysis in Unrestricted Text for Information Retrieval  David A. Evans, Chengxiang Zhai  Laboratory for Computational Linguistics  Carnegie Mellon Univeristy  Pittsburgh, PA 15213  dae@cmu.edu, cz25@andrew.cmu.edu  Abstract  IR is an important ap-  plication area of natural-language pro-  cessing where one encounters the gen-  uine challenge of processing large quanti-  ties of unrestricted natural-language t xt.	IR	Information Retrieva$information retrieva$information retrieval$Information retrieval$inside region$iterative re$Information Retrieval$Informtion Retrieval$informal register$	3
IR and machine learning techniques were integrated to determine sentence importance (Kupiec et al, 1995; Wong et al, 2008).	IR	Information Retrieva$information retrieva$information retrieval$Information retrieval$inside region$iterative re$Information Retrieval$Informtion Retrieval$informal register$	3
IR: when MWEs like pop star are indexed as a unit, the accuracy of the system im- proves on multiword queries (Acosta et al, 2011).	IR	Information Retrieva$information retrieva$information retrieval$Information retrieval$inside region$iterative re$Information Retrieval$Informtion Retrieval$informal register$	3
IR with conceptual graph matching.	IR	Information Retrieva$information retrieva$information retrieval$Information retrieval$inside region$iterative re$Information Retrieval$Informtion Retrieval$informal register$	3
The first three measures are known to be used in context of IR to capture top- ical informations.	IR	Information Retrieva$information retrieva$information retrieval$Information retrieval$inside region$iterative re$Information Retrieval$Informtion Retrieval$informal register$	3
Our adapted strategy for selecting a dense subgraph of GI is based on the IRmoval of low-coherence vertices, i.e., fragment interpreta- tions.	IR	Information Retrieva$information retrieva$information retrieval$Information retrieval$inside region$iterative re$Information Retrieval$Informtion Retrieval$informal register$	5
After IR-ranking,  the performance of alignment models over the 300  sentence pairs is calculated.	IR	Information Retrieva$information retrieva$information retrieval$Information retrieval$inside region$iterative re$Information Retrieval$Informtion Retrieval$informal register$	5
Then the IRinforcement algorithm is used until the node weight values converge (the difference be- tween scores at two iterations is below 0.0001 for all nodes) or 5,000 iterations are reached.	IR	Information Retrieva$information retrieva$information retrieval$Information retrieval$inside region$iterative re$Information Retrieval$Informtion Retrieval$informal register$	5
A natural way to incorporate ordering information is IR-estimation of the model parameters, since the content model itself provides such information through its transition structure.	IR	Information Retrieva$information retrieva$information retrieval$Information retrieval$inside region$iterative re$Information Retrieval$Informtion Retrieval$informal register$	5
It would be interesting to use our proposed estimates as initialization for EM-style IR-estimation.	IR	Information Retrieva$information retrieva$information retrieval$Information retrieval$inside region$iterative re$Information Retrieval$Informtion Retrieval$informal register$	5
3 Model Construction We employ an IR-estimation procedure that al- ternates between (1) creating clusters of text spans with similar word distributions to serve as representatives of within-document topics, and (2) computing models of word distributions and topic changes from the clusters so derived.3 Formalism preliminaries We treat texts as sequences of pre-defined text spans, each presumed to convey infor- mation abou	IR	Information Retrieva$information retrieva$information retrieval$Information retrieval$inside region$iterative re$Information Retrieval$Informtion Retrieval$informal register$	5
This work was supported in  part by the Center for Intelligent IR,  in part by SPAWARSYSCEN-SD grant number  N66001-02-1-8903 and in part by Advanced Research  and Development Activity under contract number  MDA904-01-C-0984.	IR	Information Retrieva$information retrieva$information retrieval$Information retrieval$inside region$iterative re$Information Retrieval$Informtion Retrieval$informal register$	6
In Proceedings of the Span- ish Conference in IR.	IR	Information Retrieva$information retrieva$information retrieval$Information retrieval$inside region$iterative re$Information Retrieval$Informtion Retrieval$informal register$	6
For each entity mention e in the evaluation set, we  first locate the truth chain TC that contains that mention  (it can be in only one truth chain) and the system?s  hypothesized chain HC that contains it (again, there can  Chung Heong Gooi and James Allan  Center for Intelligent IR  Department of Computer Science  University of Massachusetts  Amherst, MA 01003  {cgooi,allan}@cs.umass.edu  be only one hypothesis chain).	IR	Information Retrieva$information retrieva$information retrieval$Information retrieval$inside region$iterative re$Information Retrieval$Informtion Retrieval$informal register$	6
Readings in IR.	IR	Information Retrieva$information retrieva$information retrieval$Information retrieval$inside region$iterative re$Information Retrieval$Informtion Retrieval$informal register$	6
83  Learning the Space of Word Meanings  for IR Systems  Ko ich i  HORI ,  Se inosuke  TODA and  H isash i  YASUNAGA  Nat iona l  Ins t i tu te  of  Japanese  L i te ra ture   1 -16-10  Yutakacho  Sh inagawaku Tokyo  142 Japan   Abstract :  Several methods  to represent  mean ings   of words have been proposed.	IR	Information Retrieva$information retrieva$information retrieval$Information retrieval$inside region$iterative re$Information Retrieval$Informtion Retrieval$informal register$	6
Center for Intelligent  IR, Department of Computer  Science, University of Massachusetts, 2002.	IR	Information Retrieva$information retrieva$information retrieval$Information retrieval$inside region$iterative re$Information Retrieval$Informtion Retrieval$informal register$	6
keypads and, last but not least, the fact that people mostly communicate between friends and relatives in an IR.	IR	Information Retrieva$information retrieva$information retrieval$Information retrieval$inside region$iterative re$Information Retrieval$Informtion Retrieval$informal register$	8
If IR, ?	IR	Information Retrieva$information retrieva$information retrieval$Information retrieval$inside region$iterative re$Information Retrieval$Informtion Retrieval$informal register$	8
This is an interesting problem because a good solution to it could be applied to many other tasks as well: to enhancing access to digital libraries (containing diachronic and dialectal variants), for example, or to improving treatment of IRs such as SMS messages and blogs, etc.	IR	Information Retrieva$information retrieva$information retrieval$Information retrieval$inside region$iterative re$Information Retrieval$Informtion Retrieval$informal register$	8
A reasonable conjecture is that in this IR, the acronyms AI and CS largely replaced the expansions.	IR	Information Retrieva$information retrieva$information retrieval$Information retrieval$inside region$iterative re$Information Retrieval$Informtion Retrieval$informal register$	8
Given that the author has chosen an IR, the phrase goin to is likely to be replaced by gonna.	IR	Information Retrieva$information retrieva$information retrieval$Information retrieval$inside region$iterative re$Information Retrieval$Informtion Retrieval$informal register$	8
are undoubtedly representative of casual speech.3 However, what may not be so obvious is that an IR does not license the use of subject ellipsis at all times.	IR	Information Retrieva$information retrieva$information retrieval$Information retrieval$inside region$iterative re$Information Retrieval$Informtion Retrieval$informal register$	8
2  Feedback-3  Non-Word Errors  Remain Corrected  3,049 3,457  2,816 3,690  2,791 3,715  2,784 3,722  RW Errors  Remain Corrected  1,692 0  1,692 0  1,692 : 0  1,692 !	RW	Real-Word$Random Walk$	0
Non-Word Errors  Number 6,506  % 79.4  RW Errors Total Errors  i,692 8,198  20.6 100  Table 1: OCR Errors Originating from Literal Words  We conducted three experiments:  1.	RW	Real-Word$Random Walk$	0
RW Errors Introduced Errors  Pass Unknown Wds Lex Wds  First 182 0  Feedback-1 182 0  Feedback-2  Feedback-3  Remain Corrected  2,684 3,822  1,972 4,354  1943 4,563  1948 4,558  Remain Corrected  1,692 0  1,692 0  1,692 0  1,692 0  182 0  182 0  Total Error  Errors Reduction (%)  4,558 44.4  3,846 53.1  3,817 53.4  3,822 53.4  Table 3: Results from Context-Dependent Non-Word Error Correctio	RW	Real-Word$Random Walk$	0
RW Errors Introduced Errors  Pass Unknown Wds Lex Wds  First 182 0  Feedback-1 182 0  Feedback-	RW	Real-Word$Random Walk$	0
c?2009 ACL and AFNLP RW Spelling Correction using Google Web 1T 3-grams Aminul Islam Department of Computer Science University of Ottawa Ottawa, ON, K1N 6N5, Canada mdislam@site.uottawa.ca Diana Inkpen Department of Computer Science University of Ottawa Ottawa, ON, K1N 6N5, Canada diana@site.uottawa.ca Abstract We present a method for detecting and correcting multiple real-word spelling er- rors using the Googl	RW	Real-Word$Random Walk$	0
Context-Dependent Non- and RW Error Correction: The system treated all input strings  as possible errors and tried to correct hem by taking into account he contexts in which the  strings appeared.	RW	Real-Word$Random Walk$	0
4 3,822  1,972 4,354  1943 4,563  1948 4,558  Remain Corrected  1,692 0  1,692 0  1,692 0  1,692 0  182 0  182 0  Total Error  Errors Reduction (%)  4,558 44.4  3,846 53.1  3,817 53.4  3,822 53.4  Table 3: Results from Context-Dependent Non-Word Error Correction  Pass  First  Feedback-1  Feedback-2  Feedback-3  Non-Word Errors  Remain Corrected  2,529 3,977  1,978 4,528  1,935 4,571  1,926 4,580  RW Errors  Remain Corrected  1,225 467  1,031 661  1,008 684  1,015 677  Introduced Errors Total Error  Unknown Wds Lex Wds Errors Reduction (%)  182 54 3,990 51.3  182 119 3,310 59.6  182 141 3,266 60.2  182 , 147 3,270 60.1  Table 4: Results from Context-Dependent Real- and Non-Word Error Correction  4 Analysis  Based on the results, we can see that the predominant, positive ffect in corr	RW	Real-Word$Random Walk$	0
Pass  F~st  Feedback-1  Feedback-2  Feedback-3  Non-Word Errors  Remain Corrected  3,049 3,457  2,816 3,690  2,791 3,715  2,784 3,722  RW Errors  Remain Corrected  1,692 0  1,692 0  1,692 : 0  1,692 !	RW	Real-Word$Random Walk$	0
To attack the problem, Wan et al proposed two models, i.e., the Cluster-based conditional Markov RW model and the Cluster-based HITS model, both make use of the theme clusters in the document set (Wan and Yang, 2008).	RW	Real-Word$Random Walk$	1
c?2007 Association for Computational Linguistics WIT: Web People Search Disambiguation using RWs Jose?	RW	Real-Word$Random Walk$	1
c?2011 Association for Computational Linguistics Bilingual RW Models for Automated Grammar Correction of ESL Author-Produced Text Randy West and Y. Albert Park Department of Computer Science & Engineering University of California, San Diego La Jolla, CA 92093-0533 {rdwest,yapark}@cs.ucsd.edu Roger Levy Department of Linguistics University of California, San Diego La Jolla, CA 92093-0533 rlevy@ucsd.edu Abstract We present a novel noisy channel mod	RW	Real-Word$Random Walk$	1
2.3 RWs Model We aim to determine the similarity between any two nodes of type Webpage in the graph.	RW	Real-Word$Random Walk$	1
Improving Diversity in Ranking using Absorbing RWs.	RW	Real-Word$Random Walk$	1
Answering Opinion Questions with  RWs on Graphs.	RW	Real-Word$Random Walk$	1
- MW: The annotators identify a correspondence of the same paraphrase class between two word strings which differ only in one word.	MW	Missing Word$Missing$	0
For 4 of the 20 sections in WSJ2-21, we apply the noise introduction procedure to its own output to 222 Error Type WSJ00 MW likely to bring new attention to the problem ?	MW	Missing Word$Missing$	0
Clusters Error Type Genitives Noun Preposition All Error (%) compounds WSD System 57 36 26 119 53.85% NERD module 11 5 9 25 11.31% Brill?s tagger 7 4 8 19 8.6% MWNet sense 10 1 7 18 8.14% Part and Whole identification 5 10 0 15 6.79% Classification rules 7 2 5 14 6.33% Unseen examples Noun compound annotation 0 9 2 11 4.98% Total 97 67 57 221 100% sentences prefers the PURPOSE interpretation (bag for cotton clothes) of the noun com- pound cotton bag over the PART?WHOLE meaning (bag made of cotton) (cf. (	MW	Missing Word$Missing$	0
\[38\] Ward, W.H.; Hauptmann, A.G.; Stern, R.M.; and  Chanak, T. 1988 Parsing Spoken Phrases Despite  MWs.	MW	Missing Word$Missing$	0
5 Conclusion We have shown that it is possible to tune a WSJ- trained statistical parser to ungrammatical text with- Error Type P R F P R F E0 E2-prob MW 88.5 83.7 86.0 88.9 84.3 86.5 Extra Word 87.2 89.4 88.3 89.2 89.7 89.4 Real Word Spell 84.3 83.0 83.7 89.5 88.2 88.9 Agreement 90.4 88.8 89.6 90.3 88.6 89.4 Verb Form 88.6 87.0 87.8 89.1 87.9 88.5 Table 5: Noisy23: Breakdown by Error Type out affecting its performance on grammatical text.	MW	Missing Word$Missing$	0
2Recent results indicate that test set adaptation by test set sampling of the training corpus achieves a cased Bleu of 53.26 on MT03 whereas a general system trained on all data achieves only 51.02 Verb Placement 3 MW 5 Extra Word 5 Word Choice 26 Word Order 3 Other error 1 Total 43 Table 4: Errors on last 25 sentences of MT-03.	MW	Missing Word$Missing$	0
We re- move the recognized metaphors in riddle sentences, 850 Feature Description Correct Radical number of radicals matched MW Radical number of radicals not matched Disappearing Radical number of radicals that disappear in all characters of riddle descriptions Single Matching number of clues derived from character itself Alignment Matching number of clues derived from alignments Rule Matching number of clues derived from rules Length Rate ratio of the length of clues Frequency prior probability of this character	MW	Missing Word$Missing$	1
MW/ExtraToken errors: this is a general- ized form of conjunction errors: either ?[	MW	Missing Word$Missing$	1
Here we use the GPSG notations:  (I) modification schema  S-> \[l\[sem~0 , Conj *l \], lt\[\]semc~ 1\]  where~Y~\[ (0,1),  (O,O) \]  (2) eoordinat:i .on schema  S -> \[i\[semrm , Conj .,'- \], II\[sem~l \]  where~,(:{ (I,0), (1,1) \]  5.2 MW construction  Korean and Japanese allow one of" the constituents  of a sentence not to be explicitly stated when it is  understandable fr~ll the context.	MW	Missing Word$Missing$	1
Level Task Percentages of Noises Extra line break deletion 49.53  Paragraph  Paragraph boundary detection   Extra space deletion 15.58  Extra punctuation mark deletion 0.71  MW space insertion 1.55  MW punctuation mark insertion 3.85  Misused punctuation mark correction 0.64  Sentence  Sentence boundary detection   Case restoration 15.04  Unnecessary token deletion 9.69 Word  Misspelled word correction 3.41  Table 1.	MW	Missing Word$Missing$	1
MW spaces and  missing punctuation marks were added and marked.	MW	Missing Word$Missing$	1
German English  Exact Match (w/o TT) 46,3% 55,4%  hlcorrect parses 50,3% 39,3%  Not parsed 3,4% 5,3%  Exact Match (after 77) 53,8% 61,2%  Incorrect parses (after TT) 42,8% 33,5%  Labeled Precision (w/o 7T) 90,2% 90,6%  German English  Labeled Precision (after TT) 90,8% 91,4%  LR (all 83,5% 78,5%  utterances, w/o TT)  LR (all 84,0% 79,2%  utterances, after TT)  LR (parsed 91,0% 90,9%  utterances, w/o TT)  LR (parsed 91,6% 91,7%  utterances, after TT)  6 Conclusion  In this article we have extended probabilistic shift-  reduce parsing to be more context-sensitive than  previous works and have demonstrated that a bigger  context improves the performance of a probabilistic  shift-reduce parser.	LR	Labeled Recall$labeled recall$Lagrangian relaxation$larger$LexRank$lexical reordering$logistic regression$Logistic Regression$	0
German English  Exact Match (w/o TT) 46,3% 55,4%  hlcorrect parses 50,3% 39,3%  Not parsed 3,4% 5,3%  Exact Match (after 77) 53,8% 61,2%  Incorrect parses (after TT) 42,8% 33,5%  Labeled Precision (w/o 7T) 90,2% 90,6%  German English  Labeled Precision (after TT) 90,8% 91,4%  LR (all 83,5% 78,5%  utterances, w/o TT)  LR (all 84,0% 79,2%  utterances, after TT)  LR (parsed 91,0% 90,9%  utterances, w/o TT)  LR (parsed 91,6% 91,7%  utterances, after TT)  6 Conclusion  In this article we have extended probabilistic shift-  reduce parsing to be more context-sensitive than  previous works and have demonstrated that a bigger  con	LR	Labeled Recall$labeled recall$Lagrangian relaxation$larger$LexRank$lexical reordering$logistic regression$Logistic Regression$	0
5,4%  hlcorrect parses 50,3% 39,3%  Not parsed 3,4% 5,3%  Exact Match (after 77) 53,8% 61,2%  Incorrect parses (after TT) 42,8% 33,5%  Labeled Precision (w/o 7T) 90,2% 90,6%  German English  Labeled Precision (after TT) 90,8% 91,4%  LR (all 83,5% 78,5%  utterances, w/o TT)  LR (all 84,0% 79,2%  utterances, after TT)  LR (parsed 91,0% 90,9%  utterances, w/o TT)  LR (parsed 91,6% 91,7%  utterances, after TT)  6 Conclusion  In this article we have extended probabilistic shift-  reduce parsing to be more context-sensitive than  previous works and have demonstrated that a bigger  context improves the performance of a probabilistic  shift-reduce parser.	LR	Labeled Recall$labeled recall$Lagrangian relaxation$larger$LexRank$lexical reordering$logistic regression$Logistic Regression$	0
The results of this  ewtluation are given in the following table:  7)'aining set/trees\]  Test set \[utterances\]  GelTilall  19.750  1.000  English  17.793  1.000  Eract Match 46,3% 55,4%  Incorrect parses 50,3% 39,3%  Not pmwed 3,4% 5,3%  contextj'ree rules 988 2.205  Labeled Precision 90,2% 90,6%  LR (all 83,5% 78,5%  utterances)  LR  (parsed utterances) 91,0% 90,9%  Japan.	LR	Labeled Recall$labeled recall$Lagrangian relaxation$larger$LexRank$lexical reordering$logistic regression$Logistic Regression$	0
German English  Exact Match (w/o TT) 46,3% 55,4%  hlcorrect parses 50,3% 39,3%  Not parsed 3,4% 5,3%  Exact Match (after 77) 53,8% 61,2%  Incorrect parses (after TT) 42,8% 33,5%  Labeled Precision (w/o 7T) 90,2% 90,6%  German English  Labeled Precision (after TT) 90,8% 91,4%  LR (all 83,5% 78,5%  utterances, w/o TT)  LR (all 84,0% 79,2%  utterances, after TT)  LR (parsed 91,0% 90,9%  utterances, w/o TT)  LR (parsed 91,6% 91,7%  utterances, after TT)  6 Conclusion  In this article we have extended probabilistic shift-  reduce parsing to be more context-sensitive than  previous works and have demonstrated that a bigger  context improves the performance of a probabilistic  shif	LR	Labeled Recall$labeled recall$Lagrangian relaxation$larger$LexRank$lexical reordering$logistic regression$Logistic Regression$	0
F1<40 is the F-Measure combining labeled precision and LR for sentences of less than 40 words.	LR	Labeled Recall$labeled recall$Lagrangian relaxation$larger$LexRank$lexical reordering$logistic regression$Logistic Regression$	1
But  2The figure indicates unLR and preci-  sion.	LR	Labeled Recall$labeled recall$Lagrangian relaxation$larger$LexRank$lexical reordering$logistic regression$Logistic Regression$	1
Most of the decrease in F1 is due to the drop in unLR.	LR	Labeled Recall$labeled recall$Lagrangian relaxation$larger$LexRank$lexical reordering$logistic regression$Logistic Regression$	1
For example, for the correct proposition: v1f1: ACT|EFF, ADDR the system that generates the following output for the same argument tokens: v1f1: ACT, ADDR|PAT receives a labeled precision score of 3/4 because the PAT is incorrect and LR 3/4 be- cause the EFF is missing (should the ACT|EFF and ADDR|PAT be taken as atomic values, the scores would then be zero).	LR	Labeled Recall$labeled recall$Lagrangian relaxation$larger$LexRank$lexical reordering$logistic regression$Logistic Regression$	1
Differences to LR/precision are small,  since the number of different non-terminal categories  is very restricted.	LR	Labeled Recall$labeled recall$Lagrangian relaxation$larger$LexRank$lexical reordering$logistic regression$Logistic Regression$	1
8 Czech English Unlabeled precision 99.09 96.03 UnLR 94.81 93.07 Unlabeled F-1 96.90 94.53 Labeled precision 78.38 81.58 Labeled recall 74.99 79.06 Labeled F-1 76.65 80.30 Frame selection accuracy 79.10 84.95 Ambiguous verbs baseline 66.68 68.44 classifier 72.41 80.03 Table 1: Experimental results of errors in the Czech evaluation data were caused just by idioms or light verb constructions not be- ing recognized by our system.	LR	Labeled Recall$labeled recall$Lagrangian relaxation$larger$LexRank$lexical reordering$logistic regression$Logistic Regression$	1
2.2 LR Dual decomposition (Komodakis et al, 2007) and LR in general are often used for solving joint inference problems which are decomposable into individual subproblems linked by equality constraints (Koo et al, 2010; Rush et al, 2010; Rush and Collins, 2011; DeNero and Macherey, 2011; Martins et al, 2011; Das et al, 2012; Almeida and Martins, 2013).	LR	Labeled Recall$labeled recall$Lagrangian relaxation$larger$LexRank$lexical reordering$logistic regression$Logistic Regression$	2
We explore instead the use of LR to decou- ple the two subproblems and solve them separately.	LR	Labeled Recall$labeled recall$Lagrangian relaxation$larger$LexRank$lexical reordering$logistic regression$Logistic Regression$	2
The LR of this optimization problem is L(a,b, c(a), c(b),u) = f(a, c(a))+ g(b, c(b))+ ?	LR	Labeled Recall$labeled recall$Lagrangian relaxation$larger$LexRank$lexical reordering$logistic regression$Logistic Regression$	2
We will also mention various possibilities for  performing the inference, from commercial Integer Linear Programming  packages to search techniques to LR approximation methods.	LR	Labeled Recall$labeled recall$Lagrangian relaxation$larger$LexRank$lexical reordering$logistic regression$Logistic Regression$	2
Multiple approaches to generate good ap- proximate solutions for joint multi-structure compression, based on LR to enforce equality between the sequential and syntactic inference subproblems.	LR	Labeled Recall$labeled recall$Lagrangian relaxation$larger$LexRank$lexical reordering$logistic regression$Logistic Regression$	2
Thus more efficient graph decoding algorithms, e.g., based on LR or approximate algorithms, may be explored in future work.	LR	Labeled Recall$labeled recall$Lagrangian relaxation$larger$LexRank$lexical reordering$logistic regression$Logistic Regression$	2
Also, as the present analysis is based on a small sample of manually annotated adjectives, we intend to obtain a LR Gold Standard, in order to establish statistically more reliable results.	LR	Labeled Recall$labeled recall$Lagrangian relaxation$larger$LexRank$lexical reordering$logistic regression$Logistic Regression$	3
The next section looks at how transducers of single con-  straints or small hierarchies can be combined into single  transducers for LR hierarchies.	LR	Labeled Recall$labeled recall$Lagrangian relaxation$larger$LexRank$lexical reordering$logistic regression$Logistic Regression$	3
Thus, they tend to form LR constituents that are mostly placed in predicative position.	LR	Labeled Recall$labeled recall$Lagrangian relaxation$larger$LexRank$lexical reordering$logistic regression$Logistic Regression$	3
AT&T Labs, 33 Thomas Street, New York, NY 10007, USA iosife@telecom.tuc.gr, taniya@research.att.com Abstract We propose a multi-step system for the analysis of children?s stories that is in- tended to be part of a LR text-to-speech- based storytelling system.	LR	Labeled Recall$labeled recall$Lagrangian relaxation$larger$LexRank$lexical reordering$logistic regression$Logistic Regression$	3
Frame Semantics suggests that the meanings of  lexical items (lexical units (LU)) are best defined  with respect to LR conceptual chunks, called  Frames.	LR	Labeled Recall$labeled recall$Lagrangian relaxation$larger$LexRank$lexical reordering$logistic regression$Logistic Regression$	3
This story analysis system was developed to be part of a LR TTS-based storyteller system aimed at children.	LR	Labeled Recall$labeled recall$Lagrangian relaxation$larger$LexRank$lexical reordering$logistic regression$Logistic Regression$	3
We also implement  the MEAD, LR baselines and our method                                                    5 www-01.ibm	LR	Labeled Recall$labeled recall$Lagrangian relaxation$larger$LexRank$lexical reordering$logistic regression$Logistic Regression$	4
We compare our system with five baseline sys- tems: MEAD-WT, LR-WT, ARWG-WT,  MEAD and LR.	LR	Labeled Recall$labeled recall$Lagrangian relaxation$larger$LexRank$lexical reordering$logistic regression$Logistic Regression$	4
We also implement  the MEAD, LR baselines and our method                                                    5 www-01.ibm.com/software/integration/optimization/cplex- optimizer/  6 http://www.summarization.com/mead/  7 In our experiments, LR performs much better than  the more complex variant - C-LR (Qazvinian and  Radev, 2008), and thus we choose LR, rather than C- LR, to represent graph-based summari	LR	Labeled Recall$labeled recall$Lagrangian relaxation$larger$LexRank$lexical reordering$logistic regression$Logistic Regression$	4
2011) compared four different approaches  for multi-document scientific articles summariza- tion: MEAD, MEAD with corpus specific vo- cabulary, LR and W3SS.	LR	Labeled Recall$labeled recall$Lagrangian relaxation$larger$LexRank$lexical reordering$logistic regression$Logistic Regression$	4
We also implement  the MEAD, LR baselines and our method                                                    5 www-01.ibm.com/software/integration/optim	LR	Labeled Recall$labeled recall$Lagrangian relaxation$larger$LexRank$lexical reordering$logistic regression$Logistic Regression$	4
We also implement  the MEAD, LR baselines and our method                                                    5 www-01.ibm.com/software/integration/optimization/cplex- optimizer/  6 http://www.summarization.com/mead/  7 In our experiments, LR p	LR	Labeled Recall$labeled recall$Lagrangian relaxation$larger$LexRank$lexical reordering$logistic regression$Logistic Regression$	4
We also implement  the MEAD, LR baselines and our method                                                    5 www-01.ibm.com/software/integration/optimization/cplex- optimizer/  6 http://www.summarization.com/mead/  7 In our experiments, LR performs much better than  the more complex variant - C-LR (Qazvinian and  Radev, 2008), and thus we choose LR, rather than C- LR, to represent graph-based summarization methods  for comparison in this paper.	LR	Labeled Recall$labeled recall$Lagrangian relaxation$larger$LexRank$lexical reordering$logistic regression$Logistic Regression$	4
LR 7  (Eran and  Radev, 2004) is a multi-document summarization  system which is based on a random walk on the  similarity graph of sentences.	LR	Labeled Recall$labeled recall$Lagrangian relaxation$larger$LexRank$lexical reordering$logistic regression$Logistic Regression$	4
Lopez (2008) explores whether LR or the phrase discontiguity in- herent in hierarchical rules explains improvements over phrase-based systems.	LR	Labeled Recall$labeled recall$Lagrangian relaxation$larger$LexRank$lexical reordering$logistic regression$Logistic Regression$	5
The base system includes a distance distortion model; the lexical system adds LR; rule is the rule preordering system of Genzel (2010) plus LR; 1-step and 2-step are our classifier-based systems plus LR.	LR	Labeled Recall$labeled recall$Lagrangian relaxation$larger$LexRank$lexical reordering$logistic regression$Logistic Regression$	5
From the merged alignments we also extracted a bidirectional LR model conditioned on the source and the target phrases (Koehn et al, 2007).	LR	Labeled Recall$labeled recall$Lagrangian relaxation$larger$LexRank$lexical reordering$logistic regression$Logistic Regression$	5
From the aligned data, we also extracted a hierarchical reordering model that is similar to popular LR models (Koehn et al, 2007) but that models swaps containing more than just one phrase (Galley and returned during an earlier iteration of MERT.	LR	Labeled Recall$labeled recall$Lagrangian relaxation$larger$LexRank$lexical reordering$logistic regression$Logistic Regression$	5
The 2-step classifier preordering approach pro- vides statistically significant improvements over the LR baseline on three out of the eight language pairs: English-Spanish (en-es: 1.4 BLEU), German-English (de-en: 1.2 BLEU), and English- French (en-fr: 1.0 BLEU).	LR	Labeled Recall$labeled recall$Lagrangian relaxation$larger$LexRank$lexical reordering$logistic regression$Logistic Regression$	5
Our approach com- bines the strengths of LR and syntactic preordering models by performing long-distance reorderings using the structure of the parse tree, while utilizing a discrimina- tive model with a rich set of features, includ- ing lexical features.	LR	Labeled Recall$labeled recall$Lagrangian relaxation$larger$LexRank$lexical reordering$logistic regression$Logistic Regression$	5
3.3 Logistic regression The use of discrete cells over the Earth?s sur- face allows any classification strategy to be em- ployed, including discriminative classifiers such as LR.	LR	Labeled Recall$labeled recall$Lagrangian relaxation$larger$LexRank$lexical reordering$logistic regression$Logistic Regression$	6
We also show that LR performs fea- ture selection effectively, assigning high weights to geocentric terms.	LR	Labeled Recall$labeled recall$Lagrangian relaxation$larger$LexRank$lexical reordering$logistic regression$Logistic Regression$	6
The first-level grid is constructed the same as for Naive Bayes or flat LR and is controlled by its own parameter.	LR	Labeled Recall$labeled recall$Lagrangian relaxation$larger$LexRank$lexical reordering$logistic regression$Logistic Regression$	6
In addition, because LR does not assume feature independence, complex and over- lapping features of various sorts can be employed.	LR	Labeled Recall$labeled recall$Lagrangian relaxation$larger$LexRank$lexical reordering$logistic regression$Logistic Regression$	6
4 allows us to take advan- tage of LR without incurring such a high training cost.	LR	Labeled Recall$labeled recall$Lagrangian relaxation$larger$LexRank$lexical reordering$logistic regression$Logistic Regression$	6
Unlike for LR, the top IGR features are mostly obscure words, only some of 344 Salt Lake San Francisco New Orleans Phoenix Denver Houston Montreal Seattle Tulsa Los Angeles utah sacramento orleans tucson denver houston montreal seattle tulsa knotts slc hella jtfo az colorado antonio mtl portland okc sd salt sac prelaw phoenix broncos texans quebec tacoma oklahoma pasadena byu niners saints a	LR	Labeled Recall$labeled recall$Lagrangian relaxation$larger$LexRank$lexical reordering$logistic regression$Logistic Regression$	6
N Table 3: Syllable Organization for the       LR Model 4.1 LR Model for Combining Syllables The model to combine syllables is built upon Binary LR whose answers are either combine or not combine.	LR	Labeled Recall$labeled recall$Lagrangian relaxation$larger$LexRank$lexical reordering$logistic regression$Logistic Regression$	7
6 LR Model To improve the accuracy of the Q?A algorithm and to learn about the importance of the single features for predicting whether an answer from A is correct, we want to learn optimal scores ?	LR	Labeled Recall$labeled recall$Lagrangian relaxation$larger$LexRank$lexical reordering$logistic regression$Logistic Regression$	7
1095   Combining Prediction by Partial Matching and LR                for Thai Word Segmentation Ohm Sornil Department of Computer Science National Institute of Development Administration, Bangkok, Thailand osornil@as.nida.ac.th Paweena Chaiwanarom National Statistical Office Bangkok, Thailand paweena@nso.go.th Abstract Word segmentation is an important part of many applications, including information retrieval, information filteri	LR	Labeled Recall$labeled recall$Lagrangian relaxation$larger$LexRank$lexical reordering$logistic regression$Logistic Regression$	7
We ran LR learners and eval- uated their performance using QWK scores.	LR	Labeled Recall$labeled recall$Lagrangian relaxation$larger$LexRank$lexical reordering$logistic regression$Logistic Regression$	7
V (D) is the VR space, which is the set of weightswi that classify the training data correctly, and |V (D)| is the size of the VR space.	VR	version$vocabulary richness$Virtual Reality$	0
Combining these  two notions into tin extended VR of the automaton pro-  duct operation allows us to build tip transducers capturing a  hierarchy of constraints fiom single constraint transducers.	VR	version$vocabulary richness$Virtual Reality$	0
We have im-  plemented a slightly modified VR of Hobbs  algorithm for the Tree-bank parse trees.	VR	version$vocabulary richness$Virtual Reality$	0
Also thanks to Nadjet Bouayad, Katrin Erk and 5 anonymous reviewers for revision and criticism of pre- vious VRs of the paper.	VR	version$vocabulary richness$Virtual Reality$	0
In practice, to explore the VR space of weights consistent with the training data, BPM trains a few different perceptrons (Collins, 2002) by shuffling the samples.	VR	version$vocabulary richness$Virtual Reality$	0
We also evaluated thispenalized VR, varying the trade-off parameter C. Bayes Point Machines (BPM) for structured prediction (Corston-Oliver et al, 2006) is an en- semble learning algorithm that attempts to set the weight w to be the Bayes Point which approxi- mates to Bayesian inference for linear classifiers.	VR	version$vocabulary richness$Virtual Reality$	0
mean word and sentence length) and the type-token ratio (which indicates VR) are also represented (tok).	VR	version$vocabulary richness$Virtual Reality$	1
14 The remaining features (extended) attempt to model the following five dimensions of the lyrics: VOCABULARY: These features estimate the VR (type-token ratio for n-grams up to n = 3) and the use of non-standard words, i.e., uncommon and slang words.	VR	version$vocabulary richness$Virtual Reality$	1
We can mea- sure the VR or lexical diversity of a narrative sample using a number of different metrics (see Table 7).	VR	version$vocabulary richness$Virtual Reality$	1
The measures of VR do not distin- guish between the SD and control groups, suggest- ing it is the words themselves, and not the number of different words being used, that is important.	VR	version$vocabulary richness$Virtual Reality$	1
The combination  808  of the best VR functions in a  lnultivariate model can then be used tbr  capturing the characteristics of a stylistic  category (llohnes, 1992).	VR	version$vocabulary richness$Virtual Reality$	1
Specifically, various  functions that attempt to represent the  VR have been proposed  (Honore 1979; Sichel, 1975).	VR	version$vocabulary richness$Virtual Reality$	1
We examine several different types of fea- tures, including part-of-speech, complex- ity, context-free grammar, fluency, psy- cholinguistic, VR, and acoustic, and discuss the circumstances under which they can be extracted.	VR	version$vocabulary richness$Virtual Reality$	1
Technical Correspondence The Extraction of a Minimum Set of Semantic Primitives from a Monolingual Dictionary is NP-Complete  Computational Linguistics, Volume 12, Number 4, October-December 1986 307   A Spoken Language Interface to a VR System (Video)  Stephanie S. Everett, Kenneth Wauchope  Navy Ctr.	VR	version$vocabulary richness$Virtual Reality$	2
through a VR town.	VR	version$vocabulary richness$Virtual Reality$	2
When we extend this to a real-time immersive  VR environment, a Virtual Kitchen in  this case, the ECAs will actually perform the task  of cooking a recipe together in the virtual kitchen  while conversing about the steps involved in doing  so, as laid out by the AI plan.	VR	version$vocabulary richness$Virtual Reality$	2
ACM SIGSOFT  Software Engineering Notes, Volume 21 Issue  2, March 1996  12.Nikos Kladias, Tassos Pantazidis, Manolis  Avagianos, A VR Learning  Environment Providing Access to Digital  Museums, 1998 MultiMedia Modeling October,  1998, p193  13.	VR	version$vocabulary richness$Virtual Reality$	2
DSTO and VR.	VR	version$vocabulary richness$Virtual Reality$	2
Shun-tzu Tsai, Chun-ko Hsieh, Diversity and  Aesthetic Appeal for a VR World of  Chinese Art, proceeding of the Seventh  International Conference on Virtual System and  Multimedia,  2001  5. ?	VR	version$vocabulary richness$Virtual Reality$	2
Table 4: Example output for original sentences (O) as generated by the RT baseline (R) and our tree labeling system (T), as well as the headline-generated Google compressions (C).	RT	Reluctant Trimmer$untime$Recall$re-posting$Reference Time$role text$retweet$	0
our conclusion is that the tradeoff between coreference  quality versus rRT in our agglomerative approach is  definitely worthwhile if the number of same-named  entities to be disambiguated is relatively small.	RT	Reluctant Trimmer$untime$Recall$re-posting$Reference Time$role text$retweet$	1
7.3 RRT comparison.	RT	Reluctant Trimmer$untime$Recall$re-posting$Reference Time$role text$retweet$	1
Is the improvement in our results worth the  difference in rRT?	RT	Reluctant Trimmer$untime$Recall$re-posting$Reference Time$role text$retweet$	1
The rRTs of incremental  approaches are linear whereas the rRT of our  agglomerative vector space approach is O(n?).	RT	Reluctant Trimmer$untime$Recall$re-posting$Reference Time$role text$retweet$	1
If  the mean size of entities to be disambiguated is  relatively small, then there will not be a significant  degrade in rRT on the agglomerative approach.	RT	Reluctant Trimmer$untime$Recall$re-posting$Reference Time$role text$retweet$	1
20  30  40  50  60  70  80  90  100  20  30  40  50  60  70  80  90  100 Pr ec isi o n RT Incremental VS KL Divergence Agglomerative VS Breck and Bagga Figure 2: RT and precision tradeoff of three  algorithms on the John Smith Corpus.	RT	Reluctant Trimmer$untime$Recall$re-posting$Reference Time$role text$retweet$	2
3.2 Gold Standard RT that we could not use any previously well- established classification.	RT	Reluctant Trimmer$untime$Recall$re-posting$Reference Time$role text$retweet$	2
RT also from the Introduction that we wanted to restrict ourselves to shallow dis- tributional features.	RT	Reluctant Trimmer$untime$Recall$re-posting$Reference Time$role text$retweet$	2
RT that GEN is the function which produces  the initial set of candidate forms which is reduced by the  constraints.	RT	Reluctant Trimmer$untime$Recall$re-posting$Reference Time$role text$retweet$	2
The systems achieved  an average of 55% RT while the precision was  68.8%.	RT	Reluctant Trimmer$untime$Recall$re-posting$Reference Time$role text$retweet$	2
The Reference Broadcast act consists mostly of usernames and urls.9 Also prominent is the word rt, which has special significance on Twitter, indicating that the user is RT another user?s post.	RT	Reluctant Trimmer$untime$Recall$re-posting$Reference Time$role text$retweet$	3
Twitter provides two methods to respond to messages: replies and retweets (RT of a message to one?s follow- ers).	RT	Reluctant Trimmer$untime$Recall$re-posting$Reference Time$role text$retweet$	3
Time Adverbials in English and RT.	RT	Reluctant Trimmer$untime$Recall$re-posting$Reference Time$role text$retweet$	4
\[18\] Nerbonne J.. RT and Time in Narration;  Linguistics and PhUosophy 9 N 1 (1986) 63-82.	RT	Reluctant Trimmer$untime$Recall$re-posting$Reference Time$role text$retweet$	4
Assum-  ing the Reichenbachian threefold istinction between  Event Time (E), RT (R), and Speaking  Time (S) (the Basic Tense Structure, BTS, (Reichen-  bach 1947)), we observe that the constraints imposed  by a marker on verb tense concern the underlying re-  lation between E and S of both clauses: Selecting  either a/s or wenn to express imultaneous events in  the main clause (era) and in the subordinate clause  (es) depends on whether the event times precede	RT	Reluctant Trimmer$untime$Recall$re-posting$Reference Time$role text$retweet$	4
Figure 1: Algorithm for Computing   RT (tval)    CTYPE: clause is a regular clause,  complement clause, or relative clause   CINDEX: subclause index   PARA: paragraph number   SENT: sentence number   SCONJ: subordinating conjunction  (e.g., while, since, before)   TPREP: preposition in a TIMEX2  PP  TIMEX2: string in the TIMEX2 tag   TMOD:  temporal modifier not at- tached to a TIMEX2, (e.g., after [an  altercation])   Q	RT	Reluctant Trimmer$untime$Recall$re-posting$Reference Time$role text$retweet$	4
I: RT, Tense and Adverbs, p. 71-110.	RT	Reluctant Trimmer$untime$Recall$re-posting$Reference Time$role text$retweet$	4
3 RT Dynamic-choosing  Mechanism  3.1 Referential feature in Implicit Time  In this paper, we define the Implicit Time con- sists of the modifier and the temporal noun  which is modified by modifiers.	RT	Reluctant Trimmer$untime$Recall$re-posting$Reference Time$role text$retweet$	4
I: RT, Tense and Adverbs.	RT	Reluctant Trimmer$untime$Recall$re-posting$Reference Time$role text$retweet$	4
slot.modifiers do applyModifier(RT,modifier) end for In question text, replace slot with RT end for return question text yield.	RT	Reluctant Trimmer$untime$Recall$re-posting$Reference Time$role text$retweet$	5
t.plaintext slots do RT?	RT	Reluctant Trimmer$untime$Recall$re-posting$Reference Time$role text$retweet$	5
Even in the absence of modifiers, all RT re- ceives some additional processing before being in- serted into its corresponding slot.	RT	Reluctant Trimmer$untime$Recall$re-posting$Reference Time$role text$retweet$	5
Modifiers apply transformations to the RT in- serted into a slot, and filters enforce finer-grained matching criteria.	RT	Reluctant Trimmer$untime$Recall$re-posting$Reference Time$role text$retweet$	5
A slot inside the plaintext acts as a variable to be replaced by the correspond- ing semantic RT from a matching sentence, while any slots appearing outside the plaintext serve only to provide additional pattern match- ing criteria.	RT	Reluctant Trimmer$untime$Recall$re-posting$Reference Time$role text$retweet$	5
If a predicate frame and template are matched, they are passed to Algorithm 2, which fills tem- plate slots with RT to produce a question.	RT	Reluctant Trimmer$untime$Recall$re-posting$Reference Time$role text$retweet$	5
Tweet, tweet, RT: Conversational aspects of RTing on twitter.	RT	Reluctant Trimmer$untime$Recall$re-posting$Reference Time$role text$retweet$	6
They are: RT; hashtag; reply; link, if the tweet con- tains a link; punctuation (exclamation and ques- tions marks); emoticons (textual expression rep- resenting facial expressions); and upper cases (the number of words that starts with upper case in the tweet).	RT	Reluctant Trimmer$untime$Recall$re-posting$Reference Time$role text$retweet$	6
659 There are also other measures to consider, e.g., the follower number and the times of being RTed.	RT	Reluctant Trimmer$untime$Recall$re-posting$Reference Time$role text$retweet$	6
The features we employed were: k-top words, k-top digrams and trigrams, k-top hashtags, k-top mentions, tweet/RT/hashtag/link/mention frequencies, and out/in-neighborhood size.	RT	Reluctant Trimmer$untime$Recall$re-posting$Reference Time$role text$retweet$	6
However, few of these studies have considered measures beyond simple hashtag fre- quencies, relative mention counts among politicians, and RT counts.	RT	Reluctant Trimmer$untime$Recall$re-posting$Reference Time$role text$retweet$	6
An Approach with MaxiMEMMs.	MEMMs	mum Entropy Markov Models$Maximum Entropy Markov  Models$	0
68  CoNLL 2008: Proceedings of the 12th Conference on Computational Natural Language Learning, pages 243?247 Manchester, August 2008 The Integration of Dependency Relation Classification and Semantic Role Labeling Using Bilayer MaxiMEMMs Weiwei Sun and Hongzhan Li and Zhifang Sui Institute of Computational Linguistics Peking University {weiwsun, lihongzhan.pku}@gmail.com, szf@pku.edu.cn Abstract This paper describes a system to solve the joint learning of syntactic and seman- tic dependencies.	MEMMs	mum Entropy Markov Models$Maximum Entropy Markov  Models$	0
An aproach with MaxiMEMMs Ben?at Zapirain, Eneko Agirre IXA NLP Group University of the Basque Country Donostia, Basque Country {benat.zapirain,e.agirre}@ehu.es Llu??s Ma`rquez TALP Research Center Technical University of Catalonia Barcelona, Catalonia lluism@lsi.upc.edu Abstract We present a sequential Semantic Role La- beling system that describes the tagging problem as a Maximum Entropy Markov	MEMMs	mum Entropy Markov Models$Maximum Entropy Markov  Models$	0
track only) 2.3 MaxiMEMMs MaxiMEMMs are a discrimi- native model for sequential tagging that models the local probability P (sn | sn?1, o), where o is the context of the observation.	MEMMs	mum Entropy Markov Models$Maximum Entropy Markov  Models$	0
Rudnick et al(2013) present a combina- tion of MaxiMEMMs and HMM to perform lexical selection in the sense of cross-lingual word sense disam- biguation (i.e. by choice from the set of trans- lation alternatives).	MEMMs	mum Entropy Markov Models$Maximum Entropy Markov  Models$	0
Note that we expect the REL structure L to be cycle free.	REL	relational$relations$relative pronoun$relevance$	0
described by three design decisions: (1) instead of building a pipeline using local classifier technology, we design and learn a joint probabilistic model over events in a sentence; (2) instead of developing spe- cific inference and learning algorithms for our joint model, we apply Markov Logic, a general purpose Statistical Relation Learn- ing language, for this task; (3) we represent events as REL structures over the to- kens of a sentence, as opposed to structures that explicitly mention abstract event en- tities.	REL	relational$relations$relative pronoun$relevance$	0
Then, the term arguments of each REL triple are assigned to the latter, originating a wordnet.	REL	relational$relations$relative pronoun$relevance$	0
By mapping to REL structure over grounded text, we also show a direct connection to recent formulations of Semantic Role Labelling which may be helpful in the future.	REL	relational$relations$relative pronoun$relevance$	0
Let us introduce Markov Logic by considering the event extraction task (as REL structure over tokens as generated by algorithm 1).	REL	relational$relations$relative pronoun$relevance$	0
which is called the REL  model), a word naziva can be initially tagged ei- ther as a noun naziv (Engl.	REL	relational$relations$relative pronoun$relevance$	0
It is use- ful for low-level tasks such as parsing (e.g. for PP-attachment ambiguity within NPs), but also for tasks oriented to semantics, such as the extraction of RELhips between individuals or concepts.	REL	relational$relations$relative pronoun$relevance$	1
The former captures more distant REL compared to the latter.	REL	relational$relations$relative pronoun$relevance$	1
Thus, unary adjectives denote properties and binary adjec- tives denote REL.	REL	relational$relations$relative pronoun$relevance$	1
The passive char- acters were identified via the following REL extracted by dependency parsing: nsubjpass (passive nominal subject) and pobj (object of a preposition).	REL	relational$relations$relative pronoun$relevance$	1
However, although there is indeed a certain corre- lation between morphological class and semantic class, we claim that morphology is not sufficient for a reliable classification because it is by no means a one-to-one RELhip.	REL	relational$relations$relative pronoun$relevance$	1
these kinds of RELhips could be automatically extracted if information on the class were available.	REL	relational$relations$relative pronoun$relevance$	1
Rule 7 attempts REL disam- biguation when it finds the three tokens ?	REL	relational$relations$relative pronoun$relevance$	2
The CLOSESTLINK (Soon et al 2001) method picks the closest mention to mj that is positively classified, while the BESTLINK (Ng and Cardie, 2002) method links mj to the preced- Types Features String- Similarity mention string match, head string match, head substring match, head word pair, men- tion substring match, acronym Syntax number match, gender match, apposition, REL, mention type, modifier match, head word POS tags Semantic synonym, antonym, hypernym, modifier re- lations, both mentions are surrounded by a verb meaning ?	REL	relational$relations$relative pronoun$relevance$	2
Examples 1 and 2 show the representation that would be obtained for two imaginary English sen- 2Clause delimiters are punctuation marks other than com- mata, RELs and subordinating conjunctions.	REL	relational$relations$relative pronoun$relevance$	2
Another systematic error is faulty classification of RELs/determiners as wh-question pronouns/determiners, e.g., ?	REL	relational$relations$relative pronoun$relevance$	2
In our account, the REL which does not  specify an interclausal coherence link, and therefore  sentences (26-28) are parallel constructions.	REL	relational$relations$relative pronoun$relevance$	2
ker Ha, Response conditional la + future marker Ha, Jussive li, Preposition li, Preposition min, Future marker sa, Preposition ta, Particle wa, Preposition wa, Vocative wA, vocative yA Proclitic Article proclitic: No proclitic, Not applicable, Demonstrative particle Aa, Determiner, Determiner Al + negative particle mA, Negative particle lA, Negative particle mA, Negative particle mA, Particle mA, REL mA Enclitics Pronominals: No enclitic, Not applicable, 1st person plural/singular, 2nd person dual/plural, 2nd person fem- inine plural/singular, 2nd person masculine plural/singular, 3rd person dual/plural, 3rd person feminine plu- ral/singular, 3rd person masculine plural/singular, Vocative particle, Negative particle lA, Interrogative pronoun mA, Interrogative pronoun mA, Inter	REL	relational$relations$relative pronoun$relevance$	2
lgorithm helps  us find the REL probability of higher level  nodes as a lexical category for the frame element  through a bottom up computation of the inter-node  messages.	REL	relational$relations$relative pronoun$relevance$	3
Therefore, we defined the REL metric for  the WordNet nodes to achieve a larger coverage.	REL	relational$relations$relative pronoun$relevance$	3
1 0\)1( 1101000,1 )()|()()( N Nk k NmNNPNPNm   We should note that based on the WordNet?s  hypernym relation, the conditional REL  probability of each parent node (given any child  node) is equal to 1.	REL	relational$relations$relative pronoun$relevance$	3
For each of the WordNet nodes we  defined the REL of the node based on the  proportion of the occurrence of the node in IE related  Text (Orel) to the	REL	relational$relations$relative pronoun$relevance$	3
For each of the  frame elements, we took the terms in FrameNet FE  annotations as ground truth which means that the  REL probability of the WordNet nodes for those  terms is equal to 1.	REL	relational$relations$relative pronoun$relevance$	3
For each of the WordNet nodes we  defined the REL of the node based on the  proportion of the occurrence of the node in IE related  Text (Orel) to the occurrence of the node in the  general text (Ogen).	REL	relational$relations$relative pronoun$relevance$	3
The Sum Product algorithm helps  us find the REL probability of higher level  nodes as a lexical category for the frame element  through a bottom up computation of the inter-node  messages.	REL	relational$relations$relative pronoun$relevance$	3
1 0\)1( 1101000,1 )()|()()( N Nk k NmNNPNPNm   We should note that based on the WordNet?s  hypernym relation, the conditional REL  probability of ea	REL	relational$relations$relative pronoun$relevance$	3
gen rel O O Nl =)(Re   Using this REL metric, we evaluated all of  the WordNet nodes for the training data (found in the  previous step) and re-ranked and picked the top ?	REL	relational$relations$relative pronoun$relevance$	3
An intermediate approach would be to  mark up a "core" set of RE	RE	referring expression$Reordering$relation extraction$Relation Extraction$rank of expected$Relation extraction$Regular Expressions$regular expression$Reduce$	0
An extreme alternative is to have a first pass  where only REs which look like  anaphors are marked up, such as pronouns, def-  inite NPs and reduced forms of proper names.	RE	referring expression$Reordering$relation extraction$Relation Extraction$rank of expected$Relation extraction$Regular Expressions$regular expression$Reduce$	0
An intermediate approach would be to  mark up a "core" set of REs on  the first pass, allowing for further eferring ex-  pressions to be identified on subsequent passes  if this is necessary to resolve coreference.	RE	referring expression$Reordering$relation extraction$Relation Extraction$rank of expected$Relation extraction$Regular Expressions$regular expression$Reduce$	0
e number of "basic" REs.	RE	referring expression$Reordering$relation extraction$Relation Extraction$rank of expected$Relation extraction$Regular Expressions$regular expression$Reduce$	0
In the case of (3) we still have no modal and we use two different but co-REs to refer to the same in- dividual.	RE	referring expression$Reordering$relation extraction$Relation Extraction$rank of expected$Relation extraction$Regular Expressions$regular expression$Reduce$	0
This relation can  be annotated by adding new tags for composite  REs, but it is obviously unde-  sirable to encode these tags in advance for every  possible combination ofreferents in a text, since  the number would increase xponentially with  the number of "basic" REs.	RE	referring expression$Reordering$relation extraction$Relation Extraction$rank of expected$Relation extraction$Regular Expressions$regular expression$Reduce$	0
The problem with this interpretation is that if it is indeed the case that different co-REs simply pick out different guises of the same individual, then a sentence like (6) should have a non-contradictory reading, while this seems not to be the case.	RE	referring expression$Reordering$relation extraction$Relation Extraction$rank of expected$Relation extraction$Regular Expressions$regular expression$Reduce$	0
An extreme alternative is to have a first pass  where only REs which look like  anaphors are marked up, such as pronouns, def-  inite NPs a	RE	referring expression$Reordering$relation extraction$Relation Extraction$rank of expected$Relation extraction$Regular Expressions$regular expression$Reduce$	0
An intermediate approach would be to  mark up a "core" set of REs on  the first pass, allowing for further eferring ex-  pressions to be identified	RE	referring expression$Reordering$relation extraction$Relation Extraction$rank of expected$Relation extraction$Regular Expressions$regular expression$Reduce$	0
The Use of MMR, Diversity-Based Reranking for RE Docu- ments and Producing Summaries.	RE	referring expression$Reordering$relation extraction$Relation Extraction$rank of expected$Relation extraction$Regular Expressions$regular expression$Reduce$	1
RE methods are effective, but need reliable parsers to ex- tract the syntactic structure of the source sentences.	RE	referring expression$Reordering$relation extraction$Relation Extraction$rank of expected$Relation extraction$Regular Expressions$regular expression$Reduce$	1
Tables KOR-CHN  baseline  KOR-CHN  modified  Word 12.39 MB 12.52 MB  Phrase 19.41 MB 19.04 MB  RE 10.01 MB 9.83 MB    Table 3.	RE	referring expression$Reordering$relation extraction$Relation Extraction$rank of expected$Relation extraction$Regular Expressions$regular expression$Reduce$	1
RE strategies that use syntactic infor- mation have proved successful, but they are likely to magnify parsing errors if their reordering rules heavily rely on abundant parse information.	RE	referring expression$Reordering$relation extraction$Relation Extraction$rank of expected$Relation extraction$Regular Expressions$regular expression$Reduce$	1
c?2013 Association for Computational Linguistics Anchor Graph: Global RE Contexts for Statistical Machine Translation Hendra Setiawan ?	RE	referring expression$Reordering$relation extraction$Relation Extraction$rank of expected$Relation extraction$Regular Expressions$regular expression$Reduce$	1
Thomson Reuters 3 Times Square NY 10036, USA Abstract RE poses one of the greatest chal- lenges in Statistical Machine Translation re- search as the key contextual information may well be beyond the c	RE	referring expression$Reordering$relation extraction$Relation Extraction$rank of expected$Relation extraction$Regular Expressions$regular expression$Reduce$	1
RE in- variable grammatical particles in our framework can be summarized as: i) Find dependents of a verbal head (Vb-H) whose POS tags are in the Oth-DEP entry of Table 1.	RE	referring expression$Reordering$relation extraction$Relation Extraction$rank of expected$Relation extraction$Regular Expressions$regular expression$Reduce$	1
It is well known that dependency informa- tion plays a key role in many NLP problems,  such as syntactic parsing, semantic role label- ing as well as semantic RE.	RE	referring expression$Reordering$relation extraction$Relation Extraction$rank of expected$Relation extraction$Regular Expressions$regular expression$Reduce$	2
Indeed, the kernel-based methods have  been successfully applied to mine structured  information in various NLP applications like  syntactic parsing (Collins and Duffy, 2001;  Moschitti, 2004), semantic RE  (Zelenko et al, 2003; Zhao and Grishman,  2005; Zhou et al 2007; Qian et al, 2008), se- mantic role labeling (Moschitti, 2004); corefer- ence resolution (Yang et al, 2006; Zhou et al,  2008).	RE	referring expression$Reordering$relation extraction$Relation Extraction$rank of expected$Relation extraction$Regular Expressions$regular expression$Reduce$	2
Tree kernel-based RE with con- text-sensitive structured parse tree information.	RE	referring expression$Reordering$relation extraction$Relation Extraction$rank of expected$Relation extraction$Regular Expressions$regular expression$Reduce$	2
Kernel methods for RE.	RE	referring expression$Reordering$relation extraction$Relation Extraction$rank of expected$Relation extraction$Regular Expressions$regular expression$Reduce$	2
Qian et al (2008)  dynamically determined the parse tree structure  for semantic RE by exploiting  constituent dependencies to keep the necessary  information in the parse tree as well as remove  the noisy information.	RE	referring expression$Reordering$relation extraction$Relation Extraction$rank of expected$Relation extraction$Regular Expressions$regular expression$Reduce$	2
Dependency tree kernels for RE.	RE	referring expression$Reordering$relation extraction$Relation Extraction$rank of expected$Relation extraction$Regular Expressions$regular expression$Reduce$	2
3 RE Performing a syntax-based NLP task in most real- world scenarios requires that the incoming data first be parsed using a pre-trained parsing model.	RE	referring expression$Reordering$relation extraction$Relation Extraction$rank of expected$Relation extraction$Regular Expressions$regular expression$Reduce$	3
.0 42.6 21.3 28.4 Baseline-Ent 87.2 65.4 74.8 85.8 64.4 73.6 55.2 31.1 39.8 51.2 29.4 37.4 Oracle D-Parse 89.3 67.4 76.8 89.3 66.2 75.4 60.0 32.6 42.2 58.1 31.3 40.7 Hidden D-Parse 87.8 69.8 77.7 85.3 67.8 75.6 48.0 32.0 38.4 47.2 30.0 36.7 Oracle C-Parse 89.1 68.7 77.6 87.5 67.5 76.2 66.8 37.8 48.3 63.8 37.0 46.8 Hidden C-Parse 90.5 69.9 78.9 88.8 68.6 77.4 56.3 32.3 41.0 53.4 31.6 39.7 Table 1: RE Results.	RE	referring expression$Reordering$relation extraction$Relation Extraction$rank of expected$Relation extraction$Regular Expressions$regular expression$Reduce$	3
4.2 Discourse RE System Our work follows the approach and features de- scribed in the state-of-the-art Ruby-based discourse system of (Lin et al, 2010), to build an in- house Java-based discourse relation extraction sys- tem.	RE	referring expression$Reordering$relation extraction$Relation Extraction$rank of expected$Relation extraction$Regular Expressions$regular expression$Reduce$	3
RE for Drug-Drug Interactions using Ensemble Learn- ing Proceedings of the 1st Challenge task on Drug- Drug Interaction Extraction Md. Faisal Mahbub Chowdhury, Asma Ben Abacha, Alberto Lavelli, and Pierre Zweigenbau.	RE	referring expression$Reordering$relation extraction$Relation Extraction$rank of expected$Relation extraction$Regular Expressions$regular expression$Reduce$	3
Dan Roth has written the first paper on formulating global NLP decisions as ILP  problems with his student Scott Yih, presented in CoNLL'04, and since then has worked  on further developing Constrained Conditional Models, on learning and inference issues  within this framework and on applying it to several NLP problems, including Semantic  Role Labeling, Information and RE and Transliteration.	RE	referring expression$Reordering$relation extraction$Relation Extraction$rank of expected$Relation extraction$Regular Expressions$regular expression$Reduce$	3
We will show that this kind of Web- based RE requires different tech- niques than the state-of-the-art seed-based ap- proaches as it has to acquire information from the long-tail of the World Wide Web.	RE	referring expression$Reordering$relation extraction$Relation Extraction$rank of expected$Relation extraction$Regular Expressions$regular expression$Reduce$	3
loglike infection infection 1 infection .38, aids .27, tract .18, infected .18, positive .17 Table 2: Example results; R = RE target English word for source French word print out the top 5 ranked words.	RE	referring expression$Reordering$relation extraction$Relation Extraction$rank of expected$Relation extraction$Regular Expressions$regular expression$Reduce$	4
RE using convolution tree kernel expanded with entity features.	RE	referring expression$Reordering$relation extraction$Relation Extraction$rank of expected$Relation extraction$Regular Expressions$regular expression$Reduce$	5
RE is the task of identifying se- mantic relations between sets of entities in text (as 812 illustrated in Fig.	RE	referring expression$Reordering$relation extraction$Relation Extraction$rank of expected$Relation extraction$Regular Expressions$regular expression$Reduce$	5
RE using dependency parse trees.	RE	referring expression$Reordering$relation extraction$Relation Extraction$rank of expected$Relation extraction$Regular Expressions$regular expression$Reduce$	5
2.2 Supervised Relation Extraction RE can be naturally cast as a su- pervised classification problem.	RE	referring expression$Reordering$relation extraction$Relation Extraction$rank of expected$Relation extraction$Regular Expressions$regular expression$Reduce$	5
RE with matrix factorization and universal schemas.	RE	referring expression$Reordering$relation extraction$Relation Extraction$rank of expected$Relation extraction$Regular Expressions$regular expression$Reduce$	5
1.1 Learning RE The problem of inducing regular languages from positive and negative examples has been studied in the past, even outside the context of information extraction (Alquezar and Sanfeliu, 1994; Dupont, 1996; Firoiu et al, 1998; Garofalakis et al, 2000; Denis, 2001; Denis et al, 2004; Fernau, 2005; Galassi and Giordana, 2005; Bex et al, 2006).	RE	referring expression$Reordering$relation extraction$Relation Extraction$rank of expected$Relation extraction$Regular Expressions$regular expression$Reduce$	6
Java RE.	RE	referring expression$Reordering$relation extraction$Relation Extraction$rank of expected$Relation extraction$Regular Expressions$regular expression$Reduce$	6
We used RE to implement the  linguistic motivated patterns that check for the in- formation just mentioned in a part of speech tagged  corpus.	RE	referring expression$Reordering$relation extraction$Relation Extraction$rank of expected$Relation extraction$Regular Expressions$regular expression$Reduce$	6
doc))[a-zA-Z]{2,3}))(/[?\s]+){0,20}\b Table 4: Sample RE Learned by ReLIE(R0: input regex; Rfinal: final regex learned; the parts of R0 modified by ReLIE and the corresponding parts in Rfinal are highlighted.)	RE	referring expression$Reordering$relation extraction$Relation Extraction$rank of expected$Relation extraction$Regular Expressions$regular expression$Reduce$	6
4.1 RE     We use regular expression pattern to detect.errors  in words by using word weight (Wazn) and affixes.	RE	referring expression$Reordering$relation extraction$Relation Extraction$rank of expected$Relation extraction$Regular Expressions$regular expression$Reduce$	6
Clustered Lan- guage Models Based on RE for  SMT, Proc.	RE	referring expression$Reordering$relation extraction$Relation Extraction$rank of expected$Relation extraction$Regular Expressions$regular expression$Reduce$	6
Morpho-syntactic filters describe general  term formation patterns, and are imple- mented as generic REs.	RE	referring expression$Reordering$relation extraction$Relation Extraction$rank of expected$Relation extraction$Regular Expressions$regular expression$Reduce$	7
The product of the RE or automaton  produced by GEN with all of the constraints in order pro-  ducts a transducer encoding the harmony evaluations of  all candidates.	RE	referring expression$Reordering$relation extraction$Relation Extraction$rank of expected$Relation extraction$Regular Expressions$regular expression$Reduce$	7
As an example, (5) shows a RE giving a  subset of the candidate syllabifications of alqalamu accor-  cling to the syllabification rules of P&S(p25).	RE	referring expression$Reordering$relation extraction$Relation Extraction$rank of expected$Relation extraction$Regular Expressions$regular expression$Reduce$	7
In another  approach, she used RE patterns to  extract term collocations from a morpho- syntactically tagged corpus.	RE	referring expression$Reordering$relation extraction$Relation Extraction$rank of expected$Relation extraction$Regular Expressions$regular expression$Reduce$	7
For brevity, then, the algorithms will  be phrased in terms of the states and arcs of an automa-  ton, while, for clarity, REs will be used to  present the inputs and outputs of examples.	RE	referring expression$Reordering$relation extraction$Relation Extraction$rank of expected$Relation extraction$Regular Expressions$regular expression$Reduce$	7
Adj | Noun)* Noun     Although these patterns are REs,  the filters are implemented as unification-like  LR(1) rules (Mima et al, 1995) in order to facili- tate processing of grammatical agreements (if  any) within term candidates.	RE	referring expression$Reordering$relation extraction$Relation Extraction$rank of expected$Relation extraction$Regular Expressions$regular expression$Reduce$	7
There are four possible operations (Right, Left,  Shift and RE) for the configuration at hand.	RE	referring expression$Reordering$relation extraction$Relation Extraction$rank of expected$Relation extraction$Regular Expressions$regular expression$Reduce$	8
RE B to the cores of C1 and C2.	RE	referring expression$Reordering$relation extraction$Relation Extraction$rank of expected$Relation extraction$Regular Expressions$regular expression$Reduce$	8
RE: If there is no word 'n  ( In ?' )	RE	referring expression$Reordering$relation extraction$Relation Extraction$rank of expected$Relation extraction$Regular Expressions$regular expression$Reduce$	8
The sentences were extracted from Arabic News (LDC2004T17), eTIRR (LDC2004E72), English translation of Arabic Treebank (LDC2005E46), and Ummah (LDC2004T18).4 For Arabic pre- processing, we follow previously reported best to- kenization scheme (TB)5 and orthographic word normalization condition (REd) when translat- ing from English to Arabic (El Kholy and Habash, 2010b).	RE	referring expression$Reordering$relation extraction$Relation Extraction$rank of expected$Relation extraction$Regular Expressions$regular expression$Reduce$	8
RE: Pop TOP from the stack.	RE	referring expression$Reordering$relation extraction$Relation Extraction$rank of expected$Relation extraction$Regular Expressions$regular expression$Reduce$	8
which may  depend on t, and t has a parent on its left side, the  parser removes t from the stack S.  Shift: If there is no dependency between n and t,  and the triple does not satisfy the conditions for  RE, then push n onto the stack S.  In this work, we adopt SVMs for estimating the  word dependency attachments.	RE	referring expression$Reordering$relation extraction$Relation Extraction$rank of expected$Relation extraction$Regular Expressions$regular expression$Reduce$	8
RE S to the separating synsets for {C1,C2}.	RE	referring expression$Reordering$relation extraction$Relation Extraction$rank of expected$Relation extraction$Regular Expressions$regular expression$Reduce$	8
In J. Carroll, N. Oostdijk, and R. Sutcliffe, editors, Proceedings of the Workshop on Grammar EG and Evalua- tion, COLING 19, pages 8?14, Taipei, Taiwan.	EG	Engineering$example$exponentiated gradient$Egypt$Entity Grid$	0
Natural Lan- guage EG, 7(3):207?223.	EG	Engineering$example$exponentiated gradient$Egypt$Entity Grid$	0
Knowledge and Data EG, 19(3):370?383.	EG	Engineering$example$exponentiated gradient$Egypt$Entity Grid$	0
In J. Carroll, N. Oostdijk, and R. Sutcliffe, editors, Pro- ceedings of the Workshop on Grammar EG and Evaluation at COLING 19, pages 1?7.	EG	Engineering$example$exponentiated gradient$Egypt$Entity Grid$	0
In Proceedings of the Workshop on Grammar EG and Evalua- tion, COLING 19, Taipei, Taiwan.	EG	Engineering$example$exponentiated gradient$Egypt$Entity Grid$	0
ACL Workshop Incremental Parsing: Bringing EG and Cognition Together, pages 42?49, Barcelona, Spain.	EG	Engineering$example$exponentiated gradient$Egypt$Entity Grid$	0
For EG:  Mar ie  Giraud carries historical sig-  nificance as one of the last women to  be ezecuted in France.	EG	Engineering$example$exponentiated gradient$Egypt$Entity Grid$	1
Here we would compare the degree to which  each possible candidate antecedent (A Japanese  company, television picture tubes, Japan, TV   sets, and Malaysia in this EG) could serve  as the direct object of "export".	EG	Engineering$example$exponentiated gradient$Egypt$Entity Grid$	1
A canonical EG of selectional  restriction is that of the verb "eat", which se-  lects food as its direct object.	EG	Engineering$example$exponentiated gradient$Egypt$Entity Grid$	1
However a singular noun phrase can be the ref-  erent of a plural pronoun, as illustrated by the  following EG:  "I think if I tell Viacom I need more  time, they will take 'Cosby' across the  street," says the general manager ol a  network a~liate.	EG	Engineering$example$exponentiated gradient$Egypt$Entity Grid$	1
For EG:  A Japanese company might make tele-  vision picture tubes in Japan, assem-  ble the TV  sets in Malaysia and extort  them to I	EG	Engineering$example$exponentiated gradient$Egypt$Entity Grid$	1
For EG:  A Japanese company might make tele-  vision picture tubes in Japan, assem-  ble the TV  sets in Malaysia and extort  them to Indonesia.	EG	Engineering$example$exponentiated gradient$Egypt$Entity Grid$	1
3.1.1 The  ment ion  count  stat ist ics   The referents range from being mentioned only  once to begin mentioned 120 times in the train-  hag EGs.	EG	Engineering$example$exponentiated gradient$Egypt$Entity Grid$	1
The set of potential edges was pruned using the marginals produced by a first-order parser trained using EG descent (Collins et al, 2008) as in Koo and Collins (2010).	EG	Engineering$example$exponentiated gradient$Egypt$Entity Grid$	2
t ewpos?fpos(x,t) We use the same feature set fpos defined in Sec- tion 2.1, and adopt the EG algo- rithm to learn the weight vector wpos (Collins et al, 2008).	EG	Engineering$example$exponentiated gradient$Egypt$Entity Grid$	2
Matrix EG updates for on- line learning and bregman projection.	EG	Engineering$example$exponentiated gradient$Egypt$Entity Grid$	2
EG has a more erratic be- haviour, and requires a careful tuning of ?	EG	Engineering$example$exponentiated gradient$Egypt$Entity Grid$	2
Applying an EG rerank- ing algorithm (Bartlett et al, 2004) to the n-best out- put of our morphologically-informed Spanish pars- ing model gives us similar improvements.	EG	Engineering$example$exponentiated gradient$Egypt$Entity Grid$	2
Some of the proposed algorithms, such as EG corresponds to block- coordinate descent in the dual, and uses the exact gradient with respect to the block being updated.	EG	Engineering$example$exponentiated gradient$Egypt$Entity Grid$	2
In Proceedings of MEDAR International Conference on Arabic Language Resources and Tools, Cairo, EG.	EG	Engineering$example$exponentiated gradient$Egypt$Entity Grid$	3
Texts included in  ArabiCorpus almost exclusively belong to the  written genre, save for a small sub-corpus of  spoken EGian Arabic.	EG	Engineering$example$exponentiated gradient$Egypt$Entity Grid$	3
In Pro- ceedings of the 2nd International Conference  on Arabic Language Resources and Tools  (MEDAR), pages 102?109, Cairo, EG.	EG	Engineering$example$exponentiated gradient$Egypt$Entity Grid$	3
In Proceedings of the 2nd International Con- ference on Arabic Language Resources and Tools (MEDAR), Cairo, EG.	EG	Engineering$example$exponentiated gradient$Egypt$Entity Grid$	3
News articles included in  this sub-section of ArabiCorpus cover issues  from 1996 to 2010 and are extracted from peri- odicals published in different parts of the Arab  world (North Africa, EG, Arabian Gulf, the  Levant, etc.).	EG	Engineering$example$exponentiated gradient$Egypt$Entity Grid$	3
In NEMLAR Conference on Arabic Language Resources and Tools, Cairo, EG.	EG	Engineering$example$exponentiated gradient$Egypt$Entity Grid$	3
3 Relaxed EG In this section, we relax the troublesome assump- tion of independence between entities, thus mov- ing the probability distribution over documents away from blank sentences.	EG	Engineering$example$exponentiated gradient$Egypt$Entity Grid$	4
2 The EG Model Barzilay and Lapata (2005; 2008) introduced the entity grid, a method for local coherence modeling that captures the distribution of discourse entities across sentences in a text.	EG	Engineering$example$exponentiated gradient$Egypt$Entity Grid$	4
Naive EG .17 81 Relaxed EG .02 87 Topic-based (naive) .39 85 Topic-based (relaxed) .54 96 Table 2: Results for 10-fold cross-validation on AIR- PLANE training data.	EG	Engineering$example$exponentiated gradient$Egypt$Entity Grid$	4
2 Naive EGs Entity grids, first described in (Lapata and Barzilay, 2005), are designed to capture some ideas of Cen- tering Theory (Grosz et al, 1995), namely that ad- jacent utterances in a locally coherent discourses are likely to contain the same nouns, and that important nouns often appear in syntactically important roles such as subject or object.	EG	Engineering$example$exponentiated gradient$Egypt$Entity Grid$	4
In their EG model, a text is represented by a matrix with rows corresponding to each sen- tence in a text, and columns to each entity men- tioned anywhere in the text.	EG	Engineering$example$exponentiated gradient$Egypt$Entity Grid$	4
As ~ CHAs, the points in M~ move continuously.	CHA	ehange$characteristics$	0
stylistic wu'iants or the typical versions which can be used  interCHA:tbly in any c(mtexl \[l~rguvanh 1979\].	CHA	ehange$characteristics$	0
eaning a t   t h e  k i n d 8  of u t tQrancer  t h a t  occur i n  rpaatanooua dialog, When  tnr detinltianr of the  grammar R t a V i d ~  tnformrtfon t h a t  h t l ~ s  a  prtrcr ehaora  these rule8 ao8z likely t o  l a a d  td correct   int@tpretatlonr of uttrrtanCs&, t h e  grrar@ar 1 8  r a i d  t o  be 9tunrd*,  Uhan t h e  tuning 1.8 crarily chrncraa wnrn t h r  dornrin o t  dircaurrr,   CHAs, thc grasmrr 1 6  s a i d  t o  br  *tunrrblr#.	CHA	ehange$characteristics$	0
In January  1994, the TEl isstled its Guidelines for the Fmcoding and  hiterCHA of Machine-Readable Texts, which provide  standardized encoding conventions for a large range of  text types and features relevant for a broad range of  applications.	CHA	ehange$characteristics$	0
Term Variant Type  ECHA d'ion (ion exchange)  Culture de eellules (cell culture)  Propridtd chimique  (chemical property)  Gestion d ' eau (water management)  Eau de surface  (surface water)  Huile de palme (palm oil)  Initiation de bourgeon  (bud initiation)  dchange ionique (ionic exchange) N to A  cultures primaires de cellules (primary cell cultures) Modif.	CHA	ehange$characteristics$	0
E-dictionaries contain  exhaustive description of morpho-syntactic  CHA and are used for lexical  recognition and initial lemmatisation of words  that occur in a text.	CHA	ehange$characteristics$	1
We compare the results with a set of ad- jectives classified by human judges according to se- mantic CHA.	CHA	ehange$characteristics$	1
Among approaches to multilingual grammar engineering, the Grammar Matrix?s distinguishing CHA include the deployment of a shared core grammar for crosslinguistically consistent con- straints and a series of libraries modeling vary- ing linguistic properties.	CHA	ehange$characteristics$	1
They received instructions which referred only to semantic CHA, not to the expected syn- tactic behaviour.	CHA	ehange$characteristics$	1
Or they  can pursue a multi-dimensional inquiry, for  instance, the relations between a theory in physics  and the broader context of knowledge and action,  including identifiers uch as aesthetic value,  dominant cognitive CHA, the state of  technology, etc.	CHA	ehange$characteristics$	1
Sec- tion 3 presents desirable CHA of restricted domains for the development of NLP research in general, and question answering in particular.	CHA	ehange$characteristics$	1
We are also working  4Our approach allows morn than a single COP to appear at a  given level, however, as the ditransitive VP rule above shows.	COP	complement constituent$copula$	0
Furthermore, the CLaC system will choose to trigger the subject passive focus heuristic in the case where the verb COP is empty, and the passive noun subject is present.	COP	complement constituent$copula$	0
NegFocus will only make this assumption when the verb COP is empty, otherwise the baseline focus heuristic will be triggered, as depicted in Example (16).	COP	complement constituent$copula$	0
Since we have only one  entry for a verb, then any semantic differences that are associated with variant subcat-  egorizations will have to be built from the COPs in a completely  compositional way.	COP	complement constituent$copula$	0
The authors claim an advantage over chunker-based approaches with respect to the an- notation of markable adjectives due to the fact that the dependency relation between COP verb and predicative adjective is available.	COP	complement constituent$copula$	1
The COP itself constitutes the predicate.	COP	complement constituent$copula$	1
2.2 Unary vs. binary Unary adjectives have only one argument, usu- ally corresponding to the modified noun (a red ball  ) or the subject in a COPr sentence (this ball  is red).	COP	complement constituent$copula$	1
For example, if the preceding verb is a COP, the adjective is flagged as a markable.	COP	complement constituent$copula$	1
In English, the COPr verb is considered the syntactic head of the clause, with the pronoun being the subject and the predicate adjective be- ing an XCOMP.	COP	complement constituent$copula$	1
Verbs which express a connection between subjects and  objects are referred to as COPs, e.g. "is" in "John is a boy".	COP	complement constituent$copula$	1
The other main function of the adjective is that of predicate in a COPr sentence (6% of the tokens).	COP	complement constituent$copula$	1
A constraint that de- scribes this individual word pair would be trivial to write, but it is not feasible to model the general phenomenon in this way; thousands of constraints would be needed just to reflect the more impor- tant COs in a language, and the exact set of collocating words is impossible to predict ac- curately.	CO	collocation$confessions$concession$co-training$country$Campus Outdoor$convertible$	0
In another  approach, she used regular expression patterns to  extract term COs from a morpho- syntactically tagged corpus.	CO	collocation$confessions$concession$co-training$country$Campus Outdoor$convertible$	0
The  statistical approach was based on the mutual ex- pectation and LocalMax measures, and involved  CO extraction from raw text.	CO	collocation$confessions$concession$co-training$country$Campus Outdoor$convertible$	0
The ex- tracted COs were filtered with a stop- word list, and only COs containing sin- gle-word terms (devised previously by bilingual  alignment) were accepted as relevant.	CO	collocation$confessions$concession$co-training$country$Campus Outdoor$convertible$	0
atistical approach was based on the mutual ex- pectation and LocalMax measures, and involved  CO extraction from raw text.	CO	collocation$confessions$concession$co-training$country$Campus Outdoor$convertible$	0
For ex- ample, Vintar (2000) presented two methods for  extraction of terminological COs in order  to assist the translation process in Slovene.	CO	collocation$confessions$concession$co-training$country$Campus Outdoor$convertible$	0
This web- site represents an interactive platform that allows  people to share personal experiences, thoughts,  opinions, feelings, passions, and CO  through the network of personal stories.	CO	collocation$confessions$concession$co-training$country$Campus Outdoor$convertible$	1
n Q1 R2 comaker, imported, deceitful, huston, send, bright, remainderman?s Topic 2 in Q1 R1 descendent, younger, administrator?s, documentary, agreeable, emancipated Topic 2 in Q1 R2 younger, administrator?s, grandmother?s, plaintiffs, emancipated, learnedly Topic 3 in Q2 R1 heir-at-law, reconsidered, manumissions, birthplace, mon, mother-in-law Topic 3 in Q2 R2 heir-at-law, reconsideration, mon, CO, birthplace, father-in-law?s Topic 4 in Q2 R1 indentured, apprenticed, deputy collector, stepfather?s, traded, seizes Topic 4 in Q2 R2 deputy collector, seizes, traded, hiring, stepfather?s, indentured, teaching Topic 5 in Q4 R1 constitutionality, constitutional, unconstitutionally, Federal Army, violated Topic 5 in Q4 R2 petition, convictions, criminal court, murdered, constitutionali	CO	collocation$confessions$concession$co-training$country$Campus Outdoor$convertible$	1
(In)accuracy at detecting true and false CO and denials: An initial test of a projected motive model of veracity judgments.	CO	collocation$confessions$concession$co-training$country$Campus Outdoor$convertible$	1
In  order to evaluate the performance of our algo- rithm, we created the data set of sentences ex- tracted from personal stories about life expe- riences that were anonymously published on the  Experience Project website  (www.experienceproject.com), where  people share personal experiences, thoughts,  opinions, feelings, passions, and CO  through the network of personal stories.	CO	collocation$confessions$concession$co-training$country$Campus Outdoor$convertible$	1
The technique was adapted for author identifi-  cation by Morton (1978) - see also Farringdon (1996)  - and achieved some notoriety for its use in court cases  (e.g. to identify faked or coerced CO) as well as  in literary studies.	CO	collocation$confessions$concession$co-training$country$Campus Outdoor$convertible$	1
We also evaluate the model?s ability to predict sentiment distributions on a new dataset based on CO from the experience project.	CO	collocation$confessions$concession$co-training$country$Campus Outdoor$convertible$	1
What we get is  5.1 ; (A  ~. > I;((;))  ()ur counterlhctual becomes,intuitively, "You,the  opponent,will fail in showing that C fails, aftcr A  has been added to the COs".	CO	collocation$confessions$concession$co-training$country$Campus Outdoor$convertible$	2
and in (5), as a CO for failing to do the  action (washing the dishes) denied by (a).	CO	collocation$confessions$concession$co-training$country$Campus Outdoor$convertible$	2
f negation  which is even weaker than that of minimal  calculus,and the introduction of an opportuuity for  the opponent  to make us(" of his own conccssiot~s  as exception rules+ The second of these is easily  etlk'ctuated: by inlroducing flK+ fail- operator twice  we cause a first change of rolcs which gives the op-  ponent, tmw as a ternporary proponent, the op+  portunity to britlg additional COs into play,  The second fail operator then restores the initial  order of roles.	CO	collocation$confessions$concession$co-training$country$Campus Outdoor$convertible$	2
Suppose your adversary  accepts the invitation to discuss, and takes the  antecedent of the counterfactual s a temporary  additional CO.	CO	collocation$confessions$concession$co-training$country$Campus Outdoor$convertible$	2
r twice  we cause a first change of rolcs which gives the op-  ponent, tmw as a ternporary proponent, the op+  portunity to britlg additional COs into play,  The second fail operator then restores the initial  order of roles.	CO	collocation$confessions$concession$co-training$country$Campus Outdoor$convertible$	2
- -B -> -~A = yes  min  Suppose we add to tile COs A and B a  CO to the effect that stepping on tile brake  is the only reason for Jones' being alive (--, B ->  -, A) .	CO	collocation$confessions$concession$co-training$country$Campus Outdoor$convertible$	2
Steedman et al(2003b) and Hwa et al (2003) also used several versions of cor- rected CO which are not comparable to ours and other suggested methods because their evalua- tion requires different measures (e.g. reviewed and corrected constituents are separately counted).	CO	collocation$confessions$concession$co-training$country$Campus Outdoor$convertible$	3
Moreover, CO model (Bian et al.,	CO	collocation$confessions$concession$co-training$country$Campus Outdoor$convertible$	3
Table 2 compares our results with self-training and CO results reported by (Steedman et al 20003a; 2003b).	CO	collocation$confessions$concession$co-training$country$Campus Outdoor$convertible$	3
pars- ing model using a small seed dataset (500 sentences for both methods and 1,000 sentences for CO only).	CO	collocation$confessions$concession$co-training$country$Campus Outdoor$convertible$	3
2003a), which applied CO (Blum and Mitchell, 1998) and self-training to Collins?	CO	collocation$confessions$concession$co-training$country$Campus Outdoor$convertible$	3
Steedman et al (2003b) followed a similar CO protocol except that the selection function (three functions were explored) considered the differences between the confidence scores of the two parsers.	CO	collocation$confessions$concession$co-training$country$Campus Outdoor$convertible$	3
~he f i r s t  conta ins  an unreso lved  reverence  to  some CO!	CO	collocation$confessions$concession$co-training$country$Campus Outdoor$convertible$	4
E.g, if one here selects the option of having the results presented by CO of origin of the hit texts, one is not presented directly with the KWIC list of results, but rather with a bar representation of the number of hits per CO.	CO	collocation$confessions$concession$co-training$country$Campus Outdoor$convertible$	4
Another example:  "President and wife came to capital"  (Articles and pronouns ere dropped to reflect Russian),  This phrase is processed as  "President roof-CO with xhis wife came to capital Eof-  CO".	CO	collocation$confessions$concession$co-training$country$Campus Outdoor$convertible$	4
For example, these queries im- ply that Afghanistan is a CO, Netscape is a web browser, Bushwackers are shoes, and BidFind is an auction search engine.	CO	collocation$confessions$concession$co-training$country$Campus Outdoor$convertible$	4
Nouns that are readily converted are marked as ei- ther strongly countable (for countable nouns that can be converted to uncountable, such as cake) or weakly countable (for uncountable nouns that are readily CO to countable, such as beer).	CO	collocation$confessions$concession$co-training$country$Campus Outdoor$convertible$	6
The format should be easily CO to the specific formats required by statistical packages such as R or SPSS.	CO	collocation$confessions$concession$co-training$country$Campus Outdoor$convertible$	6
Additionally, META?s forward index (used for classifica- tion), is easily CO to LIBSVM format.	CO	collocation$confessions$concession$co-training$country$Campus Outdoor$convertible$	6
Human: IMG_6892 Lookn up in the sky its a  bird its a plane its ah..... you  ILP: This is a sporty little red CO made for  a great day in Key West FL.	CO	collocation$confessions$concession$co-training$country$Campus Outdoor$convertible$	6
,Oct,... 3,A1,C1,C3,... Trading,Interest,Demand,Production,... 30 Treasury,mortgage-backed 50,1.50,0.50,0.05,... York,York-based age,identity,integrity,identification,... bond,floating-rate books,words,budgets,clothes,... consumer CO,bonus,Brady,subordinated,... increase interest loss months no-fly,year-ago,corresponding,buffer,... people quarter results rose there Figure 1: The bipartite graphs show the top 40 non-stopword Brown cluster pair features for all four classification tasks.	CO	collocation$confessions$concession$co-training$country$Campus Outdoor$convertible$	6
b~ / \ pit head(p~ol>body) ~ "\\  / ,~  corner /  goal i~cl>thing) \[.41 obj mod \[  ~left :  Figm'e I. 1: A UNL graph deCO as "Ronaldo  has headed the ball into the left corner of the net"  In a UNL graph, UWs appear with attributes  describing what is said from tile speaker's point  of view.	CO	collocation$confessions$concession$co-training$country$Campus Outdoor$convertible$	6
2.4 Agreement in ACs The present honorification system in the KPSG can offer us a streamlined way of explaining the agreement in auxiliary verb constructions we noted in section 1.1.	AC	Auxiliary Construction$Ambiguity Classes$Active Chain$Adpositional Case$	0
NN plan (.03, .9), offer (.2, .74), issues (0, 0), increase (.34, .66), end (.18, .81) 17 115 DT, NNP As (0, 0), One (0, .01), First (0, .82), Big (0, .91), On (0, .01) 18 11 NN, JJ market (.99, 0), U.S. (0, 0), bank (1, 0), cash (.98, 0), high (.06, .9) 20 22 VBN, JJ estimated (.58, .15), lost (.43, .03), failed (.35, .04), related (.74, .23), re- duced (.57, .12) Table 3: Selection of Predicted AC: Common ambiguity classes from the predicted part-of-speech assignments from the WSJ data set, and the five most common word types associated with each ambiguity class.	AC	Auxiliary Construction$Ambiguity Classes$Active Chain$Adpositional Case$	1
Elles  permettent de localiser la plupart des contraiutes syn-  taxiques (par exemple, sujet-VT, VT-objet) tout  en ddcrivant la syntaxe sous forme d'arbres.	VT	verbe$Veins Theory$	0
In M. Loflin  and J. SilVTrg, eds.,	VT	verbe$Veins Theory$	0
oVTaring pride evi- denced by a superior manner toward the weak?.	VT	verbe$Veins Theory$	0
Grammaire transformation-  nelle du frangais: le VT, Paris, Larousse  II. - -	VT	verbe$Veins Theory$	0
Morpholoo  gie et ggndration des VTs fran~ais.	VT	verbe$Veins Theory$	0
1998:  VT: A Model of Global Discourse Cohe- sion and Coherence, in Proceedings of the 17th in- ternational conference on Computational linguis- tics.	VT	verbe$Veins Theory$	1
Formal proofs in Incremental  Discourse Processing and VT, Research  Report TR98-2 Dept.	VT	verbe$Veins Theory$	1
Following VT, the predecessor of approve(fda, elixirplus) is ban(fda, elixir); its linear predecessor contain(elixir, gestodene) (an embedded satellite) is inaccessible.	VT	verbe$Veins Theory$	1
It is worth noting that Tetreault (2005) has employed Grosz and Sid- ner?s (1986) discourse theory and VT (Ide and Cristea, 2000) to identify and remove candidate antecedents that are not referentially ac- cessible to an anaphoric pronoun in his heuristic pronoun resolvers.	VT	verbe$Veins Theory$	1
At each level, the parser goes on with a forest  of developing trees in parallel, ranking them by a  global score (Figure 6) based on heuristics that  are suggested by both VT (Cristea et  al.,	VT	verbe$Veins Theory$	1
184  VT:  A Model of Global Discourse Cohesion and Coherence  Dan CRISTEA  Dept.	VT	verbe$Veins Theory$	1
AZ: Information Extraction from Scientific Text.	AZ	Argumentative Zoning$argumentative zoning$	0
AZ for Im- proved Citation Indexing.	AZ	Argumentative Zoning$argumentative zoning$	0
AZ: Infor- mation Extraction from Scientific Text.	AZ	Argumentative Zoning$argumentative zoning$	0
We examine the features used to achieve this result and experiment with AZ as a sequence tag- ging task, decoded with Viterbi using up to four previous classification decisions.	AZ	Argumentative Zoning$argumentative zoning$	0
c?2009 ACL and AFNLP Accurate AZ with Maximum Entropy models Stephen Merity and Tara Murphy and James R. Curran School of Information Technologies University of Sydney NSW 2006, Australia {smerity,tm,james}@it.usyd.edu.au Abstract We present a maximum entropy classifier that significantly improves the accuracy of AZ in scientific litera- ture.	AZ	Argumentative Zoning$argumentative zoning$	0
Ac- curate AZ with maximum entropy models.	AZ	Argumentative Zoning$argumentative zoning$	1
To- wards discipline-independent AZ: Evidence from chemistry and computational linguis- tics.	AZ	Argumentative Zoning$argumentative zoning$	1
In our ex- periments that treat AZ as a se- quence labelling task, the context xi incorporates history information ?	AZ	Argumentative Zoning$argumentative zoning$	1
To- wards domain-independent AZ: Ev- idence from chemistry and computational linguistics.	AZ	Argumentative Zoning$argumentative zoning$	1
Accurate AZ with maximum entropy models.	AZ	Argumentative Zoning$argumentative zoning$	1
Proceedings of the TREC-7, pp.	TREC-7	Seventh Text REtrieval Conference$Seventh Text Retrieval Conference$	0
In The TREC-7.	TREC-7	Seventh Text REtrieval Conference$Seventh Text Retrieval Conference$	1
c?2016 Association for Computational Linguistics Insertion Position Selection Model for Flexible Non-Terminals in Dependency Tree-to-Tree Machine Translation Toshiaki Nakazawa JST Agency 5-3, Yonbancho, Chiyoda-ku, Tokyo, 102-8666, Japan nakazawa@pa.jst.jp John Richardson and Sadao Kurohashi Kyoto University Yoshida-honmachi, Sakyo-ku, Kyoto, 606-8501, Japan john@nlp.ist.i.kyoto-u.ac.jp kuro@i.kyoto-u.ac.jp Abstract Dependency tree-to-tree translation models are powerful because they can naturally han- dle long range reorderings which is importa	JST	Japan Science and Technology$JUSTIFICATION$	0
Acknowledgments The research presented in this paper was partly funded by PREST, JST Corporation.	JST	Japan Science and Technology$JUSTIFICATION$	0
PSG Parsing Takashi Ninomiya Information Technology Center University of Tokyo Takuya Matsuzaki Department of Computer Science University of Tokyo Yoshimasa Tsuruoka School of Informatics University of Manchester Yusuke Miyao Department of Computer Science University of Tokyo Jun?ichi Tsujii Department of Computer Science, University of Tokyo School of Informatics, University of Manchester SORST, JST Agency Hongo 7-3-1, Bunkyo-ku, Tokyo, 113-0033, Japan {ninomi, matuzaki, tsuruoka, yusuke, tsujii}@is.s.u-tokyo.ac.jp Abstract This paper describes an extremely lexi- calized probabilistic model for fast and accurate HPSG parsing.	JST	Japan Science and Technology$JUSTIFICATION$	0
The JST Agency Dictionary,? (	JST	Japan Science and Technology$JUSTIFICATION$	0
c?2005 Association for Computational Linguistics A Machine Learning Approach to Acronym Generation Yoshimasa Tsuruoka     CREST JST Agency Japan Sophia Ananiadou School of Computing Salford University United Kingdom tsuruoka@is.s.u-tokyo.ac.jp S.Ananiadou@salford.ac.uk tsujii@is.s.u-tokyo.ac.jp Jun?ichi Tsujii    Department of Computer Science The University of Tokyo Japan Abstract This paper presents a machine learning approach to acronym generation.	JST	Japan Science and Technology$JUSTIFICATION$	0
CREST, JST Corporation #Department of Complexity Science and Engineering, University of Tokyo?	JST	Japan Science and Technology$JUSTIFICATION$	0
JST: Let me make one thing perfectly clear.	JST	Japan Science and Technology$JUSTIFICATION$	1
On the basis of the data derived from the corpus ,anal-  ysis, the algorithm hypothesizes the following set of re-  lations between the textual units:  rhet_rel(JST, 1 2) V  rhet..rel(CONDITION, 1,2)  rhet_rel(ELABORATION, 3, \[1,2\]) V  rhet_reI(ELABORATION, \[3, 6\], \[ 1,2\])  rhet_rel(El_ABOgATlON, \[4, 6\], 3) V  rhet_ret(ELABOr~YlON, \[4  6\], \[1, 3\])  rhet_rel(CONTRAST, 4, 5)  (4) rhet_rel(EVIDENCE, 6, 5)  rhet_reI(ELABORATION, \[7, 10\], \[1,6\])  rhet_rel(CONCESSION, 7, 8)  rhet_rel(EXAMPLE, 9 \[7, 8\]) V  rhet_rel(EXAMPLE, \[9, 10\],	JST	Japan Science and Technology$JUSTIFICATION$	1
This  hypothesis is consistent with the information given in Table 6, which shows that, in  the corpus, the marker With consistently signaled BACKGROUND and JST  relations between a satellite, the unit that contained the marker, and a nucleus, the  unit that followed it.	JST	Japan Science and Technology$JUSTIFICATION$	1
es 17 0 0 0 0 0 17 0 * incident-location 91 22 9 6 7 0 69 13 54 0 phys-effects 29 3 1 1 1 0 26 5 50 0 human-effects 28 1 0 0 0 1 28 0 0 10 0 MATCHED ONLY 382 284 110 20 55 99 197 31 42 3 5 MATCHED/MISSING 1117 284 110 20 55 99 932 11 42 3 5 ALL TEMPLATES 1117 665 110 20 55 480 932 11 18 7 2 SET FILLS ONLY 463 94 47 5 26 16 385 11 53 17 Table 3 .0 : 	 TST1-MUC3 rescored after Phase II developmen t JST AND ANALYSIS OF SCORE S Although the above stated scores seem rather discouraging or low, there are several valid justifications for suc h occurrences.	JST	Japan Science and Technology$JUSTIFICATION$	1
The original PDTB sense tags for meanwhile and as were respectively <COMPARISONCONTRASTJUXTAPOSITION> and <CONTINGENCYPRAGMATICCAUSE- JST>, where JUXTAPOSITION and JST were dropped because they stem from the third level of the PDTB sense hierarchy: 1.	JST	Japan Science and Technology$JUSTIFICATION$	1
Rule  when so-series demonst ra t ive  adverb   cataphor ica l ly  Refers  to  the  Verb  Phrase   in the  SS   Candidate numerating rule 10  When an anaphor is "sou/soushite/sonoyouni" and is  in the subordinate clause which has a conjunctive par-  ticle such as "9a", "daga ", and "keredo "or an adjective  conjunction such as "youni",  {(the main clause, 45)}  4 Heur i s t i c  Ru le  fo r  Persona l  P ronouns   Candidate numerating rule 1  When an anaphor is a first personal pronoun,  {(t	SS	SpellingSim$Same Sentence$structure$Sentence Splitting$Stanford parser style$semantic class$surface shape of the  sentence$subjects$Surface Structure$Subpath Set$	1
LiveJournal Blogspot JDPA ACE Tokens Per Sentence 19.2 18.6 16.5 19.7 Relations Per Sentence 1.08 1.71 2.56 0.56 Relations Not In SS 33% 30% 27% 0% Training Mention Pairs in One Sentence 58,452 54,480 95,630 77,572 Mentions Per Sentence 4.26 4.32 4.03 3.16 Mentions Per Entity 1.73 1.63 1.33 2.36 Mentions With Only One Token 77.3% 73.2% 61.2% 56.2% Table 2: Selected document statistics for three JDPA Corpus document sources.	SS	SpellingSim$Same Sentence$structure$Sentence Splitting$Stanford parser style$semantic class$surface shape of the  sentence$subjects$Surface Structure$Subpath Set$	1
0 conserve_VVIB~.T.~ \[nbarl \[nla space_NNIMEASURE nla\] nbarlJ v40\]  and_CCAND Iv40 reduce_WIALTER \[nbarl \[nla weight_NNIMEASURE nla\]  nbarl\] v40\] v41\] \[pl in_IIIN \[nbar12 \[jl new_JJTIME jl\] \[nla  cars_NN2DEVICE nla\] nbar12\] pl\] v2\] vibarl\] tl\] ilb\] nbarq4\] pl\] v4\]  vbar2\] sdl\] sprime2\] ._. sprpdl\] start\]  Figure 2: IBM/Lancaster Treebank and ATl~/Lancaster Parses For SS  rougbJy 3,000 lexica\] tags and about 1,100 d~erent non-terminal node labels, s?	SS	SpellingSim$Same Sentence$structure$Sentence Splitting$Stanford parser style$semantic class$surface shape of the  sentence$subjects$Surface Structure$Subpath Set$	1
SSs: EN-ES-P ?	SS	SpellingSim$Same Sentence$structure$Sentence Splitting$Stanford parser style$semantic class$surface shape of the  sentence$subjects$Surface Structure$Subpath Set$	1
SS/Paragraph 5 True if the two opinions are in the same sentence/paragraph.	SS	SpellingSim$Same Sentence$structure$Sentence Splitting$Stanford parser style$semantic class$surface shape of the  sentence$subjects$Surface Structure$Subpath Set$	1
SS Indicator  This feature is either 0 or 1 indicating whether an  anaphor and a candidate antecedent are in the  same sentence.	SS	SpellingSim$Same Sentence$structure$Sentence Splitting$Stanford parser style$semantic class$surface shape of the  sentence$subjects$Surface Structure$Subpath Set$	1
In the conditioning events, h is the  head constituent above p, l~ r is the list of candi-  date antecedents o be considered, t is the type  of phrase of the proposed antecedent (always  a noun-phrase in this study), I is the type of  the head constituent, sp describes the syntactic  SS in which p appears, dspecifies the dis-  tance of each antecedent from p and M" is the  number of times the referent is mentioned.	SS	SpellingSim$Same Sentence$structure$Sentence Splitting$Stanford parser style$semantic class$surface shape of the  sentence$subjects$Surface Structure$Subpath Set$	2
In clustering, objects are grouped together according to their feature value distribution, not to a predefined classification (as is the case when using supervised techniques), so that we achieve a better guarantee that we are learning a SS already present in the data.	SS	SpellingSim$Same Sentence$structure$Sentence Splitting$Stanford parser style$semantic class$surface shape of the  sentence$subjects$Surface Structure$Subpath Set$	2
A broad semantic classification like the one we pro- pose is a first step for characterising their meaning and argument SS.	SS	SpellingSim$Same Sentence$structure$Sentence Splitting$Stanford parser style$semantic class$surface shape of the  sentence$subjects$Surface Structure$Subpath Set$	2
We have not, however, been able to  duplicate exactly the syntactic SSs as-  sumed by Hobbs.	SS	SpellingSim$Same Sentence$structure$Sentence Splitting$Stanford parser style$semantic class$surface shape of the  sentence$subjects$Surface Structure$Subpath Set$	2
Toward an adequate taxonomy of personality attributes: Replicated factor SS in peer nomination personality rating.	SS	SpellingSim$Same Sentence$structure$Sentence Splitting$Stanford parser style$semantic class$surface shape of the  sentence$subjects$Surface Structure$Subpath Set$	2
The syntnctic SS st, and the distance  from the pronoun da are independent of the  number of times the referent is mentioned.	SS	SpellingSim$Same Sentence$structure$Sentence Splitting$Stanford parser style$semantic class$surface shape of the  sentence$subjects$Surface Structure$Subpath Set$	2
PESA: Phrase Pair Extraction as  SS.	SS	SpellingSim$Same Sentence$structure$Sentence Splitting$Stanford parser style$semantic class$surface shape of the  sentence$subjects$Surface Structure$Subpath Set$	3
7 FirstLevel Preprocessing Second Level Preprocessing Editors and Interfaces Models and Other Applications Higher Level Multilingual NLP Applications Text Language-Encoding Identification Encoding Converters Text Normalization SS Tokenization Morphological Analyzer Encoding Converter Generator Model of Scripts Spell Checker Model of Morphology Part Of Speech Tagger Other Specialized Interfaces Text Editor Annotation Interfaces Local Word Grouper or Chunker Figure 1: One view of the basic computational in- frastructure required for Natural Language Process- ing or Computational Linguistics.	SS	SpellingSim$Same Sentence$structure$Sentence Splitting$Stanford parser style$semantic class$surface shape of the  sentence$subjects$Surface Structure$Subpath Set$	3
SS (40 Operations).	SS	SpellingSim$Same Sentence$structure$Sentence Splitting$Stanford parser style$semantic class$surface shape of the  sentence$subjects$Surface Structure$Subpath Set$	3
79    Input SS and Translating  Takao Doi,  Eiichiro Sumita  ATR Spoken Language Translation Research Laboratories   2-2-2 Hikaridai, Kansai Science City, Kyoto, 619-0288 Japan  {takao.doi, eiichiro.sumita}@atr.co.jp         Abstract  We propose a method to split and translate  input sentences for speech translation in order  to overcome the long sentence problem.	SS	SpellingSim$Same Sentence$structure$Sentence Splitting$Stanford parser style$semantic class$surface shape of the  sentence$subjects$Surface Structure$Subpath Set$	3
1 http://lhncbc.nlm.nih.gov/project/consumer-health- question-answering 30 SS  Request  Question  Sentence  Ignore  Sentence  Background  Sentence  Candidate Generation  UMLS  SVM Candidate Ranking  Boundary Fixing  Focus  Focus Recognition  Sentence Classification  Background Classification  SVM Comorbidity Classification  SVM Diagnosis Classification  SVM Family History Classification  SVM ISF Classification  SVM Lifestyle Classification  SVM Symptom Cl	SS	SpellingSim$Same Sentence$structure$Sentence Splitting$Stanford parser style$semantic class$surface shape of the  sentence$subjects$Surface Structure$Subpath Set$	3
SS, Text Stemming and  Chunking: This module splits the context into sen- tences, then stems out the words and chunks those.	SS	SpellingSim$Same Sentence$structure$Sentence Splitting$Stanford parser style$semantic class$surface shape of the  sentence$subjects$Surface Structure$Subpath Set$	3
Our aim is to estab- lish SSes for adjectives in Catalan by means of clustering, using only shallow syntactic evidence.	SS	SpellingSim$Same Sentence$structure$Sentence Splitting$Stanford parser style$semantic class$surface shape of the  sentence$subjects$Surface Structure$Subpath Set$	5
2 Classification and Hypothesis As mentioned above, the SSification of adjectives is not settled in theoretical linguistics.	SS	SpellingSim$Same Sentence$structure$Sentence Splitting$Stanford parser style$semantic class$surface shape of the  sentence$subjects$Surface Structure$Subpath Set$	5
A broad SSification like the one we pro- pose is a first step for characterising their meaning and argument structure.	SS	SpellingSim$Same Sentence$structure$Sentence Splitting$Stanford parser style$semantic class$surface shape of the  sentence$subjects$Surface Structure$Subpath Set$	5
2.4 Morphology vs. syntax It could seem that the SSes established for the second parameter amount to morphological classes: not derived (basic adjectives), denominal (object adjectives), and deverbal (event adjectives).	SS	SpellingSim$Same Sentence$structure$Sentence Splitting$Stanford parser style$semantic class$surface shape of the  sentence$subjects$Surface Structure$Subpath Set$	5
Our hypothesis, which will be tested on Sec- tion 4.3, is that syntax is more reliable than mor- phology as a basis for SSification.	SS	SpellingSim$Same Sentence$structure$Sentence Splitting$Stanford parser style$semantic class$surface shape of the  sentence$subjects$Surface Structure$Subpath Set$	5
In recent research in the field, the main effort has been to infer SSes for verbs, in English (Stevenson et al, 1999) and German (Schulte im Walde and Brew, 2002).	SS	SpellingSim$Same Sentence$structure$Sentence Splitting$Stanford parser style$semantic class$surface shape of the  sentence$subjects$Surface Structure$Subpath Set$	5
Because it often takes the same nominative SS such as ?	SS	SpellingSim$Same Sentence$structure$Sentence Splitting$Stanford parser style$semantic class$surface shape of the  sentence$subjects$Surface Structure$Subpath Set$	7
5=no, 1=quickly) 2.35 3.35 +43% Table 5: Dierence in response with ten SS when viewing 1-agent and 2-agent versions of Mike Announcer interrupts expert Sorry, E-MIKE.	SS	SpellingSim$Same Sentence$structure$Sentence Splitting$Stanford parser style$semantic class$surface shape of the  sentence$subjects$Surface Structure$Subpath Set$	7
All these SS were familiar with the RoboCup domain and the Soccer Server en- vironment.	SS	SpellingSim$Same Sentence$structure$Sentence Splitting$Stanford parser style$semantic class$surface shape of the  sentence$subjects$Surface Structure$Subpath Set$	7
The system also con-  tains domain knowledge including the domain  concepts, specific list of SS and verbs, and  topic headings.	SS	SpellingSim$Same Sentence$structure$Sentence Splitting$Stanford parser style$semantic class$surface shape of the  sentence$subjects$Surface Structure$Subpath Set$	7
We first obtained from 10,000 to 50,000 unique  documents from the TREC 1, 2 and 3 volumes using the  Inquery search engine from UMass Amherst for each of  the following SS: art, business, education,  government, healthcare, movies, music, politics,  Number of  occurrences  Percentage  of entities  1 46.66  2 18.78  3 9.03  4 4.55  5 1.86  6 1.16  7 0.83  8 0.46  9 or more 16.67    Table 1: Breakdown of distribution by number of  occurrences within the Person X corpus.	SS	SpellingSim$Same Sentence$structure$Sentence Splitting$Stanford parser style$semantic class$surface shape of the  sentence$subjects$Surface Structure$Subpath Set$	7
5=no, 1=quickly) 3.97 Table 4: Average responses of 20 SS to rst questionnaire evaluation of (two-agent) Mike Question Scale 1-agent 2-agent Di Is the game better with or without...? (	SS	SpellingSim$Same Sentence$structure$Sentence Splitting$Stanford parser style$semantic class$surface shape of the  sentence$subjects$Surface Structure$Subpath Set$	7
SS and In- terpretation.	SS	SpellingSim$Same Sentence$structure$Sentence Splitting$Stanford parser style$semantic class$surface shape of the  sentence$subjects$Surface Structure$Subpath Set$	8
SS and Inter- pretation.	SS	SpellingSim$Same Sentence$structure$Sentence Splitting$Stanford parser style$semantic class$surface shape of the  sentence$subjects$Surface Structure$Subpath Set$	8
A relatively recent approach to this problem is to use  a set of rules like (61) which Kuroda (1976) calls  Canonical SS Filters and Miyagawa  (1980) calls Case Redundancy Rules.	SS	SpellingSim$Same Sentence$structure$Sentence Splitting$Stanford parser style$semantic class$surface shape of the  sentence$subjects$Surface Structure$Subpath Set$	8
\[Lesmo, Lombardo 91\] Lesmo L., Lombardo V., A  Dependency Syntax for the SS of  Sentences, Pro,:.	SS	SpellingSim$Same Sentence$structure$Sentence Splitting$Stanford parser style$semantic class$surface shape of the  sentence$subjects$Surface Structure$Subpath Set$	8
REFERENCES  Chomsky, N. (1971), Deep Structure,  SS and Semantic Inter-  wretation, in: Semantics (ed.	SS	SpellingSim$Same Sentence$structure$Sentence Splitting$Stanford parser style$semantic class$surface shape of the  sentence$subjects$Surface Structure$Subpath Set$	8
We propose more efficient al- gorithms approximating Tree Kernel: Tree Overlapping and SS.	SS	SpellingSim$Same Sentence$structure$Sentence Splitting$Stanford parser style$semantic class$surface shape of the  sentence$subjects$Surface Structure$Subpath Set$	9
4 SS 4.1 Definition of similarity SS similarity between two trees is de- fined as the number of subpaths shared by the trees.	SS	SpellingSim$Same Sentence$structure$Sentence Splitting$Stanford parser style$semantic class$surface shape of the  sentence$subjects$Surface Structure$Subpath Set$	9
The results of the experiments comparing these three algo- rithms showed that structural retrieval with Tree Overlapping and SS were faster than that with Tree Kernel by 100 times and 1,000 times respectively.	SS	SpellingSim$Same Sentence$structure$Sentence Splitting$Stanford parser style$semantic class$surface shape of the  sentence$subjects$Surface Structure$Subpath Set$	9
The experiments comparing these three algorithms showed that Tree Overlapping is 100 times faster and SS is 1,000 times faster than Tree Kernel when being used for struc- tural retrieval.	SS	SpellingSim$Same Sentence$structure$Sentence Splitting$Stanford parser style$semantic class$surface shape of the  sentence$subjects$Surface Structure$Subpath Set$	9
In this paper, we propose two efficient algo- 399 rithms to calculate similarity of syntactic struc- tures: Tree Overlapping and SS.	SS	SpellingSim$Same Sentence$structure$Sentence Splitting$Stanford parser style$semantic class$surface shape of the  sentence$subjects$Surface Structure$Subpath Set$	9
For syntactic parsing, Goodman (1996) proposed a vari- ational method for summing out spurious ambiguity that was equivalent to MBR decoding (Goel and Byrne, 2000; Kumar and Byrne, 2004) with a constituent-recall loss function.	MBR	minimum Bayes risk$Minimum Bayes Risk$	0
The book concludes with a discussion of MBR decod- ing, and a few other variants.	MBR	minimum Bayes risk$Minimum Bayes Risk$	0
7 A MBR decoding procedure to pick an output clustering.	MBR	minimum Bayes risk$Minimum Bayes Risk$	0
Minimum Bayes Retrieval Risk {4}: Calculates the translation probability for the translation having the MBR among the re- trieved training instances.	MBR	minimum Bayes risk$Minimum Bayes Risk$	0
Such rescoring is implemented using a MBR technique (Kumar and Byrne 2004; Tromble et al2008).	MBR	minimum Bayes risk$Minimum Bayes Risk$	0
Generalized MBR System  Combination.	MBR	minimum Bayes risk$Minimum Bayes Risk$	1
Modifying the multitask objective to incorpo- rate application-specific loss/decoding, such as MBR (Kumar and Byrne, 2004) ?	MBR	minimum Bayes risk$Minimum Bayes Risk$	1
An improved consensus-like method for MBR decoding and lattice combi- nation.	MBR	minimum Bayes risk$Minimum Bayes Risk$	1
de Gispert et al, 2009) adopted the MBR decoding strategy to combine output from identical SMT system, which is trained on alternative morphological decompositions of the source language.	MBR	minimum Bayes risk$Minimum Bayes Risk$	1
MBR  Decoding for Statistical Machine Translation.	MBR	minimum Bayes risk$Minimum Bayes Risk$	1
MBR Com- bination of Translation Hypotheses from Alterna- tive Morphological Decompositions.	MBR	minimum Bayes risk$Minimum Bayes Risk$	1
50  60  70  80  90  100  110  120  130  140  0  1  2  3  4 A v e r a g e   D F Etymology depth Average document frequency for words in the training data ENIT Figure 5: DF changes with the addition of etymological features The shape of the document frequency curves mirror the LSA results ?	DF	Document frequency$Dyadic Features$document frequency$	0
We therefore only used the Balanced Winnow algorithm for our classification experiments, which were run with the following LCS configuration, based on tuning experiments on the same data by Koster et al(2011):  Global term selection (GTS): DF minimum is 2, term frequency minimum is 3.	DF	Document frequency$Dyadic Features$document frequency$	0
DF (df) is borrowed for the information  * Institute of Information Sciences and Electronics, 1-1-1 Tennodai, Tsukuba 305-8573, Japan  t 180 Park Avenue, Florham Park, NJ 07932  Computational Linguistics Volume 27, Number 1  retrieval iterature (Sparck Jones 1972); it counts the number of documents that con-  tain a type at least once.	DF	Document frequency$Dyadic Features$document frequency$	0
(5) DF of bigrams in the sen- tence:  1 1( ) i i D i iw w S df w w + +?? .	DF	Document frequency$Dyadic Features$document frequency$	0
DF (df ) counts are derived from the definitions contained in all our resources.	DF	Document frequency$Dyadic Features$document frequency$	0
The first hall' is used to count document  frequency rl/: (DF will be used  instead of standard (term) frequency.)	DF	Document frequency$Dyadic Features$document frequency$	0
c?2013 Association for Computational Linguistics Automatic Prediction of Friendship via Multi-model DF  Zhou Yu, David Gerritsen, Amy Ogan, Alan W Black, Justine Cassell  School of Computer Science, Carnegie Mellon University  {zhouyu, dgerrits, aeo, awb, justine }@cs.cmu.edu    Abstract  In this paper we focus on modeling  friendships between humans as a way of  working towards technology that can initiate  and sustain a lifelong relationship with users.	DF	Document frequency$Dyadic Features$document frequency$	1
4.4 Re-estimating Bilingual Term Correspondences with Monolin- gual Web Documents For the 100 target English terms evaluated in the previous section, this section describes the result of applying the technique presented in Section 3.3, i.e., re-estimating bilingual term 8When the co-occurrence DF of t E and t J in the whole news articles is less than x, all the co-occurring dates are included.	DF	Document frequency$Dyadic Features$document frequency$	2
Each element of the vector )(ts  is an index term in the sentence, weighted by its text frequency (tf) and inverse DF (idf) where tf is defined as the frequency of the word in that particular sentence, and idf  is the inverse frequency of the word in the larger document collection N dflog?	DF	Document frequency$Dyadic Features$document frequency$	2
Figure 6: Inter-DF statistics 7 Conclusions and Future Work In this paper we have presented a statistical approach for the extraction of a lexicon which contains the verbs and nouns that can be considered as candidates for use as predicates for the induction of predicate/argument structures that we call messages.	DF	Document frequency$Dyadic Features$document frequency$	2
A first remark that we can make in respect to those graphs is that con- cerning the collection frequency, DF and tf.idf measures, for small threshold numbers we have more or less high precision values while the recall and fallout values are low.	DF	Document frequency$Dyadic Features$document frequency$	2
Since we represent word to- kens rather than word types in the cohesion graph, we do not need to model the term frequency tf separately, instead we set salience to the log value of the inverse DF idf : salience(t i ) = log |D| |{d : t i ?	DF	Document frequency$Dyadic Features$document frequency$	2
2 statistic estimation is re- stricted to certain portion of the whole news articles so that the following condition be satis- fied: i) co-occurrence DF of a target English term and its reference Japanese translation is fixed to be x,8 ii) the number of days be greater than or equal to y. For each news articles data set, Table 3 shows document frequencies df(tE) of a target English term tE , co-occurrence document frequencies df(tE, tJ ) of tE and its reference Japanese translation tJ , and the numbers of days for English as wel	DF	Document frequency$Dyadic Features$document frequency$	2
The following is the category structure that we need:  (27) a. F = {PRONOUN, CASE, PERSON, GENDER, NUMBER,  ANIMACY, PROXIMITY}  b. A = {question, personal, DEM, subjec-  tive, objective, reflexive, possessive, posses-  sive-determiner, first, second, third, feminine  masculine, neuter, singular, plural}  C. T O  d. p = {<PRONOUN, {question, personal,  DEM}),  <CASE, {subjective, objective, reflexive, posses-  sive, possessive-determiner}),  <PERSON, {first, second third}),  <GENDER, {feminine, masculine, neuter}),  <NUMBER, {singu	DEM	demonstrative$demonstratives$d~monstrat i f$	0
The following is the category structure that we need:  (27) a. F = {PRONOUN, CASE, PERSON, GENDER, NUMBER,  ANIMACY, PROXIMITY}  b. A = {question, personal, DEM, subjec-  tive, objective, reflexive, possessive, posses-  sive-determiner, first, second, third, feminine  masculine, neuter, singular, plural}  C. T O  d. p = {<PRONOUN, {question, personal,  DEM}),  <CASE, {subjective, objective, reflexive, posses-  sive, possessive-determiner}),  <PERSON, {first, second third}),  <GENDER, {feminine, masculine, neuter}),  <NUMBER, {singular, plural}),  <ANIMACY, 2>,  <PROXIMITY, 2>}  The constraints that must be imposed are the following:  (28) a. PRONOUN  b. (PRONOUN:question) ~ (CASE /~ -'-I PERSON /'k ---I  NUMBER /~ ANIMACY /~ 7 PROXIMIT	DEM	demonstrative$demonstratives$d~monstrat i f$	0
For a clue c, an associated Clues  feature d takes one of the four values, depending  on the way c appears in A and B. c' = 0 if c ap-  pears in neither A or B; d = 1 if c appears in both  A and B; d = 2 if c appears in A and not in B;  and d = 3 if c appears not in A but in B. We con-  sider clue expressions from the following grammat-  ical classes: nominals, adjectives, DEMs,  adverbs, sentence connectives, verbs, sentence-final  particles, topic-marking particles, and punctuation  marks.	DEM	demonstrative$demonstratives$d~monstrat i f$	0
The former is morphosyntactically very heterogeneous:   some pronouns inflect for gender (e.g., the DEM pronoun ten, the possessive pronoun m?j, but not the interrogative pro- noun kto or the negative pronoun nikt);   some pronouns, but not all, inflect for per- son;   some pronouns, but not all, inflect for num- ber;   the short reflexive pronoun sie does not overtly inflect at all, although it may be con- strued as a weak form of the anaphoric pro- noun siebie.	DEM	demonstrative$demonstratives$d~monstrat i f$	0
Lexical adjectives, including DEMs ad- verbs, numerals, and possessive adjectives, as well as ordinary intersective adjectives ?	DEM	demonstrative$demonstratives$d~monstrat i f$	0
<PERSON, {first, second third}),  <GENDER, {feminine, masculine, neuter}),  <NUMBER, {singular, plural}),  <ANIMACY, 2>,  <PROXIMITY, 2>}  The constraints that must be imposed are the following:  (28) a. PRONOUN  b. (PRONOUN:question) ~ (CASE /~ -'-I PERSON /'k ---I  NUMBER /~ ANIMACY /~ 7 PROXIMITY)  C. (PRONOUN:personaD <--> (CASE /~ PERSON /~ NUMBER  /k -q ANIMACY /~ "7 PROXIMITY)  d. (PRONOUN:DEM) <--) (7 CASE /~ 7 PERSON  /~ NUMBER /~ -3 ANIMACY /~ PROXIMITY)  e. GENDER ~ (PRONOUN A (PERSON:thirD A (NUMBER:  singular))  Note that this description of the pronominal system of  English is artificially complicated by its isolation from  the rest of the grammar.	DEM	demonstrative$demonstratives$d~monstrat i f$	0
Pronouns and DEM are ignored.	DEM	demonstrative$demonstratives$d~monstrat i f$	1
We compute a number of shallow features that provide a cheap way of capturing the above intu- itions: the number of DEM, pronouns, and definite descriptions as well as the number of sentence-initial discourse connectives.	DEM	demonstrative$demonstratives$d~monstrat i f$	1
For a clue c, an associated Clues  feature d takes one of the four values, depending  on the way c appears in A and B. c' = 0 if c ap-  pears in neither A or B; d = 1 if c appears in both  A and B; d = 2 if c appears in A and not in B;  and d = 3 if c appears not in A but in B. We con-  sider clue expressions from the following grammat-  ical classes: nominals, adjectives, DEM,  adverbs, sentence connectives, verbs, sentence-final  particles, topic-marking particles, and punctuation  marks.	DEM	demonstrative$demonstratives$d~monstrat i f$	1
Lexical adjectives, including DEM ad- verbs, numerals, and possessive adjectives, as well as ordinary intersective adjectives ?	DEM	demonstrative$demonstratives$d~monstrat i f$	1
For each of the cohesive devices discussed in Section 3.4?DEM, pronouns, definite descriptions, and sentence-initial discourse con- nectives?we compare the previous sentence in the summary with the previous sentence in the in- put article.	DEM	demonstrative$demonstratives$d~monstrat i f$	1
To distinguish between DEM and definite de- terminers, a gradation of givenness markers as sug- gested by Gundel et al (Gundel et al, 1989) is nec- essary: ?	DEM	demonstrative$demonstratives$d~monstrat i f$	1
le latin) seuls les d~-  finis anaphoriaue et DEM  existent, ce qui con-  firme le caract~re redondant du "d~fini g~n~rique".	DEM	demonstrative$demonstratives$d~monstrat i f$	2
RONNE-  MENT INTER-  PHRASTIQUE  le  ce  ENVIRONNE-  MENT  REFLEXIF  (le)  ENVIRONNE-  MENT  RELATIF  lequel  il - le~uel  gui  le se gue  lui se ~ qu i   auquel  quoi  son son dont  lui soi lequel  lui gui  guoi  Nous avons mis ainsi sous un m~me paradiqme  des morphemes grammaticaux qui appart iennent  aux Dara-  digmes tradit ionnels  suivants:  I- art ic le d~fini  II- adject i fs  et pronoms DEMs   II I- pronom personnel  (3e personne)  IV- adject i f  et pronom possess i f  (3e personne)  V- pronom r~f lexi f  (3e personne)  35 -  Remaruuons aue les noms personnels  {mo__~i, to__ii,  nous et vous) n 'admettent pas la pronominal isat ion cf.	DEM	demonstrative$demonstratives$d~monstrat i f$	2
XV CBNTEhTLbIL ANALIS I S   4.1 Basilq r(pproac'lr t o  C e n t o x t u a l  RHSlv.	RHS	rhs$right hand side$	0
TTT also supports constructive functions, with bound variables as arguments, in the RHS tem- plates, such as join-with-dash!,	RHS	rhs$right hand side$	0
When a match to the transduction lhs pattern oc- curs, the resulting bindings and transduction RHS are used to create a new tree, which then replaces the tree (or subtree) that matched the lhs.	RHS	rhs$right hand side$	0
However, variables that appear in both a rule?s lhs and RHS must occur at a depth less than two on the left, and Tiburon cannot easily simulate our verti- cal path or sequence operators.	RHS	rhs$right hand side$	0
Formally, the reordering rule is a triple {p, lhs,  RHS}, where p is the reordering probability, lhs is  the left hand side of the rule, i.e., the constituent  label sequence of a parse tree node, and RHS is the  reordering permutation derived either from hand- crafted rules as in (Collins et.	RHS	rhs$right hand side$	0
The RHS of such a transduction is allowed to reference the bindings of variables that appear in the enclos- ing pattern.	RHS	rhs$right hand side$	0
In anal--  ogy to a eontext-flee phrase structure rule, a DATR  sentence has a left hand side that consists of exactly  one non-terminal symbol (i.e. a node-path pair) and  a RHS that consists of an arbitrary num-  ber of non-terminal and terminal symbols (i.e. DATR  atoms).	RHS	rhs$right hand side$	1
RHSs of the sentences o\[ the DATR theory.	RHS	rhs$right hand side$	1
In the  case of an active edge, a rewriting rule in the grammar  and a position on the RHS of that rule are  provided, thus indicating what is still in order to com-  plete the recognition of the constituent.	RHS	rhs$right hand side$	1
Epenlhetic  material arises when syllabic slots which are not occultied  by segments are realised, l lerc the nlarks are given on the  RHS of the cohm in each pair.	RHS	rhs$right hand side$	1
C, there are specifications R and i, with 0  < i < n, where i is the position on the RHS of  rule R. A word in the string is itself represented as an  inactive edge connecting two adjoining vertices.	RHS	rhs$right hand side$	1
For example, (9b)  says that for any slot name o-, a constituent labelled {(o-,  mNc)} may have the immediate constituent analysis  seen on the RHS of the equation.	RHS	rhs$right hand side$	1
Thus the left  hand side of a formula (before the equality sign) consists  of an atomic label, and the RHS is a string of  tagmemes, which are ordered triples (a, b, c) where a is  an indication of optional (-+) or obligatory (+) status, b  is a slot or function name, and c is a filler or category  label.	RHS	rhs$right hand side$	1
d) AER with Soft-Union.	AER	Alignment Error Rate$alignment error rate$alignment error$	0
(f) AER with with the Best Strategy.	AER	Alignment Error Rate$alignment error rate$alignment error$	0
Firstly, the (bidirec- tional) reference alignments used in the computation of the AER were split into two sets of unidirectional alignments.	AER	Alignment Error Rate$alignment error rate$alignment error$	0
Figure 2: Precision/Recall Curve and AER with Different Models and Strategies.	AER	Alignment Error Rate$alignment error rate$alignment error$	0
Use of this technique dramati- cally reduces the AER  of the extracted corpora over heuristic  methods based on position of the sen- tences in the text.	AER	Alignment Error Rate$alignment error rate$alignment error$	0
10 Corpus English French English French English French English French 10K 47% 50% 61% 66% 74% 77% 84% 87% 25K 43% 44% 57% 59% 69% 72% 80% 83% 50K 42% 44% 55% 57% 67% 69% 78% 81% Table 2: AER by Model and Corpus Size Corpus Baseline Null SG Tags Tags+SG Tags+Null Tags+Null+SG 5K 17.53 16.86 16.72 16.20 15.31 15.36 15.14 15K 15.03 14.29 13.52 13.90 12.63 13.22 12.52 25K 13.85 13.05 12.79 13.10 11.91 12.30 11.79 35K 13.19 11.98 12.03 12.60 11.45 11.56 11.07 50K 12.63 11.76 11.78 12.10 11.19 11.11 10.69 ing size increases the Och model catches up with the Tags model a	AER	Alignment Error Rate$alignment error rate$alignment error$	0
In  section 5, we present word alignment results that  show significant AER reductions  compared to the baseline HMM and IBM model 4.	AER	Alignment Error Rate$alignment error rate$alignment error$	1
interp to reduce AER (Koehn, 2005) over a hand-aligned development set.	AER	Alignment Error Rate$alignment error rate$alignment error$	1
0,3 we trained several mod- els on a 10,000-sentence-pair subset of the French- English Hansards, and chose values that minimized the AER, as evaluated on a 447 sen- tence set of manually created alignments (Mihalcea and Pedersen, 2003).	AER	Alignment Error Rate$alignment error rate$alignment error$	1
Our most successful systems were based on classifier combination, and we found different combination methods performed best under the target evalua- tion metrics of F-measure and AER.	AER	Alignment Error Rate$alignment error rate$alignment error$	1
We can see that switching the search method from weighted maximum matching to a cohesion- constrained ITG (D-ITG) has produced a 34% rel- ative reduction in AER.	AER	Alignment Error Rate$alignment error rate$alignment error$	1
This constraint produces a sig- nificant reduction in AER.	AER	Alignment Error Rate$alignment error rate$alignment error$	1
it will be fine) JSL3 FINE / DREAM Alignment pairs that consist of many more or fewer sign words than Japanese words are dis- carded as AERs.	AER	Alignment Error Rate$alignment error rate$alignment error$	2
These spurious words cause signifi- cant word AERs (as shown with dash lines), which in turn directly affect the q	AER	Alignment Error Rate$alignment error rate$alignment error$	2
As some of the links are correctly aligned in the HMM and BM alignments (shown with solid lines), the combined alignment corrects some AERs while still contains com- mon incorrect alignment links.	AER	Alignment Error Rate$alignment error rate$alignment error$	2
In this paper, we regard the alignment pair as the AER when n sign > (N JP + ?)	AER	Alignment Error Rate$alignment error rate$alignment error$	2
These spurious words cause signifi- cant word AERs (as shown with dash lines), which in turn directly affect the quality of phrase translation tables or translation rules that are learned based on word alignment.	AER	Alignment Error Rate$alignment error rate$alignment error$	2
and (Ittycheriah and Roukos, 2005)), which also introduce many word AERs.	AER	Alignment Error Rate$alignment error rate$alignment error$	2
In this paper we introduce a confidence mea- sure for word alignment, which is robust to extra or missing words in the bilingual sentence pairs, as well as word AERs.	AER	Alignment Error Rate$alignment error rate$alignment error$	2
This relatively small improve- ment is mainly due to the selection of the whole sentence alignment: for many sentences the best alignment still contains AERs, some of which could be fixed by other aligners.	AER	Alignment Error Rate$alignment error rate$alignment error$	2
In addition, it may include  MNR  of motion and the inclusion of a COMITATIVE  phrase (i.e. accompaniment by an ob- ject/individual in the GO or COME event).	MNR	MANNER$manner$	0
TENSE PRESENT  ASPECT SIMPLE MORPH_ASP/  MOOD  IMPERFEC- TIVE  SUBJ_NUM SINGULAR SUBJ_PER 3RD  SUBJ_GEN FEM SUBJ_CAT GROUP  INTEROG NO NEGATION NO  SVC NO PP YES  LOC_ADV NO ADVERBIAL YES  GOAL NO SOURCE NO  MNR YES SETTING YES  PATH NO PURPOSIVE NO  COMITATIVE NO TEMPORAL NO  DEGREE NO      4 Statistical analyses  A wide range of statistical tests can be ap- plied in order to explore the data frames de- scribed above for various purposes.	MNR	MANNER$manner$	0
PERFECTIVE,  SUBJUNCTIVE, JUSSIVE, IMPER- ATIVE  SUBJECT PERSON 1ST, 2ND, 3RD   SUBJECT NUMBER SINGULAR, DUAL, PLURAL  SUBJECT GENDER FEMININE, MASCULINE, NIL (for  1st person inflections)                                                    1 The data frame was, in fact, coded for more variables than  the set laid out in Table 4, such as the different  morphosyntactic realizations of GOAL, SOURCE, MNR,  etc.,	MNR	MANNER$manner$	0
B  PHRASE  YES, NO  ADVERBIAL PHRASE YES, NO  SERIAL VERB CON- STRUCTION  YES, NO    Semantic variables    Levels   SUBJECT CATEGORY ACTIVITY, ANIMAL, ATTRIB- UTE, BODY, COGNITION, COM- MUNICATION, CONTENT (of a  document/speech), DEMON- STRATIVE, DUMMY SUBJECT,  EVENT, GROUP, HUMAN, LOCA- TION, NOTION, OB- JECT/ARTIFACT, SENSE, STATE,  SUBSTANCE, TIME  GOAL PHRASE YES, NO  SOURCE PHRASE YES, NO  MNR PHRASE YES, NO  SETTING PHRASE YES, NO  PATH PHRASE YES, NO  PURPOSIVE PHRASE YES, NO  COMITATIVE PHRASE YES, NO  TEMPORAL PHRASE YES, NO  DEGREE PHRASE YES, NO    Table 1.	MNR	MANNER$manner$	0
a = 0.247  qadima = 0.416  contextual features used (in  the model):  TESNE.PAST + ASPECT.SIMPLE +  SUBJ_PER.3RD +  SUBJ_CAT.HUMAN + PP.YES +  LOC.ADV.YES + MNR.YES +  COMITATIVE.YES     ????? ?? ???? ???? ????? ??? ??? ??? ???	MNR	MANNER$manner$	0
Michael decided to go to the beach), we can add an  adjunct of type MNR to it (Michael quickly  decided to go to the beach) but we cannot add an adjunct  of type PROPERTY (*Michael important(ly) decided  to go to the beach) 6.	MNR	MANNER$manner$	0
Section 3 presents the Posterior Regularization (PR) framework and describes how to encode such constraints in an efficient MNR, requiring only repeated inference in the original model to enforce the constraints.	MNR	MANNER$manner$	1
Unfortunately, the selection of paradigm words rarely receives sufficient attention and is typically done in an ad hoc MNR.	MNR	MANNER$manner$	1
Since in queries, quantifiers rarely interact with alternative phrases in the MNR discussed in Hoeksema and von Fintel?s work, their analyses have not been carried over into the present work.	MNR	MANNER$manner$	1
However, a variety of  pronouns indicate the same class: Plural pro-  pronoun gender class  he,himself, him,his HE  she,herself, her,hers SHE  it,itself, its IT  nouns like "they" and "us" reveal no gender in-  formation about their referent and consequently  aren't useful, although this might be a way to  learn pluralization in an unsupervised MNR.	MNR	MANNER$manner$	1
7.2 Domain-specific sub-corpora         The person-x corpus may appear to be biased due to  the MNR of its construction.	MNR	MANNER$manner$	1
Lexical adverbs, including MNR, time, and loca- tion, and adverbs of negation, which vary by clause type (declarative, imperative, or interrogative) ?	MNR	MANNER$manner$	1
In attempting to  b.ootstraPLexical information about referents'  gender, we consider two strategies, both com-  pletely blind to any kind of semantics.	PL	p l$Probability Loss$Phrase Levels$	0
I f  the master   i s  p resent ,  the master -s lave  re la t ion  need not to  be express -   ed syntact i ca l l y :   "Peter  ~ave the so.._nn an apPLe . "	PL	p l$Probability Loss$Phrase Levels$	0
When NL text  has been t ransformed in to  a se t  of   p roper ty  l i s t s ,  fe tched  from the  vocabu lar ies  and augmented by  morpholog ica l  preeoan,  there  are many ways to o rder  the apPL i c -   a t ion  of  the re levant  GSRs, fo r ,  wh i le  each GSR i s  descr ibed   procedura l ly ,  the descr ip t ion  as a whole i s  dec la ra t ive .	PL	p l$Probability Loss$Phrase Levels$	0
S ign i f i cant  words are devided in to  c lasses  depending on  the  ro le  they  PLay  in  n~ing  cor responding not ions .	PL	p l$Probability Loss$Phrase Levels$	0
a l l  i n fo rmat ion  from  supernot ion  i s  re levant  to ac tua l  not ion ,  i f  i t  i sn ' t  exPL i c i t -   l y  euperoeded.	PL	p l$Probability Loss$Phrase Levels$	0
- 52  -  Indeed,  un l i ke  a t t r ibutes  of  c lass  A, Subetant iona l   A t t r ibutes  may be used wi thout  exPL i c i t  re fe rence  to i t s   master :  e ,g ,  " the  t ra in  goes to the cap i te?" .	PL	p l$Probability Loss$Phrase Levels$	0
Before the flood season comes , it is necessary to seize the time to formulate plans for forecasting floods and to carry out work with clear  	 Figure 1: Chinese/English Parallel Corpus Aligned at the Sentence, Word, and PL: horizontal lines denote the segmentations of a sentence alignment and arrows denote a word-level mapping.	PL	p l$Probability Loss$Phrase Levels$	2
In fact, it also pat-  terns closely with SPATIAL-SEQUENCE; such simplifications of semantic diversity are found in several areas,  as where the semantic relations PW, PLAN-STEP, ABSTRACT-INSTANCE all pattern with rhetori-  cal ELABORATION.	PW	PART-WHOLE$pw$	0
Most existing approaches target either a single relation, e.g., PW (Girju et al.,	PW	PART-WHOLE$pw$	0
Also assueiated with each entry is an  image-seheme (Lakoff 1987) such as the COrCI'AIN~R Schema, or the  PW Schema.	PW	PART-WHOLE$pw$	0
Not all the relations of RST are rhetorical  - -  for example JUSTIFY and MOTIVATE are clearly intentional, and CAUSE and PW are clearly  semantic.	PW	PART-WHOLE$pw$	0
There are propositions that differentiate  these two relations; thus, combinations in (a),  with PW relation are possible with  a preposition U, while combinations in (b),  with a SET-ELEMENT relation, are not:  a. nozka 'leg' U stula 'chair'  pugovica 'button'  U paljto ~coat'  b. *chaschka 'cup' U serviza 'service'  *korova 'cow' U stada 'herd'  ACTES DE COL	PW	PART-WHOLE$pw$	0
These include relations such as CAUSE, PW, IS-A, TEMPORAL-SEQUENCEp  SPATIAL-SEQUENCE.	PW	PART-WHOLE$pw$	0
As was stated earlier, domains  II.1, II.2 define the following relations:  1) PW;  2) SET-ELEMENT.	PW	PART-WHOLE$pw$	0
3.1 Similarity measure  Punctuation and uninformative words are removed  from each sentence using a simple regular expression  pattern mateher and a stoPWord list.	PW	PART-WHOLE$pw$	1
We sus-  pect this is due to the use of a different stoPWord list  and stemming algorithm.	PW	PART-WHOLE$pw$	1
a) a new constituent is started by projecting a  complete rule uPWards; (b) the constituent then takes left and right modifiers (or none if it is unary). (	PW	PART-WHOLE$pw$	1
At the start of the conversation, the belief (probability) that the connection is working p(allOk) is 56% and the belief that the power to the DSL modem is on p(PWrOn) is 98.0% (these are 2 of the 19 compo- nents in the product state x).	PW	PART-WHOLE$pw$	1
Practi- cally, the effect may be that some local rules will have variants propagating uPWards a certain va- lency: ?? %	PW	PART-WHOLE$pw$	1
ID p(allOk) p(PWrOn) Transcript *S1 56% 98.0% I?m going to try pinging your DSL modem from here.	PW	PART-WHOLE$pw$	1
In  (Conolly et al, 1994) a decision tree is trained on  a small number of 15 features concerning anaphor  type, GF, recency, morphosyntac-  tic agreement and subsuming concepts.	GF	grammatical function$Grammatical Framework$Grammatical Function$	0
References    Baker, M. C. (1988) Incorporation, A Theory of  GF changing.	GF	grammatical function$Grammatical Framework$Grammatical Function$	0
Our evaluation results indicate that the basic performance of the parser trained with the treebank almost equals bunsetsus-based parsers and has the potential to supply detailed syntactic information by GF labels for se- mantic analysis, such as predicate-argument struc- ture analysis.	GF	grammatical function$Grammatical Framework$Grammatical Function$	0
Our goal is to construct a practical constituent parser that can deal with appropriate grammatical units and output GFs as semi- semantic information, e.g., grammatical or seman- tic roles of arguments and gapping types of relative clauses.	GF	grammatical function$Grammatical Framework$Grammatical Function$	0
Most of the Korean corpus was built using grammat- ical chunks eojeols, which resemble Japanese bun- setsus and consist of content words and morphemes that represent GFs.	GF	grammatical function$Grammatical Framework$Grammatical Function$	0
Base contains only ba- sic tags, not GF tags.	GF	grammatical function$Grammatical Framework$Grammatical Function$	0
Expressivity and Complexity of the GF.	GF	grammatical function$Grammatical Framework$Grammatical Function$	1
c?2007 Association for Computational Linguistics Converting GF to Regulus Peter Ljungl?f Department of Linguistics G?teborg University Gothenburg, Sweden peb@ling.gu.se Abstract We present an algorithm for converting GF grammars (Ranta, 2004) into the Regulus unification-based framework (Rayner et al, 2006).	GF	grammatical function$Grammatical Framework$Grammatical Function$	1
Speech recognition grammar com- pilation in GF.	GF	grammatical function$Grammatical Framework$Grammatical Function$	1
Compared to GF, the Regu- lus formalism is quite restricted.	GF	grammatical function$Grammatical Framework$Grammatical Function$	1
Since GF is more ex- pressive than Regulus,	GF	grammatical function$Grammatical Framework$Grammatical Function$	1
The Expressivity and Complexity of GF.	GF	grammatical function$Grammatical Framework$Grammatical Function$	1
1.1 GF GF (Ranta, 2004) is a gram- mar formalism based on type theory.	GF	grammatical function$Grammatical Framework$Grammatical Function$	1
In each FrameNet sentence, a single target  predicate is identified and all of its relevant Frame  Elements are tagged with their element-type (e.g.,  Agent, Judge), their syntactic Phrase Type (e.g., NP,  PP), and their GF (e.g., External  Argument, Object Argument).	GF	grammatical function$Grammatical Framework$Grammatical Function$	2
We classify an adjective as object-biased if the mean of the judgments for the object interpretation of this particular adjective is larger than the mean for the subject interpretation; subject-biased adjec- tives are classified accordingly, whereas adjectives for which no effect of GF is found are classified as equi-biased.	GF	grammatical function$Grammatical Framework$Grammatical Function$	2
GFs and  Verb Subcategorization in Madarin Chinese.	GF	grammatical function$Grammatical Framework$Grammatical Function$	2
3.4.1 GF Label Evaluation Ku?bler et al (2006) present the results shown in Ta- ble 3 for the parsing performance of the unlexical- ized model of the Stanford Parser (Klein and Man- ning, 2002).	GF	grammatical function$Grammatical Framework$Grammatical Function$	2
The bi- ases and the significance of the GF effect (p) are shown in Table 6.	GF	grammatical function$Grammatical Framework$Grammatical Function$	2
3 Parsing with GFs 3.1 Model As explained above, this paper focuses on unlexi- calized grammars.	GF	grammatical function$Grammatical Framework$Grammatical Function$	2
TREC-9?,	TREC-9	The Ninth Text Retrieval Conference$Eighth Text REtrieval Conference$	0
Proceedings of the TREC-9, pp.	TREC-9	The Ninth Text Retrieval Conference$Eighth Text REtrieval Conference$	1
In Proceedings of the TREC-9 (TREC-8), pages 83?106, Gaithersburg, MD.	TREC-9	The Ninth Text Retrieval Conference$Eighth Text REtrieval Conference$	1
In Proceedings of The TREC-9 (TREC-8), http://trec.	TREC-9	The Ninth Text Retrieval Conference$Eighth Text REtrieval Conference$	1
In Proceedings of the TREC-9 (TREC-8).	TREC-9	The Ninth Text Retrieval Conference$Eighth Text REtrieval Conference$	1
Proceedings of the TREC-9, November, 1999.	TREC-9	The Ninth Text Retrieval Conference$Eighth Text REtrieval Conference$	1
In the TREC-9 (TREC-8).	TREC-9	The Ninth Text Retrieval Conference$Eighth Text REtrieval Conference$	1
In the area of lexicon acquisition, many  researchers have employed public KBs  such as WordNet in IE systems.	KB	knowledge base$knowledge bases$knowledge-based$Knowledge Base$	0
We will look at  ways in which Constrained Conditional Models can be used to augment  probabilistic models with declarative KBd constraints and how these  support expressive global decisions.	KB	knowledge base$knowledge bases$knowledge-based$Knowledge Base$	0
Methods based on manually built lexical KBs, such as WordNet, compute the shortest path be- tween two concepts in the KB and/or look at word overlap in the glosses (see Budan- itsky and Hirst (2006) for an overview).	KB	knowledge base$knowledge bases$knowledge-based$Knowledge Base$	0
Our approach demonstrates the  use of two human built KBs  (WordNet and FrameNet) for the task of  semantic extraction.	KB	knowledge base$knowledge bases$knowledge-based$Knowledge Base$	0
2.2 A Thesaurus -based  Approach   Morris and Hirst \[1991\] used Roget's thesaurus as  KB for determining whether or not two  words are semantically related.	KB	knowledge base$knowledge bases$knowledge-based$Knowledge Base$	0
The ongoing development of public  KBs such as WordNet, FrameNet, CYC,  etc.	KB	knowledge base$knowledge bases$knowledge-based$Knowledge Base$	0
In the area of lexicon acquisition, many  researchers have employed public KB  such as WordNet in IE systems.	KB	knowledge base$knowledge bases$knowledge-based$Knowledge Base$	1
Computing power has increased since the first such systems were developed, and the general methodology has changed from the use of hand-encoded KB about simple domains to the use of text collections as the main knowledge source over more complex domains.	KB	knowledge base$knowledge bases$knowledge-based$Knowledge Base$	1
MOSES (Basili et al 2004) is an ontology-based QA system in which users pose questions in natural language to KB of facts extracted from a federation of Web sites and organized in topic map repositories.	KB	knowledge base$knowledge bases$knowledge-based$Knowledge Base$	1
Methods based on manually built lexical KB, such as WordNet, compute the shortest path be- tween two concepts in the knowledge base and/or look at word overlap in the glosses (see Budan- itsky and Hirst (2006) for an overview).	KB	knowledge base$knowledge bases$knowledge-based$Knowledge Base$	1
Our approach demonstrates the  use of two human built KB  (WordNet and FrameNet) for the task of  semantic extraction.	KB	knowledge base$knowledge bases$knowledge-based$Knowledge Base$	1
The ongoing development of public  KB such as WordNet, FrameNet, CYC,  etc.	KB	knowledge base$knowledge bases$knowledge-based$Knowledge Base$	1
Introduction to the Articles in this Special Section Demner-Fushman and Lin?s article (Answering clinical questions with KB and statistical techniques) extends previous work by the authors (Demner-Fushman and Lin 2005) on a QA system in the medical domain.	KB	knowledge base$knowledge bases$knowledge-based$Knowledge Base$	2
These structures (with sorne enhancements) are to  provide the basis for KB parsing.	KB	knowledge base$knowledge bases$knowledge-based$Knowledge Base$	2
In fact, as a not- too-long-term vision, we are convinced that research in restricted domains will drive the convergence between structured KB and free text-based question answering.	KB	knowledge base$knowledge bases$knowledge-based$Knowledge Base$	2
Whereas structured KB QA systems are well adapted to applications managing complex queries in a very structured information environment, the kind of research developed in TREC, CLEF, and NTCIR is probably better suited to broad-purpose generic applications dealing with simple factual ques- tions such as World Wide Web?based ques	KB	knowledge base$knowledge bases$knowledge-based$Knowledge Base$	2
Whereas structured KB QA systems are well adapted to applications managing complex queries in a very structured information environment, the kind of research developed in TREC, CLEF, and NTCIR is probably better suited to broad-purpose generic applications dealing with simple factual ques- tions such as World Wide Web?based question answering.	KB	knowledge base$knowledge bases$knowledge-based$Knowledge Base$	2
Both trends have developed in parallel and represent the opposite ends of a spec- trum connecting what we might label as structured KB and free text- based question answering.	KB	knowledge base$knowledge bases$knowledge-based$Knowledge Base$	2
Whereas KB ystems like (Carbonell  and Brown, 1988) and (Rich and LuperFoy, 1988)  combining multiple resolution strategies are expen-  sive in the cost of human effort at development time  and limited ability to scale to new domains, more re-  cent knowledge-poor approaches like (Kennedy and  Boguraev, 1996) and (Mitkov, 1998) address the  problem without sophisticated linguistic knowledge.	KB	knowledge base$knowledge bases$knowledge-based$Knowledge Base$	2
616  Bui ld ing  KBs  for the Generat ion  of   Sof tware Documentat ion  *  C4cile Paristand Keith Vander  L inden  :I  ITRI, University of Brighton  Lewes Road  Brighton BN2 4AT, UK  {clp,knvl}~itri.brighton.ac.uk  Abst rac t   Automated text generation requires a  underlying knowledge base fl'om which  to generate, which is often difficult to  produce.	KB	knowledge base$knowledge bases$knowledge-based$Knowledge Base$	3
ANGUAGE  SYSTEM  I lubcrt l.chnmnn  IBM l)cutschhmd Gmbl l ,  Scientific ('enter  institute for KBd Systems  Wilckensstr.	KB	knowledge base$knowledge bases$knowledge-based$Knowledge Base$	3
es  Abstract  This paper explores the automatic onstruction  of a multilingual Lexical KB from  pre-existing lexical resources.	KB	knowledge base$knowledge bases$knowledge-based$Knowledge Base$	3
c?2015 Association for Computational Linguistics Semantic Parsing via Staged Query Graph Generation: Question Answering with KB Wen-tau Yih Ming-Wei Chang Xiaodong He Jianfeng Gao Microsoft Research Redmond, WA 98052, USA {scottyih,minchang,xiaohe,jfgao}@microsoft.com Abstract We propose a novel semantic parsing framework for question answering using a knowledge base.	KB	knowledge base$knowledge bases$knowledge-based$Knowledge Base$	3
LREC 2002 Workshop on Ontologies and  Lexical KBs (2002)  22.	KB	knowledge base$knowledge bases$knowledge-based$Knowledge Base$	3
Representational Interoperability of Linguistic and Collaborative KBs.	KB	knowledge base$knowledge bases$knowledge-based$Knowledge Base$	3
The unLPs and recalls of the pre- vious model and models 1, 2, and 3 were signifi- cantly different as measured using stratified shuf- fling tests (Cohen, 1995) with p-values < 0.05.	LP	labeled precision$linear programming$Linear Precedence$lip protrusion$label propagation$linear precedence$Linear Program$Linear Preeedenc$linear precedenc$Labeled precision$Labeled Precision$labeled$less privileged$Libertarian Party}$	0
Parsing performance is measured by f-score, f = 2?P?RP+R , where P, R are LP and recall.	LP	labeled precision$linear programming$Linear Precedence$lip protrusion$label propagation$linear precedence$Linear Program$Linear Preeedenc$linear precedenc$Labeled precision$Labeled Precision$labeled$less privileged$Libertarian Party}$	0
The columns LP, recall, f-score and accuracy represent aggregates over sentences 1 . . .	LP	labeled precision$linear programming$Linear Precedence$lip protrusion$label propagation$linear precedence$Linear Program$Linear Preeedenc$linear precedenc$Labeled precision$Labeled Precision$labeled$less privileged$Libertarian Party}$	0
F1<40 is the F-Measure combining LP and labeled recall for sentences of less than 40 words.	LP	labeled precision$linear programming$Linear Precedence$lip protrusion$label propagation$linear precedence$Linear Program$Linear Preeedenc$linear precedenc$Labeled precision$Labeled Precision$labeled$less privileged$Libertarian Party}$	0
6.4 Evaluation metrics We evaluate parsing performance using labeled F- Measure (combining LP and labeled 80 DEV SET TERMINAL SYMBOLS F1<40 F1 UAS Tagging Acc.	LP	labeled precision$linear programming$Linear Precedence$lip protrusion$label propagation$linear precedence$Linear Program$Linear Preeedenc$linear precedenc$Labeled precision$Labeled Precision$labeled$less privileged$Libertarian Party}$	0
The LPs and recalls were signifi- cantly different among models 1, 2, and 3 and between the previous model and model 3, but were not significantly different between the previ- ous model and mod	LP	labeled precision$linear programming$Linear Precedence$lip protrusion$label propagation$linear precedence$Linear Program$Linear Preeedenc$linear precedenc$Labeled precision$Labeled Precision$labeled$less privileged$Libertarian Party}$	0
The LPs and recalls were signifi- cantly different among models 1, 2, and 3 and between the previous model and model 3, but were not significantly different between the previ- ous model and model 1 and between the previous model and model 2.	LP	labeled precision$linear programming$Linear Precedence$lip protrusion$label propagation$linear precedence$Linear Program$Linear Preeedenc$linear precedenc$Labeled precision$Labeled Precision$labeled$less privileged$Libertarian Party}$	0
On the other hand, inference in PSL reduces to a LP problem, which is theoretically and practically much more efficient.	LP	labeled precision$linear programming$Linear Precedence$lip protrusion$label propagation$linear precedence$Linear Program$Linear Preeedenc$linear precedenc$Labeled precision$Labeled Precision$labeled$less privileged$Libertarian Party}$	1
Multi-lingual dependency parsing with incremental integer LP.	LP	labeled precision$linear programming$Linear Precedence$lip protrusion$label propagation$linear precedence$Linear Program$Linear Preeedenc$linear precedenc$Labeled precision$Labeled Precision$labeled$less privileged$Libertarian Party}$	1
A LP formulation for global inference in natural language tasks.	LP	labeled precision$linear programming$Linear Precedence$lip protrusion$label propagation$linear precedence$Linear Program$Linear Preeedenc$linear precedenc$Labeled precision$Labeled Precision$labeled$less privileged$Libertarian Party}$	1
Grammatical error correction using integer LP.	LP	labeled precision$linear programming$Linear Precedence$lip protrusion$label propagation$linear precedence$Linear Program$Linear Preeedenc$linear precedenc$Labeled precision$Labeled Precision$labeled$less privileged$Libertarian Party}$	1
Thadani and McKeown (2011) substituted MANLI?s simulated annealing-based decoding with integer LP, and achieved a consider- able speed-up.	LP	labeled precision$linear programming$Linear Precedence$lip protrusion$label propagation$linear precedence$Linear Program$Linear Preeedenc$linear precedenc$Labeled precision$Labeled Precision$labeled$less privileged$Libertarian Party}$	1
i=1 score(wi, h, l i)ewi,li subject to C(s) Constraints In the {0, 1} LP for- mulation described above, we can encode linguis- tic constraints that reflect the interactions among the linguistic phenomena.	LP	labeled precision$linear programming$Linear Precedence$lip protrusion$label propagation$linear precedence$Linear Program$Linear Preeedenc$linear precedenc$Labeled precision$Labeled Precision$labeled$less privileged$Libertarian Party}$	1
NOTES  This work was supported by SITRA Foundation (Finnish National  Fund for Research and Development), Helsinki, Finland  Current address:  SITRA Foundation  P.O. Box 329  SF-00121 Helsinki, Finland  272 Computational Linguistics, Volume 12, Number 4, October-December 1986   Topological Dependency Trees: A Constraint-Based Account of LP Denys Duchier Programming Systems Lab Universita?t des Saarlandes, Geb.	LP	labeled precision$linear programming$Linear Precedence$lip protrusion$label propagation$linear precedence$Linear Program$Linear Preeedenc$linear precedenc$Labeled precision$Labeled Precision$labeled$less privileged$Libertarian Party}$	2
Duchier D. & R. Debusmann (2001) "Topological  Dependency Trees: A Constraint-Based Account of  LP'', in proceedings of  ACL.	LP	labeled precision$linear programming$Linear Precedence$lip protrusion$label propagation$linear precedence$Linear Program$Linear Preeedenc$linear precedenc$Labeled precision$Labeled Precision$labeled$less privileged$Libertarian Party}$	2
4.4 Underspecification of LP  In our proposed tree description language, we provide for underspecified dominance  but not for underspecified linear precedence.	LP	labeled precision$linear programming$Linear Precedence$lip protrusion$label propagation$linear precedence$Linear Program$Linear Preeedenc$linear precedenc$Labeled precision$Labeled Precision$labeled$less privileged$Libertarian Party}$	2
2.4 LP Constraints The elaboration above has assumed the absence of any linear precedence constraints.	LP	labeled precision$linear programming$Linear Precedence$lip protrusion$label propagation$linear precedence$Linear Program$Linear Preeedenc$linear precedenc$Labeled precision$Labeled Precision$labeled$less privileged$Libertarian Party}$	2
LP in D&eontinuous  Constituents."	LP	labeled precision$linear programming$Linear Precedence$lip protrusion$label propagation$linear precedence$Linear Program$Linear Preeedenc$linear precedenc$Labeled precision$Labeled Precision$labeled$less privileged$Libertarian Party}$	2
de- pendency graph, where each dimension (e.g. Im- mediate Dominance and LP) is as- sociated with its own set of well-formedness con- ditions (called principles).	LP	labeled precision$linear programming$Linear Precedence$lip protrusion$label propagation$linear precedence$Linear Program$Linear Preeedenc$linear precedenc$Labeled precision$Labeled Precision$labeled$less privileged$Libertarian Party}$	2
For bootstrapping purposes, however, it is appropriate for the resulting score vector f c to depend on the seed vector y. 3.2 Laplacian LP To make f seed dependent, Komachi et al(2008) noted that we should use a power series of a ma- trix rather than a simple power of a matrix.	LP	labeled precision$linear programming$Linear Precedence$lip protrusion$label propagation$linear precedence$Linear Program$Linear Preeedenc$linear precedenc$Labeled precision$Labeled Precision$labeled$less privileged$Libertarian Party}$	4
Komachi et al(2008) shows that simple bootstrapping algorithms can be inter- preted as LP on graphs (Komachi et al 2008).	LP	labeled precision$linear programming$Linear Precedence$lip protrusion$label propagation$linear precedence$Linear Program$Linear Preeedenc$linear precedenc$Labeled precision$Labeled Precision$labeled$less privileged$Libertarian Party}$	4
irst reduced bootstrapping al- gorithms to LP using Komachi et al 44 (2008)?s theorization.	LP	labeled precision$linear programming$Linear Precedence$lip protrusion$label propagation$linear precedence$Linear Program$Linear Preeedenc$linear precedenc$Labeled precision$Labeled Precision$labeled$less privileged$Libertarian Party}$	4
This accords with the fact that many papers such as (Talukdar and Pereira, 2010; Kozareva et al 2011) suggest that graph-based semi-supervised learning, or LP, is another effective met	LP	labeled precision$linear programming$Linear Precedence$lip protrusion$label propagation$linear precedence$Linear Program$Linear Preeedenc$linear precedenc$Labeled precision$Labeled Precision$labeled$less privileged$Libertarian Party}$	4
To this end, we first reduced bootstrapping al- gorithms to LP using Komachi et al 44 (2008)?s theorization.	LP	labeled precision$linear programming$Linear Precedence$lip protrusion$label propagation$linear precedence$Linear Program$Linear Preeedenc$linear precedenc$Labeled precision$Labeled Precision$labeled$less privileged$Libertarian Party}$	4
Zhou et al (2008) integrate the advantages of  SVM bootstrapping in learning critical instances  and LP in capturing the manifold  structure in both the labeled and unlabeled data,  by first bootstrapping a moderate number of  weighted support vectors through a co-training  procedure from all the available data, and then  applying LP algorithm via the  bootstrapped support vectors.	LP	labeled precision$linear programming$Linear Precedence$lip protrusion$label propagation$linear precedence$Linear Program$Linear Preeedenc$linear precedenc$Labeled precision$Labeled Precision$labeled$less privileged$Libertarian Party}$	4
This accords with the fact that many papers such as (Talukdar and Pereira, 2010; Kozareva et al 2011) suggest that graph-based semi-supervised learning, or LP, is another effective method for this harvesting task.	LP	labeled precision$linear programming$Linear Precedence$lip protrusion$label propagation$linear precedence$Linear Program$Linear Preeedenc$linear precedenc$Labeled precision$Labeled Precision$labeled$less privileged$Libertarian Party}$	4
13 Other solutions are possible that do not require extended erivations or LP constraints.	LP	labeled precision$linear programming$Linear Precedence$lip protrusion$label propagation$linear precedence$Linear Program$Linear Preeedenc$linear precedenc$Labeled precision$Labeled Precision$labeled$less privileged$Libertarian Party}$	5
SMTNs  can impose generalized LP on labeled  substrings.	LP	labeled precision$linear programming$Linear Precedence$lip protrusion$label propagation$linear precedence$Linear Program$Linear Preeedenc$linear precedenc$Labeled precision$Labeled Precision$labeled$less privileged$Libertarian Party}$	5
It is obtained in  compiling LP, requirement and  exclusion properties described in the previous  sections together with, indirectly, that of  constituency.	LP	labeled precision$linear programming$Linear Precedence$lip protrusion$label propagation$linear precedence$Linear Program$Linear Preeedenc$linear precedenc$Labeled precision$Labeled Precision$labeled$less privileged$Libertarian Party}$	5
Topo- logical dependency trees: A constraint-based ac- count of LP.	LP	labeled precision$linear programming$Linear Precedence$lip protrusion$label propagation$linear precedence$Linear Program$Linear Preeedenc$linear precedenc$Labeled precision$Labeled Precision$labeled$less privileged$Libertarian Party}$	5
b. imposing restrictions on the spaces of search for  pieces to be linked together, possibly taking into  account general criteria of LP.	LP	labeled precision$linear programming$Linear Precedence$lip protrusion$label propagation$linear precedence$Linear Program$Linear Preeedenc$linear precedenc$Labeled precision$Labeled Precision$labeled$less privileged$Libertarian Party}$	5
FERGUS consists of three models: tree chooser, unraveler, and LP chooser.	LP	labeled precision$linear programming$Linear Precedence$lip protrusion$label propagation$linear precedence$Linear Program$Linear Preeedenc$linear precedenc$Labeled precision$Labeled Precision$labeled$less privileged$Libertarian Party}$	5
POLY2 is implemented in Java and it uses lp-solve software (Berkelaar, 1999) in order to perform LPming.	LP	labeled precision$linear programming$Linear Precedence$lip protrusion$label propagation$linear precedence$Linear Program$Linear Preeedenc$linear precedenc$Labeled precision$Labeled Precision$labeled$less privileged$Libertarian Party}$	6
Bergsma and Kondrak (2007b) present a method for identifying sets of cognates across groups of languages using the global inference framework of Integer LPming.	LP	labeled precision$linear programming$Linear Precedence$lip protrusion$label propagation$linear precedence$Linear Program$Linear Preeedenc$linear precedenc$Labeled precision$Labeled Precision$labeled$less privileged$Libertarian Party}$	6
LPming.	LP	labeled precision$linear programming$Linear Precedence$lip protrusion$label propagation$linear precedence$Linear Program$Linear Preeedenc$linear precedenc$Labeled precision$Labeled Precision$labeled$less privileged$Libertarian Party}$	6
In Proceedings of the Workshop on In- teger LPming for Natural Langauge Pro- cessing, pages 1?9.	LP	labeled precision$linear programming$Linear Precedence$lip protrusion$label propagation$linear precedence$Linear Program$Linear Preeedenc$linear precedenc$Labeled precision$Labeled Precision$labeled$less privileged$Libertarian Party}$	6
We will also mention various possibilities for  performing the inference, from commercial Integer LPming  packages to search techniques to Lagrangian relaxation approximation methods.	LP	labeled precision$linear programming$Linear Precedence$lip protrusion$label propagation$linear precedence$Linear Program$Linear Preeedenc$linear precedenc$Labeled precision$Labeled Precision$labeled$less privileged$Libertarian Party}$	6
InProceedings of the  Workshop on Integer LPming for  Natural Langauge Processing, pp.	LP	labeled precision$linear programming$Linear Precedence$lip protrusion$label propagation$linear precedence$Linear Program$Linear Preeedenc$linear precedenc$Labeled precision$Labeled Precision$labeled$less privileged$Libertarian Party}$	6
13 Other solutions are possible that do not require extended erivations or LPe constraints.	LP	labeled precision$linear programming$Linear Precedence$lip protrusion$label propagation$linear precedence$Linear Program$Linear Preeedenc$linear precedenc$Labeled precision$Labeled Precision$labeled$less privileged$Libertarian Party}$	8
SMTNs  can impose generalized LPe on labeled  substrings.	LP	labeled precision$linear programming$Linear Precedence$lip protrusion$label propagation$linear precedence$Linear Program$Linear Preeedenc$linear precedenc$Labeled precision$Labeled Precision$labeled$less privileged$Libertarian Party}$	8
It is obtained in  compiling LPe, requirement and  exclusion properties described in the previous  sections together with, indirectly, that of  constituency.	LP	labeled precision$linear programming$Linear Precedence$lip protrusion$label propagation$linear precedence$Linear Program$Linear Preeedenc$linear precedenc$Labeled precision$Labeled Precision$labeled$less privileged$Libertarian Party}$	8
Topo- logical dependency trees: A constraint-based ac- count of LPe.	LP	labeled precision$linear programming$Linear Precedence$lip protrusion$label propagation$linear precedence$Linear Program$Linear Preeedenc$linear precedenc$Labeled precision$Labeled Precision$labeled$less privileged$Libertarian Party}$	8
b. imposing restrictions on the spaces of search for  pieces to be linked together, possibly taking into  account general criteria of LPe.	LP	labeled precision$linear programming$Linear Precedence$lip protrusion$label propagation$linear precedence$Linear Program$Linear Preeedenc$linear precedenc$Labeled precision$Labeled Precision$labeled$less privileged$Libertarian Party}$	8
FERGUS consists of three models: tree chooser, unraveler, and LPe chooser.	LP	labeled precision$linear programming$Linear Precedence$lip protrusion$label propagation$linear precedence$Linear Program$Linear Preeedenc$linear precedenc$Labeled precision$Labeled Precision$labeled$less privileged$Libertarian Party}$	8
LP reflects the number of cor-  rectly labeled constituents identified by the rhetorical parser with respect o the total  number of labeled constituents identified by the parser.	LP	labeled precision$linear programming$Linear Precedence$lip protrusion$label propagation$linear precedence$Linear Program$Linear Preeedenc$linear precedenc$Labeled precision$Labeled Precision$labeled$less privileged$Libertarian Party}$	9
"Metric  'Precision  Recall  LP  Labeled recall  Tagging accuracy  Number of crossing brackets J  Operations  Operation sequence  Syntax Semantics  -0.63 -0.63  -0.64 -0.66  -0.75 -0.78  -0.65 -0.65  -0.66 -0.56  0.58 0.54  -0.45 -0.41  -0.39 -0.36  Table 5: Correla	LP	labeled precision$linear programming$Linear Precedence$lip protrusion$label propagation$linear precedence$Linear Program$Linear Preeedenc$linear precedenc$Labeled precision$Labeled Precision$labeled$less privileged$Libertarian Party}$	9
LP has  the strongest correlation with both the syntactic and  semantic translation evaluation grades.	LP	labeled precision$linear programming$Linear Precedence$lip protrusion$label propagation$linear precedence$Linear Program$Linear Preeedenc$linear precedenc$Labeled precision$Labeled Precision$labeled$less privileged$Libertarian Party}$	9
Segmentation/ Regular Simulating  Tagging seg/tag as perfect  ( "seg/tag" ) implemented seg/tag  LP  Labeled recall  Tagging accuracy  Crossings/sentence  0 crossings  < 2 crossings  Structure&Label  86.9%  85 .O%  94.2%  1.63  42.9%  74.2%  16.0%  93.4%  92.9%  100.0%  1.13  48.5%  85.3%  28.8%  Table 2: Impact of segmentation/tagging errors  in the process of becoming available for Korean, will  greatly improve parsing accuracy.	LP	labeled precision$linear programming$Linear Precedence$lip protrusion$label propagation$linear precedence$Linear Program$Linear Preeedenc$linear precedenc$Labeled precision$Labeled Precision$labeled$less privileged$Libertarian Party}$	9
We use the PARSEVAL measures (Black et al 1991) to compare performance: LP = number of correct constituents in proposed parse number of constituents in proposed parse Labeled recall = number of correct constituents in proposed parse number of constituents in treebank parse Crossing brackets = number of constituents that violate constituent boundaries with a constituent in the treebank parse For a constituent to be ?	LP	labeled precision$linear programming$Linear Precedence$lip protrusion$label propagation$linear precedence$Linear Program$Linear Preeedenc$linear precedenc$Labeled precision$Labeled Precision$labeled$less privileged$Libertarian Party}$	9
8 Czech English Unlabeled precision 99.09 96.03 Unlabeled recall 94.81 93.07 Unlabeled F-1 96.90 94.53 LP 78.38 81.58 Labeled recall 74.99 79.06 Labeled F-1 76.65 80.30 Frame selection accuracy 79.10 84.95 Ambiguous verbs baseline 66.68 68.44 classifier 72.41 80.03 Table 1: Experimental results of errors in the Czech evaluation data were caused just by idioms or light verb constructions not be- ing recognized by our system.	LP	labeled precision$linear programming$Linear Precedence$lip protrusion$label propagation$linear precedence$Linear Program$Linear Preeedenc$linear precedenc$Labeled precision$Labeled Precision$labeled$less privileged$Libertarian Party}$	9
However, we decided not to do so,  because many training sentences were also used for  feature set and background knowledge development  121  Training sentences 32 64 128 256 512 1024  Precision  Recall  LP  Labeled recall  Tagging accuracy  Crossings/sentence  0 crossings  < 1 crossing  < 2 crossings  < 3 crossings  < 4 crossings  Correct operations  Operation Sequence  Structure&Label  88.6%  87.3%  84.1%  81.2%  94.3%  1.97  27.6%  56.4%  70.6%  81.0%  88.3%  63.0%  2.5%  5.5%  88.1%  87.4%  83.9%  81.9%  92.9%  2.00  35.0%  58.9%  72.4%  81.6%  84.0%  68.3%  6.1%  12.9%  90.0%	LP	labeled precision$linear programming$Linear Precedence$lip protrusion$label propagation$linear precedence$Linear Program$Linear Preeedenc$linear precedenc$Labeled precision$Labeled Precision$labeled$less privileged$Libertarian Party}$	9
Negra Tu?Ba-D/Z Unlabeled Precision 78.69 89.92 Unlabeled Recall 82.29 86.48 LP 64.08 75.36 Labeled Recall 67.01 72.47 Coverage 97.00 99.90 Table 2: PARSEVAL Evaluation The parser trained on Tu?Ba-D/Z performs much better than the one trained on Negra on all labeled and unlabeled bracketing scores.	LP	labeled precision$linear programming$Linear Precedence$lip protrusion$label propagation$linear precedence$Linear Program$Linear Preeedenc$linear precedenc$Labeled precision$Labeled Precision$labeled$less privileged$Libertarian Party}$	10
German English  Exact Match (w/o TT) 46,3% 55,4%  hlcorrect parses 50,3% 39,3%  Not parsed 3,4% 5,3%  Exact Match (after 77) 53,8% 61,2%  Incorrect parses (after TT) 42,8% 33,5%  LP (w/o 7T) 90,2% 90,6%  German English  LP (after TT) 90,8% 91,4%  Labeled Recall (all 83,5% 78,5%  utterances, w/o TT)  Labeled Recall (all 84,0% 79,2%  utterances, after TT)  Labeled Recall (parsed 91,0% 90,9%  utterances, w/o TT)  Labeled Recall (parsed 91,6% 91,7%  utterances, after TT)  6 Conclusion  In this article we have extended probabilistic shift-  reduce	LP	labeled precision$linear programming$Linear Precedence$lip protrusion$label propagation$linear precedence$Linear Program$Linear Preeedenc$linear precedenc$Labeled precision$Labeled Precision$labeled$less privileged$Libertarian Party}$	10
German English  Exact Match (w/o TT) 46,3% 55,4%  hlcorrect parses 50,3% 39,3%  Not parsed 3,4% 5,3%  Exact Match (after 77) 53,8% 61,2%  Incorrect parses (after TT) 42,8% 33,5%  LP (w/o 7T) 90,2% 90,6%  German English  LP (after TT) 90,8% 91,4%  Labeled Recall (all 83,5% 78,5%  utterances, w/o TT)  Labeled Recall (all 84,0% 79,2%  utterances, after TT)  Labeled Recall (parsed 91,0% 90,9%  utterances, w/o TT)  Labeled Recall (parsed 91,6% 91,7%  utterances, after TT)  6 Conclusion  In this article we have extended probabilistic shift-  reduce parsing to be more context-sensitive than  previous work	LP	labeled precision$linear programming$Linear Precedence$lip protrusion$label propagation$linear precedence$Linear Program$Linear Preeedenc$linear precedenc$Labeled precision$Labeled Precision$labeled$less privileged$Libertarian Party}$	10
4http://bllip.cs.brown.edu/biomedical/ 103 0 25000 50000 75000 100000 125000 150000 175000 200000 225000 250000 275000 Number of sentences added 80.0 80.2 80.4 80.6 80.8 81.0 81.2 81.4 81.6 81.8 82.0 82.2 82.4 82.6 82.8 83.0 83.2 83.4 83.6 83.8 84.0 84.2 84.4 R e r a n k i n g   p a r s e r   f - s c o r e WSJ+Medline WSJ+BioBooks WSJ+NANC WSJ (baseline) Figure 2: LP-Recall results on development data for four versions of the parser as a function of number of self-training sentences References Michiel Bacchiani, Michael Riley, Brian Roark, and Richard Sproat.	LP	labeled precision$linear programming$Linear Precedence$lip protrusion$label propagation$linear precedence$Linear Program$Linear Preeedenc$linear precedenc$Labeled precision$Labeled Precision$labeled$less privileged$Libertarian Party}$	10
The results of this  ewtluation are given in the following table:  7)'aining set/trees\]  Test set \[utterances\]  GelTilall  19.750  1.000  English  17.793  1.000  Eract Match 46,3% 55,4%  Incorrect parses 50,3% 39,3%  Not pmwed 3,4% 5,3%  contextj'ree rules 988 2.205  LP 90,2% 90,6%  Labeled Recall (all 83,5% 78,5%  utterances)  Labeled Recall  (parsed utterances) 91,0% 90,9%  Japan.	LP	labeled precision$linear programming$Linear Precedence$lip protrusion$label propagation$linear precedence$Linear Program$Linear Preeedenc$linear precedenc$Labeled precision$Labeled Precision$labeled$less privileged$Libertarian Party}$	10
i, A, j?|A spans from i to j} represent the test/gold parses, respectively, and we calculate:8 LP = #(G ?	LP	labeled precision$linear programming$Linear Precedence$lip protrusion$label propagation$linear precedence$Linear Program$Linear Preeedenc$linear precedenc$Labeled precision$Labeled Precision$labeled$less privileged$Libertarian Party}$	10
Reevaluation was conducted on the LP test set released by the shared task organizers after our system?s output had been initially evaluated.	LP	labeled precision$linear programming$Linear Precedence$lip protrusion$label propagation$linear precedence$Linear Program$Linear Preeedenc$linear precedenc$Labeled precision$Labeled Precision$labeled$less privileged$Libertarian Party}$	11
We make use of partial syntac- tic information together with features ob- tained from the unLP corpus, and con- vert the task into one of sequential BIO- tagging.	LP	labeled precision$linear programming$Linear Precedence$lip protrusion$label propagation$linear precedence$Linear Program$Linear Preeedenc$linear precedenc$Labeled precision$Labeled Precision$labeled$less privileged$Libertarian Party}$	11
For each stem, we acquire a histogram of sur- rounding words, with a window size of 3, from the unLP corpus.	LP	labeled precision$linear programming$Linear Precedence$lip protrusion$label propagation$linear precedence$Linear Program$Linear Preeedenc$linear precedenc$Labeled precision$Labeled Precision$labeled$less privileged$Libertarian Party}$	11
Similarly to previous work in hedge cue detec- tion (Morante and Daelemans, 2009), we first con- vert the task into a sequential labeling task based on the BIO scheme, where each word in a hedge cue is LP as B-CUE, I-CUE, or O, indicating respectively the LP word is at the beginning of a cue, inside of a cue, or outside of a hedge cue; this is similar to the tagging scheme from the CoNLL-2001 shared task.	LP	labeled precision$linear programming$Linear Precedence$lip protrusion$label propagation$linear precedence$Linear Program$Linear Preeedenc$linear precedenc$Labeled precision$Labeled Precision$labeled$less privileged$Libertarian Party}$	11
For each stem, we acquired the distribu- tion of surrounding words from the unLP cor- pus, and calculated the similarity between these distributions and the distribution of hedge cues in the training corpus.	LP	labeled precision$linear programming$Linear Precedence$lip protrusion$label propagation$linear precedence$Linear Program$Linear Preeedenc$linear precedenc$Labeled precision$Labeled Precision$labeled$less privileged$Libertarian Party}$	11
The problem is especially acute for LP languages, but even in the case of English, we are aware of many category types entirely missing from the Penn Treebank (Clark et al, 2004).	LP	labeled precision$linear programming$Linear Precedence$lip protrusion$label propagation$linear precedence$Linear Program$Linear Preeedenc$linear precedenc$Labeled precision$Labeled Precision$labeled$less privileged$Libertarian Party}$	12
Kin- yarwanda is a LP language characterised by lack of electronic resources and insignificant presence on the Internet.	LP	labeled precision$linear programming$Linear Precedence$lip protrusion$label propagation$linear precedence$Linear Program$Linear Preeedenc$linear precedenc$Labeled precision$Labeled Precision$labeled$less privileged$Libertarian Party}$	12
We know that any HLT project related to  a LP language should follow those  guidelines, but from our experience we know that  in most cases they do not.	LP	labeled precision$linear programming$Linear Precedence$lip protrusion$label propagation$linear precedence$Linear Program$Linear Preeedenc$linear precedenc$Labeled precision$Labeled Precision$labeled$less privileged$Libertarian Party}$	12
As  Gujarati is currently a LP lan- guage in the sense of being resource poor,  manually tagged data is only around 600  sentences.	LP	labeled precision$linear programming$Linear Precedence$lip protrusion$label propagation$linear precedence$Linear Program$Linear Preeedenc$linear precedenc$Labeled precision$Labeled Precision$labeled$less privileged$Libertarian Party}$	12
i.alegria@ehu.es        Abstract  We present some Language Technology  applications that have proven to be effec- tive tools to promote the use of Basque, a  European LP language.	LP	labeled precision$linear programming$Linear Precedence$lip protrusion$label propagation$linear precedence$Linear Program$Linear Preeedenc$linear precedenc$Labeled precision$Labeled Precision$labeled$less privileged$Libertarian Party}$	12
Abstract Sinhala, spoken in Sri Lanka as an official language, is one of the LP lan- guages; still there are no established text in- put methods.	LP	labeled precision$linear programming$Linear Precedence$lip protrusion$label propagation$linear precedence$Linear Program$Linear Preeedenc$linear precedenc$Labeled precision$Labeled Precision$labeled$less privileged$Libertarian Party}$	12
Each feature is weighted according to a set of hand-crafted or machine-learned weights over 165 the DevSet.	DevSet	development dataset$Development Set$	0
3http://www-tsujii.is.s.u-tokyo.ac.jp/GENIA/home/wiki.cgi Feature Set R P F1 Baseline (current word) 44.82 2.86 05.38 + POS & char 3-gram 77.41 27.96 41.09 + previous POS tag 79.77 29.32 42.88 + lexicon (final tagger) 80.44 29.65 43.33 Table 1: Recall (R), precision (P), and F1-measure for the trigger words tagger (in %s) on the DevSet for different feature sets using MIRA training with false negatives as a loss function.	DevSet	development dataset$Development Set$	0
4 Experimental Setup 4.1 Datasets For our DevSet, we used a subset of the reddit irony corpus (Wallace et al, 2014) com- prising annotated comments from the progressive and conservative subreddits.	DevSet	development dataset$Development Set$	0
The DevSet includes 1,825 anno- tated comments (876 and 949 from the progressive and conservative subreddits, respectively).	DevSet	development dataset$Development Set$	0
` 1 ` 2 ) +0.035; +0.034 (+0.000, +0.062) +0.001; +0.000 (-0.011, +0.011) Table 2: Summary results over 500 random train/test splits of the DevSet.	DevSet	development dataset$Development Set$	0
= 5 is tuned by grid search using the DevSet.	DevSet	development dataset$Development Set$	0
.01/72.48 73.56/73.79 73.44/73.61 (RR-PCFG) 65.86/66.86 71.84/72.76 74.06/74.28 75.13/75.29 BaseDef (PCFG) 67.68/68.86 71.17/72.47 74.13/74.39 72.54/72.79 (RR-PCFG) 66.65/67.86 73.09/74.13 74.59/74.59 76.05/76.34 BaseDefAcc (PCFG) 68.11/69.30 71.50/72.75 74.16/ 74.41 72.77/73.01 (RR-PCFG) 67.13/68.01 73.63/74.69 74.65/74.79 76.15/ 76.43 Table 1: Parsing Results for Sentences of Length < 40 in the DevSet: Averaged F-Measure With/Without Punctuation.	DevSet	development dataset$Development Set$	1
339 DevSet (KBP) KBP MPQA Positive Negative Positive Negative Positive Negative P R F1 P R F1 P R F1 P R F1 P R F1 P R F1 KM Gold 90.9 2.5 4.8 93.8 8.6 15.8 93.9 4.3 8.3 93.5 6.6 12.4 61.5 1.3 2.5 90.0 5.2 9.8 Random 16.6 13.1 14.7 4.9 4.0 4.4 13.3 12.7 13.0 10.1 6.9 8.2 10.9 15.4 12.8 8.9 6.7 7.7 Sentence 60.0 16.3 25.7 21.7 43.1 28.8 40.9 20.6 27.4 21.0 31.4 25.2 18.9 3.7 6.2 16.7 18.2 17.4	DevSet	development dataset$Development Set$	1
no Table 4: Illustrative examples from the RTE3 test suite RTE3 DevSet (800 problems) System % yes precision recall accuracy Core +coref 50.25 68.66 66.99 67.25 Core -coref 49.88 66.42 64.32 64.88 NatLog 18.00 76.39 26.70 58.00 Hybrid, bal.	DevSet	development dataset$Development Set$	1
of Evidence Instances Training Set 677 DevSet 178 Test Set 183 Table 1: Evidence per data set The evidence instances were obtained from the corpus developed by Molla-Aliod and Santiago- Martinez (2011).	DevSet	development dataset$Development Set$	1
In the DevSetup, we use ABST as the training corpus and FULL as the de- velopment corpus.	DevSet	development dataset$Development Set$	1
Training Set Set Positive Negative Neutral Total Train 3,640 (37%) 1,458 (15%) 4,586 (48%) 9,684 DevSet Set Positive Negative Neutral Total Dev 575 (35%) 340(20%) 739 (45%) 1,654 Testing Sets Set Positive Negative Neutral Total LiveJournal 427 (37%) 304 (27%) 411 (36%) 1,142 SMS2013 492 (23%) 394(19%) 1,207 (58%) 2,093 Twitter2013 1,572 (41%) 601 (16%) 1,640 (43%) 3,813 Twitter2014 982 (53%) 202 (11%) 669 (36%) 1,853 Twitter2014Sar 33 (38%) 40 (47%) 13 (15%) 86 Table 4: Class distrib	DevSet	development dataset$Development Set$	1
c?2009 Association for Computational Linguistics Automatic Agenda Graph Construction from Human-Human Dialogs  using Clustering Method    Cheongjae Lee, Sangkeun Jung, Kyungduk Kim, Gary Geunbae Lee  Department of Computer Science and Engineering  POSTECH  Pohang, South Korea  {lcj80,hugman,getta,gblee}@postech.ac.kr         Abstract  Various knowledge sources are used for spo- ken dialog systems such as task model, do- main model, and agenda.	POSTECH	Pohang University of Science and Technology$Pohang University of Science & Technology$Pohang University of Computer Science and Technolog$	0
of Electrical and  Computer Engineering24, POSTECH  Advanced Information Technology Research Center(Altrc) {meixunj1,colorful2,jhlee4}@  postech.ac.kr  Language Engineering Institute3 Div.	POSTECH	Pohang University of Science and Technology$Pohang University of Science & Technology$Pohang University of Computer Science and Technolog$	0
c?2008 Association for Computational Linguistics Robust Dialog Management with N-best Hypotheses Using Dialog Examples and Agenda Cheongjae Lee, Sangkeun Jung and Gary Geunbae Lee POSTECH Department of Computer Science and Engineering Pohang, Republic of Korea {lcj80,hugman,gblee}@postech.ac.kr Abstract This work presents an agenda-based approach to improve the robustness of the dialog man- ager by using dialog examples and n-best recognition hypotheses.	POSTECH	Pohang University of Science and Technology$Pohang University of Science & Technology$Pohang University of Computer Science and Technolog$	0
939  Heuristic Methods for Reducing Errors of Geographic Named Entities Learned by Bootstrapping Seungwoo Lee and Gary Geunbae Lee Department of Computer Science and Engineering, POSTECH, San 31, Hyoja-dong, Nam-gu, Pohang, 790-784, Republic of Korea Abstract.	POSTECH	Pohang University of Science and Technology$Pohang University of Science & Technology$Pohang University of Computer Science and Technolog$	0
c?2013 Association for Computational Linguistics Enriching Entity Translation Discovery using Selective Temporality Gae-won You, Young-rok Cha, Jinhan Kim, and Seung-won Hwang POSTECH, Republic of Korea {gwyou, line0930, wlsgks08, swhwang}@postech.edu Abstract This paper studies named entity trans- lation and proposes ?	POSTECH	Pohang University of Science and Technology$Pohang University of Science & Technology$Pohang University of Computer Science and Technolog$	0
c?2009 Association for Computational Linguistics A Local Tree Alignment-based Soft Pattern Matching Approach for Information Extraction Seokhwan Kim, Minwoo Jeong, and Gary Geunbae Lee Department of Computer Science and Engineering POSTECH San 31, Hyoja-dong, Nam-gu, Pohang, 790-784, Korea {megaup, stardust, gblee}@postech.ac.kr Abstract This paper presents a new soft pattern match- ing method which aims to improve the recall with minimized precision loss in information extraction tasks.	POSTECH	Pohang University of Science and Technology$Pohang University of Science & Technology$Pohang University of Computer Science and Technolog$	0
POSTECH, Pohang, Korea ?	POSTECH	Pohang University of Science and Technology$Pohang University of Science & Technology$Pohang University of Computer Science and Technolog$	1
In short, I am making a plea for making the  speci f icat ion language used for theory  development in natural language  understanding be a communicat ion language  intended and engineered for human  139   Dec is ion -Tree  based Error  Cor rec t ion  for Stat is t ica l  Phrase  Break   Pred ic t ion  in Korean  *  Byeongchang K im and Geunbae Lee  Del)artment of Computer  Science & Engineering  POSTECH  Pohang, 790-784, South Korea  {bckim, gblee}((~postech.ac.kr  Abst ract   tn this paper, we present a new 1)hrase break  prediction architecture that integrates proba-  bilistic apt)roach with decision-tree based error  correction.	POSTECH	Pohang University of Science and Technology$Pohang University of Science & Technology$Pohang University of Computer Science and Technolog$	1
60  St ructura l  d i sambiguat ion  of  morpho-syntact i c  categor ia l  pars ing  for Korean  *  Jeongwon Cha and Geunbae Lee  Department of Computer Science & Engineering  POSTECH  Pohang, Korea  {himen, gblee}@postech.ac.kr  Abstract  The Korean Combinatory Categorial Grammar  (KCCG) tbrmalism can unitbrmly handle word  order variation among arguments and adjuncts  within a clause as well as in complex clauses  and across clause boundaries, i.e., long distance  scrambling.	POSTECH	Pohang University of Science and Technology$Pohang University of Science & Technology$Pohang University of Computer Science and Technolog$	1
of Computer Science and Engineering  POSTECH  San 31, Hyoja-Dong, Pohang, 790-784, Korea  {leeck,gblee }@postech.ac.kr  Seo JungYun  Natural Language Processing Lab  Dept.	POSTECH	Pohang University of Science and Technology$Pohang University of Science & Technology$Pohang University of Computer Science and Technolog$	1
1361  Unlimited Vocabulary Grapheme to Phoneme Conversion for  Korean TTS  Byeongchang Kim and WonI1 Lee and Geunbae Lee and Jong-Hyeok  Lee  Department of Computer Science & Engineering  POSTECH  Pohang, Korea  {bckim, bdragon, gblee, jhlee)@postech.ac.kr  Abst ract   This paper describes a grapheme-to-phoneme  conversion method using phoneme connectivity  and CCV conversion rules.	POSTECH	Pohang University of Science and Technology$Pohang University of Science & Technology$Pohang University of Computer Science and Technolog$	1
Similarly, Asher (1993) argues that a simple ontology of eventualities is too coarse-grained, and that DR need to distinguish different kinds of abstract objects, including actions, propositions, and facts as well as eventualities.	DR	discourse representations$Discourse Relations$DivRank$Direct Reversal$	0
Some features contain first-order formulas like  the conditions in DR.	DR	discourse representations$Discourse Relations$DivRank$Direct Reversal$	0
As an additional contri- bution, we have presented a novel family of met- rics which operate at the semantic level by analyz- ing DR.	DR	discourse representations$Discourse Relations$DivRank$Direct Reversal$	0
Since segmentation is the first stage of discourse parsing, quality discourse segments are critical to build- ing quality DR (Soricut and Marcu, 2003).	DR	discourse representations$Discourse Relations$DivRank$Direct Reversal$	0
1 Introduction Our proposal is based on a rich set of individual metrics operating at different linguistic levels: lex- ical (i.e., on word forms), shallow-syntactic (e.g., on word lemmas, part-of-speech tags, and base phrase chunks), syntactic (e.g., on dependency and con- stituency trees), shallow-semantic (e.g., on named entities and semantic roles), and semantic (e.g., on DR).	DR	discourse representations$Discourse Relations$DivRank$Direct Reversal$	0
The important point  for our present purposes is that the representa-  tion of the meaning of the S is built up from the  DR of the subject and the  predicate.	DR	discourse representations$Discourse Relations$DivRank$Direct Reversal$	0
In Workshop on DR, pages 114?120.	DR	discourse representations$Discourse Relations$DivRank$Direct Reversal$	1
Posters and Demonstrations, pages 87?90 Manchester, August 2008 Easily Identifiable DR Emily Pitler, Mridhula Raghupathy, Hena Mehta, Ani Nenkova, Alan Lee, Aravind Joshi University of Pennsylvania 3330 Walnut Street Philadelphia, PA 19104 Abstract We present a corpus study of local dis- course relations based on the Penn Dis- course Tree Bank, a large manually anno- tated corpus of explicitly or implicitly re- alized relations.	DR	discourse representations$Discourse Relations$DivRank$Direct Reversal$	1
In COLING/ACL Workshop on DR and Discourse Markers, pages 86?92, Montreal, Quebec, Canada.	DR	discourse representations$Discourse Relations$DivRank$Direct Reversal$	1
Inferring DR in Context.	DR	discourse representations$Discourse Relations$DivRank$Direct Reversal$	1
In  Intentionality And Structure In DR:  Proceedings Of A Workshop Sponsored By The Special  Interest Group On Generation Of The Association For  Computational Linguistics, Columbus, OH, June.	DR	discourse representations$Discourse Relations$DivRank$Direct Reversal$	1
4.1 DR PDTB contains annotations for four coarse-grained discourse relation types, as shown in the left column of Table 1.	DR	discourse representations$Discourse Relations$DivRank$Direct Reversal$	1
We incorporate DR in our general ranking framework, which creates a dynamicM during each iteration, rather than a static one.	DR	discourse representations$Discourse Relations$DivRank$Direct Reversal$	2
By incorporating DR, we obtain rank r?i and the global biased ranking score Gi for sentence si from date t to summarize Ct.	DR	discourse representations$Discourse Relations$DivRank$Direct Reversal$	2
2007) and DR (Mei et al.,	DR	discourse representations$Discourse Relations$DivRank$Direct Reversal$	2
Most re- cently diversity rank DR is another solution to diversity penalization in (Mei et al, 2010).	DR	discourse representations$Discourse Relations$DivRank$Direct Reversal$	2
4) DR (Mei et al.,	DR	discourse representations$Discourse Relations$DivRank$Direct Reversal$	2
R-2 RSU4 R-1 R-2 RSU4 R-1 R-2 RSU4 R-1 R-2 RSU4 R-1 R-2 RSU4 Random 0.4091 0.1046 0.1576 0.4496 0.1100 0.1925 0.4442 0.1192 0.1858 0.4154 0.1130 0.1693 0.4309 0.1115 0.1771 Centroid 0.4029 0.0993 0.1484 0.4228 0.1100 0.1764 0.4235 0.1192 0.1722 0.3763 0.0787 0.1386 0.4133 0.1077 0.1640 LexRank 0.4396 0.1451 0.1891 0.4406 0.1296 0.1955 0.4304 0.1397 0.1859 0.4032 0.0992 0.1661 0.4350 0.1331 0.1894 DR 0.4534 0.1504 0.1888 0.4473 0.1161 0.1925 0.4391 0.1167 0.1804 0.4275 0.1180 0.1733 0.4487 0.1317 0.1888 GMDS 0.3918 0.0890 0.1415 0.4339 0.1066 0.1784 0.4064 0.0845 0.1576 0.3846 0.0809 0.1413 0.4045 0.0916 0.1553 ILP-BL 0.4635 0.1650 0.2000 0.4948 0.1731 0.2333 0.4691 0.1613 0.2073 0.4545 0.1445 0.1981 0.4755 0.1654 0.2136 Our Method 0.4723 0.1655 0.2035 0.5078 0.1787 0.2397 0.4716 0.171	DR	discourse representations$Discourse Relations$DivRank$Direct Reversal$	2
Examples 1 and 2 show the representation that would be obtained for two imaginary EN sen- 2Clause delimiters are punctuation marks other than com- mata, relative pronouns and subordinating conjunctions.	EN	English$Englis$E - n$	0
5 Unsuperv ised  Learn ing  o f  Gender   In fo rmat ion   The importance of gender information as re-  vealed in the previous experiments caused us to  consider automatic methods for estimating the  probability that nouns occurring in a large cor-  pus of EN text deonote inanimate, mascu-  line or feminine things.	EN	English$Englis$E - n$	0
The second half of the paper describes a  method for using (portions of) t~e aforemen-  tioned program to learn automatically the typi-  cal gender of EN words, information that is  itself used in the pronoun resolution program.	EN	English$Englis$E - n$	0
We believe that the approach could be straightforwardly extended to other Indoeuropean languages, such as Spanish, German or EN.	EN	English$Englis$E - n$	0
The Grammar Ma- trix was developed initially on the basis of broad- coverage grammars for EN (Flickinger, 2000) and Japanese (Siegel and Bender, 2002), and has since been extended and refined as it has been used in the development of broad-coverage grammars for Norwegian (Hellan and Haugereid, 2003), Modern Greek (Kordoni and Neu, 2005), and Spanish (Ma- rimon et al, 2007), as well as being applied to 42 other languages from a variety of language families in a classroom context (B	EN	English$Englis$E - n$	0
In recent research in the field, the main effort has been to infer semantic classes for verbs, in EN (Stevenson et al, 1999) and German (Schulte im Walde and Brew, 2002).	EN	English$Englis$E - n$	0
Examples 1 and 2 show the representation that would be obtained for two imaginary ENh sen- 2Clause delimiters are punctuation marks other than com- mata, relative pronouns and subordinating conjunctions.	EN	English$Englis$E - n$	1
5 Unsuperv ised  Learn ing  o f  Gender   In fo rmat ion   The importance of gender information as re-  vealed in the previous experiments caused us to  consider automatic methods for estimating the  probability that nouns occurring in a large cor-  pus of ENh text deonote inanimate, mascu-  line or feminine things.	EN	English$Englis$E - n$	1
The second half of the paper describes a  method for using (portions of) t~e aforemen-  tioned program to learn automatically the typi-  cal gender of ENh words, information that is  itself used in the pronoun resolution program.	EN	English$Englis$E - n$	1
We believe that the approach could be straightforwardly extended to other Indoeuropean languages, such as Spanish, German or ENh.	EN	English$Englis$E - n$	1
The Grammar Ma- trix was developed initially on the basis of broad- coverage grammars for ENh (Flickinger, 2000) and Japanese (Siegel and Bender, 2002), and has since been extended and refined as it has been used in the development of broad-coverage grammars for Norwegian (Hellan and Haugereid, 2003), Modern Greek (Kordoni and Neu, 2005), and Spanish (Ma- rimon et al, 2007), as well as being applied to 42 other languages from a variety of language families in a classroom context (B	EN	English$Englis$E - n$	1
In recent research in the field, the main effort has been to infer semantic classes for verbs, in ENh (Stevenson et al, 1999) and German (Schulte im Walde and Brew, 2002).	EN	English$Englis$E - n$	1
l as t - s ide   CONTROL a * NATIVE-OR-HOT  VALUENon-nat ive   NORTH/YOUNG NORTH/OLD ? . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .	EN	English$Englis$E - n$	2
HATIVE-OR-NOT  VALUENo,-ne~.lve  HORTH/YOUNG NORTH/OLD . . . . . . . . . . . . . . . . . . . . . .	EN	English$Englis$E - n$	2
Sri and ubc: Simple similarity features for STS.	STS	Similarity Core task$semantic textual similarity$Semantic Textual Similari-ty$Semantic Textual Similarit$Segmenters$Semantic Textual Similarity$	1
Semeval-2012 task 6: A pilot on STS.	STS	Similarity Core task$semantic textual similarity$Semantic Textual Similari-ty$Semantic Textual Similarit$Segmenters$Semantic Textual Similarity$	1
Ukp: Computing STS by combining multiple content similarity measures.	STS	Similarity Core task$semantic textual similarity$Semantic Textual Similari-ty$Semantic Textual Similarit$Segmenters$Semantic Textual Similarity$	1
SemEval-2012 Task 6: A pilot on STS.	STS	Similarity Core task$semantic textual similarity$Semantic Textual Similari-ty$Semantic Textual Similarit$Segmenters$Semantic Textual Similarity$	1
SemEval-2012 Task 6: A Pilot on STSy.	STS	Similarity Core task$semantic textual similarity$Semantic Textual Similari-ty$Semantic Textual Similarit$Segmenters$Semantic Textual Similarity$	3
For details on annotation guidelines and                                                    1 In *SEM?2013, the shared task is changed with focus on  "STSy".	STS	Similarity Core task$semantic textual similarity$Semantic Textual Similari-ty$Semantic Textual Similarit$Segmenters$Semantic Textual Similarity$	3
UMBC EBIQUITY-CORE: STSy Systems.	STS	Similarity Core task$semantic textual similarity$Semantic Textual Similari-ty$Semantic Textual Similarit$Segmenters$Semantic Textual Similarity$	3
SEM 2013 shared task: STSy.	STS	Similarity Core task$semantic textual similarity$Semantic Textual Similari-ty$Semantic Textual Similarit$Segmenters$Semantic Textual Similarity$	3
F-93430, Villetaneuse, France {buscaldi,joseph.le-roux,jgflores} @lipn.univ-paris13.fr Adrian Popescu CEA, LIST, Vision & Content Engineering Laboratory F-91190 Gif-sur-Yvette, France adrian.popescu@cea.fr Abstract This paper describes the system used by the LIPN team in the STSy task at *SEM 2013.	STS	Similarity Core task$semantic textual similarity$Semantic Textual Similari-ty$Semantic Textual Similarit$Segmenters$Semantic Textual Similarity$	3
c?2014 Association for Computational Linguistics Probabilistic Soft Logic for STSy Islam Beltagy ?	STS	Similarity Core task$semantic textual similarity$Semantic Textual Similari-ty$Semantic Textual Similarit$Segmenters$Semantic Textual Similarity$	3
3.2 Consistency Analysis of Different STS In Section 3.1 we have refuted two hypotheses.	STS	Similarity Core task$semantic textual similarity$Semantic Textual Similari-ty$Semantic Textual Similarit$Segmenters$Semantic Textual Similarity$	4
3.1 Character-based, Lexicon-based and Feature-based STS The training data for the segmenter is two orders of magnitude smaller than for the MT system, it is not terribly well matched to it in terms of genre and variety, and the information an MT system learns about alignment of Chinese to English might be the basis for a task appropriate segmentation style for Chinese-English MT.	STS	Similarity Core task$semantic textual similarity$Semantic Textual Similari-ty$Semantic Textual Similarit$Segmenters$Semantic Textual Similarity$	4
5.2 Compar ing  STS   Word segmentation is a big issue for Chinese since  linguistics-strong applications uch as POS tagging,  sentence parsing, machine translation, text to voice,  etc.	STS	Similarity Core task$semantic textual similarity$Semantic Textual Similari-ty$Semantic Textual Similarit$Segmenters$Semantic Textual Similarity$	4
6 Evaluation of Automatic STS Having looked at how S, WD, and B perform at a small scale in ?	STS	Similarity Core task$semantic textual similarity$Semantic Textual Similari-ty$Semantic Textual Similarit$Segmenters$Semantic Textual Similarity$	4
3.4 Evaluating Automatic STS Coders often disagree in segmentation tasks (Hearst, 1997, p. 56), making it improbable that a single, correct, reference segmentation could be identified from human codings.	STS	Similarity Core task$semantic textual similarity$Semantic Textual Similari-ty$Semantic Textual Similarit$Segmenters$Semantic Textual Similarity$	4
C5  <- 28 Long Queries @~- 28 Short Queries --->  Queens UMASS Queens UMASS  RR 2059 2070 1972 1991  AvPre .467 .460 .417 .414  P@10 .625 .589 .554 .561  R.Pre .471 .453 .413 .412  RR  AvPre  P@IO  R.Pre  TREC6  ~.- 26 Long Queries @~-26 Short Queries--~  Queens UMASS Queens UMASS  2791 2761 2547 2488  .603 .587 .476 .491  .869 .850 .712 .750  .567 .557 .463 .476  Table4: Comparing Queens & UMASS STS  134  to segment he string into short-words, thereby also  discovering unknown words.	STS	Similarity Core task$semantic textual similarity$Semantic Textual Similari-ty$Semantic Textual Similarit$Segmenters$Semantic Textual Similarity$	4
SemEval-2012 Task 6: A Pilot on STS.	STS	Similarity Core task$semantic textual similarity$Semantic Textual Similari-ty$Semantic Textual Similarit$Segmenters$Semantic Textual Similarity$	5
For details on annotation guidelines and                                                    1 In *SEM?2013, the shared task is changed with focus on  "STS".	STS	Similarity Core task$semantic textual similarity$Semantic Textual Similari-ty$Semantic Textual Similarit$Segmenters$Semantic Textual Similarity$	5
UMBC EBIQUITY-CORE: STS Systems.	STS	Similarity Core task$semantic textual similarity$Semantic Textual Similari-ty$Semantic Textual Similarit$Segmenters$Semantic Textual Similarity$	5
SEM 2013 shared task: STS.	STS	Similarity Core task$semantic textual similarity$Semantic Textual Similari-ty$Semantic Textual Similarit$Segmenters$Semantic Textual Similarity$	5
F-93430, Villetaneuse, France {buscaldi,joseph.le-roux,jgflores} @lipn.univ-paris13.fr Adrian Popescu CEA, LIST, Vision & Content Engineering Laboratory F-91190 Gif-sur-Yvette, France adrian.popescu@cea.fr Abstract This paper describes the system used by the LIPN team in the STS task at *SEM 2013.	STS	Similarity Core task$semantic textual similarity$Semantic Textual Similari-ty$Semantic Textual Similarit$Segmenters$Semantic Textual Similarity$	5
c?2014 Association for Computational Linguistics Probabilistic Soft Logic for STS Islam Beltagy ?	STS	Similarity Core task$semantic textual similarity$Semantic Textual Similari-ty$Semantic Textual Similarit$Segmenters$Semantic Textual Similarity$	5
The performance of the experiments on two  datasets indicates feasibility and potentiality of  the QC.	QC	quantum classifier$Question Classification$Question Classificatio$question classification$query classes$	0
1008  A Language Independent Method for QC Thamar Solorio1, Manuel Pe?rez-Coutin?o1, Manuel Montes-y-Go?mez1,2 Luis Villasen?or-Pineda1 and Aurelio Lo?pez-Lo?pez1 1Language Technologies Group, Computer Science Department National Institute of Astrophysics, Optics and Electronics 72840 Tonantzintla, Puebla, Mexico 2Departamento de Sistemas Informa?ticos y Computacio?n Universidad Polite?cnica de Valencia Espan?a {tha	QC	quantum classifier$Question Classification$Question Classificatio$question classification$query classes$	1
QC Data: We built a new taxonomy for Question Classifica- tion based on the NE categories discussed above.	QC	quantum classifier$Question Classification$Question Classificatio$question classification$query classes$	1
3.2.2 QC We classify the question to the anticipated type of the answer.	QC	quantum classifier$Question Classification$Question Classificatio$question classification$query classes$	1
6.1 QC Question classification is defined as a task similar to text categorization; it maps a given question into a question type.	QC	quantum classifier$Question Classification$Question Classificatio$question classification$query classes$	1
5.2 QC Module We evaluated the classifier based on our proposed taxonomy using 230 Arabic questions.	QC	quantum classifier$Question Classification$Question Classificatio$question classification$query classes$	1
43 3 QC  First, we bought five language test books for  second grade students and one of them, pub- lished by KUMON, was used as a training text  to develop our system.	QC	quantum classifier$Question Classification$Question Classificatio$question classification$query classes$	1
1008  A Language Independent Method for QCn Thamar Solorio1, Manuel Pe?rez-Coutin?o1, Manuel Montes-y-Go?mez1,2 Luis Villasen?or-Pineda1 and Aurelio Lo?pez-Lo?pez1 1Language Technologies Group, Computer Science Department National Institute of Astrophysics, Optics and Electronics 72840 Tonantzintla, Puebla, Mexico 2Departamento de Sistemas Informa?ticos y Computacio?n Universidad Polite?cnica de Valencia Espan?a {tha	QC	quantum classifier$Question Classification$Question Classificatio$question classification$query classes$	2
QCn Data: We built a new taxonomy for Question Classifica- tion based on the NE categories discussed above.	QC	quantum classifier$Question Classification$Question Classificatio$question classification$query classes$	2
3.2.2 QCn We classify the question to the anticipated type of the answer.	QC	quantum classifier$Question Classification$Question Classificatio$question classification$query classes$	2
6.1 QCn Question classification is defined as a task similar to text categorization; it maps a given question into a question type.	QC	quantum classifier$Question Classification$Question Classificatio$question classification$query classes$	2
5.2 QCn Module We evaluated the classifier based on our proposed taxonomy using 230 Arabic questions.	QC	quantum classifier$Question Classification$Question Classificatio$question classification$query classes$	2
43 3 QCn  First, we bought five language test books for  second grade students and one of them, pub- lished by KUMON, was used as a training text  to develop our system.	QC	quantum classifier$Question Classification$Question Classificatio$question classification$query classes$	2
2006), QC (Nguyen et al, 2008) and POS tagging (S?gaard, 2010).	QC	quantum classifier$Question Classification$Question Classificatio$question classification$query classes$	3
Given that the QA system does not do any kind of QC and it does not use any NE recogniser, the results are sat- isfactory.	QC	quantum classifier$Question Classification$Question Classificatio$question classification$query classes$	3
In Section 6, we compare the performance of conven- tional methods with that of the proposed method by using real NLP tasks: QC and sentence modality identification.	QC	quantum classifier$Question Classification$Question Classificatio$question classification$query classes$	3
Medical domain  EpoCare (Niu and Hirst 2004)  system by University of Maryland (Demner-Fushman and Lin 2005)  QC by Columbia University and Cooper Union (Yu, Sable, and Zhu 2005)  IMIX 12.	QC	quantum classifier$Question Classification$Question Classificatio$question classification$query classes$	3
Analysis of statistical QC for fact-based ques- tions.	QC	quantum classifier$Question Classification$Question Classificatio$question classification$query classes$	3
Task Classification for Multitask Learning: We explore two simple QC schemes.	QC	quantum classifier$Question Classification$Question Classificatio$question classification$query classes$	3
The  subscript i ranges over all the QC.	QC	quantum classifier$Question Classification$Question Classificatio$question classification$query classes$	4
To distinguish between these QC, we introduce novel met- rics based on the entropy of the click distri- butions of individual searchers.	QC	quantum classifier$Question Classification$Question Classificatio$question classification$query classes$	4
To maximize the little information we have  about the QC, we treat the words in  query class names as additional example queries.	QC	quantum classifier$Question Classification$Question Classificatio$question classification$query classes$	4
Definitions of QC: we now more for- mally define the QC we consider: ?	QC	quantum classifier$Question Classification$Question Classificatio$question classification$query classes$	4
To maximize the little information we have  about the QC, we treat the words in  query class names as	QC	quantum classifier$Question Classification$Question Classificatio$question classification$query classes$	4
4.2 Baseline classifier  Since the QC are not mutually  exclusive, we treat the query classification task  as 67 binary classification problems.	QC	quantum classifier$Question Classification$Question Classificatio$question classification$query classes$	4
Table 2 reports the distribution of QC in our dataset.	QC	quantum classifier$Question Classification$Question Classificatio$question classification$query classes$	4
The first clause (monotone, reverse) in- dicates whether the TO follows the source order; the second (adjacent, gap) indicates whether the anchor and its neighboring constituent are adja- cent or separated by an intervening when projected.	TO	target order$to$	0
2.2 Recurs ive  Head Transduct ion   We can apply a set of head transducers recursively  to derive a pair of source-TOed depen-  dency trees.	TO	target order$to$	0
In this paper, we take 843 Figure 1: An example with a source sentence F re- ordered into TO F ?,	TO	target order$to$	0
Unlike with the Bleu score, rank order centroid weights (rather than the geometric mean) are used to combine scores of different orders, which avoids problems with scoring partial realizations which have no n- gram matches of the TO.	TO	target order$to$	0
4.1 Calculating Oracle Orderings In order to calculate reordering quality, we first define a ranking function r(fj |F,A), which indi- cates the relative position of source word fj in the proper TO (Figure 2 (a)).	TO	target order$to$	0
Source reordering is based on a set of learned rewrite rules that non- deterministically reorder the input words so as to match the TO thereby generating a lattice of possible reorderings.	TO	target order$to$	0
We incorpo-  rate multiple anaphora resolution facTOrs inTO  a statistical framework - -  specifically the dis-  tance between the pronoun and the proposed  antecedent, gender/number/animaticity of the  proposed antecedent, governing head informa-  tion and noun phrase repetition.	TO	target order$to$	1
In particular, the scheme infers the gender of a  referent from the gender of the pronouns that  161  refer TO it and selects referents using the pro-  noun anaphora program.	TO	target order$to$	1
A Statistical Approach TO Anaphora Resolution  Niyu  Ge, John  Hale and Eugene Charn iak   Dept.	TO	target order$to$	1
The second half of the paper describes a  method for using (portions of) t~e aforemen-  tioned program TO learn auTOmatically the typi-  cal gender of English words, information that is  itself used in the pronoun resolution program.	TO	target order$to$	1
We combine  them inTO a single probability that enables us  TO identify the referent.	TO	target order$to$	1
We incorpo-  rate multiple anaphora resolution facTOrs inTO  a statistical framework - -  specifically the dis-	TO	target order$to$	1
504  Proceedings of the Workshop on BioNLP: ST, pages 95?98, Boulder, Colorado, June 2009.	ST	Shared Task$stops$Surface Text$Simulated Tempering$Split Tags$Same Topic$solve time$sm'face tape$subtree$syntactic tree$same topic$	0
National Centre for Text Mining, UK {sebastian,chun,takagi}@dbcls.rois.ac.jp tsujii@is.s.u-tokyo.ac.jp Abstract In this paper we describe our entry to the BioNLP 2009 ST regarding bio- molecular event extraction.	ST	Shared Task$stops$Surface Text$Simulated Tempering$Split Tags$Same Topic$solve time$sm'face tape$subtree$syntactic tree$same topic$	0
7 Conclusion Our approach the BioNLP ST 2009 can be characterized by three decisions: (a) jointly CORE VALID FULL eventType 52.8 63.2 64.3 role 44.0 53.5 55.7 site 42.0 46.0 51.5 Total 50.7 60.1 61.9 Table 4: Ground atom F-scores for global formulae.	ST	Shared Task$stops$Surface Text$Simulated Tempering$Split Tags$Same Topic$solve time$sm'face tape$subtree$syntactic tree$same topic$	0
The CoNLL- 2010 ST: Learning to Detect Hedges and their Scope in Natural Language Text.	ST	Shared Task$stops$Surface Text$Simulated Tempering$Split Tags$Same Topic$solve time$sm'face tape$subtree$syntactic tree$same topic$	0
Proceedings of the Fourteenth Conference on Computational Natural Language Learning: ST, pages 138?143, Uppsala, Sweden, 15-16 July 2010.	ST	Shared Task$stops$Surface Text$Simulated Tempering$Split Tags$Same Topic$solve time$sm'face tape$subtree$syntactic tree$same topic$	0
Proceedings of the Workshop on BioNLP: ST, pages 41?49, Boulder, Colorado, June 2009.	ST	Shared Task$stops$Surface Text$Simulated Tempering$Split Tags$Same Topic$solve time$sm'face tape$subtree$syntactic tree$same topic$	0
The algorithm ST until all features are pruned.	ST	Shared Task$stops$Surface Text$Simulated Tempering$Split Tags$Same Topic$solve time$sm'face tape$subtree$syntactic tree$same topic$	1
The chosen fragment is composed with the current subanalysis to produce a new one; the process ST when an analysis results with no non-terminal leaves.	ST	Shared Task$stops$Surface Text$Simulated Tempering$Split Tags$Same Topic$solve time$sm'face tape$subtree$syntactic tree$same topic$	1
We run each training procedure until the area under the precision/recall curve measured on a development corpus ST increasing (see Figure 4 for an example of such a curve).	ST	Shared Task$stops$Surface Text$Simulated Tempering$Split Tags$Same Topic$solve time$sm'face tape$subtree$syntactic tree$same topic$	1
The moni tor  mon i tors  the whole reconst ruct ion  pro-  cess and ST the process by raising the th resho ld  to  judge the incons istency when it judges that the  reconst rnet ion  takes too much t ime.	ST	Shared Task$stops$Surface Text$Simulated Tempering$Split Tags$Same Topic$solve time$sm'face tape$subtree$syntactic tree$same topic$	1
Announcer speaks when expert ST Thanks.	ST	Shared Task$stops$Surface Text$Simulated Tempering$Split Tags$Same Topic$solve time$sm'face tape$subtree$syntactic tree$same topic$	1
In order to rule out the possibility that the sampling process never ST, we use a maximum sample size of 10,000 derivations.	ST	Shared Task$stops$Surface Text$Simulated Tempering$Split Tags$Same Topic$solve time$sm'face tape$subtree$syntactic tree$same topic$	1
5.1 ST Statistics Out of 57,809 sentences, 6,047 (10.5%) are anno- tated (see Table 2); and 4,934 (8.5%) have multi- token bracketings.	ST	Shared Task$stops$Surface Text$Simulated Tempering$Split Tags$Same Topic$solve time$sm'face tape$subtree$syntactic tree$same topic$	2
Automatic Event Classification Using ST Features.	ST	Shared Task$stops$Surface Text$Simulated Tempering$Split Tags$Same Topic$solve time$sm'face tape$subtree$syntactic tree$same topic$	2
Learning ST Patterns for a Question An- swering System.	ST	Shared Task$stops$Surface Text$Simulated Tempering$Split Tags$Same Topic$solve time$sm'face tape$subtree$syntactic tree$same topic$	2
Learn- ing ST Patterns for a Question Answering  System.	ST	Shared Task$stops$Surface Text$Simulated Tempering$Split Tags$Same Topic$solve time$sm'face tape$subtree$syntactic tree$same topic$	2
4.1 Extraction with ST Patterns To extract information about roles, we used the set of surface patterns originally developed for the QA system we used at TREC 2003 (Jijkoun et al, 2004).	ST	Shared Task$stops$Surface Text$Simulated Tempering$Split Tags$Same Topic$solve time$sm'face tape$subtree$syntactic tree$same topic$	2
4.2 Generating ST  SUMO-based story plans provide a form of inter- lingua where story details are represented in logi- cal form.	ST	Shared Task$stops$Surface Text$Simulated Tempering$Split Tags$Same Topic$solve time$sm'face tape$subtree$syntactic tree$same topic$	2
One way to speed up Gibbs sampling is by ST (Marinari and Parisi, 1992), which performs simulation in a gener- alized ensemble, and can rapidly achieve an equilibrium state.	ST	Shared Task$stops$Surface Text$Simulated Tempering$Split Tags$Same Topic$solve time$sm'face tape$subtree$syntactic tree$same topic$	3
ST: A newMonte Carlo scheme.	ST	Shared Task$stops$Surface Text$Simulated Tempering$Split Tags$Same Topic$solve time$sm'face tape$subtree$syntactic tree$same topic$	3
We use three Markov chain Monte Carlo (MCMC) algorithms, including Gibbs sampling, ST, as well as MC-SAT, andMaximum a posteriori/Most Probable Expla- nation (MAP/MPE) algorithm for probabilistic inference in MLNs.	ST	Shared Task$stops$Surface Text$Simulated Tempering$Split Tags$Same Topic$solve time$sm'face tape$subtree$syntactic tree$same topic$	3
There are 476 En- glish Wikipedia articles in the Controversial corpus 187 Classification Frequency Totally Different 2 Same General Topic 3 Overlapping Topics 11 STs 33 Parallel 1 and 384 associated ?	ST	Shared Task$stops$Surface Text$Simulated Tempering$Split Tags$Same Topic$solve time$sm'face tape$subtree$syntactic tree$same topic$	5
10 Conclusion We presented the first context-dependent semantic parsing system to detect and reST expres- sions.	ST	Shared Task$stops$Surface Text$Simulated Tempering$Split Tags$Same Topic$solve time$sm'face tape$subtree$syntactic tree$same topic$	6
5 Experiments In the following sub-sections we present experi- mental results on learning to reST expres- sions in Twitter using minimal supervision.	ST	Shared Task$stops$Surface Text$Simulated Tempering$Split Tags$Same Topic$solve time$sm'face tape$subtree$syntactic tree$same topic$	6
First we turn our attention to the STs obtained using ILP (for the sentences for which the solution was found within 30 min- utes).	ST	Shared Task$stops$Surface Text$Simulated Tempering$Split Tags$Same Topic$solve time$sm'face tape$subtree$syntactic tree$same topic$	6
Recently, Angeli et al introduced the idea of learning semantic parsers to reST expres- sions (Angeli et al, 2012) and showed that the ap- proach can generalize to multiple languages (An- geli and Uszkoreit, 2013).	ST	Shared Task$stops$Surface Text$Simulated Tempering$Split Tags$Same Topic$solve time$sm'face tape$subtree$syntactic tree$same topic$	6
The table shows that the average ST is under one minute per sentence.	ST	Shared Task$stops$Surface Text$Simulated Tempering$Split Tags$Same Topic$solve time$sm'face tape$subtree$syntactic tree$same topic$	6
The sparse sequence alignment technique mentioned  above is especially intriguing, because the complexity introduced in dealing with sparsity resembles in certain  respects that of the time warp problem; further, the fact that the sparse alignment problem can be solved efficiently  gives rise to hope that the same techniques can be used to ST warping.	ST	Shared Task$stops$Surface Text$Simulated Tempering$Split Tags$Same Topic$solve time$sm'face tape$subtree$syntactic tree$same topic$	6
As we increase the sentence length we see the ST increases, however, we never see an order of magnitude in- crease between brackets as witnessed by Germann et al (2004) thus optimal decoding is more practi- ca	ST	Shared Task$stops$Surface Text$Simulated Tempering$Split Tags$Same Topic$solve time$sm'face tape$subtree$syntactic tree$same topic$	6
As we increase the sentence length we see the ST increases, however, we never see an order of magnitude in- crease between brackets as witnessed by Germann et al (2004) thus optimal decoding is more practi- cal than previously suggested.	ST	Shared Task$stops$Surface Text$Simulated Tempering$Split Tags$Same Topic$solve time$sm'face tape$subtree$syntactic tree$same topic$	6
2.2  Decomposition operations and Fragments The fragments for LFG-DOP consist of connected STs whose nodes are in ?-	ST	Shared Task$stops$Surface Text$Simulated Tempering$Split Tags$Same Topic$solve time$sm'face tape$subtree$syntactic tree$same topic$	8
Af- ter ranking the first level, the beam zooms in on the top-ranked cells and constructs a finer k-d tree under each one (one such ST is shown in the top-right map callout).	ST	Shared Task$stops$Surface Text$Simulated Tempering$Split Tags$Same Topic$solve time$sm'face tape$subtree$syntactic tree$same topic$	8
disjunct from the ST above).	ST	Shared Task$stops$Surface Text$Simulated Tempering$Split Tags$Same Topic$solve time$sm'face tape$subtree$syntactic tree$same topic$	8
nsist of connected STs whose nodes are in ?-	ST	Shared Task$stops$Surface Text$Simulated Tempering$Split Tags$Same Topic$solve time$sm'face tape$subtree$syntactic tree$same topic$	8
To give a precise definition of LFG-DOP fragments, it is convenient to recall the decomposition operations employed by the orginal DOP model which is also known as the "Tree-DOP" model (Bod 1993, 1998): (1)  Root: the Root operation selects any node of a tree to be the root of the new ST and erases all nodes except the selected node and the nodes it dominates.	ST	Shared Task$stops$Surface Text$Simulated Tempering$Split Tags$Same Topic$solve time$sm'face tape$subtree$syntactic tree$same topic$	8
To give a precise definition of LFG-DOP fragments, it is convenient to recall the decomposition operations employed by the orginal DOP model which is also known as the "Tree-DOP" model (Bod 1993, 1998): (1)  Root: the Root operation selects any node of a tree to be the root of the new ST and erases	ST	Shared Task$stops$Surface Text$Simulated Tempering$Split Tags$Same Topic$solve time$sm'face tape$subtree$syntactic tree$same topic$	8
When a node is selected by the Root operation, all nodes outside of that node's ST are erased, just as in Tree-DOP.	ST	Shared Task$stops$Surface Text$Simulated Tempering$Split Tags$Same Topic$solve time$sm'face tape$subtree$syntactic tree$same topic$	8
loyed by the orginal DOP model which is also known as the "Tree-DOP" model (Bod 1993, 1998): (1)  Root: the Root operation selects any node of a tree to be the root of the new ST and erases all nodes except the selected node and the nodes it dominates.	ST	Shared Task$stops$Surface Text$Simulated Tempering$Split Tags$Same Topic$solve time$sm'face tape$subtree$syntactic tree$same topic$	8
ient to recall the decomposition operations employed by the orginal DOP model which is also known as the "Tree-DOP" model (Bod 1993, 1998): (1)  Root: the Root operation selects any node of a tree to be the root of the new ST and erases all nodes except the selected node and the nodes it dominates.	ST	Shared Task$stops$Surface Text$Simulated Tempering$Split Tags$Same Topic$solve time$sm'face tape$subtree$syntactic tree$same topic$	8
(2)  Frontier : the Frontier operation then chooses a set (possibly empty) of nodes in the new ST different from its root and erases all STs dominated by the chosen nodes.	ST	Shared Task$stops$Surface Text$Simulated Tempering$Split Tags$Same Topic$solve time$sm'face tape$subtree$syntactic tree$same topic$	8
F the forest made of the STs of all the  considered lexical entries plus the output (the type  we want to derive).	ST	Shared Task$stops$Surface Text$Simulated Tempering$Split Tags$Same Topic$solve time$sm'face tape$subtree$syntactic tree$same topic$	9
For others it is unclear exactly how much of the syntactic annotations can be ef- fectively leveraged with current models, and what structures in the STs are most relevant to the current task.	ST	Shared Task$stops$Surface Text$Simulated Tempering$Split Tags$Same Topic$solve time$sm'face tape$subtree$syntactic tree$same topic$	9
For En- glish, not only is the hidden marginalization method a suitable replacement for the STs pro- vided by pre-trained, state-of-the-art models, but in both configurations we find that inducing an optimal hidden structure is preferable to the parser-produced annotations.	ST	Shared Task$stops$Surface Text$Simulated Tempering$Split Tags$Same Topic$solve time$sm'face tape$subtree$syntactic tree$same topic$	9
Similarly, dependency-based  models (e.g., Collins, 1996; Chelba et al, 1997) use  a dependency structure D of W instead of a parse  tree T, where D is extracted from STs.	ST	Shared Task$stops$Surface Text$Simulated Tempering$Split Tags$Same Topic$solve time$sm'face tape$subtree$syntactic tree$same topic$	9
3.0.1 The  Hobbs a lgor i thm  The Hobbs algorithm makes a few assumptions  about the STs upon which it operates  that are not satisfied by the tree-bank trees that  form the substrate for our algorithm.	ST	Shared Task$stops$Surface Text$Simulated Tempering$Split Tags$Same Topic$solve time$sm'face tape$subtree$syntactic tree$same topic$	9
1 ) *l-R-?llq -',4?I 'ffd-<: 11,II?1 -~.:~.  This utterance has two distinct STs:  ( 1 ) S/PP j i / \  / \  PP\[de\]  '\] ~J ~b lJ, 6 #,/PP VP  / " . . . .	ST	Shared Task$stops$Surface Text$Simulated Tempering$Split Tags$Same Topic$solve time$sm'face tape$subtree$syntactic tree$same topic$	9
The more the feature terms  illustrates the ST feature, the higher the  weight of the topic feature.	ST	Shared Task$stops$Surface Text$Simulated Tempering$Split Tags$Same Topic$solve time$sm'face tape$subtree$syntactic tree$same topic$	10
ftjfi  shows feature term set illustrating the ST  feature fi.	ST	Shared Task$stops$Surface Text$Simulated Tempering$Split Tags$Same Topic$solve time$sm'face tape$subtree$syntactic tree$same topic$	10
ftjfi  shows feature term set illustrating the ST	ST	Shared Task$stops$Surface Text$Simulated Tempering$Split Tags$Same Topic$solve time$sm'face tape$subtree$syntactic tree$same topic$	10
To test the hypothesis, we compare the performance of our proposed model trained on documents from all available out-of-topic data with two models, one trained on single out-of-topic data and another trained on the ST (intra-topic) This work is licenced under a Creative Commons Attribution 4.0 International License.	ST	Shared Task$stops$Surface Text$Simulated Tempering$Split Tags$Same Topic$solve time$sm'face tape$subtree$syntactic tree$same topic$	10
Since the repetition of lexi-  cal items occurs more frequently within regions of a text  which are about the ST or group of topics, the  visually apparent squares along the main diagonal of  the plot correspond to regions of the text.	ST	Shared Task$stops$Surface Text$Simulated Tempering$Split Tags$Same Topic$solve time$sm'face tape$subtree$syntactic tree$same topic$	10
rates the ST feature, the higher the  weight of the topic feature.	ST	Shared Task$stops$Surface Text$Simulated Tempering$Split Tags$Same Topic$solve time$sm'face tape$subtree$syntactic tree$same topic$	10
1 In t roduct ion   Even moderately ong documents typically address  sew~ral topics or different aspects of the ST.	ST	Shared Task$stops$Surface Text$Simulated Tempering$Split Tags$Same Topic$solve time$sm'face tape$subtree$syntactic tree$same topic$	10
A FU Q can be used to ask (i) a similar question as the previous one but with different constraints or different participants (topic extension); (ii) a ques- tion concerning a different aspect of the ST (topic exploration); (iii) a question concerning a related activity or a related entity (topic shift).	ST	Shared Task$stops$Surface Text$Simulated Tempering$Split Tags$Same Topic$solve time$sm'face tape$subtree$syntactic tree$same topic$	10
Kiros et al (2015) also use GRUs instead of LSTM.	GRU	Gated Recurrent Unit$gated recurrent unit$	0
Kiros et al (2015) train an encoder-decoder model to encode a sentence into a fixed-length vector and subsequently decode both the following and preceding sentence, using GRUs (Chung et al, 2014).	GRU	Gated Recurrent Unit$gated recurrent unit$	0
Two independent GRUs are used to model the word embeddings and utterance embeddings separately on word sequence view and utterance sequence view, the former of which captures dependencies in word level and the latter captures utterance-level semantic and discourse information.	GRU	Gated Recurrent Unit$gated recurrent unit$	0
The encoder is a bidirectional neural network with GRUs (Cho et al, 2014) that reads an input sequence x = (x 1 , ..., x m ) and calculates a forward sequence of hidden states ( ??	GRU	Gated Recurrent Unit$gated recurrent unit$	1
The BASIC NMT system is built using the Blocks framework (van Merri?enboer et al, 2015) based on the Theano library (Bastien et al, 2012) with standard hyper-parameters (Bahdanau et al, 2015): the encoder and decoder networks consist of 1000 GRUs (Cho et al, 2014).	GRU	Gated Recurrent Unit$gated recurrent unit$	1
Recurrent neural networks Recent years have shown a resurgence of interest in RNN, particularly variants with long short-term memory (Hochreiter and Schmidhuber, 1997) or GRUs (Cho et al, 2014).	GRU	Gated Recurrent Unit$gated recurrent unit$	1
They used a GRU for f (see, e.g., (Cho et al, 2014b)).	GRU	Gated Recurrent Unit$gated recurrent unit$	1
Kumar et al (2015) proposed a similar model where a memory was designed to change the gate of the GRU for each iteration.	GRU	Gated Recurrent Unit$gated recurrent unit$	1
s (W r x t +U r h t?1 ) (4) Our GRUs use steep sigmoids for gate activations: ?	GRU	Gated Recurrent Unit$gated recurrent unit$	1
As an example, the left of Figure 4 shows (solid line) the change of the importance score with time when an INT takes place (the dotted line represents the importance score without interrup- tion).	INT	interruption$interrupted$integrated$	0
Further, it illustrates that discourse issues such as controlling INT, abbreviation, and main- taining consistency can also be decom- posed: rather than considering them at the single level of one linear explana- tion they can also be tackled separately within each individual agent.	INT	interruption$interrupted$integrated$	0
Further, discourse issues such as control- ling INT, abbreviation, and maintaining consistency can also be decomposed: rather than considering them at the single level of one linear explanation they can be tackled separately within each individual agent and then also at the level of inter-agent cooperation.	INT	interruption$interrupted$integrated$	0
Similarly, it may happen that when the two most important propositions in shared memory Importance Score/Time Ends the first utterrance without INT An Important event occurs with INT without INT time < > ?	INT	interruption$interrupted$integrated$	0
The discourse control techniques of INT, rep- etition, abbreviation, and silence are used to con- trol both the dialogue strategies of each individual agent and also the interaction between them.	INT	interruption$interrupted$integrated$	0
He takes her out arg1 arg2 (4) This example demonstrates that we cannot sim- ply treat MWEs as contiguous word strings and include those in the lexicon, since the MWE takes out is INT by the object her in (3).	INT	interruption$interrupted$integrated$	1
We also show a graph on how many of the utterances would have been INT in Figure 5.	INT	interruption$interrupted$integrated$	1
However, at this point, about 20% of the callers would be INT.	INT	interruption$interrupted$integrated$	1
Figure 5: Percentage of utterances INT by maximum speech time-out.	INT	interruption$interrupted$integrated$	1
Specically, we add a bonus to the scores of propo- sitions uttered directly before a period where both commentators are silent (the longer that a com- mentary continues unINT, the higher the silence bonus).	INT	interruption$interrupted$integrated$	1
As it anecdotally happened to callers that they were INT by the dialog system, on the one hand, some voice user interface designers tend to chose rather large values for this time-out setting, e.g., 15 or 20 seconds.	INT	interruption$interrupted$integrated$	1
An  INT cognate identification algorithm would  take as input unordered wordlists from two or more  related languages, and produce alist of aligned cog-  nate pairs as output.	INT	interruption$interrupted$integrated$	2
However, it could well be good enough to be INT into a full parser and provide a benefit to it.	INT	interruption$interrupted$integrated$	2
This  finite bound on path length can be INT into our  algorithm by modifying the add-item procedure such  that only items with a path shorter than the permitted  maximum path length are added to the chart.	INT	interruption$interrupted$integrated$	2
This is still rather different from our setup, where PP attachment is fully INT into the parsing problem.	INT	interruption$interrupted$integrated$	2
In our approach, the recognition of foreign  words has been INT into the ATR process  for Serbian.	INT	interruption$interrupted$integrated$	2
They can be  INT in Web interactions, but not in  cooperative forms of learning.	INT	interruption$interrupted$integrated$	2
IWPT,  Pittsburgh, PA.	IWPT	International Workshop on Parsing Technologies$International Conference on Parsing Technologies$	0
In Proceedings of the IWPT, pp.	IWPT	International Workshop on Parsing Technologies$International Conference on Parsing Technologies$	0
In Proceedings of  the Third IWPT, Tilburg, The Netherlands.	IWPT	International Workshop on Parsing Technologies$International Conference on Parsing Technologies$	0
In Proceedings of the Fourth IWPT.	IWPT	International Workshop on Parsing Technologies$International Conference on Parsing Technologies$	0
In: Proceedings of the  IWPT,  Carnegie Mellon University, Pittsburgh, August  1989, pp.	IWPT	International Workshop on Parsing Technologies$International Conference on Parsing Technologies$	0
Third IWPT, pages 277?292.	IWPT	International Workshop on Parsing Technologies$International Conference on Parsing Technologies$	0
10th IWPT, ACL: 1-10.	IWPT	International Workshop on Parsing Technologies$International Conference on Parsing Technologies$	1
In Proceedings of the 11th IWPT.	IWPT	International Workshop on Parsing Technologies$International Conference on Parsing Technologies$	1
In Proceedings of the 11th IWPT, pages 192- 201.	IWPT	International Workshop on Parsing Technologies$International Conference on Parsing Technologies$	1
In Proceedings of the 11th IWPT, pages 138?141.	IWPT	International Workshop on Parsing Technologies$International Conference on Parsing Technologies$	1
s positive correlation with frequency; i.e., since frequency and poly- semy are HPly correlated, one would expect them to have similar effects on seman- tic change, but we found that the effect of poly- semy completely reversed after controlling for fre- quency.	HP	highly positive$Head of Phrase$hashtag pattern$hydrophobic-hydrophilic$home page$	0
More- over, the data exhibits a HP-skewed distribution.	HP	highly positive$Head of Phrase$hashtag pattern$hydrophobic-hydrophilic$home page$	0
which would be HP.	HP	highly positive$Head of Phrase$hashtag pattern$hydrophobic-hydrophilic$home page$	0
If the test is HPly corre- lated with external tagging quality measures (e.g., those based on gold standard tagging), Q(B) will produce better results than B with high probability.	HP	highly positive$Head of Phrase$hashtag pattern$hydrophobic-hydrophilic$home page$	0
The  feedback was globally HP.	HP	highly positive$Head of Phrase$hashtag pattern$hydrophobic-hydrophilic$home page$	0
We therefore collected a total of 1,000 movie review videos that were either HP or negative.	HP	highly positive$Head of Phrase$hashtag pattern$hydrophobic-hydrophilic$home page$	0
14 Association for Computational Linguistics Learning Emotion Indicators from Tweets: Hashtags, Hashtag Patterns, and Phrases Ashequl Qadir School of Computing University of Utah Salt Lake City, UT 84112, USA asheq@cs.utah.edu Ellen Riloff School of Computing University of Utah Salt Lake City, UT 84112, USA riloff@cs.utah.edu Abstract We present a weakly supervised approach for learning hashtags, HPs, and phrases associated with five emotions: AFFEC- TION, ANGER/RAGE, FEAR/ANXIETY, JOY, and SADNESS/DISAPPOINTMENT.	HP	highly positive$Head of Phrase$hashtag pattern$hydrophobic-hydrophilic$home page$	2
We then harvest emotion phrases from the hash- tags and HPs for contextual emotion clas- sification.	HP	highly positive$Head of Phrase$hashtag pattern$hydrophobic-hydrophilic$home page$	2
3.2 Learning Hashtag Patterns We learn HPs in a similar but separate boot- strapping process.	HP	highly positive$Head of Phrase$hashtag pattern$hydrophobic-hydrophilic$home page$	2
We use probabil- ity estimates to determine which HPs are reliable indicators for an emotion.	HP	highly positive$Head of Phrase$hashtag pattern$hydrophobic-hydrophilic$home page$	2
Conse- quently, we generalize beyond specific hashtags to cre- ate HPs that will match all hashtags with the same prefix, such as	HP	highly positive$Head of Phrase$hashtag pattern$hydrophobic-hydrophilic$home page$	2
Our research learns three types of emotion in- dicators for tweets: hashtags, HPs, and phrases for one of five emotions: AFFEC- TION, ANGER/RAGE, FEAR/ANXIETY, JOY, or SAD- NESS/DISAPPOINTMENT.	HP	highly positive$Head of Phrase$hashtag pattern$hydrophobic-hydrophilic$home page$	2
Conse- quently, we generalize beyond specific hashtags to cre- ate HPs that will match all hashtags with the same prefix, such as the pattern #angryat* which will match both #angryattheworld and #angryatlife.	HP	highly positive$Head of Phrase$hashtag pattern$hydrophobic-hydrophilic$home page$	2
r research learns three types of emotion in- dicators for tweets: hashtags, HPs, and phrases for one of five emotions: AFFEC- TION, ANGER/RAGE, FEAR/ANXIETY, JOY, or SAD- NESS/DISAPPOINTMENT.	HP	highly positive$Head of Phrase$hashtag pattern$hydrophobic-hydrophilic$home page$	2
At a higher level, (Mann and Yarowsky, 2003) disambiguated personal names by clustering people?s HPs using a TFIDF similarity, and several other researchers have ap- plied clustering at the same level in the context of the entity identification problem (Bilenko et al, 2003; Mc- Callum and Wellner, 2003; Li et al, 2004).	HP	highly positive$Head of Phrase$hashtag pattern$hydrophobic-hydrophilic$home page$	4
These features however only covered small amount of disambiguation evidence and certain types of web pages (such as personal HPs).	HP	highly positive$Head of Phrase$hashtag pattern$hydrophobic-hydrophilic$home page$	4
Conll-99 HP.	HP	highly positive$Head of Phrase$hashtag pattern$hydrophobic-hydrophilic$home page$	4
s HP is a Zamboni game in celebration of Frank Zam- boni ?	HP	highly positive$Head of Phrase$hashtag pattern$hydrophobic-hydrophilic$home page$	4
Corpus Selection The corpus selection page (tool HP) provides information about all available corpora, and allows for corpora upload and deletion.	HP	highly positive$Head of Phrase$hashtag pattern$hydrophobic-hydrophilic$home page$	4
The retrieved English  HPs are presented in Chinese and/or  English.	HP	highly positive$Head of Phrase$hashtag pattern$hydrophobic-hydrophilic$home page$	4
2 Lexical and SF  We view VSD as a supervised learning problem,  solving which requires three groups of features:  lexical, syntactic, and semantic.	SF	Sentence Formalism$Syntactic Features$segmentation F$Segment frequency$Slot filling$SameFrame$short form$subcat features$	1
Combin- ing Lexical and SF for Supervised Word Sense Disambiguation. (	SF	Sentence Formalism$Syntactic Features$segmentation F$Segment frequency$Slot filling$SameFrame$short form$subcat features$	1
4.4 SyntaLex-4: Combination of Lexical and Simple SF SyntaLex-4 also relies on a combination of PoS and bigram features but uses unified decision trees that can have either kind of feature at a particular node.	SF	Sentence Formalism$Syntactic Features$segmentation F$Segment frequency$Slot filling$SameFrame$short form$subcat features$	1
4.3 SyntaLex-3: Ensemble of Lexical and Simple SF Prior research has shown that both lexical and syn- tactic features can individually achieve a reasonable quality of disambiguation.	SF	Sentence Formalism$Syntactic Features$segmentation F$Segment frequency$Slot filling$SameFrame$short form$subcat features$	1
400  Complementarity of Lexical and Simple SF: The SyntaLex Approach to SENSEVAL-3 Saif Mohammad University of Toronto Toronto, ON M5S1A1 Canada smm@cs.toronto.edu http://www.cs.toronto.edu/?smm Ted Pedersen University of Minnesota Duluth, MN 55812 USA tpederse@d.umn.edu http://www.d.umn.edu/?tpederse Abstract This paper describes the SyntaLex entries in the English Lexical Sample Task of SENSEVAL-3.	SF	Sentence Formalism$Syntactic Features$segmentation F$Segment frequency$Slot filling$SameFrame$short form$subcat features$	1
Exploring SF for Pronoun Reso- lution Using Context-Sensitive Convolution Tree  Kernel.	SF	Sentence Formalism$Syntactic Features$segmentation F$Segment frequency$Slot filling$SameFrame$short form$subcat features$	1
When performing joint word segmen- tation on the Buckeye corpus, our Bigram model reaches around above 55% F-score for recovering deleted /t/s with a word SF-score of around 72% which is 2% better than running a Bi- gram model that does not model /t/-deletion.	SF	Sentence Formalism$Syntactic Features$segmentation F$Segment frequency$Slot filling$SameFrame$short form$subcat features$	2
al Random N?best Candidates Field Local Features Global Features Average Perceptron Input Sentence Average Perceptron Output Conditional RandomField (10?Fold Split) Figure 1: Outline of the segmentation process 2.1 Learning Algorithm Given an unsegmented sentence x, the word seg- mentation problem can be defined as finding the 143 Sixth SIGHAN Workshop on Chinese Language Processing most probable SF (x) from a set of pos- sible segmentations of x. F (x) = argmax y?GEN(x) ?(	SF	Sentence Formalism$Syntactic Features$segmentation F$Segment frequency$Slot filling$SameFrame$short form$subcat features$	2
Experiments show that our system outperforms existing systems on newswire, broadcast news and Egyptian dialect, im- proving SF 1 score on a recently released Egyptian Arabic corpus to 95.1%, compared to 90.8% for another segmenter designed specifically for Egyptian Arabic.	SF	Sentence Formalism$Syntactic Features$segmentation F$Segment frequency$Slot filling$SameFrame$short form$subcat features$	2
At the same  Chinese word SF-measure,  the number of bigrams in the model can  be reduced by up to 90%.	SF	Sentence Formalism$Syntactic Features$segmentation F$Segment frequency$Slot filling$SameFrame$short form$subcat features$	2
When combined with the joint segmentation and POS-tagging system, the SF-score, joint segmentation and POS-tagging F-score were 95.00% and 90.17%, respectively, and the joint segmentation and parsing F-score for non-root words (excluding punctuations) was 75.09%, where the corresponding accuracy with gold-standard segmented and POS- tagged input was 86.21%, as shown in Table 18.	SF	Sentence Formalism$Syntactic Features$segmentation F$Segment frequency$Slot filling$SameFrame$short form$subcat features$	2
54.61 70.12 NO-VAR 54.12 73.99 Table 6: Word SF-scores for the /t/- recovery F-scores in Table 5 averaged over two runs (standard errors less than 2% unless given).	SF	Sentence Formalism$Syntactic Features$segmentation F$Segment frequency$Slot filling$SameFrame$short form$subcat features$	2
SF can be framed as a sequential label- ing problem in which the most probable semantic slot labels are estimated for each word of the given word sequence.	SF	Sentence Formalism$Syntactic Features$segmentation F$Segment frequency$Slot filling$SameFrame$short form$subcat features$	4
SF is a sequential labeling task to map a sequence of T words xT1 to a sequence of T slot labels yT1 .	SF	Sentence Formalism$Syntactic Features$segmentation F$Segment frequency$Slot filling$SameFrame$short form$subcat features$	4
SF is a traditional task and tremendous efforts have been done, especially since the 1980s when the Defense Advanced Research Program Agency (DARPA) Airline Travel Informa- tion System (ATIS) projects started (Price, 1990).	SF	Sentence Formalism$Syntactic Features$segmentation F$Segment frequency$Slot filling$SameFrame$short form$subcat features$	4
Figure 2: Input and output to topic grouping for TST-MUC3-0099 157 SF SF consists of five parts : (1) pure set fills, (2) string fills, (3) cross-referenced slots, (4) date extraction, and (5) location extraction.	SF	Sentence Formalism$Syntactic Features$segmentation F$Segment frequency$Slot filling$SameFrame$short form$subcat features$	4
4 Related Work SF is used in dialogue systems such as the Ravenclaw-Olympus system5, but the slots are filled by using output from a chart parser (Ward, 2008).	SF	Sentence Formalism$Syntactic Features$segmentation F$Segment frequency$Slot filling$SameFrame$short form$subcat features$	4
at the end of a name (for in- stance, a noun phrase) hints on a family-member or 4The original algorithm decides whether a given SF can be explained by a given long form.	SF	Sentence Formalism$Syntactic Features$segmentation F$Segment frequency$Slot filling$SameFrame$short form$subcat features$	6
a r e  s ty led   "template-like" here  only to ind ica te  t h a t  they have not  been matched  with input t e x t ,  And hence the infer red  proposit ions they represent   have not necessar i ly  been s ta ted  e x p l i c i t l y  i n  the input tex t ,  Let  us f i r s t  see t h e  e f f e c t  of doing t h i s ,  and then the mechanism tha t   does i t ,  In  what follows, we extend the "SF" of templates  (obtained by wr i t ing  square brackets round English words, c lus tered  a t  th ree  nodes to show the d i s t r ibu t ion  of formulas i n  the f u l l   template) by wr i t ing  extract ions  a s  English words ins ide  double  square brackets,   L e t  us consider  (37) John f i r e d  a t  a l i n e  of s tags  with a shotgun  The r e s u l t  of matching t h i s	SF	Sentence Formalism$Syntactic Features$segmentation F$Segment frequency$Slot filling$SameFrame$short form$subcat features$	6
can be trans- ferred to all synonyms from other entries in the dic- tionary that have the same long or SFs (but possibly do not mention the respective other in any synonym.)	SF	Sentence Formalism$Syntactic Features$segmentation F$Segment frequency$Slot filling$SameFrame$short form$subcat features$	6
Sentences start their life in SF, s, are ranked by a source language model, p(s), and then probabilistically ex- panded to form the long sentence, p(l|s).	SF	Sentence Formalism$Syntactic Features$segmentation F$Segment frequency$Slot filling$SameFrame$short form$subcat features$	6
Finally, the subjects were asked to fill in  a SF and describe their reactions to using the system.	SF	Sentence Formalism$Syntactic Features$segmentation F$Segment frequency$Slot filling$SameFrame$short form$subcat features$	6
More esoterically, perhaps, we also exploit SFs of attribute names so as to minimize the size of elementize d messages.	SF	Sentence Formalism$Syntactic Features$segmentation F$Segment frequency$Slot filling$SameFrame$short form$subcat features$	6
As Collins notes, removing both the distance metric and SF results in a gigantic drop in performance, since without both of these features, the model has no way to encode the fact that flatter structures should be avoided in several crucial cases, such as for PPs, which tend to prefer one argument to the right of their head-children.	SF	Sentence Formalism$Syntactic Features$segmentation F$Segment frequency$Slot filling$SameFrame$short form$subcat features$	7
For a  maximum number of four, this will take the form:  \[(A, (A ...... )), (B, (_,B .... )), (C, ( .... C,_)), (D, ( ...... D))\]  The declaration will also allow us to work out the mnemonic values  (npl , f l ,  etc) needed for the scat and SF (types of  323  Computational Linguistics Volume 22, Number 3  booZ_comb_value f ature).	SF	Sentence Formalism$Syntactic Features$segmentation F$Segment frequency$Slot filling$SameFrame$short form$subcat features$	7
Edges with  SF are combined with other elements  in the chart in accordance with general combinatory  schemata.	SF	Sentence Formalism$Syntactic Features$segmentation F$Segment frequency$Slot filling$SameFrame$short form$subcat features$	7
has 2 correct sub- cat frames but receives 8 predictions which differ only in the SF.	SF	Sentence Formalism$Syntactic Features$segmentation F$Segment frequency$Slot filling$SameFrame$short form$subcat features$	7
However, the cor- rect SF for it are intransitive and sbar.	SF	Sentence Formalism$Syntactic Features$segmentation F$Segment frequency$Slot filling$SameFrame$short form$subcat features$	7
We extracted the SF from their type definitions in the Alpino lexicon to cre- ate a gold standard of subcat frames.	SF	Sentence Formalism$Syntactic Features$segmentation F$Segment frequency$Slot filling$SameFrame$short form$subcat features$	7
10) Intuitively, the above link confidence definition compares the lexical translation probability of the aligned word pair with the translation probabilities of all the TW given the source word.	TW	target words$twlist$trigger word$tweets$	0
J} corresponds to a sequence of source word positions, where J is the source sentence length, and with null representing un- aligned TW.	TW	target words$twlist$trigger word$tweets$	0
Baseline: For the baseline system, we applied  two different algorithms for sentences which  have opinion-bearing verbs as TW and  for those that have opinion-bearing adjectives as  TW.	TW	target words$twlist$trigger word$tweets$	0
IBM Models 3, 4, and 5 attempt to capture fertility (the tendency of each source word to generate several TW), resulting in probabilis- tically deficient, intractable models that require local heuristic search and are difficult to implement and extend.	TW	target words$twlist$trigger word$tweets$	0
 Non-bijective: Multiple TW can be linked to a single source word.	TW	target words$twlist$trigger word$tweets$	0
The reason this happens is that the generative model has to distribute translation probability for each source word among different candidate TW.	TW	target words$twlist$trigger word$tweets$	0
Subsequently, we trained several sequence taggers in order to identify the TWs in text.	TW	target words$twlist$trigger word$tweets$	2
In stage (2), we generate relations between a TW and one or more proteins, while in stage (3), we look for complex in- teractions between simple events, TWs and proteins.	TW	target words$twlist$trigger word$tweets$	2
Our approach is based on a three-stage clas- sification: (1) TW tagging, (2) sim- ple event extraction, and (3) complex event extraction.	TW	target words$twlist$trigger word$tweets$	2
In our experiments, we found that simultaneously identifying TWs and the event types they trigger yielded low recall; thus, we settled on	TW	target words$twlist$trigger word$tweets$	2
In our experiments, we found that simultaneously identifying TWs and the event types they trigger yielded low recall; thus, we settled on identifying TWs in text as one kind of entity, regardless of event types.	TW	target words$twlist$trigger word$tweets$	2
Our system is based on a three-stage classification process: (1) TW tagging using a linear se- quence model, (2) simple event extraction, and (3) complex event extraction.	TW	target words$twlist$trigger word$tweets$	2
media, such as chat texts, SMS and TW.	TW	target words$twlist$trigger word$tweets$	3
2013) likewise cluster TW using K-means but predict location only at the country level.	TW	target words$twlist$trigger word$tweets$	3
Analysis of text to estimate affect or sentiment is a relatively recent research topic that has at- tracted great interest, as reflected by a series of shared evaluation tasks, e.g., analysis of news headlines (Strapparava and Mihalcea, 2007) and TW (Nakov et al.,	TW	target words$twlist$trigger word$tweets$	3
Dis- advantages of these methods are the dependency on time-specific population data, making them un- suitable for some corpora (e.g. 19th-century doc- uments); the difficulty in adjusting grid resolution in a principled fashion; and the fact that not all documents are near a city (Han14 find that 8% of TW are ?	TW	target words$twlist$trigger word$tweets$	3
Like- wise, TW in Twitter are often geotagged; in this case, it is possible to view either an individual tweet or the collection of TW for a given user as a document, respectively identifying the loca- tion as the place from which the tweet was sent or the home location of the user.	TW	target words$twlist$trigger word$tweets$	3
Notice that we normalize the LD by the length of the lexicon element, as we em- pirically found it performing better compared with normalizing by the length of the segment.	LD	lab-dental$linear dominance)$Levenshtein distance$Labeled Data$Large Dataset$	2
if one of the words to be compared is not in WordNet, their similarity is calculated using the LD.	LD	lab-dental$linear dominance)$Levenshtein distance$Labeled Data$Large Dataset$	2
Lev(xuj :vj , l)/|l| (4) where Lev represents the LD.	LD	lab-dental$linear dominance)$Levenshtein distance$Labeled Data$Large Dataset$	2
This is a dynamic programming algorithm sim- ilar to computing LD between two strings, except the cost function used to compute the match between tokens is as shown below.	LD	lab-dental$linear dominance)$Levenshtein distance$Labeled Data$Large Dataset$	2
The most common definition of distance be- tween two strings is the LD, also known as edit distance (ED).	LD	lab-dental$linear dominance)$Levenshtein distance$Labeled Data$Large Dataset$	2
The typical sentences computed as the centroids of the clusters created using Algorithm 2 are them- selves color coded using the same identification sys- 1We have also experimented with a symmetric version of LD, but we prefer the n-gram overlap score due to its linear run time complexity.	LD	lab-dental$linear dominance)$Levenshtein distance$Labeled Data$Large Dataset$	2
Dis- criminative Induction of Sub-Tree Alignment using  Limited LD.	LD	lab-dental$linear dominance)$Levenshtein distance$Labeled Data$Large Dataset$	3
Text Mining Techniques for Leveraging Positively LD.	LD	lab-dental$linear dominance)$Levenshtein distance$Labeled Data$Large Dataset$	3
LD For experiments within the paral- lel text, we manually labeled 1320 of the 94K co- ordinate NP examples.	LD	lab-dental$linear dominance)$Levenshtein distance$Labeled Data$Large Dataset$	3
Distant Supervision for Relation Ex- traction Without LD.	LD	lab-dental$linear dominance)$Levenshtein distance$Labeled Data$Large Dataset$	3
Variable Possible Values Domain (D) {Movie Review classifica- tion (MR), Webpage classi- fication (WebKB)} Instance Granu- larity (G) {document (doc), sentence (sent)} Feature Space (F ) {unigram only (u), uni- gram+dependency (u+d)} LD (#AUs) (L) {64, 128, 256, 512, 1024} Irrelevant Text (I) {0, 200, 400, 600,? }	LD	lab-dental$linear dominance)$Levenshtein distance$Labeled Data$Large Dataset$	3
Limitations of Co-Training for Natural Language Learning from LDs.	LD	lab-dental$linear dominance)$Levenshtein distance$Labeled Data$Large Dataset$	4
5.2 Result on LD Table 2 shows the translation results.	LD	lab-dental$linear dominance)$Levenshtein distance$Labeled Data$Large Dataset$	4
Eliminating  Class Noise in LDs.	LD	lab-dental$linear dominance)$Levenshtein distance$Labeled Data$Large Dataset$	4
The other is the Stanford LD 2 (Maas et al 2011), a collection of 50,000 comments from IMDB, evenly divided into training and test sets.	LD	lab-dental$linear dominance)$Levenshtein distance$Labeled Data$Large Dataset$	4
4 Using LDs Effectively In the previous section, we found our crowdsourced data was good at predicting AAC-like test sets.	LD	lab-dental$linear dominance)$Levenshtein distance$Labeled Data$Large Dataset$	4
6.2 LD Experiment The large experiment considers 968 English words (468,028 pairs) over a range of sampling rates.	LD	lab-dental$linear dominance)$Levenshtein distance$Labeled Data$Large Dataset$	4
While using UD (Nivre et al, 2016) could potentially simplify porting the rules, we chose not to investigate this option due to the on- going nature of the project and focused on the estab- lished representations for now.	UD	Universal Dependencies$underspecified domain$Unlabeled Data$	0
4.1 Experimental Setup We first investigated our model on 19 lan- guages from the UD Tree- banks v1.2.	UD	Universal Dependencies$underspecified domain$Unlabeled Data$	0
3 74.5 80.4 78.1 76.2 Pipeline P tag 73.7 83.6 72.0 73.0 79.3 79.5 63.0 78.0 66.9 78.5 87.8 73.5 84.2 75.4 70.3 83.6 73.4 79.5 79.4 76.6 RBGParser 75.8 83.6 73.9 73.5 79.9 79.6 68.0 78.5 65.4 78.9 87.7 74.2 84.7 77.6 72.4 83.9 75.4 81.3 80.7 77.6 Stackprop 77.0 84.3 73.8 74.2 80.7 80.7 70.1 78.5 74.5 80.0 88.9 74.1 85.8 77.5 73.6 84.7 79.2 80.4 81.8 78.9 Table 2: Labeled Attachment Score (LAS) on UD Treebank.	UD	Universal Dependencies$underspecified domain$Unlabeled Data$	0
We evaluate our approach on 19 languages from the UD treebank in Section 4.	UD	Universal Dependencies$underspecified domain$Unlabeled Data$	0
UD v1: A Multilingual Treebank Collection.	UD	Universal Dependencies$underspecified domain$Unlabeled Data$	0
On 19 lan- guages from the UD, our method is 1.3% (absolute) more accu- rate than a state-of-the-art graph-based ap- proach and 2.7% more accurate than the most comparable greedy model.	UD	Universal Dependencies$underspecified domain$Unlabeled Data$	0
then the description S used to instantiate the UD is the conjunction of the properties in the description SD with the value of the differentiation	UD	Universal Dependencies$underspecified domain$Unlabeled Data$	1
V } 3: for all U(N, t) sorted by Givenness do 4: if U(N, t) matches D then 5: restructure(D, N , RS) 6: return U(N, t) 7: end if 8: end for 9: return failure Figure 2: Reference algorithms, relying on the same UDs 5 Example In this section we present the interpretation side of some expressions we generated in the GIVE set- ting (table 2).	UD	Universal Dependencies$underspecified domain$Unlabeled Data$	1
then the description S used to instantiate the UD is the conjunction of the properties in the description SD with the value of the differentiation criterion used to create the partition namely, properties of val(c) true of the referent (line 2).	UD	Universal Dependencies$underspecified domain$Unlabeled Data$	1
hich refer- ence domain, referring will be processed and thereby, which description will be used for instantiating the UDs.	UD	Universal Dependencies$underspecified domain$Unlabeled Data$	1
Then this domain is used to match a so called UD (Salmon-Alt and Romary, 2001).	UD	Universal Dependencies$underspecified domain$Unlabeled Data$	1
In D0, the first matching UD is the definite ?	UD	Universal Dependencies$underspecified domain$Unlabeled Data$	1
The first step (line 1?2) determines in which refer- ence domain, referring will be processed and thereby, which description will be used for instantiating the UDs.	UD	Universal Dependencies$underspecified domain$Unlabeled Data$	1
Self-taught Learn- ing : Transfer Learning from UD.	UD	Universal Dependencies$underspecified domain$Unlabeled Data$	2
7 Previous Work: Combining Labeled and UD The two-step procedure used in our Co-Training method for statistical parsing was incipient in the SuperTag- ger (Srinivas, 1997) which is a statistical model for tag- ging sentences with elementary lexicalized structures.	UD	Universal Dependencies$underspecified domain$Unlabeled Data$	2
Detection of Agreement vs. Disagreement in  Meetings: Training with UD.	UD	Universal Dependencies$underspecified domain$Unlabeled Data$	2
Enhancing Chinese Word Segmentation Using UD.	UD	Universal Dependencies$underspecified domain$Unlabeled Data$	2
c?2006 Association for Computational Linguistics Agreement/Disagreement Classication: Exploiting UD using Contrast Classiers Sangyun Hahn Richard Ladner Dept.	UD	Universal Dependencies$underspecified domain$Unlabeled Data$	2
03, Workshop on the Continuum from Labeled to UD in Machine Learning and Data Mining.	UD	Universal Dependencies$underspecified domain$Unlabeled Data$	2
The speech corpora consist of one MS sponta- neous corpus, containing twenty segments and totaling fty minutes, and one read corpus of ve segments, read by a single speaker and to- taling eleven minutes of speech.	MS	multi-speaker$Missed Samples$minimum statistics$Marecek and Straka$Mel?c?uk style$Mel?cuk style$	0
We tried to restrict ourselves to features whose inclusion is motivated by previous work (pauses, speech rate) and added features that are specific to MS speech (overlap, changes in speaker activity).	MS	multi-speaker$Missed Samples$minimum statistics$Marecek and Straka$Mel?c?uk style$Mel?cuk style$	0
Guest actors and MS turns are not considered in the gen- der analysis.	MS	multi-speaker$Missed Samples$minimum statistics$Marecek and Straka$Mel?c?uk style$Mel?cuk style$	0
In the future, we hope  to further enlarge our MS database.	MS	multi-speaker$Missed Samples$minimum statistics$Marecek and Straka$Mel?c?uk style$Mel?cuk style$	0
Performance measures used to date within the DARPA spo-  ken language research community (and included in this  paper) do not conform to the recommended approach, since  the scoring software, in general, generates a single measure-  merit for the ensemble of test data (e.g., one datum indicat-  ing word or utterance error rate for the entire MS,  multi-utterance, t st subset, rather than the mean error rate  for the ensemble of speakers).	MS	multi-speaker$Missed Samples$minimum statistics$Marecek and Straka$Mel?c?uk style$Mel?cuk style$	0
Towards MS unsupervised speech pattern discovery.	MS	multi-speaker$Missed Samples$minimum statistics$Marecek and Straka$Mel?c?uk style$Mel?cuk style$	0
8 In an experiment, we used the MS (2013)?s parser as an initializer for the Gillenwater et al (2011)?s parser.	MS	multi-speaker$Missed Samples$minimum statistics$Marecek and Straka$Mel?c?uk style$Mel?cuk style$	3
5.3 Tuning multi-phase IR Because MS (2013)?s parser does not distinguish training data from test data, we pos- tulate S 0 = S 1 .	MS	multi-speaker$Missed Samples$minimum statistics$Marecek and Straka$Mel?c?uk style$Mel?cuk style$	3
The contribution of Iterated Reranking We compare the quality of the treebank resulted in the end of phase 1 against the quality of the treebank given by the initialier MS (2013).	MS	multi-speaker$Missed Samples$minimum statistics$Marecek and Straka$Mel?c?uk style$Mel?cuk style$	3
We then search for what IR (with the MaxEnc op- tion) contributes to the overall performance by com- paring the quality of the treebank resulted in the end of phase 1 against the quality of the treebank given by its initialier, i.e. MS (2013).	MS	multi-speaker$Missed Samples$minimum statistics$Marecek and Straka$Mel?c?uk style$Mel?cuk style$	3
The position and shape of N(D i ) is thus deter- mined by two factors: how well P i can fit D i , and k. Intuitively, the lower the fitness is, the more N(D i ) goes far away fromD i ; and the larger k is, the larger 3 MS (2013) did not report any experimental result on the WSJ corpus.	MS	multi-speaker$Missed Samples$minimum statistics$Marecek and Straka$Mel?c?uk style$Mel?cuk style$	3
Naseem and Barzilay (2011), Tu and Honavar (2012), Spitkovsky et al (2012), Spitkovsky et al (2013), and MS (2013) employ ex- tensions of the DMV but with different learning strategies.	MS	multi-speaker$Missed Samples$minimum statistics$Marecek and Straka$Mel?c?uk style$Mel?cuk style$	3
e conjunctions (or a punctuation fulfilling its role); in the Moscow family, the con- juncts form a chain where each node in the chain depends on the previous (or following) node; in the Stanford family, the conjuncts are siblings ex- cept for the first (or last) conjunct, which is the 10Names are chosen purely as a mnemonic device, so that Prague Dependency Treebank belongs to the Prague family, MS belongs to the Moscow family, and Stanford parser style belongs to the Stanford family.	MS	multi-speaker$Missed Samples$minimum statistics$Marecek and Straka$Mel?c?uk style$Mel?cuk style$	4
taient ADJ pendant environ 2 mois, tandis que les corpus de test devaient ?	ADJ	disponibles$adjectives$Adjective$ad jec t ives$	0
Il  s'agit  de cr?er et rendre ADJ de nouvelles ressources aux chercheurs  en  TAL,  d'une  part  et  de  d'?quiper  les  langues  africaines  de  ressources num?riques nouvelles et indispensable ?	ADJ	disponibles$adjectives$Adjective$ad jec t ives$	0
et une interface d??dition sp?cifique pour contribuer directement aux dictionnaires ADJ sur la plate-forme.	ADJ	disponibles$adjectives$Adjective$ad jec t ives$	0
lectroniques ADJ sont rares, mal distribu?es, voire inexistantes.	ADJ	disponibles$adjectives$Adjective$ad jec t ives$	0
Les dictionnaires seront donc ADJ sur Internet d'ici la fin de l'ann?e 2012 sous licence Creative Commons. ?	ADJ	disponibles$adjectives$Adjective$ad jec t ives$	0
Les sous-objets de cet objet sont alors  ADJ pour de nouvelles ilff6rences.	ADJ	disponibles$adjectives$Adjective$ad jec t ives$	0
We now justify each of the choices: why ADJ, why clustering, and why shallow features.	ADJ	disponibles$adjectives$Adjective$ad jec t ives$	1
Our aim is to estab- lish semantic classes for ADJ in Catalan by means of clustering, using only shallow syntactic evidence.	ADJ	disponibles$adjectives$Adjective$ad jec t ives$	1
8 08003 Barcelona eloi@iua.upf.es Abstract In this paper, we present a clustering exper- iment directed at the acquisition of semantic classes for ADJ in Catalan, using only shallow distributional features.	ADJ	disponibles$adjectives$Adjective$ad jec t ives$	1
In this paper, we con- centrate on ADJ, which have received less at- tention (see though Hatzivassiloglou and McKeown (1993) and Lapata (2001)).	ADJ	disponibles$adjectives$Adjective$ad jec t ives$	1
We define a broad-coverage classification for ADJ based on Ontological Semantics.	ADJ	disponibles$adjectives$Adjective$ad jec t ives$	1
pronouns 12 count of quote tokens 13 count of 1st person plural pronouns 14 count of 2nd person singular pronouns 15 count of quote positive words 16 count of quote negative words 17 count of nouns 18 count of verbs 19 count of ADJ 20 count of adverbs 21 up to 3-grams extracted from quote Table 1: Common feature set.	ADJ	disponibles$adjectives$Adjective$ad jec t ives$	1
Predicting the semantic ori-  entation of ADJ.	ADJ	disponibles$adjectives$Adjective$ad jec t ives$	1
Major type Minor type  Kanji Reading, Writing, Radical, The  order of writing, Classification  Word knowl- edge  Katakana, How to use Kana, Ap- propriate Noun, To fill blanks for  Verb, ADJ, and Adjunct,  Synonym, Antonym, Particle,  Conjunction, Onomatopoeia, Po- lite Expression, Punctuation mark Reading com- prehension  Who, What, When, Where, How,  Why question, Extract specific  phrases, Progress order of a story,  Prose and Verse   Composition Constructing sentence, How to  write composition  Table 1.	ADJ	disponibles$adjectives$Adjective$ad jec t ives$	2
1,2 V, N Sem Generation Spelling correction 1,2,3 Any Syn/Sem Generation ADJ ordering 1,2 Adj Sem Generation Compound bracketing 1,2 N Syn Analysis Compound interpret.	ADJ	disponibles$adjectives$Adjective$ad jec t ives$	2
ADJs which have an event component in their meaning (event adjectives for short) denote a state that is directly dependent on an event, be it simultaneous or previous to the state.	ADJ	disponibles$adjectives$Adjective$ad jec t ives$	2
ADJs are predicates, equivalent to verbs when appearing in predicative environments.	ADJ	disponibles$adjectives$Adjective$ad jec t ives$	2
This parameter can again be used for basic tasks such as POS-tagging: ADJ-noun ambiguity is notoriously the most difficult one to solve, and the ordering restrictions on the classes of adjectives can help to reduce it.	ADJ	disponibles$adjectives$Adjective$ad jec t ives$	2
701,3   Acquisition of Semantic Classes for ADJs from Distributional Evidence Gemma Boleda GLiCom Universitat Pompeu Fabra La Rambla 30-32 08002 Barcelona gemma.boleda@upf.edu Toni Badia GLiCom Universitat Pompeu Fabra La Rambla 30-32 08002 Barcelona toni.badia@upf.edu Eloi Batlle Audiovisual Institute Universitat Pompeu Fabra Pg.	ADJ	disponibles$adjectives$Adjective$ad jec t ives$	2
Some words,  such as  =oat ADJ   and adverbs ,  do not bu i ld  s t ruc tures  but ra ther  modi fy   s t ruc tures  bu i l t  by other  words.	ADJ	disponibles$adjectives$Adjective$ad jec t ives$	3
I f  John is n o t  s ing le ,  then John is  married,   j?"his kind of r e l a t i o n  seems t o  hold pr imari ly  between two ADJ  o r   two adverbs belonging t o  t h e  same pr imi t ive  concept.	ADJ	disponibles$adjectives$Adjective$ad jec t ives$	3
The arguments can apparently be verbs, ADJ  o r   Rouns.	ADJ	disponibles$adjectives$Adjective$ad jec t ives$	3
This c lass  covers  most ADJ  and adverbs .	ADJ	disponibles$adjectives$Adjective$ad jec t ives$	3
Action verbs and adjectives can appear in imperative  sentences but  non-action verbs and ADJ  cannot.	ADJ	disponibles$adjectives$Adjective$ad jec t ives$	3
This c lass is  qu i te   substant ia l ,  Inc luding many du l l  nouns and near ly  a l l   ADJ  and adverbs .	ADJ	disponibles$adjectives$Adjective$ad jec t ives$	3
8 08003 Barcelona eloi@iua.upf.es Abstract In this paper, we present a clustering exper- iment directed at the acquisition of semantic classes for adJJs in Catalan, using only shallow distributional features.	JJ	jective$Japanese Kanj$	0
We define a broad-coverage classification for adJJs based on Ontological Semantics.	JJ	jective$Japanese Kanj$	0
To that end, we  devised a more obJJ test, useful only for  scoring the subset of referents that are names  of people.	JJ	jective$Japanese Kanj$	0
pronouns 12 count of quote tokens 13 count of 1st person plural pronouns 14 count of 2nd person singular pronouns 15 count of quote positive words 16 count of quote negative words 17 count of nouns 18 count of verbs 19 count of adJJs 20 count of adverbs 21 up to 3-grams extracted from quote Table 1: Common feature set.	JJ	jective$Japanese Kanj$	0
701,3   Acquisition of Semantic Classes for AdJJs from Distributional Evidence Gemma Boleda GLiCom Universitat Pompeu Fabra La Rambla 30-32 08002 Barcelona gemma.boleda@upf.edu Toni Badia GLiCom Universitat Pompeu Fabra La Rambla 30-32 08002 Barcelona toni.badia@upf.edu Eloi Batlle Audiovisual Institute Universitat Pompeu Fabra Pg.	JJ	jective$Japanese Kanj$	0
Predicting the semantic ori-  entation of adJJs.	JJ	jective$Japanese Kanj$	0
It can  be a Chinese character, a sub-string of English  words, a Korean Hangual or a JJi or  several Japanese Katakanas.	JJ	jective$Japanese Kanj$	1
Site?ID (h) Japanese transliterated to JJi Figure 2: Accuracy in top-1, F -score and MRR for standard runs.	JJ	jective$Japanese Kanj$	1
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8Accuracy in top-10 0.10.2 0.30.4 0.50.6 0.70.8 0.91 F-score English to ChineseEnglish to HindiEnglish to TamilEnglish to KannadaEnglish to RussianEnglish to KoreanEnglish to JapaneseJapanese transliterated to JJi Figure 4: Accuracy in top-1 vs. F -score for dif- ferent tasks.	JJ	jective$Japanese Kanj$	1
glish Tamil Microsoft Research India 7,974 987 1,000 EnTa English Kannada Microsoft Research India 7,990 968 1,000 EnKa English Russian Microsoft Research India 5,977 943 1,000 EnRu English Chinese Institute for Infocomm Research 31,961 2,896 2,896 EnCh English Korean Hangul CJK Institute 4,785 987 989 EnKo English Japanese Katakana CJK Institute 23,225 1,492 1,489 EnJa Japanese name (in English) JJi CJK Institute 6,785 1,500 1,500 JnJk Table 1: Source and target languages for the shared task on transliteration.	JJ	jective$Japanese Kanj$	1
Site?ID (h) Japanese transliterated to JJi Figure 3: MAPref , MAP10 and MAPsys scores for standard runs.	JJ	jective$Japanese Kanj$	1
First is previous Korean  Hanja, JJi (Chinese characters in  Japanese language) and Chinese Pinyin input  methods, the second one is English-Korean  transliteration.	JJ	jective$Japanese Kanj$	1
Therefore, in our scenario we would like to place a sparsifying ` 1 regularizer over the contextual (interaction) features while still leveraging the squared ` 2 -norm penalty for the standard BOW (BoW) features.	BOW	bag-of-words$bag of words$	0
For text input, we use raw BOW as features, with different vocabularies for the state side and action side.	BOW	bag-of-words$bag of words$	0
The system is basically an information re- trieval system which selects a sentence, instead  of a document, based on the BOW  method.	BOW	bag-of-words$bag of words$	0
Shown are the micro-averaged F 1 scores for a BOW baseline, a system trained on Polyglot embeddings, the two strongest systems of Hermann and Blunsom (2014), and our Joint w/ Aux system with ` 1 and ` 2 regularization.	BOW	bag-of-words$bag of words$	0
The obvious approach to our unsupervised ver- sion of the problem would be to segment the text  (if necessary), represent each of the resulting units  of text as a BOW, and then use clustering  algorithms to find natural clusters.	BOW	bag-of-words$bag of words$	0
Shown are histograms with smoothed kernel density estimates of differences in recall and precision between the baseline BOW based approach and each feature space/method (one per row).	BOW	bag-of-words$bag of words$	0
We frame the classification task as follows: The input features to the classifier are the words in the blog post i.e each blog post is treated as a BOW and the output variable is the binary comment polarity computed in the previ- 14 SentiWordNet PMI Blog SVM sLDA SVM sLDA cb 0.56 0.58 0.79 0.79 dk 0.61 0.64 0.75 0.77 my 0.67 0.59 0.87 0.87 rs 0.53 0.55 0.74 0.76 rwn 0.57 0.59 0.90 0.90 Table 3: Accuracy: Using blog posts to predict comment sentiment polarity ous section.	BOW	bag-of-words$bag of words$	1
Context word features record the presence of a word within a fixed window around the tar- get word (BOW); collocational features capture the syntactic environment of the target word and are usu- ally represented by a small number of words and/or part- of-speech tags to the left or right of the target word.	BOW	bag-of-words$bag of words$	1
Making the most of BOW: Sentence regularization with alter- nating direction method of multipliers.	BOW	bag-of-words$bag of words$	1
Near queries use Altavista?s NEAR operator to ex- pand the n-gram; a NEAR b means that a has to oc- cur in the same ten word window as b; the window is treated as a BOW (e.g., history changes expands to "history" NEAR "changes").	BOW	bag-of-words$bag of words$	1
Each blog post is then represented as a BOW from the post.	BOW	bag-of-words$bag of words$	1
We are also interested in studying how to conduct queries not as a BOW but bind by syntactic relations (Wilson et al, 2005).	BOW	bag-of-words$bag of words$	1
The dotted line cor- responds to a random choice out of fourteen items and to the perplexity of a HM trained on the corpus.	HM	histogram model$Harabagiu and Maiorano$head match$	0
HM  (1999) argued that indexing in question answering  should be based on 1)aragraphs.	HM	histogram model$Harabagiu and Maiorano$head match$	1
HM (2000) and Postolache et al (2006) projected English corpora to Roma- nian to bootstrap human annotation, either manu- ally or via automatic alignments.	HM	histogram model$Harabagiu and Maiorano$head match$	1
HM (2000) designed a CR system for English-Romanian bitexts while Mitkov and Barbu (2003) focused on the English-French language pair.	HM	histogram model$Harabagiu and Maiorano$head match$	1
Also, studies by  Harabagiu, Moldovan, and Maiorano (HM 1996; Harabagiu and  Moldovan 1999) show that cohesion can be used to determine rhetorical relations that  hold between smaller discourse constituents as well.	HM	histogram model$Harabagiu and Maiorano$head match$	1
SWIZZLE is a multilingual en-  hancement of COCKTAIL (HM,  1999), a coreference r solution system that operates  on a mixture of heuristics that combine semantic  and textual cohesive information 2.	HM	histogram model$Harabagiu and Maiorano$head match$	1
There are also systems aimed at extracting par- tial knowledge from texts, by either filling seman- tic templates (Hobbs et al, 1996) or by generating a set of linguistic patterns for information extrac- tion (HM, 2000), to name but a few.	HM	histogram model$Harabagiu and Maiorano$head match$	1
Proper HM: Two proper names have the same head, and they do not contain numeric or location pre-modifiers.	HM	histogram model$Harabagiu and Maiorano$head match$	2
Compatible HM: Two mentions have the same head, and the pre-modifiers of the anaphor are a subset of the pre-modifiers of the antecedent.	HM	histogram model$Harabagiu and Maiorano$head match$	2
The Stanford system is the winner of the CoNLL2011 shared 650 OntoNotes-Dev Same speaker > Compatible HM > Substring > String match > Proper HM > Acronym ACE2004-nwire Compatible HM > Substring > Proper HM > String match > Demonym > Apposition > Same speaker > Role apposition > Relative pronoun > Acronym > Predicate nominative Table 3: The resulting ranking of informativeness scores on different data sets.	HM	histogram model$Harabagiu and Maiorano$head match$	2
The routing information (context vector,  Boolean terms and match threshold) is stored in a  table along with the UID of the route.	UID	user ID$Uniform Information Density$user identity$	0
URL harvesting: social network traversal, obvious spam and non-text documents filter- ing, optional spell check of the short message to see if it could be English text, optional record of UIDs for later crawls.	UID	user ID$Uniform Information Density$user identity$	0
In the re- sulting list of tokens, all UIDs and web URLs are replaced with placeholders.2 Any remaining punctu- ation is stripped from the tokens and empty tokens are deleted.	UID	user ID$Uniform Information Density$user identity$	0
The request includes the source sen- tence, source and target language, and optionally a UID.	UID	user ID$Uniform Information Density$user identity$	0
kish 1,415 7.2 German 1,289 6.6 Spanish 954 4.9 French 703 3.6 Italian 658 3.4 Portuguese 357 1.8 Arabic 263 1.3 Table 2: 10 most frequent languages of spell- check-filtered URLs gathered on FriendFeed 4.2 identi.ca The results of the two strategies followed on identi.ca led to a total of 1,113,783 URLs checked for redirection, which were collected in about a week (the deep crawler reached 37,485 UIDs).	UID	user ID$Uniform Information Density$user identity$	0
Among others, we defined: a) userID, which is matched against a list of known UIDs to select a UM profile for answer extraction (see Section 5); b) the current query, which is used to dynamically up- date the stack of recent user questions used by the clar- ification request detection module to perform reference resolution; c) the topic of conversation, i.e. the keywords of the last question issued by the user which received an an- swer.	UID	user ID$Uniform Information Density$user identity$	0
The second is the UID hypothesis (Jaeger and Levy, 2006; Jaeger, 2010), as it applies to syntactic structure.	UID	user ID$Uniform Information Density$user identity$	1
However, we observe that priors based on UID (e.g the ?	UID	user ID$Uniform Information Density$user identity$	2
Comparison with it allows us to check the usefulness of UID in the task.	UID	user ID$Uniform Information Density$user identity$	2
Our model is a principled generative latent variable model which captures three important factors: view- point specific topic preference, UID and user interactions.	UID	user ID$Uniform Information Density$user identity$	2
They assume these two concepts are orthogonal and they do not con- sider UID.	UID	user ID$Uniform Information Density$user identity$	2
The novelty of our approach is to leverage UID, allowing us to construct a corpus of language-labeled Twitter messages without using automated tools to determine the languages of the messages.	UID	user ID$Uniform Information Density$user identity$	2
4 Analysis While the main goal of this paper is to document the AAWD corpus, we also performed several sta- tistical analyses of authority and alignment, in or- der to demonstrate the relevance of these social acts as markers of UID and social dynamics within our corpus.	UID	user ID$Uniform Information Density$user identity$	2
SA, the biggest oil pro- ducer in the world, was once a sup- porter of Osama bin Laden and his associates who led attacks against the United States.?	SA	Saudi Arabia$Sentiment analysis$Sentiment Analysis$speaker adaptive$sentence  adverbs$speecb act$sentiment analysis$statistical analysis$Spreading Activation$	0
Most Compositional labour union, tax authority, health council, market counterparty, employment policy Most Lexical study design, family motto, wood shaving, avoidance behaviour, smash hit Most Lexical (including Proper Nouns) Vo Quy, Bonito Oliva, Mamur Zapt, Evander Holyfield, SA Table 5: Top lexical and compositional nouns for the BIN-CMPD model using Ilex(c) ent impact of both constituents.	SA	Saudi Arabia$Sentiment analysis$Sentiment Analysis$speaker adaptive$sentence  adverbs$speecb act$sentiment analysis$statistical analysis$Spreading Activation$	0
Arabic language is   the official language in all Arab nations as  Egypt, SA and Algeria.	SA	Saudi Arabia$Sentiment analysis$Sentiment Analysis$speaker adaptive$sentence  adverbs$speecb act$sentiment analysis$statistical analysis$Spreading Activation$	0
The Arabic Online Commentary dataset that we created was based on reader commentary from the online versions of three Arabic newspapers: Al- Ghad from Jordan, Al-Riyadh from SA, and Al-Youm Al-Sabe?	SA	Saudi Arabia$Sentiment analysis$Sentiment Analysis$speaker adaptive$sentence  adverbs$speecb act$sentiment analysis$statistical analysis$Spreading Activation$	0
He collected Arabizi words from tweets and trained a character-level language 52 Reference Year Location Participants Data Size of Data Arabizi English Arabic (Keong et al, 2015) 2015 Malysia 20 Arab Post Graduates SMS 200 Messages 35% 50% 10% (Bies et al, 2014) 2014 Egypt 26 Native Arabic Speakers SMS 101,292 Messages 77% - 23% (Alabdulqader et al, 2014) 2014 SA 61 Students and Non-students SMS, BBM, and Whatsapp 3236 Messages 15% 8% 74% (Bianchi, 2012) 2012 Jordan - Online Forum 460,220 Posts 35.5% 17.5% 32% (Al-Khatib and Sabbah, 2008) 2008 Jordan 46 Students SMS 181 Messages 37% 54% 9% Table 1: Percentage of Arabizi Usage in Related Work model and a statistical sequence labelling algo- rithm.	SA	Saudi Arabia$Sentiment analysis$Sentiment Analysis$speaker adaptive$sentence  adverbs$speecb act$sentiment analysis$statistical analysis$Spreading Activation$	0
SA is the world?s biggest oil exporter.?	SA	Saudi Arabia$Sentiment analysis$Sentiment Analysis$speaker adaptive$sentence  adverbs$speecb act$sentiment analysis$statistical analysis$Spreading Activation$	0
2.2 Sentiment Analysis SA in computational linguistics has focused on examining what textual features (lexi- cal, syntactic, punctuation, etc) contribute to affec- tive content of text and how these features can be detected automatically to derive a sentiment metric for a word, sentence or whole text.	SA	Saudi Arabia$Sentiment analysis$Sentiment Analysis$speaker adaptive$sentence  adverbs$speecb act$sentiment analysis$statistical analysis$Spreading Activation$	1
Semeval 2013 task 2: SA in twitter.	SA	Saudi Arabia$Sentiment analysis$Sentiment Analysis$speaker adaptive$sentence  adverbs$speecb act$sentiment analysis$statistical analysis$Spreading Activation$	1
SA: SA deter- mines positive or negative opinions expressed on  topics (Liu, 2012; Pang and Lee, 2008).	SA	Saudi Arabia$Sentiment analysis$Sentiment Analysis$speaker adaptive$sentence  adverbs$speecb act$sentiment analysis$statistical analysis$Spreading Activation$	1
SA is often  used to extract the opinions in blog pages.	SA	Saudi Arabia$Sentiment analysis$Sentiment Analysis$speaker adaptive$sentence  adverbs$speecb act$sentiment analysis$statistical analysis$Spreading Activation$	1
A sentimental educa- tion: SA using subjectivity summa- rization based on minimum cuts.	SA	Saudi Arabia$Sentiment analysis$Sentiment Analysis$speaker adaptive$sentence  adverbs$speecb act$sentiment analysis$statistical analysis$Spreading Activation$	1
c?2012 Association for Computational Linguistics Social Event Radar: A Bilingual Context Mining and SA  Summarization System      Wen-Tai Hsieh Chen-Ming Wu  Department of IM,   National Taiwan University  Institute for Information Industry  wentai@iii.org.tw cmwu@iii.org.tw     Tsun Ku Seng-cho T. Chou  Institute for Information Industry Department of IM,   National Taiwan University  cujing@iii.org.tw chou@im.ntu.edu.tw          Abstract  Social Event Radar is a new social  net	SA	Saudi Arabia$Sentiment analysis$Sentiment Analysis$speaker adaptive$sentence  adverbs$speecb act$sentiment analysis$statistical analysis$Spreading Activation$	2
1331  Proceedings of the 3rd Workshop on Computational Approaches to Subjectivity and SA, pages 99?103, Jeju, Republic of Korea, 12 July 2012.	SA	Saudi Arabia$Sentiment analysis$Sentiment Analysis$speaker adaptive$sentence  adverbs$speecb act$sentiment analysis$statistical analysis$Spreading Activation$	2
c?2012 Association for Computational Linguistics Multimodal SA (Abstract of Invited Talk) Rada Mihalcea Department of Computer Science and Engineering University of North Texas P. O. Box 311366 Denton, TX 76203-6886, U.S.A. rada@cs.unt.edu Abstract With more than 10,000 new videos posted online every day on social websites such as YouTube and Facebook, the internet is be- coming an almost infinite source of informa- tion.	SA	Saudi Arabia$Sentiment analysis$Sentiment Analysis$speaker adaptive$sentence  adverbs$speecb act$sentiment analysis$statistical analysis$Spreading Activation$	2
A Sentimental Education:  SA Using Subjectivity  Summarization Based on Minimum Cuts.	SA	Saudi Arabia$Sentiment analysis$Sentiment Analysis$speaker adaptive$sentence  adverbs$speecb act$sentiment analysis$statistical analysis$Spreading Activation$	2
c?2012 Association for Computational Linguistics Multimodal SA (Abstract of Invited Talk) Rada Mihalcea Department of Computer Science and Engineering University of North Texas P. O. Box 311366 Denton, TX 76203-6886, U.S.A. rada@cs.unt.edu Abstract With more than 10,000 new videos posted online every day on social w	SA	Saudi Arabia$Sentiment analysis$Sentiment Analysis$speaker adaptive$sentence  adverbs$speecb act$sentiment analysis$statistical analysis$Spreading Activation$	2
35  Proceedings of the 3rd Workshop on Computational Approaches to Subjectivity and SA, page 1, Jeju, Republic of Korea, 12 July 2012.	SA	Saudi Arabia$Sentiment analysis$Sentiment Analysis$speaker adaptive$sentence  adverbs$speecb act$sentiment analysis$statistical analysis$Spreading Activation$	2
Recognizing Contextual Polarity in Phrase- Level SA.	SA	Saudi Arabia$Sentiment analysis$Sentiment Analysis$speaker adaptive$sentence  adverbs$speecb act$sentiment analysis$statistical analysis$Spreading Activation$	2
In the past, Dragon had focused primarily on  speaker dependent and SA recognition , so  that speaker independent research was a new departure  for us in this past year.	SA	Saudi Arabia$Sentiment analysis$Sentiment Analysis$speaker adaptive$sentence  adverbs$speecb act$sentiment analysis$statistical analysis$Spreading Activation$	3
The 1290 ex- cluded words are OOVs to both the word and hybrid 4We use the IBM system with SA training based on maximum likelihood with no discriminative training.	SA	Saudi Arabia$Sentiment analysis$Sentiment Analysis$speaker adaptive$sentence  adverbs$speecb act$sentiment analysis$statistical analysis$Spreading Activation$	3
In this paper, we describe the SA capabilities of the BBN BYBLOS continuous  speech recognition system.	SA	Saudi Arabia$Sentiment analysis$Sentiment Analysis$speaker adaptive$sentence  adverbs$speecb act$sentiment analysis$statistical analysis$Spreading Activation$	3
Definition of procedures for evaluation of vocabu-  lary/SA systems.	SA	Saudi Arabia$Sentiment analysis$Sentiment Analysis$speaker adaptive$sentence  adverbs$speecb act$sentiment analysis$statistical analysis$Spreading Activation$	3
We also  anticipate moving from speaker dependent recognition to a  SA mode which will require far less training  data for new speakers.	SA	Saudi Arabia$Sentiment analysis$Sentiment Analysis$speaker adaptive$sentence  adverbs$speecb act$sentiment analysis$statistical analysis$Spreading Activation$	3
The SA transforms are then applied to the future sentences.	SA	Saudi Arabia$Sentiment analysis$Sentiment Analysis$speaker adaptive$sentence  adverbs$speecb act$sentiment analysis$statistical analysis$Spreading Activation$	3
In Proceedings of the 1st international CIKM workshop on Topic-SA for mass opinion, pages 45?52.	SA	Saudi Arabia$Sentiment analysis$Sentiment Analysis$speaker adaptive$sentence  adverbs$speecb act$sentiment analysis$statistical analysis$Spreading Activation$	6
Sentiws: a publicly available german- language resource for SA.	SA	Saudi Arabia$Sentiment analysis$Sentiment Analysis$speaker adaptive$sentence  adverbs$speecb act$sentiment analysis$statistical analysis$Spreading Activation$	6
Indeed, the main goal of our work is to extract and organize the major opinions about a topic that are buried in many scattered opinionated sources rather than perform deeper understanding of opin- ions (e.g., distinguishing positive from negative opinions), which can be done by using any exist- ing SA technique as an orthogonal post-processing step after applying our method.	SA	Saudi Arabia$Sentiment analysis$Sentiment Analysis$speaker adaptive$sentence  adverbs$speecb act$sentiment analysis$statistical analysis$Spreading Activation$	6
In CIKM workshop on Topic-SA for mass opinion, pages 53?56.	SA	Saudi Arabia$Sentiment analysis$Sentiment Analysis$speaker adaptive$sentence  adverbs$speecb act$sentiment analysis$statistical analysis$Spreading Activation$	6
Sentiwordnet 3.0: An enhanced lexical resource for SA and opinion mining.	SA	Saudi Arabia$Sentiment analysis$Sentiment Analysis$speaker adaptive$sentence  adverbs$speecb act$sentiment analysis$statistical analysis$Spreading Activation$	6
There has been a growing interest in the NLP treatment of subjectivity and SA ?	SA	Saudi Arabia$Sentiment analysis$Sentiment Analysis$speaker adaptive$sentence  adverbs$speecb act$sentiment analysis$statistical analysis$Spreading Activation$	6
formation patterns) and SA.	SA	Saudi Arabia$Sentiment analysis$Sentiment Analysis$speaker adaptive$sentence  adverbs$speecb act$sentiment analysis$statistical analysis$Spreading Activation$	7
The approach is hybrid: it combines morpho- syntactic filters for extraction of term candidates,  and SA that ranks term candidates  according to their termhood.	SA	Saudi Arabia$Sentiment analysis$Sentiment Analysis$speaker adaptive$sentence  adverbs$speecb act$sentiment analysis$statistical analysis$Spreading Activation$	7
This kind of SA as independently sug-  gested in (Resnik, 1991) can be made with LTAGs be-  cause of their extended omain of locality but also be-  cause of their lexiealized property.	SA	Saudi Arabia$Sentiment analysis$Sentiment Analysis$speaker adaptive$sentence  adverbs$speecb act$sentiment analysis$statistical analysis$Spreading Activation$	7
To sum up, the results from the machine learning method are consistent with that from the  multivariate SA in Section 3.1.	SA	Saudi Arabia$Sentiment analysis$Sentiment Analysis$speaker adaptive$sentence  adverbs$speecb act$sentiment analysis$statistical analysis$Spreading Activation$	7
A SA of the TREC-3 data.	SA	Saudi Arabia$Sentiment analysis$Sentiment Analysis$speaker adaptive$sentence  adverbs$speecb act$sentiment analysis$statistical analysis$Spreading Activation$	7
In non-parametric SA, one com-  pares the rank of data sets when the qualitative be-  haviour is similar but the absolute quantities are un-  reliable.	SA	Saudi Arabia$Sentiment analysis$Sentiment Analysis$speaker adaptive$sentence  adverbs$speecb act$sentiment analysis$statistical analysis$Spreading Activation$	7
Similarity between Words  Computed by SA on an English Dictionary  Hideki Kozima  Course in Computer Science  . . . .	SA	Saudi Arabia$Sentiment analysis$Sentiment Analysis$speaker adaptive$sentence  adverbs$speecb act$sentiment analysis$statistical analysis$Spreading Activation$	8
Word Sense Disambiguation with SA Networks Generated from Thesauri.	SA	Saudi Arabia$Sentiment analysis$Sentiment Analysis$speaker adaptive$sentence  adverbs$speecb act$sentiment analysis$statistical analysis$Spreading Activation$	8
A Parallel Constraint Satisfaction and  SA Model for Resolving Syntactic Ambiguity."	SA	Saudi Arabia$Sentiment analysis$Sentiment Analysis$speaker adaptive$sentence  adverbs$speecb act$sentiment analysis$statistical analysis$Spreading Activation$	8
Appendix B. Function of Paradigme  SA Rules  Each node Pi of the semantic network Paradigme  computes its activity value vi(T+ 1) at time T+I  as  follows:  v'(T+ l) = ? (	SA	Saudi Arabia$Sentiment analysis$Sentiment Analysis$speaker adaptive$sentence  adverbs$speecb act$sentiment analysis$statistical analysis$Spreading Activation$	8
Similarity between Words  Computed by SA on an English Diction-  ary.	SA	Saudi Arabia$Sentiment analysis$Sentiment Analysis$speaker adaptive$sentence  adverbs$speecb act$sentiment analysis$statistical analysis$Spreading Activation$	8
Order Logic Programming for Semantic Interpretation of Coordinate Con- structs (ACL95), S. Kulick 72 9511001 Countability and Number in Japanese-to-English Machine Translation (COLING94), F. Bond et al 442 Computational Linguistics Volume 28, Number 4 73 9511006 Disambiguating Noun Groupings with Respect to WordNet Senses (ACL95 Workshop), P. Resnik 74 9601004 Similarity between Words Computed by SA on an English Dictionary (EACL93), H. Kozima, T. Furugori 75 9604019 Magic for Filter Optimization in Dynamic Bottom-up Processing (ACL96), G. Minnen 76 9604022 Unsupervised Learning of Word-Category Guessing Rules (ACL96), A. Mikheev 77 9605013 Learning Dependencies between Case Frame Slots (COLING96), H. Li, N. Abe 78 9605014 Clustering Words with the MDL Principle (COLING96	SA	Saudi Arabia$Sentiment analysis$Sentiment Analysis$speaker adaptive$sentence  adverbs$speecb act$sentiment analysis$statistical analysis$Spreading Activation$	8
The function first gathers AO of D that have the persis- tent part of description S (Gp and Sp), and if there is already a domain composed by these objects, its salience is increased such that it is the most salient (line 4).	AO	all objects$Object Accusative$	0
The information displayed is not a complete his-  tory including the creation, deformations and  deletion of AO, but rather only a record  of the construction steps (dependencies) of the  stored objects.	AO	all objects$Object Accusative$	0
is an environment composed of E, the universe of AO and V , the set of ground predicates that hold in the environment.	AO	all objects$Object Accusative$	0
The tree is recursively expanded by selecting the best test for each sub- set and so on, until AO of the current subset belong to the same class.	AO	all objects$Object Accusative$	0
The root node is a RD whose ground is AO of the room.	AO	all objects$Object Accusative$	0
i) its POS tag is in the RM-D entry of Table 1, ii) its dependency head is inside of the verbal block, and iii) is the right-most object among AO of the verbal block.	AO	all objects$Object Accusative$	0
Only the most frequent relations are shown (a key for the English relations is given in Table 2; in German the relations are SB (Subject), OA (AO), CJ (Conjunct), DA (Dative), CD (Coordinator), MO (Modifier), RE (Subordinate Clause), RS (Reported Speech), OC (Object Clausal), OP (Object Prepositional), NK (Noun Kernel), and CVC (Collocational Verb Construction).	AO	all objects$Object Accusative$	1
TACL, 3:211?225.	TACL	Transactions of the ACL$Transactions of the Association for Computational Linguistics$	0
In TACL.	TACL	Transactions of the ACL$Transactions of the Association for Computational Linguistics$	0
TACL, 1:49?62.	TACL	Transactions of the ACL$Transactions of the Association for Computational Linguistics$	0
To ap- pear in TACL.	TACL	Transactions of the ACL$Transactions of the Association for Computational Linguistics$	0
TACL.	TACL	Transactions of the ACL$Transactions of the Association for Computational Linguistics$	0
While phrasal alignments are important and have 219 TACL, 2 (2014) 219?230.	TACL	Transactions of the ACL$Transactions of the Association for Computational Linguistics$	1
In contrast to this trend, research in knowledge acquisition is now heading towards the seamless in- 231 TACL, 2 (2014) 231?244.	TACL	Transactions of the ACL$Transactions of the Association for Computational Linguistics$	1
TACL, 2:27?40.	TACL	Transactions of the ACL$Transactions of the Association for Computational Linguistics$	1
TACL, 1:1?12.	TACL	Transactions of the ACL$Transactions of the Association for Computational Linguistics$	1
1977), though EM does not 105 TACL, 2 (2014) 105?118.	TACL	Transactions of the ACL$Transactions of the Association for Computational Linguistics$	1
Our NEWSSPIKE-RE system encapuslates these intuitions in a novel graphical model making 117 TACL, vol.	TACL	Transactions of the ACL$Transactions of the Association for Computational Linguistics$	1
the semantic description pointed out is coded  in the LKB as a definitional  graph.	LKB	lexical knowledge base$Lexical Knowledge Bases$Linguistic Knowledge Builder$Lexical Knowledge Base$	0
Methods based on manually built LKBs, such as WordNet, compute the shortest path be- tween two concepts in the knowledge base and/or look at word overlap in the glosses (see Budan- itsky and Hirst (2006) for an overview).	LKB	lexical knowledge base$Lexical Knowledge Bases$Linguistic Knowledge Builder$Lexical Knowledge Base$	0
Building and using a LKB of near-synonym differences.	LKB	lexical knowledge base$Lexical Knowledge Bases$Linguistic Knowledge Builder$Lexical Knowledge Base$	0
The more evident problem with WORDNET is that  it is a LKB for English, and so it  is not usable for other languages.	LKB	lexical knowledge base$Lexical Knowledge Bases$Linguistic Knowledge Builder$Lexical Knowledge Base$	0
4.1 Semant ic  verb  classi f icat ion in the   LKB  The LKB is based on a hierarchical  representation f French verbs.	LKB	lexical knowledge base$Lexical Knowledge Bases$Linguistic Knowledge Builder$Lexical Knowledge Base$	0
In LREC 2002 Workshop on Ontologies and LKB.. J. Pustejovsky.	LKB	lexical knowledge base$Lexical Knowledge Bases$Linguistic Knowledge Builder$Lexical Knowledge Base$	1
"Models for LKB".	LKB	lexical knowledge base$Lexical Knowledge Bases$Linguistic Knowledge Builder$Lexical Knowledge Base$	1
In Patrick Saint-Dizier,  editor, Predicative Forms in Natural  Language and LKB.	LKB	lexical knowledge base$Lexical Knowledge Bases$Linguistic Knowledge Builder$Lexical Knowledge Base$	1
LREC 2002 Workshop on Ontologies  and LKB.	LKB	lexical knowledge base$Lexical Knowledge Bases$Linguistic Knowledge Builder$Lexical Knowledge Base$	1
LREC 2002 Workshop on Ontologies and  LKB (2002)  22.	LKB	lexical knowledge base$Lexical Knowledge Bases$Linguistic Knowledge Builder$Lexical Knowledge Base$	1
5.3 Towards integrated polymorphic or multiple-view  LKB  Our system can be viewed as a facet of a broader  'environment for an encyclop:edic discovery' with other  mcxles of activity: sell-review, semi-tutored lessons, where  character thematic 'collections' would drive the discovery.	LKB	lexical knowledge base$Lexical Knowledge Bases$Linguistic Knowledge Builder$Lexical Knowledge Base$	1
"Models for LKBs".	LKB	lexical knowledge base$Lexical Knowledge Bases$Linguistic Knowledge Builder$Lexical Knowledge Base$	3
es  Abstract  This paper explores the automatic onstruction  of a multilingual LKB from  pre-existing lexical resources.	LKB	lexical knowledge base$Lexical Knowledge Bases$Linguistic Knowledge Builder$Lexical Knowledge Base$	3
LREC 2002 Workshop on Ontologies  and LKBs.	LKB	lexical knowledge base$Lexical Knowledge Bases$Linguistic Knowledge Builder$Lexical Knowledge Base$	3
LREC 2002 Workshop on Ontologies and  LKBs (2002)  22.	LKB	lexical knowledge base$Lexical Knowledge Bases$Linguistic Knowledge Builder$Lexical Knowledge Base$	3
A Project for the Con- struction of an Italian LKB in the  Framework of WordNet, IRST Technical Report #  9406-15.	LKB	lexical knowledge base$Lexical Knowledge Bases$Linguistic Knowledge Builder$Lexical Knowledge Base$	3
5.3 Towards integrated polymorphic or multiple-view  LKBs  Our system can be viewed as a facet of a broader  'environment for an encyclop:edic discovery' with other  mcxles of activity: sell-review, semi-tutored lessons, where  character thematic 'collections' would drive the discovery.	LKB	lexical knowledge base$Lexical Knowledge Bases$Linguistic Knowledge Builder$Lexical Knowledge Base$	3
Inference in our model is however not as straightforward and we propose an efficient MCMC sampling scheme.	MCMC	Markov chain Monte Carlo$Markov Chain Monte Carlo$	0
LogLin(M,F,w,Y(x i )) 3.2 Inference To perform inference in this model, we adopt a common MCMC estimation procedure for IBPs (G?or?ur et al, 2006; Navarro and Griffiths, 2007).	MCMC	Markov chain Monte Carlo$Markov Chain Monte Carlo$	0
Bayesian probabilistic matrix factorization using MCMC.	MCMC	Markov chain Monte Carlo$Markov Chain Monte Carlo$	0
generated using MCMC meth- ods.	MCMC	Markov chain Monte Carlo$Markov Chain Monte Carlo$	0
5 Inference Schemes In this section we give a high level description of a MCMC sampling based inference scheme for the hierarchical Pitman- Yor language model.	MCMC	Markov chain Monte Carlo$Markov Chain Monte Carlo$	0
Probabilistic inference using MCMC methods.	MCMC	Markov chain Monte Carlo$Markov Chain Monte Carlo$	0
MCMC: Stochastic Simulation for Bayesian Inference.	MCMC	Markov chain Monte Carlo$Markov Chain Monte Carlo$	1
Bayesian inference for PCFGs via MCMC.	MCMC	Markov chain Monte Carlo$Markov Chain Monte Carlo$	1
Stochastic techniques such as MCMC are exact in the limit of infinite runtime, but tend to be too slow for large problems.	MCMC	Markov chain Monte Carlo$Markov Chain Monte Carlo$	1
MCMC in Practice.	MCMC	Markov chain Monte Carlo$Markov Chain Monte Carlo$	1
We also obtained a probability distribution over each co- efficient by MCMC sampling using the R package lme4 version 0.99 (Bates, 2005).	MCMC	Markov chain Monte Carlo$Markov Chain Monte Carlo$	1
In Proceedings of the 6th Interna- tional Workshop on TAG and Re- lated Frameworks, pages 101?106, Venice, Italy.	TAG	Tree Adjoining Grammar$tree adjoining grammar$	0
An efficient parsing algorithm for TAGs.	TAG	Tree Adjoining Grammar$tree adjoining grammar$	0
Feature Structures Based TAGs.	TAG	Tree Adjoining Grammar$tree adjoining grammar$	0
Coordi- nation in TAGs: Formal- ization and implementation.	TAG	Tree Adjoining Grammar$tree adjoining grammar$	0
Some computational properties of TAGs.	TAG	Tree Adjoining Grammar$tree adjoining grammar$	0
For the latter case, the  linguistic realizer is based on TAG  \[Joshi 87\].	TAG	Tree Adjoining Grammar$tree adjoining grammar$	0
Prex probabilities from stochas- tic TAGs.	TAG	Tree Adjoining Grammar$tree adjoining grammar$	1
Conditions on consistency of proba- bilistic TAGs.	TAG	Tree Adjoining Grammar$tree adjoining grammar$	1
Using descriptions of trees in a TAG.	TAG	Tree Adjoining Grammar$tree adjoining grammar$	1
Parsing strategies with lexicalized grammars: Appli- cation to TAGs.	TAG	Tree Adjoining Grammar$tree adjoining grammar$	1
B.: D-ltag system - discourse parsing with a lexi- calized TAG.	TAG	Tree Adjoining Grammar$tree adjoining grammar$	1
Incremental seman- tic role labeling with TAG.	TAG	Tree Adjoining Grammar$tree adjoining grammar$	1
Sentiment  Classification of MR using Contextual  Valence Shifters.	MR	Medical relation$Movie Reviews$Majority Rule$meaning representation$Match Rat$meaning representations$maximum recall$	1
Emotional   Lexicon  Method  News Corpus MR  Pr.	MR	Medical relation$Movie Reviews$Majority Rule$meaning representation$Match Rat$meaning representations$maximum recall$	1
Sentiment  Classification of MR Using Contextual  Valence Shifters.	MR	Medical relation$Movie Reviews$Majority Rule$meaning representation$Match Rat$meaning representations$maximum recall$	1
Pang et al, 2002) tested various ma- chine learning algorithms on MR.	MR	Medical relation$Movie Reviews$Majority Rule$meaning representation$Match Rat$meaning representations$maximum recall$	1
5.2 Sentiment Prediction in MR The first two experiments concern the prediction of the sentiment of movie reviews in the Stanford Sentiment Treebank (Socher et al, 2013b).	MR	Medical relation$Movie Reviews$Majority Rule$meaning representation$Match Rat$meaning representations$maximum recall$	1
Features Method  News Corpus MR  Pr.	MR	Medical relation$Movie Reviews$Majority Rule$meaning representation$Match Rat$meaning representations$maximum recall$	1
For exam- 1520 Rule-MR-Sp Rule-SpSent Maxent Rel micro-avg 2.77 ?	MR	Medical relation$Movie Reviews$Majority Rule$meaning representation$Match Rat$meaning representations$maximum recall$	2
4.1 Bias-Correcting MRs We first want to explore the following idea: If a given annotator annotates most items with 0, then we might want to assign less significance to that 542 choice for any particular item.4 That is, if an an- notator appears to be biased towards a particular category, then we might want to try to correct for this bias during aggregation.	MR	Medical relation$Movie Reviews$Majority Rule$meaning representation$Match Rat$meaning representations$maximum recall$	2
Beta 00.20.40.60.81 Majority Weighte d Rule-b ased (a) Corpus-reproduction metrics 5.38 3.26 4.58 2.98 3.15 1.31 2.07 1.24 Token s Types TTR 0123456 Origin al Weigh ted MR-ba sed 0.640 .69 0.	MR	Medical relation$Movie Reviews$Majority Rule$meaning representation$Match Rat$meaning representations$maximum recall$	2
The user?s question was analyzed and a MR corresponding to the question was encoded in a knowledge representation formalism.	MR	Medical relation$Movie Reviews$Majority Rule$meaning representation$Match Rat$meaning representations$maximum recall$	3
5 Related Work and Discussion Several semantic parsing methods use a domain- independent MR derived from the combinatory categorial grammar (CCG) parses (e.g., (Cai and Yates, 2013; Kwiatkowski et al, 2013; Reddy et al, 2014)).	MR	Medical relation$Movie Reviews$Majority Rule$meaning representation$Match Rat$meaning representations$maximum recall$	3
Most state-of-the-art approaches to KB-QA are based on semantic pars- ing, where a question (utterance) is mapped to its formal MR (e.g., logical form) and then translated to a KB query.	MR	Medical relation$Movie Reviews$Majority Rule$meaning representation$Match Rat$meaning representations$maximum recall$	3
In NLP a very common paradigm for word MR is the use of the Distributional hypothesis.	MR	Medical relation$Movie Reviews$Majority Rule$meaning representation$Match Rat$meaning representations$maximum recall$	3
is that two different types of MRs are learned, reflecting the tendency for state texts to describe scenes and action texts to describe potential actions from the user.	MR	Medical relation$Movie Reviews$Majority Rule$meaning representation$Match Rat$meaning representations$maximum recall$	3
The system transforms texts  into a formal MR language based on cases.	MR	Medical relation$Movie Reviews$Majority Rule$meaning representation$Match Rat$meaning representations$maximum recall$	3
Average MRe I  4 e J i i i ' i i = ?	MR	Medical relation$Movie Reviews$Majority Rule$meaning representation$Match Rat$meaning representations$maximum recall$	4
80 q?gram MRio M ea n  R ec ip ric al  S co re SoftTFIDF?CRF SoftTFIDF?Lev CRF q?gram Best Levenstein TFIDF Figure 1: Precision, Recall, F-measure and Mean Reciprocal Rank comparisions for each string simi- larity metric across different  -gram match ratios.	MR	Medical relation$Movie Reviews$Majority Rule$meaning representation$Match Rat$meaning representations$maximum recall$	4
80 q?gram MRio M ea n  R ec ip ric al  S co re SoftTFIDF?CRF SoftTFIDF?Lev CRF q?gr	MR	Medical relation$Movie Reviews$Majority Rule$meaning representation$Match Rat$meaning representations$maximum recall$	4
8 q?gram MRio Pr ec is io n SoftTFIDF?CRF SoftTFIDF?Lev CRF q?gram Best Levenstein TFIDF 0.2 0.3 0.4 0.5 0.6 0.	MR	Medical relation$Movie Reviews$Majority Rule$meaning representation$Match Rat$meaning representations$maximum recall$	4
80 q?gram MRio M ea n  R ec ip ric al  S co re SoftTFIDF?CRF SoftTFIDF?Lev CRF q?gram Best Levenstein TFIDF Figure 1: Precision, Recall, F-measure and Mean Reciprocal Rank comparisions for each string simi- larity metric across different  -	MR	Medical relation$Movie Reviews$Majority Rule$meaning representation$Match Rat$meaning representations$maximum recall$	4
80 q?gram MRio R ec al l SoftTFIDF?CRF SoftTFIDF?Lev CRF q?gram Best Levenstein TFIDF 0.2 0.3 0.4 0.5 0.6 0.	MR	Medical relation$Movie Reviews$Majority Rule$meaning representation$Match Rat$meaning representations$maximum recall$	4
80 q?gram MRio F?	MR	Medical relation$Movie Reviews$Majority Rule$meaning representation$Match Rat$meaning representations$maximum recall$	4
0.35  0.4  0.45  0.5  0.55  0.6  0.65  0.7  0.75  0  0.2  0.4  0.6  0.8  1 Pr ec is io n q-gram MRio  0.7  0.71  0.72  0.73  0.74  0.75  0.76  0.77  0.78  0  0.2  0.4  0.6  0.8  1 R ec al l q-gram MRio Jaro JaroWinkler SmithWaterman TFIDF UnsmoothedJS Jaccard Figure 1: Performance of Approximate String Matching for Gene Normalization.	MR	Medical relation$Movie Reviews$Majority Rule$meaning representation$Match Rat$meaning representations$maximum recall$	4
A generative model for parsing natural language to MR.	MR	Medical relation$Movie Reviews$Majority Rule$meaning representation$Match Rat$meaning representations$maximum recall$	5
is that two different types of MR are learned, reflecting the tendency for state texts to describe scenes and action texts to describe potential actions from the user.	MR	Medical relation$Movie Reviews$Majority Rule$meaning representation$Match Rat$meaning representations$maximum recall$	5
Joint learning of words and MR for open-text semantic parsing.	MR	Medical relation$Movie Reviews$Majority Rule$meaning representation$Match Rat$meaning representations$maximum recall$	5
For varied selections of thermodynamical exercises, the  system has derived correct MR.	MR	Medical relation$Movie Reviews$Majority Rule$meaning representation$Match Rat$meaning representations$maximum recall$	5
Mimicking an important aspect of human language learning, it ac- quires MR for indi- vidual words from descriptions of visual scenes.	MR	Medical relation$Movie Reviews$Majority Rule$meaning representation$Match Rat$meaning representations$maximum recall$	5
-calculus formulas and their semantic models, corresponding to the semantic or MR, are directly obtained from known semantic representations which were provided with the data or learned before.	MR	Medical relation$Movie Reviews$Majority Rule$meaning representation$Match Rat$meaning representations$maximum recall$	5
We also note that the  MR of Method 2 is much larger than  that of Method 1.	MR	Medical relation$Movie Reviews$Majority Rule$meaning representation$Match Rat$meaning representations$maximum recall$	6
This approach of combining the requirements of MR and maximum precision demands sophisticated syntactic knowl- edge of Czech.	MR	Medical relation$Movie Reviews$Majority Rule$meaning representation$Match Rat$meaning representations$maximum recall$	6
We found that by using the synonyms in WordNet synsets2 as our substitution lexicon, we could achieve a MR of 53% when using an oracle to select the correct synonyms.	MR	Medical relation$Movie Reviews$Majority Rule$meaning representation$Match Rat$meaning representations$maximum recall$	6
Topline recall is the MR possible  for that number of layers.	MR	Medical relation$Movie Reviews$Majority Rule$meaning representation$Match Rat$meaning representations$maximum recall$	6
However, using the synonyms from RT as the substitution lex- icon led to a MR of 85%.	MR	Medical relation$Movie Reviews$Majority Rule$meaning representation$Match Rat$meaning representations$maximum recall$	6
The MR score attainable with our phrases is 84.64% for the development data set.	MR	Medical relation$Movie Reviews$Majority Rule$meaning representation$Match Rat$meaning representations$maximum recall$	6
statistics about the relations and co-reference deci- sions predicted by SERIF, character-based edit  distance, and token SSTs.	SST	Spontaneous  Scheduling Task$subset tree$Spontaneous Scheduling Task$	1
Syntactic Tree Kernel (STK), also known as a SST kernel (Collins and Duffy, 2002), maps objects in the space of all possible tree fragments constrained by the rule that the sibling nodes from their parents cannot be separated.	SST	Spontaneous  Scheduling Task$subset tree$Spontaneous Scheduling Task$	1
S shatters an input tree into its subparts (e. g., subtrees, SSTs, or partial trees as described in Section 3).	SST	Spontaneous  Scheduling Task$subset tree$Spontaneous Scheduling Task$	1
We tuned the regulariza- tion parameter (-c) on the dev set in the same manner as described above, providing 4 GB of memory to the kernel cache (-m 4000).6 We used SST kernels, which compute the similarity between two trees by implicitly enu- merating all possible fragments of the trees (in contrast with subtree kernels, where all frag- ments fully extend to the leaves).	SST	Spontaneous  Scheduling Task$subset tree$Spontaneous Scheduling Task$	1
o Configuration 2 includes Configuration 1 fea- tures with the addition of string similarity (edit  distance, token SSTs) algorithms for the  name variation stage.	SST	Spontaneous  Scheduling Task$subset tree$Spontaneous Scheduling Task$	1
We analyze phrase-structures using the notion of tree fragments (referred to as SSTs by Collins and Duffy, 2002).	SST	Spontaneous  Scheduling Task$subset tree$Spontaneous Scheduling Task$	1
The parser is trained and evaluated with the  SST, which is a nego-  tiation situation, in which two subjects have to  decide on time and place for a meeting.	SST	Spontaneous  Scheduling Task$subset tree$Spontaneous Scheduling Task$	2
FeasPar was trained, tested and evaluated with  the SST, and compared  with a handmodeled LR-parser.	SST	Spontaneous  Scheduling Task$subset tree$Spontaneous Scheduling Task$	2
2013 cite) have recently used RNTN to classify sentences into positive/negative categories.	RNTN	recursive neural tensor networks$Recursive Neural Tensor Network$	0
Can RNTN learn logical reasoning?	RNTN	recursive neural tensor networks$Recursive Neural Tensor Network$	0
To address this need, we introduce the Stanford Sentiment Treebank and a powerful RNTN that can accurately predict the compositional semantic effects present in this new corpus.	RNTN	recursive neural tensor networks$Recursive Neural Tensor Network$	1
2012) that represents every word as both a vector and a ma- trix, or RNTNs (Socher et al.,	RNTN	recursive neural tensor networks$Recursive Neural Tensor Network$	1
To address them, we introduce the RNTN.	RNTN	recursive neural tensor networks$Recursive Neural Tensor Network$	1
3  190  ~ure of this language is to adopt CU  instead of normal unification.	CU	constraint unification$Categorial U,,ificAtion$compound uni$	0
There are also several disjunctive unification algorithms that  exploit independence, such as CU (Hasida 1986; Nakano 1991), con-  texted unification (Maxwell and Kaplan 1989), and unification based on disjunctive  feature logic (D6rre and Eisele 1990).	CU	constraint unification$Categorial U,,ificAtion$compound uni$	0
A logical CU sys-  tern (Nakano 1991) is used in the planners.	CU	constraint unification$Categorial U,,ificAtion$compound uni$	0
In cu-Prolog, user defined  constraints can be directly added to a program clause  (constraint added Horn clause), and the constraint  unification \[12, 8\] 1 is adopted instead of the nor-  1 In these earlier papers, "CU" was called  "conditioned unification."	CU	constraint unification$Categorial U,,ificAtion$compound uni$	0
Using CU, the inference rule of  cu-Prolog is as follows:  Q, R; C. , Q' : -S ;  D.,  0 = mgu(Q, Q'), B = my(co,  DO)  $0, R6; B  (Q is an atomic formula.	CU	constraint unification$Categorial U,,ificAtion$compound uni$	0
cu-Prolog adopts CU i stead of  the normal Prolog unification.	CU	constraint unification$Categorial U,,ificAtion$compound uni$	0
The  franle is normal i zed  and  p laced  together  accord -   ing to the spec i f i ca t ion  of the compounder  to fo rm a CUt,  and  this p rocess  of normal i za t ion  and  abutt ing can  be  recurs ive ly  re-   peated.	CU	constraint unification$Categorial U,,ificAtion$compound uni$	2
y. y (a+ x) 3 Three computational paradigms Compositional semantics associates meanings to utterances by assigning meanings to atomic items, and by giving rules that allows to compute the meaning of a CUt from the meanings of its parts.	CU	constraint unification$Categorial U,,ificAtion$compound uni$	2
The score -2 is assigned to idioms and CUts with gaps.	CU	constraint unification$Categorial U,,ificAtion$compound uni$	2
The score -1 is as- signed to CUts.	CU	constraint unification$Categorial U,,ificAtion$compound uni$	2
7 A(unit) : awareness level <= -2 -1 >= 0 Mode of alert always emphasis by mouse-over none Score -2 -1 0 1 C(unit) : composition CUt with gap CUt single word D(unit) : difficulty technical term general term elementary term S(unit) : speciality specified domain general domain R(unit) : resource type encyclopaedia dictionary Table 1: Awareness levels and the scores of each characteristic are optimal from the point of view of the user?s search behaviour.	CU	constraint unification$Categorial U,,ificAtion$compound uni$	2
MERT in statistical machine translation.	MERT	Minimum error rate training$Minimum Error Rate Training$minimum error rate training$	0
MERT for statistical machine translation.	MERT	Minimum error rate training$Minimum Error Rate Training$minimum error rate training$	0
MERT in Statistical Machine Translation.	MERT	Minimum error rate training$Minimum Error Rate Training$minimum error rate training$	1
F. J. Och, 2003, MERT in Statis- tical Machine Translation.	MERT	Minimum error rate training$Minimum Error Rate Training$minimum error rate training$	1
MERT in  Statistical Machine Translation.	MERT	Minimum error rate training$Minimum Error Rate Training$minimum error rate training$	1
MERT in Statis- tical Machine Translation.	MERT	Minimum error rate training$Minimum Error Rate Training$minimum error rate training$	1
MERT in Sta- tistical Machine Translation.	MERT	Minimum error rate training$Minimum Error Rate Training$minimum error rate training$	1
Effi- cient MERT and minimum bayes- risk decoding for translation hypergraphs and lattices.	MERT	Minimum error rate training$Minimum Error Rate Training$minimum error rate training$	2
We set the feature weights by optimizing the Bleu score directly using MERT (Och, 2003) on the de- velopment set.	MERT	Minimum error rate training$Minimum Error Rate Training$minimum error rate training$	2
We tuned the weights of each model (non-augmented base- line, unigram-augmented, and unigram-augmented- filtered) with a separate MERT.	MERT	Minimum error rate training$Minimum Error Rate Training$minimum error rate training$	2
Feature weights were set with MERT (Och, 2003) on a tuning set using BLEU (Papineni et al, 2002) as the objective function.	MERT	Minimum error rate training$Minimum Error Rate Training$minimum error rate training$	2
Unfortunately, MERT can- not be directly used to optimize feature weights of max-translation decoding because Eq. (	MERT	Minimum error rate training$Minimum Error Rate Training$minimum error rate training$	2
tation performance has been improved significantly, from the earliest maximal match (dictionary-based) approaches to HMM-based (Zhang et al, 2003) ap- proaches and recent state-of-the-art machine learn- ing approaches such as maximum entropy (Max- Ent) (Xue and Shen, 2003), support vector ma- chine (SVM) (Kudo and Matsumoto, 2001), con- ditional random fields (CRF) (Peng and McCallum, 2004), and MERT (Gao et al, 2004).	MERT	Minimum error rate training$Minimum Error Rate Training$minimum error rate training$	2
The complex- ity of this step is O(n2), as the number of mini- mal phrases is bounded by the minimum of the num- ber of MP in either language.	MP	monolingual phrases$Maximal Projection$Member of Parliament$	0
Marcu and Wong (2002) address point 2 with a lexi- con constraint; MP that are above a length threshold or below a frequency threshold are excluded from the lexicon.	MP	monolingual phrases$Maximal Projection$Member of Parliament$	0
In this work, we only require representations for MP that are relatively short.	MP	monolingual phrases$Maximal Projection$Member of Parliament$	0
null, Pnull1 ) where the base distributions, PP1 and P null 1 , range over phrase pairs or MP in either language, respectively.	MP	monolingual phrases$Maximal Projection$Member of Parliament$	0
0.51 2014 Avg 0.55 0.63 0.71 0.55 0.59 0.52 0.73 0.54 forums 0.35 0.42 0.55 0.48 0.50 0.45 0.66 0.31 students 0.66 0.66 0.73 0.73 0.65 0.69 0.77 0.63 headline 0.64 0.60 0.79 0.64 0.73 0.66 0.76 0.62 belief 0.46 0.71 0.68 0.67 0.48 0.61 0.77 0.41 images 0.52 0.71 0.75 0.62 0.67 0.56 0.82 0.68 2015 Avg 0.53 0.63 0.70 0.63 0.59 0.60 0.76 0.53 SICK 0.53 0.62 0.66 0.57 0.63 0.54 0.72 0.66 results with MP, while J bi per- forms better with bilingual examples.	MP	monolingual phrases$Maximal Projection$Member of Parliament$	0
Euclidean dis- tance was consistently chosen for bilingual sen- tences and MP, while cosine sim- ilarity was chosen for bilingual phrases.	MP	monolingual phrases$Maximal Projection$Member of Parliament$	0
61   Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 833?840 Manchester, August 2008 Prediction of MP for Semantic Role Labeling Weiwei Sun ?	MP	monolingual phrases$Maximal Projection$Member of Parliament$	1
833 Figure 1: A sentence from WSJ test corpus of CoNLL-2005 shared task 2 MP and Its Government of Arguments 2.1 MP Principle and parameters theory is a framework of generative grammar.	MP	monolingual phrases$Maximal Projection$Member of Parliament$	1
First, "Della Noce" is the last name of a Cana-  dian MP.	MP	monolingual phrases$Maximal Projection$Member of Parliament$	2
Two MWTD are merged if they are syn- onymic variants obtained by derivation or conversion.	TD	T candidates$transducing dictionary$Terminology Data$typically developing$deletion$tree depth$Tunisian dialect$typical development$topic distillation$	0
The MWTD secr?tion insulinique (insulin secretion) and hypers?cr?tion insulinique (insulin 1http://www.sciences.univ-nantes.fr/ info/perso/permanents/daille/ and release LINUX.	TD	T candidates$transducing dictionary$Terminology Data$typically developing$deletion$tree depth$Tunisian dialect$typical development$topic distillation$	0
2.2 Semantic Role Features Ideally, we want to use features based on the true semantic roles of the MTD.	TD	T candidates$transducing dictionary$Terminology Data$typically developing$deletion$tree depth$Tunisian dialect$typical development$topic distillation$	0
All MWTD linked by derivational morphol- ogy or by variations inducing semantic variations are clustered.	TD	T candidates$transducing dictionary$Terminology Data$typically developing$deletion$tree depth$Tunisian dialect$typical development$topic distillation$	0
The MWTD produit de la fore?t, produit agroforestier, non-produit agro- forestier, and sous-produit forestier, sous-produit de la fore?t have also been iden- tified.	TD	T candidates$transducing dictionary$Terminology Data$typically developing$deletion$tree depth$Tunisian dialect$typical development$topic distillation$	0
For exam- ple, the following MWTD constitutes a cluster of MWTs: produit forestier/produit de la fore?t, produit non forestier, non-produit agroforestier, produit agroforestier, sous-produit forestier/sous-produit de la fore?t, produit alimentaire forestier andproduit forestier.	TD	T candidates$transducing dictionary$Terminology Data$typically developing$deletion$tree depth$Tunisian dialect$typical development$topic distillation$	0
Syntactical variants that induce se- mantic discrepancies are retrieved from the set of the candidate variants and new MWTD are created.	TD	T candidates$transducing dictionary$Terminology Data$typically developing$deletion$tree depth$Tunisian dialect$typical development$topic distillation$	0
The pa- per describes so called TD, a set of rules designed for a direct transcrip- tion of a certain category of source language words into a target language.	TD	T candidates$transducing dictionary$Terminology Data$typically developing$deletion$tree depth$Tunisian dialect$typical development$topic distillation$	1
Two main bottlenecks of full-fledged  transfer-based systems are:  8  - complexity of the syntactic dictionary  - relative unreliability of the syntactic  analysis of the source language  Even a relatively simple component  (TD) was equally complex  for English-to-Czech and Czech-to-Russian  translation  Limited text domains do not exist in real life,  it is necessary to work with a high coverage  dictionary at least for the source language.	TD	T candidates$transducing dictionary$Terminology Data$typically developing$deletion$tree depth$Tunisian dialect$typical development$topic distillation$	1
It was generally  assumed that for the pair Czech/Russian the  TD would be able to profit  from a substantially greater number of productive  rules.	TD	T candidates$transducing dictionary$Terminology Data$typically developing$deletion$tree depth$Tunisian dialect$typical development$topic distillation$	1
At the time when the work was terminated in  1990, the system had a main translation  dictionary of about 8000 words, accompanied by  so called TD covering another  2000 words.	TD	T candidates$transducing dictionary$Terminology Data$typically developing$deletion$tree depth$Tunisian dialect$typical development$topic distillation$	1
So far, the appl-  ication of TD in the framework of our experiment  has benn considered.	TD	T candidates$transducing dictionary$Terminology Data$typically developing$deletion$tree depth$Tunisian dialect$typical development$topic distillation$	1
The TD was  based on the original idea described in Kirschner  (1987).	TD	T candidates$transducing dictionary$Terminology Data$typically developing$deletion$tree depth$Tunisian dialect$typical development$topic distillation$	1
A TD Bank for Trans-  lators (TEAM)."	TD	T candidates$transducing dictionary$Terminology Data$typically developing$deletion$tree depth$Tunisian dialect$typical development$topic distillation$	2
(5) Brinkmann, Karl-Heinz, TD Banks as a Basis for High-Quality  Translation, in: COLING80 (Tokyo, 1980).	TD	T candidates$transducing dictionary$Terminology Data$typically developing$deletion$tree depth$Tunisian dialect$typical development$topic distillation$	2
TD Categories - ISO 12620  ?	TD	T candidates$transducing dictionary$Terminology Data$typically developing$deletion$tree depth$Tunisian dialect$typical development$topic distillation$	2
Carl, M., Langlais, P.: An intelligent TDbase as a pre-processor for Statistical Machine Translation.	TD	T candidates$transducing dictionary$Terminology Data$typically developing$deletion$tree depth$Tunisian dialect$typical development$topic distillation$	2
License details: http://creativecommons.org/licenses/by/4.0/  66 guage throughout the whole island of Ireland, researchers at Fiontar began development of the Nation- al TDbase for Irish, focal.ie (focal ?	TD	T candidates$transducing dictionary$Terminology Data$typically developing$deletion$tree depth$Tunisian dialect$typical development$topic distillation$	2
Categories of Systems  There are three broad categories of "computerized  translation tools" (the differences hinging on how  ambitious the system is intended to be): Machine  Translation (MT), Machine-aided Translation (MAT),  and TDbanks.	TD	T candidates$transducing dictionary$Terminology Data$typically developing$deletion$tree depth$Tunisian dialect$typical development$topic distillation$	2
2012) than their TD peers.	TD	T candidates$transducing dictionary$Terminology Data$typically developing$deletion$tree depth$Tunisian dialect$typical development$topic distillation$	3
Instances of perseveration on a partic- ular topic in the spontaneous spoken language of children with ASD, however, are not typically ex- plicitly counted in a clinical setting, making com- parisons with TD children diffi- cult to quantify.	TD	T candidates$transducing dictionary$Terminology Data$typically developing$deletion$tree depth$Tunisian dialect$typical development$topic distillation$	3
However, adults with developmental  disorders who have successfully acquired syntax  typically have mental ages of at least 6 or 7, an age  at which TD children also have  well-structured language.	TD	T candidates$transducing dictionary$Terminology Data$typically developing$deletion$tree depth$Tunisian dialect$typical development$topic distillation$	3
Also in class is Charlie, a TD child.	TD	T candidates$transducing dictionary$Terminology Data$typically developing$deletion$tree depth$Tunisian dialect$typical development$topic distillation$	3
ast as expressive of changes in language development as the Index of Productive Syntax (Scarborough, 1990), an empirically validated metric based on an inventory of grammatical structures derived from the child language literature; and (2) these parse tree features can be used to model language development without the use of an inventory of specific structures, assuming only the knowledge that in TD children the level of language development is correlated with age.	TD	T candidates$transducing dictionary$Terminology Data$typically developing$deletion$tree depth$Tunisian dialect$typical development$topic distillation$	3
Currently there are not many op- portunities for children with severe disabilities to play independently and to interact on equal terms with TD children.	TD	T candidates$transducing dictionary$Terminology Data$typically developing$deletion$tree depth$Tunisian dialect$typical development$topic distillation$	3
String alignment with  substitution, insertion, TD, squashing~ and  expansion operations.	TD	T candidates$transducing dictionary$Terminology Data$typically developing$deletion$tree depth$Tunisian dialect$typical development$topic distillation$	4
The main problem identified by the authors was boundary TD: a problem which impacts the summarization task.	TD	T candidates$transducing dictionary$Terminology Data$typically developing$deletion$tree depth$Tunisian dialect$typical development$topic distillation$	4
In order to take  this fact into account, the penalty for a gap can be  calculated as a function of its length, rather than as  a simple sum of individual TDs.	TD	T candidates$transducing dictionary$Terminology Data$typically developing$deletion$tree depth$Tunisian dialect$typical development$topic distillation$	4
quence of substitution and TD as compression  is unsatisfactory because it cannot be distinguished  from an actual sequence of substitution and dele-  tion.	TD	T candidates$transducing dictionary$Terminology Data$typically developing$deletion$tree depth$Tunisian dialect$typical development$topic distillation$	4
6 The set of operations con-  tains insertions/TDs, substitutions, and expan-  sions/compressions.	TD	T candidates$transducing dictionary$Terminology Data$typically developing$deletion$tree depth$Tunisian dialect$typical development$topic distillation$	4
often ex-  tended to cover pairs consisting of a segment and  the null character, which correspond to the opera-  288  Algorithm Calculation Calculation Dynamic Phonological  of alignment of distance programming features  Covington (1996)  Somers (1998)  Nerbonne and Heeringa (1997)  Gildea and Jurafsky (1996)  explicit  explicit  implicit  explicit  Table 1: Comparison of  lions of insertion and TD (also called indels).	TD	T candidates$transducing dictionary$Terminology Data$typically developing$deletion$tree depth$Tunisian dialect$typical development$topic distillation$	4
In  addition, we define two new features: 1) the num- ber of lexical words in a rule to control the model?s  preference for lexicalized rules over un-lexicalized  rules and 2) the average TD in a rule to bal- ance the usage of hierarchical rules and flat rules.	TD	T candidates$transducing dictionary$Terminology Data$typically developing$deletion$tree depth$Tunisian dialect$typical development$topic distillation$	5
We do average-case analysis below because the TD (height) for a sentence of n words is a random variable: in the worst-case it can be linear in n (degenerated into a linear-chain), but we assume this adversarial situation does not happen frequently, and the average TD is O(log n).	TD	T candidates$transducing dictionary$Terminology Data$typically developing$deletion$tree depth$Tunisian dialect$typical development$topic distillation$	5
0  5  10  15  20  25  0  10  20  30  40  50 Tr ee  D ep th  d (n) Sentence Length (n) Avg Depth Variance 3.5 log n Figure 6: Mean and variance of TD vs. sentence length.	TD	T candidates$transducing dictionary$Terminology Data$typically developing$deletion$tree depth$Tunisian dialect$typical development$topic distillation$	5
We first verify the assumptions we made in Sec- tion 3.3 in order to prove the theorem that TD (as a random variable) is normally-distributed with O(logn) mean and variance.	TD	T candidates$transducing dictionary$Terminology Data$typically developing$deletion$tree depth$Tunisian dialect$typical development$topic distillation$	5
Qualitatively, we veri- fied that for most n, TD d(n) does look like a normal distribution.	TD	T candidates$transducing dictionary$Terminology Data$typically developing$deletion$tree depth$Tunisian dialect$typical development$topic distillation$	5
Our TunDiaWN construction  approach is founded, in one hand, on a  corpus based method to analyze and ex- tract TD words.	TD	T candidates$transducing dictionary$Terminology Data$typically developing$deletion$tree depth$Tunisian dialect$typical development$topic distillation$	6
2013) introduced  a lexicon for the TD in order to  adapt an existing morphological analyzer initial- ly designed for Standard Arabic.	TD	T candidates$transducing dictionary$Terminology Data$typically developing$deletion$tree depth$Tunisian dialect$typical development$topic distillation$	6
3 Challenges  In the last years, TD is widely used  in new written media and web 2.0, especially in  social networks, blogs, forums, weblogs, etc.,	TD	T candidates$transducing dictionary$Terminology Data$typically developing$deletion$tree depth$Tunisian dialect$typical development$topic distillation$	6
fr                    Abstract    In this paper, we propose TunDiaWN  (TD Wordnet) a lexical re- source for the dialect language spoken in  Tunisia.	TD	T candidates$transducing dictionary$Terminology Data$typically developing$deletion$tree depth$Tunisian dialect$typical development$topic distillation$	6
c?2014 Association for Computational Linguistics TD Wordnet creation and enrichment   using web resources and other Wordnets  Rihab Bouchlaghem   LARODEC, ISG de Tunis  2000 Le Bardo, Tunisie  rihab.bouchlaghem@isg.rnu.tn                        Aymen Elkhlifi                Paris-Sorbonne University,             28 Rue Serpente, Paris, France   Aymen.Elkhlifi@paris.sorbonne.fr                    Abstract    In this paper, we propos	TD	T candidates$transducing dictionary$Terminology Data$typically developing$deletion$tree depth$Tunisian dialect$typical development$topic distillation$	6
that dealt with the automatic  processing of TD are based on spo- ken dialogue corpus.	TD	T candidates$transducing dictionary$Terminology Data$typically developing$deletion$tree depth$Tunisian dialect$typical development$topic distillation$	6
2013)  presented a method that aims to construct bilin- gual dictionary using explicit knowledge about  the relation between TD and Stand- ard Arabic.	TD	T candidates$transducing dictionary$Terminology Data$typically developing$deletion$tree depth$Tunisian dialect$typical development$topic distillation$	6
ect Wordnet creation and enrichment   using web resources and other Wordnets  Rihab Bouchlaghem   LARODEC, ISG de Tunis  2000 Le Bardo, Tunisie  rihab.bouchlaghem@isg.rnu.tn                        Aymen Elkhlifi                Paris-Sorbonne University,             28 Rue Serpente, Paris, France   Aymen.Elkhlifi@paris.sorbonne.fr                    Abstract    In this paper, we propose TunDiaWN  (TD Wordnet) a lexical re- source for the dialect language spoken in  Tunisia.	TD	T candidates$transducing dictionary$Terminology Data$typically developing$deletion$tree depth$Tunisian dialect$typical development$topic distillation$	6
A clustering  technique is adapted and applied to mine  the possible relations existing between  the TD extracted words and  to group them into meaningful groups.	TD	T candidates$transducing dictionary$Terminology Data$typically developing$deletion$tree depth$Tunisian dialect$typical development$topic distillation$	6
lkhlifi                Paris-Sorbonne University,             28 Rue Serpente, Paris, France   Aymen.Elkhlifi@paris.sorbonne.fr                    Abstract    In this paper, we propose TunDiaWN  (TD Wordnet) a lexical re- source for the dialect language spoken in  Tunisia.	TD	T candidates$transducing dictionary$Terminology Data$typically developing$deletion$tree depth$Tunisian dialect$typical development$topic distillation$	6
A clustering  technique is adapted and applied to mine  the possible relations existing between  the TD extracted words and  to group them into meaningful gro	TD	T candidates$transducing dictionary$Terminology Data$typically developing$deletion$tree depth$Tunisian dialect$typical development$topic distillation$	6
The aTD of metaphor and metonymy comprehension in children with autism.	TD	T candidates$transducing dictionary$Terminology Data$typically developing$deletion$tree depth$Tunisian dialect$typical development$topic distillation$	7
Given the vocabulary differences seen here, we expect to find not only that children with ASD are aban- doning the topic of the source narrative more fre- quently than children with TD but also that the topics they choose to pursue are related to their own individual specific interests.	TD	T candidates$transducing dictionary$Terminology Data$typically developing$deletion$tree depth$Tunisian dialect$typical development$topic distillation$	7
In this paper, we explore the  behaviour of a connectionist network, since these  systems have been widely applied to phenomena  within cognitive and language development  (Elman et al, 1996) and more recently to capturing  both aTD and acquired deficits in  adults (Thomas & Karmiloff-Smith, 2002, 2003).	TD	T candidates$transducing dictionary$Terminology Data$typically developing$deletion$tree depth$Tunisian dialect$typical development$topic distillation$	7
We find that the words used by children with TD tend to be used by other children with typ- ical development, while the words used by children with autism overlap less with those used by children with typical devel- opment and even less with those used by other children with autism.	TD	T candidates$transducing dictionary$Terminology Data$typically developing$deletion$tree depth$Tunisian dialect$typical development$topic distillation$	7
A TD model involves a parser which generates candidate anal- yses, and human annotators who manually iden- tify the desired tree structure.	TD	T candidates$transducing dictionary$Terminology Data$typically developing$deletion$tree depth$Tunisian dialect$typical development$topic distillation$	7
Autism, context/noncontext information processing, and aTD.	TD	T candidates$transducing dictionary$Terminology Data$typically developing$deletion$tree depth$Tunisian dialect$typical development$topic distillation$	7
Im- proved algorithms for TD environment in a hyperlinked.	TD	T candidates$transducing dictionary$Terminology Data$typically developing$deletion$tree depth$Tunisian dialect$typical development$topic distillation$	8
Im- proved algorithms for TD in a hyper- linked environment.	TD	T candidates$transducing dictionary$Terminology Data$typically developing$deletion$tree depth$Tunisian dialect$typical development$topic distillation$	8
Improved algo- rithms for TD in a hyperlinked environ- ment.	TD	T candidates$transducing dictionary$Terminology Data$typically developing$deletion$tree depth$Tunisian dialect$typical development$topic distillation$	8
The first state is always the  initial state "1", the second depemts on the type of  stern and its ending throughout the conjugation (dig-  its or character strings can be used indifferently for  labelling the states), l~br example, for the first verb  conjugation, whose INF end in "-at," the sec-  ond states are spread out among 10 different states.	INF	infinitives$Inf init ive$information$Inf init ive  Phrase$	0
der fiberlegene Paul  Zu-INF (marginally) and dad-clauses are possible in  lieu of the prepositional.	INF	infinitives$Inf init ive$information$Inf init ive  Phrase$	0
The complement clauses we  consider are daft (that) clauses, ob (whether) clauses, and  infinitive clauses (pure INF and infinitive clauses in-  troduced by zu (to)).	INF	infinitives$Inf init ive$information$Inf init ive  Phrase$	0
This implies that deep annotations as, for instance, have been derived so far from PennTreeBank/PropBank, in which ei- ther all syntactic nodes of the annotation are kept (as in (Bohnet et al, 2010)) or only certain syntac- tic nodes are removed (as THAT complementizers and TO INF in the shared task 2011 on sur- face realization (Belz et al, 2011)) still fall short of a genuine semantic annotation.	INF	infinitives$Inf init ive$information$Inf init ive  Phrase$	0
Tile lexicon mainly contains verb INF in an encoded form.	INF	infinitives$Inf init ive$information$Inf init ive  Phrase$	0
Like this grammar, it contains around 1300 elementary trees and covers auxiliaries, copula, raising and small clause constructions, topicalization, relative clauses, INF, gerunds, passives, adjuncts, ditransitives and datives, ergatives, it-clefts, wh- clefts, PRO constructions, noun-noun modifica- tion, extraposition, sentential adjuncts, impera- tives and resultatives.	INF	infinitives$Inf init ive$information$Inf init ive  Phrase$	0
Sentences with INFs  I. The teacher wanted Kathy to  hurry.	INF	infinitives$Inf init ive$information$Inf init ive  Phrase$	1
IF IGEN ( INF Generator)  is another   ra ther  s t ra ight fo rward  F in i te  State Pat tern   Matcher (developed by Gunnel K~llgren).	INF	infinitives$Inf init ive$information$Inf init ive  Phrase$	1
We present some experiments il-  lustrating the accuracy of the method and note  that with this INF added, our pronoun  resolution method achieves 84.2% accuracy.	INF	infinitives$Inf init ive$information$Inf init ive  Phrase$	2
The first piece of useful INF we con-  sider is the distance between the pronoun  and the candidate antecedent.	INF	infinitives$Inf init ive$information$Inf init ive  Phrase$	2
The second half of the paper describes a  method for using (portions of) t~e aforemen-  tioned program to learn automatically the typi-  cal gender of English words, INF that is  itself used in the pronoun resolution program.	INF	infinitives$Inf init ive$information$Inf init ive  Phrase$	2
This program differs  from earlier work in its almost complete lack of  hand-crafting, relying instead on a very small  corpus of Penn Wall Street Journal Tree-bank  text (Marcus et al, 1993) that has been marked  with co-reference INF.	INF	infinitives$Inf init ive$information$Inf init ive  Phrase$	2
is INF added, our pronoun  resolution method achieves 84.2% accuracy.	INF	infinitives$Inf init ive$information$Inf init ive  Phrase$	2
The second  experiment investigates a method for unsuper-  vised learning of gender/number/animaticity  INF.	INF	infinitives$Inf init ive$information$Inf init ive  Phrase$	2
Our first experiment  shows the relative contribution of each source  Of INF and demonstrates a uccess rate  of 82.9% for all sources combined.	INF	infinitives$Inf init ive$information$Inf init ive  Phrase$	2
urnal Tree-bank  text (Marcus et al, 1993) that has been marked  with co-reference INF.	INF	infinitives$Inf init ive$information$Inf init ive  Phrase$	2
For MUC-5, the following event-related thematic roles were handled : agent, patient, EXP, recipient , beneficiary, source, and location .	EXP	experiencer$Explicit$	0
18 See Verma nd Mohanan (1991) for an extensive survey of EXP subject constructions i  different  languages.	EXP	experiencer$Explicit$	0
is an  EXP of the verb upset.	EXP	experiencer$Explicit$	0
(Yi, 2007) shows agent and EXP as ARG0  accounts for 93% in all ARG0s in Propbank.	EXP	experiencer$Explicit$	0
the semantic case or  cases that may be used in that slot  ('age,in, "EXP', etc.),	EXP	experiencer$Explicit$	0
425  Computational Linguistics Volume 17, Number 4  The sentences in 34 illustrate the various syntactic onsequences of metonymy and  coercion involving EXP verbs, while those in 35 show the different metonymic  extensions possible from the causing event in a killing.	EXP	experiencer$Explicit$	0
EXP mention of speakers.	EXP	experiencer$Explicit$	1
Although the four genes are con- EXP renaming PMID 15767583 : Genetic analysis of ykvJKLM mu- tants in Acinetobacter confirmed that each was essen- tial for queuosine biosynthesis, and the genes were re- named queCDEF .	EXP	experiencer$Explicit$	1
EXP re- naming relations occur in 261 sentences, synonymy- like relations in 349 sentences, biological proof- based relations in 76 sentences.	EXP	experiencer$Explicit$	1
As expected, the overall accuracy of identify- ing contingency and expansion relations is lower, Task All relations EXP relations only Comparison 91.28% (76.54%) 97.23% (69.72%) Contingency 84.44% (76.81%) 93.99% (79.73%) Temporal 94.79% (86.54%) 95.4% (79.98%) Expansion 77.51% (55.67%) 97.61% (65.16%) Table 2: Decision tree classification accuracy us- ing only the presence of connectives as binary fea- tures.	EXP	experiencer$Explicit$	1
Atkins, Beryl S., Judy Kegl, Beth Levin (1986)  "EXP and Implicit Information in Dictionaries",  in Advances in Lexicology.	EXP	experiencer$Explicit$	1
EXP renaming relation is the easiest positive case to identify.	EXP	experiencer$Explicit$	1
Although the four genes are con- EXP renaming PMID 15767583 : Genetic analys	EXP	experiencer$Explicit$	1
Class EXP (%) Implicit (%) Total Comparison 5590 (69.05%) 2505 (30.95%) 8095 Contingency 3741 (46.75%) 4261 (53.25%) 8002 Temporal 3696 (79.55%) 950 (20.45%) 4646 Expansion 6431 (42.04%) 8868 (57.96%) 15299 Table 1: Discourse relation distribution in seman- tic and explicit/implicit classes in the PDTB 3 Distribution and ambiguity of connectives Table 1 shows the distribution of discourse rela- tio	EXP	experiencer$Explicit$	1
t Genre = Email Lexical Stop-words Stylistic Character n-grams Lexical Stop-words Stylistic Character n-grams Sex Discrimination 5.85 5.57 1.67 10.33 2.24 7.29 8.86 9.72 Legalization of Marijuana 7.86 7.76 1.57 12.19 2.91 3.32 5.21 7.39 Catholic Church 6.24 8.76 6.24 14.33 2.41 4.48 3.59 5.22 Privacy Rights 5.9 4.66 1.9 14.05 2.97 6.45 4.6 10.06 War in Iraq 8.1 7.95 3.48 15.57 3.96 7.58 2.99 7.79 GM 7.19 5.85 7.19 10.29 2.57 4.31 1.98 6.82 Table 5: Average performance gain from adding an additional topic as training data across different initial topics on dataset 1.	GM	Gay Marriage$Google Maps$graph matching$Graphical Model$	0
Test Topic Lexical Stop-words Stylistic Character n-grams Stop-words + Character n-grams Previous Work Sex Discrimination 66.67 76.19 33.33 95.24 95.24 95 Catholic Church 76.19 95.24 38.10 95.24 100 95 GM 80.95 80.95 42.86 90.48 90.48 95 Legalization of Marijuana 52.38 66.67 33.33 95.24 100 100 Privacy Rights 42.86 52.38 28.57 95.24 90.48 100 War in Iraq 57.14 71.43 38.10 100 100 81 Average 62.7 73.81 35.72 95.24 96.03 94.33 Table 7: Comparing performance of our work with previous work in the same training/testing setting.	GM	Gay Marriage$Google Maps$graph matching$Graphical Model$	0
The first corpus contains communication samples from 21 authors in six genres (Email, Essay, Blog, Chat, Phone Interview, and Discussion) on six topics (Catholic Church, GM, War in Iraq, Legalization of Marijuana, Privacy Rights, and Sex Discrimination), which we call dataset 1.	GM	Gay Marriage$Google Maps$graph matching$Graphical Model$	0
7 Model Evaluation 7.1 Qualitative Evaluation Tables 4 and 5 present the inferred topic- viewpoints words, i.e. arguing expressions, by JTV for the Obama Healthcare and GM data sets, respectively.	GM	Gay Marriage$Google Maps$graph matching$Graphical Model$	0
GM?	GM	Gay Marriage$Google Maps$graph matching$Graphical Model$	0
v. Capitalism 214 62% 2.97 46% 55% 70% 0.67 0.66 0.68 53% 0.49 0.48 0.49 Death Penalty 331 60% 2.40 45% 56% 35% 0.31 0.29 0.34 55% 0.46 0.48 0.44 Evolution 818 66% 3.74 53% 58% 82% 0.78 0.78 0.79 56% 0.49 0.48 0.50 Existence Of God 852 76% 4.16 51% 56% 75% 0.73 0.70 0.76 52% 0.49 0.47 0.51 Firefox v. IE 233 38% 1.27 15% 79% 76% 0.47 0.44 0.49 72% 0.33 0.34 0.33 GM 560 56% 2.01 28% 65% 84% 0.77 0.74 0.81 60% 0.43 0.43 0.44 Gun Control 135 59% 2.08 45% 63% 37% 0.24 0.21 0.27 53% 0.24 0.30 0.20 Healthcare 112 79% 3.11 53% 55% 73% 0.71 0.69 0.72 60% 0.49 0.56 0.44 Immigration 78 58% 1.95 33% 54% 33% 0.21 0.23 0.19 53% 0.39 0.48 0.33 Iphone v. Blackberry 25 44% 1.14 14% 67% 88% 0.80 0.86 0.75 71% 0.46 0.60 0.38 Israel v. Palestine 64 33% 3.37 53% 58	GM	Gay Marriage$Google Maps$graph matching$Graphical Model$	0
The first one is a dedi- cated webpage using the GM JavaScript API (see Figure 2).	GM	Gay Marriage$Google Maps$graph matching$Graphical Model$	1
2 Related work Mobile applications such as Siri, GM Navigation, Sygic, etc.	GM	Gay Marriage$Google Maps$graph matching$Graphical Model$	1
2 The application is re- 1 http://code.google.com/apis/kml/documentation/ 2 In order to run it, start Google Earth with KML: http://press.jrc.it/geo?type=event&format=kml&language=en 147 Figure 2: Event visualisation with GM stricted to displaying at most half the globe, but it allows expanding overlaid events.	GM	Gay Marriage$Google Maps$graph matching$Graphical Model$	1
Templates exist for referring to multimedia content, external websites, news stories, scientific sources, other on-line repositories (such as the Internet Movie Database (IMDB), medical clas- sification systems (ICD9 and ICD10), coordinates on GM, etc.	GM	Gay Marriage$Google Maps$graph matching$Graphical Model$	1
The video (dated 6/2005) shows exam- ples of two web browsing tasks, one as an exam- ple of navigating the New York Times web site, the other using GM to select and zoom in on a target area.	GM	Gay Marriage$Google Maps$graph matching$Graphical Model$	1
2 Related work Mobile apps such as Siri, GM Naviga- tion, Sygic, etc.	GM	Gay Marriage$Google Maps$graph matching$Graphical Model$	1
We show in detail our findings about syntactic levels (how often GM helped assign a relation between two clauses, a verb and its arguments, or a noun and its modifier) and about the accuracy of the suggestion.	GM	Gay Marriage$Google Maps$graph matching$Graphical Model$	2
Robust textual inference via GM.	GM	Gay Marriage$Google Maps$graph matching$Graphical Model$	2
Biometric identification based on the eye movements and GM techniques.	GM	Gay Marriage$Google Maps$graph matching$Graphical Model$	2
Information retrieval with conceptual GM.	GM	Gay Marriage$Google Maps$graph matching$Graphical Model$	2
Syntactic-semantic GM is used to produce a list of candidate assign- ments for 63.75% of the pairs analysed, and in 57% of situations the correct rela- tions is one of the system?s suggestions; in 19.6% of situations it suggests only the correct relation.	GM	Gay Marriage$Google Maps$graph matching$Graphical Model$	2
Con- ceptual GM: a flexible algorithm and ex- periments.	GM	Gay Marriage$Google Maps$graph matching$Graphical Model$	2
2 Background 2.1 GMs Factor graphs (Kschischang et al 2001) succinctly represent the joint distribution over random vari- ables by a product of factors that make the depen- dencies between the random variables explicit.	GM	Gay Marriage$Google Maps$graph matching$Graphical Model$	3
Frey B.J. 1998, GMs for Machine  Learning and Digital Communication,  Cambridge, MA, MIT Press  Gildea D., Jurafsky D. 2002, Automatic labeling of  semantic roles, Computational Linguistics,  28(3):245-288.	GM	Gay Marriage$Google Maps$graph matching$Graphical Model$	3
The GMs Toolkit: An open source software system for speech and time-series processing.	GM	Gay Marriage$Google Maps$graph matching$Graphical Model$	3
An example of the model defined over 5 m2 m1 m3 m5 m4 1 1 1 1 y 12 y 23 y 13 y 45 Figure 3: GM for Entity Resolution: defined over 5 mentions, with the setting of the vari- ables resulting in 2 entities.	GM	Gay Marriage$Google Maps$graph matching$Graphical Model$	3
k, `] 2.2 Formulation as a GM We score triples (a, pi, ?)	GM	Gay Marriage$Google Maps$graph matching$Graphical Model$	3
In M. I. Jordan, editor, Learning in GMs.	GM	Gay Marriage$Google Maps$graph matching$Graphical Model$	3
For the sole transition-based parsers trained with the selected features, we obtain for Chinese, Hungar- ian and Russian higher labeled and UAS.	UAS	Unlabeled Accuracy Score$unlabeled accuracy scores$unlabeled attachment score$Unlabelled Attachment Score$unlabeled accuracy score$	1
Furthermore, when we inspect the UAS (not shown here), we see that the unlabeled attachment score for idafa also decreases.	UAS	Unlabeled Accuracy Score$unlabeled accuracy scores$unlabeled attachment score$Unlabelled Attachment Score$unlabeled accuracy score$	1
However, it is possible to compute approximate UAS by training the constituent parsers on the NP-patched (Vadas and Curran, 2007) version of the data and then running the test output through just the first conversion script?that is, the modified version of Johansson and Nugues? (	UAS	Unlabeled Accuracy Score$unlabeled accuracy scores$unlabeled attachment score$Unlabelled Attachment Score$unlabeled accuracy score$	1
nt to remember that an UAS of 75.8% corresponds to a word-to-word score of 82.7%, which puts Turkish on a par with languages like Czech, Dutch and Spanish.	UAS	Unlabeled Accuracy Score$unlabeled accuracy scores$unlabeled attachment score$Unlabelled Attachment Score$unlabeled accuracy score$	2
The first consists of determiners and particles, which have an UAS over 80% and which are found within a distance of 1?1.4 IGs from their head.7 The second group mainly contains subjects, objects and different kinds of adjuncts, with a score in the range 60?80% and a distance of 1.8?5.2 IGs to their head.	UAS	Unlabeled Accuracy Score$unlabeled accuracy scores$unlabeled attachment score$Unlabelled Attachment Score$unlabeled accuracy score$	2
It is then important to remember that an UAS of 75.8% corresponds to a word-to-word score of 82.7%, which puts Turkish on a par with languages like Czech, Dutch and Spanish.	UAS	Unlabeled Accuracy Score$unlabeled accuracy scores$unlabeled attachment score$Unlabelled Attachment Score$unlabeled accuracy score$	2
The UASs of the converted dependencies are shown as the accuracies in Table 5, since most bunsetsu-based dependency parsers out- put only unlabeled structure.	UAS	Unlabeled Accuracy Score$unlabeled accuracy scores$unlabeled attachment score$Unlabelled Attachment Score$unlabeled accuracy score$	2
Moreover, when we break down the results according to whether the head of a dependency is part of a multiple-IG word or a complete (single-IG) word, we observe a highly significant difference in accuracy, with only 53.2% UAS for multiple-IG heads versus 83.7% for single-IG heads.	UAS	Unlabeled Accuracy Score$unlabeled accuracy scores$unlabeled attachment score$Unlabelled Attachment Score$unlabeled accuracy score$	2
Benefiting from the rich features selected in the tree kernel space, our model achieved the best reported UAS of 93.72 without using any additional resource.	UAS	Unlabeled Accuracy Score$unlabeled accuracy scores$unlabeled attachment score$Unlabelled Attachment Score$unlabeled accuracy score$	2
We tested several evaluation measures that compute the results of each model, that are LAS [labeled attachment score], LUMP 8 [(labeled attachment score + UAS + mor- phology accuracy + part-of-speech accuracy)/4] and PMLAS 9 [labeled attachment score, morphology accuracy and part-of-speech accuracy].	UAS	Unlabeled Accuracy Score$unlabeled accuracy scores$unlabeled attachment score$Unlabelled Attachment Score$unlabeled accuracy score$	2
For the sole transition-based parsers trained with the selected features, we obtain for Chinese, Hungar- ian and Russian higher labeled and UASs.	UAS	Unlabeled Accuracy Score$unlabeled accuracy scores$unlabeled attachment score$Unlabelled Attachment Score$unlabeled accuracy score$	4
Furthermore, when we inspect the UASs (not shown here), we see that the unlabeled attachment score for idafa also decreases.	UAS	Unlabeled Accuracy Score$unlabeled accuracy scores$unlabeled attachment score$Unlabelled Attachment Score$unlabeled accuracy score$	4
However, it is possible to compute approximate UASs by training the constituent parsers on the NP-patched (Vadas and Curran, 2007) version of the data and then running the test output through just the first conversion script?that is, the modified version of Johansson and Nugues? (	UAS	Unlabeled Accuracy Score$unlabeled accuracy scores$unlabeled attachment score$Unlabelled Attachment Score$unlabeled accuracy score$	4
0.006 (0.004) max -0.03 (0.001) -0.01 (0.003) -0.04 (0.001) -0.02 (0.003) mult 0.16 (0.002) 0.11 (0.006) 0.21 (0.002) 0.03 (0.006) min 0.13 (0.001) 0.11 (0.007) 0.10 (0.001) 0.09 (0.007) gm 0.14 (0.001) 0.18 (0.005) 0.12 (0.001) 0.16 (0.005) dp -0.03 (0.002) -0.09 (0.007) -0.04 (0.002) -0.09 (0.007) Table 7: Means and SEs for Increases in Cosine with respect to the hd Baseline for Proposed Higher-Order Dependency Based Approach.	SE	Standard Error$set expansion$standard error$sentence external$	0
Corpus Predictability Model Performance SE baseline model 81.98% unigram model 82.86%  0.93 Read bigram predictability model 84.41%  1.10 unigram+bigram model 85.03%  1.04 baseline model 70.03% unigram model 72.22%  0.62 Spontaneous bigram model 74.46%  0.30 unigram+bigram model 77.43%  0.51 Table 4: Ripper Results for Accent Status Prediction Model Predictability Total Accented Word Not Accented Accentability unigram	SE	Standard Error$set expansion$standard error$sentence external$	0
Bootstrap Methods for SEs, Confidence Intervals, and Other Measures of Statistical Accuracy.	SE	Standard Error$set expansion$standard error$sentence external$	0
Method % Wins SE Game only 45.7 ?	SE	Standard Error$set expansion$standard error$sentence external$	0
i Decision Trees Rule Sets  Features Accuracy SE Accuracy SE  1.	SE	Standard Error$set expansion$standard error$sentence external$	0
324 Shutova, Teufel, and Korhonen Statistical Metaphor Processing Table 4 Examples of seed SE by the system.	SE	Standard Error$set expansion$standard error$sentence external$	1
This is because the work not only adopts the same candidate SE strategy mentioned previously, but also uti- lizes monolingual information when selecting NE pairs (only a simple bigram model is used, however).	SE	Standard Error$set expansion$standard error$sentence external$	1
c?2011 Association for Computational Linguistics HITS-based Seed Selection and Stop List Construction for Bootstrapping Tetsuo Kiso Masashi Shimbo Mamoru Komachi Yuji Matsumoto Graduate School of Information Science Nara Institute of Science and Technology Ikoma, Nara 630-0192, Japan {tetsuo-s,shimbo,komachi,matsu}@is.naist.jp Abstract In bootstrapping (seed SE), select- ing good seeds and creating stop lists are two effective ways to reduce semantic drift, but these methods generally need human super- vision.	SE	Standard Error$set expansion$standard error$sentence external$	1
Furthermore, Category (V) errors (Expansion Limitation, 5%) are caused by the problem that the desired candidate (i.e., reference) is excluded during the candidate 250 Chen, Zong, and Su A Joint Model to Identify and Align Bilingual Named Entities SE stage.	SE	Standard Error$set expansion$standard error$sentence external$	1
To apply Espresso for this task, we reformulate the task to be that of seed SE, and not classification.	SE	Standard Error$set expansion$standard error$sentence external$	1
Additional guidance, 325 Computational Linguistics Volume 39, Number 2 Table 5 Examples of seed SE by the baseline.	SE	Standard Error$set expansion$standard error$sentence external$	1
0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Fraction of test data used Accu racy     Semiparametric Landwehr et al Landwehr et al (TA) 0 50 100 150 200 2500 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Number of individuals R Accu racy     Landwehr et al (T) Holland & K. (unweighted) Holland & K. (weighted) Figure 3: Multiclass accuracy over number of test observations (left) and number of individuals R (right) with SEs.	SE	Standard Error$set expansion$standard error$sentence external$	2
The Landwehr et al model and variants also assign a 592 0 0.2 0.4 0.6 0.8 10 0.2 0.4 0.6 0.8 1 Fraction of test data used Acc urac y Figure 6: Multiclass accuracy over number of test observations with SEs for Semiparametric variants.	SE	Standard Error$set expansion$standard error$sentence external$	2
one SE?.	SE	Standard Error$set expansion$standard error$sentence external$	2
ber of individuals R (right) with SEs.	SE	Standard Error$set expansion$standard error$sentence external$	2
one SE?	SE	Standard Error$set expansion$standard error$sentence external$	2
SE.	SE	Standard Error$set expansion$standard error$sentence external$	2
Note that the error bars shown in this  section show the SE.	SE	Standard Error$set expansion$standard error$sentence external$	2
The first one is fired whenever a free SE pronoun is spotted; the second one takes the results of the first submodule and checks for nominal anaphora.	SE	Standard Error$set expansion$standard error$sentence external$	3
gogo-nara yamada-ga i-ru noda  afternoon-cond PN-nom be-pres aux-pres  (If you mean) the afternoon, Yamada will be here  Figure 3: Discourse relations with and without  anaphoric force  Among discourse relations with sentence xter-  nal anaphoric binding there are two types: those  whose antecedent part is bound sentence xter-  nally and those whose conclusion part is bound  SEly.	SE	Standard Error$set expansion$standard error$sentence external$	3
The system uses two resolution submodules which work in sequence: the first one is fired whenever a free SE pronoun is spotted; the second one takes the results of the first submodule and checks for nominal anaphora.	SE	Standard Error$set expansion$standard error$sentence external$	3
MEMs for Fra- meNet Classification.	MEM	Maximum Entropy Model$ME model$	0
MEMing  Toolkit for Python and C++.	MEM	Maximum Entropy Model$ME model$	0
A MEM for Prepositional Phrase Attachment.	MEM	Maximum Entropy Model$ME model$	0
224   CoNLL 2008: Proceedings of the 12th Conference on Computational Natural Language Learning, pages 203?207 Manchester, August 2008 Parsing Syntactic and Semantic Dependencies with Two Single-Stage MEMs ?	MEM	Maximum Entropy Model$ME model$	0
A MEM for Preposi-  tional Phrase Attachment.	MEM	Maximum Entropy Model$ME model$	0
A MEM  for Part-Of-Speech Tagging.	MEM	Maximum Entropy Model$ME model$	0
wever, recent theoretical and experimental re- sults in (Laerty et al 2001) have highlighted problems with the parameter estimation method for MEMs.	MEM	Maximum Entropy Model$ME model$	1
However, recent theoretical and experimental re- sults in (Laerty et al 2001) have highlighted problems with the parameter estimation method for MEMs.	MEM	Maximum Entropy Model$ME model$	1
Laerty et al 2001) give exper- imental results suggesting that CRFs can per- form signicantly better than MEMs.	MEM	Maximum Entropy Model$ME model$	1
MEMs have the advantage of being quite  exible in the features that can be incorporated in the model.	MEM	Maximum Entropy Model$ME model$	1
For system development, we used  MEGA model optimization package6, an imple- mentation of MEMs.	MEM	Maximum Entropy Model$ME model$	1
Although MEMs provide a nice framework for  incorporating arbitrary knowledge sources that can  be encoded as a large set of constraints, training and  using MEMs is extremely computationally  expensive.	MEM	Maximum Entropy Model$ME model$	1
Automatic identifi- cation of non-compositional multi-word expressions using LSA.	LSA	latent semantic analysis$language sample analysis$Latent Semantic Analysis$Latent semantic analysis$	0
Auto- matic identification of non-compositional multi-word expressions using LSA.	LSA	latent semantic analysis$language sample analysis$Latent Semantic Analysis$Latent semantic analysis$	0
Using LSA to improve access to textual infor-mation.	LSA	latent semantic analysis$language sample analysis$Latent Semantic Analysis$Latent semantic analysis$	0
One con- clusion of our research is that formality variation is omnipresent in natural corpora, but it does not follow that the identification of these differences on the lexical level is a trivial one; nevertheless, 90 we are able to make significant progress using the methods presented here, in particular the applica- tion of LSA to blog corpora.	LSA	latent semantic analysis$language sample analysis$Latent Semantic Analysis$Latent semantic analysis$	0
Contextual spelling correction using LSA.	LSA	latent semantic analysis$language sample analysis$Latent Semantic Analysis$Latent semantic analysis$	0
Unsupervised learning by proba- bilistic LSA.	LSA	latent semantic analysis$language sample analysis$Latent Semantic Analysis$Latent semantic analysis$	0
Researchers are begin- ning to discuss the limits of structured instruments in terms of which language impairments they tap into and how well they do so, and are advocating the po- tential benefits of LSA ?	LSA	latent semantic analysis$language sample analysis$Latent Semantic Analysis$Latent semantic analysis$	1
A solution to Plato?s problem: The LSA the- ory of the acquisition, induction, and representation of knowledge.	LSA	latent semantic analysis$language sample analysis$Latent Semantic Analysis$Latent semantic analysis$	2
2008), divides the problem in two parts: first the continuous representation is obtained by an adapta- tion of the LSA; then a Gaus- sian mixture model is learned using this continu- ous representation and included in a hidden Markov model.	LSA	latent semantic analysis$language sample analysis$Latent Semantic Analysis$Latent semantic analysis$	2
They also pro- posed to use LSA to compute the association strength with seed words.	LSA	latent semantic analysis$language sample analysis$Latent Semantic Analysis$Latent semantic analysis$	2
Explicit Semantic Analysis (ESA) (Gabrilovich and Markovitch, 2007) 0.74 (reimplemented in (Yeh et al 2009)) 0.71 Compact Hierarchical ESA (Liberman and Markovitch, 2009) 0.71 Hyperlink Graph (Milne and Witten, 2008) 0.69 Graph Traversal (Agirre et al 2009)) 0.66 Distributional Similarity (Agirre et al 2009)) 0.65 LSA (Finkelstein et al 2002) 0.56 Random Graph Walk (Hughes and Ramage, 2007) 0.55 Normalized Path-length (lch) (Strube and Ponzetto, 2006) 0.55 cPMId(? :	LSA	latent semantic analysis$language sample analysis$Latent Semantic Analysis$Latent semantic analysis$	2
A solution to Plato?s problem: The LSA theory of the acquisition, induction, and representation of knowledge.	LSA	latent semantic analysis$language sample analysis$Latent Semantic Analysis$Latent semantic analysis$	2
Baldwin et al, 2003) use WordNet::Similarity to provide an evaluation tool for multiword expressions that are identified via LSA. (	LSA	latent semantic analysis$language sample analysis$Latent Semantic Analysis$Latent semantic analysis$	2
LSA as a tool  to improve automatic speech recognition perfor- mance.	LSA	latent semantic analysis$language sample analysis$Latent Semantic Analysis$Latent semantic analysis$	3
LSA for Bulgarian literature.	LSA	latent semantic analysis$language sample analysis$Latent Semantic Analysis$Latent semantic analysis$	3
LSA for Russian literature investigation.	LSA	latent semantic analysis$language sample analysis$Latent Semantic Analysis$Latent semantic analysis$	3
LSA of stream A might also be usefully employed here.	LSA	latent semantic analysis$language sample analysis$Latent Semantic Analysis$Latent semantic analysis$	3
LSA for text-based research.	LSA	latent semantic analysis$language sample analysis$Latent Semantic Analysis$Latent semantic analysis$	3
LSA.	LSA	latent semantic analysis$language sample analysis$Latent Semantic Analysis$Latent semantic analysis$	3
Using umls CUIs (cuis) for word sense disam- biguation in the biomedical domain.	CUI	concept unique identifier$C0024881$	0
A typical entry for a concept is:    ID Course  Label Course  Subclassof Work    Table 1 A concept    where ID is the CUI, label is  the readable name of the concept,  subclassof indicates  the relation to another class.	CUI	concept unique identifier$C0024881$	0
trm278 CUI L0024669 S0059711 2003AC trm656 C0000726 L0000726 S0414154 2003AC . . . . . . . . . . . . . . .	CUI	concept unique identifier$C0024881$	1
What U.S. company did Sony purchase to form Sony Pic- tures ENT (SPE)??	ENT	Entertainment$entit$ENTAIL$	0
Sony Pictures ENT (SPE)?;	ENT	Entertainment$entit$ENTAIL$	0
Proceedings 5th Interna- tional Conference ENT Computing,  Cambridge, UK.	ENT	Entertainment$entit$ENTAIL$	0
s top politician on Tuesday evening, making way for Conservative 1 http://code.google.com/p/evbcorpus/ 2 http://sourceforge.net/apps/mediawiki/opennlp/ 3 http://cogcomp.cs.illinois.edu/page/software view/4 4 http://nlp.stanford.edu/ner/index.shtml 5 http://alias-i.com/lingpipe/index.html 87 Table 2: Number of files and sentences for each topic Topic File Sentence Economy 125 4,326 ENT 11 365 Health 336 21,107 Politics 141 4,253 Science 34 1,692 Social 110 3,699 Sport 22 838 Technology 104 2,609 Misc 117 117 Total 1,000 45,531 Figure 2: Architecture of building EVNECorpus from EVBCorpus leader David Cameron.? :	ENT	Entertainment$entit$ENTAIL$	0
For example, for  organization entities, systems distinguish between  Media and ENT organizations.	ENT	Entertainment$entit$ENTAIL$	0
In Proceedings of the AI, ALife and ENT Workshop, Mon- treal, CA.	ENT	Entertainment$entit$ENTAIL$	0
<K> ENT, sports, and games 12.	ENT	Entertainment$entit$ENTAIL$	0
We used two approaches for identifying story characters motivated by (Elson and McKeown, 2010): 1) named ENTy recognition was used for identifying proper names, e.g., ?	ENT	Entertainment$entit$ENTAIL$	1
Characters in chil- dren?s stories can either be human or non-human ENTies, i.e., animals and non-living objects, ex- hibiting anthropomorphic traits.	ENT	Entertainment$entit$ENTAIL$	1
3.2 Identification of Story Characters The second step is identifying candidate charac- ters (i.e., ENTies) that appear in the stories under analysis.	ENT	Entertainment$entit$ENTAIL$	1
This includes (i) tokenization, (ii) sen- tence splitting and identification of paragraph boundaries, (iii) part-of-speech (POS) tagging, (iv) lemmatization, (v) named ENTy recognition, (vi) dependency parsing, and (vii) co-reference analysis.	ENT	Entertainment$entit$ENTAIL$	1
Two broad approaches for the iden- tification of story characters were followed: (i) named ENTy recognition, and (ii) identification of character nominals, e.g., ?	ENT	Entertainment$entit$ENTAIL$	1
The more fre-  quently an ENTy is repeated, the more likely it  is to be the topic of the story and thus to be  a candidate for pronominalization.	ENT	Entertainment$entit$ENTAIL$	1
The log-OR behaves similarly for our basic algorithm, but appears to be more robust to other partitioning algorithms or tuning (see Section 6), so, for simplicity, we present it here as well.	OR	odds ratio$Odds ratio$ocator$outside region$Omission Rate$Operating Room$onset-rhyme$	0
Specifically, we prune links between pairs of mentions that are of men- tion distance more than 100, as well as values for ai that fall below a particular OR threshold with respect to the best setting of that ai in the BASIC model; that is, those for which log ( PBASIC (ai|x) maxj PBASIC (ai = j|x) ) is below a cutoff ?.	OR	odds ratio$Odds ratio$ocator$outside region$Omission Rate$Operating Room$onset-rhyme$	0
Examples include OR (Moham- mad and Hirst, 2006) and Turney?s (2001) IR-based pointwise mutual information (PMI-IR).	OR	odds ratio$Odds ratio$ocator$outside region$Omission Rate$Operating Room$onset-rhyme$	0
Alongside each feature, we show the words with the highest and lowest log- ORs with respect to the feature.	OR	odds ratio$Odds ratio$ocator$outside region$Omission Rate$Operating Room$onset-rhyme$	0
We first identified the most informative unigrams and bigrams using the information gain measure (Yang and Pedersen 1997), and then selected only the positive outcome predictors using OR (Mladenic and Grobelnik 1999).	OR	odds ratio$Odds ratio$ocator$outside region$Omission Rate$Operating Room$onset-rhyme$	0
For both benchmarks, the scoring scheme for measuring interaction set accuracy is in the form of a log OR of gene pairs either sharing anno- tations or physically interacting.	OR	odds ratio$Odds ratio$ocator$outside region$Omission Rate$Operating Room$onset-rhyme$	0
Anderberg)  4) [M14] Third Sokal-Sneath,       [M15] Sokal-Michiner,      [M16] Rogers-Tanimoto, [M17] Hamann  5) [M18] OR, [M19] Yule?s ,?	OR	odds ratio$Odds ratio$ocator$outside region$Omission Rate$Operating Room$onset-rhyme$	1
OR ad bc   M19.	OR	odds ratio$Odds ratio$ocator$outside region$Omission Rate$Operating Room$onset-rhyme$	1
c .565 .453 .546  1/bc .502 .532 .502  [M18] OR .443 .567 .456  Table 9.	OR	odds ratio$Odds ratio$ocator$outside region$Omission Rate$Operating Room$onset-rhyme$	1
Feature OR  SE p # turns per Q 10.411 0.787 0.003 # clarifications  1.043 0.033 0.024 # no input  2.001 0.176 <0.001 Table 2: User satisfaction regression 5 Conclusion Our results demonstrate the viability of conduct-ing survey interviews of the sort from which im-portant national statistics are derived with spoken dialog systems.	OR	odds ratio$Odds ratio$ocator$outside region$Omission Rate$Operating Room$onset-rhyme$	1
As an example, the following  3 AMs: OR, Yule?s ?	OR	odds ratio$Odds ratio$ocator$outside region$Omission Rate$Operating Room$onset-rhyme$	1
3.1 ACM Classifications We tested TFM on corpora representing genres from academic publications to Usenet postings, 2OR is defined as .0/ 12354%/fi68794$/ 123:.0/;6 , where p is Pr(k|C), the probability that term k is present given category C, and q is Pr(k|!C).	OR	odds ratio$Odds ratio$ocator$outside region$Omission Rate$Operating Room$onset-rhyme$	1
If one of two entity candidates causing crossing or type conflict has a verified lOR, the other can be regarded as an error and removed.	OR	odds ratio$Odds ratio$ocator$outside region$Omission Rate$Operating Room$onset-rhyme$	2
OR Most entities are often mentioned with geographic entities where they are lo- cated, especially when they are not familiar to general readers.	OR	odds ratio$Odds ratio$ocator$outside region$Omission Rate$Operating Room$onset-rhyme$	2
4.2 Gazetteer with LOR Most entities are often mentioned with geographic entities where they are lo- cated, especially when they are not familiar to general readers.	OR	odds ratio$Odds ratio$ocator$outside region$Omission Rate$Operating Room$onset-rhyme$	2
For instance, in the OUP-Hachette  English French dictionary, under bark we find the label  Bot(anical) attached to one meaning and the collOR  (of dog) associated with the other one.	OR	odds ratio$Odds ratio$ocator$outside region$Omission Rate$Operating Room$onset-rhyme$	2
We would need to investigate further how to make better  use of dictionary information such as collORs, etc.	OR	odds ratio$Odds ratio$ocator$outside region$Omission Rate$Operating Room$onset-rhyme$	2
The lOR information was also collected from the Probert Encyclopedia.	OR	odds ratio$Odds ratio$ocator$outside region$Omission Rate$Operating Room$onset-rhyme$	2
We can say that the lOR information is a special case of the co-occurrence information.	OR	odds ratio$Odds ratio$ocator$outside region$Omission Rate$Operating Room$onset-rhyme$	2
To extract the bigram features, we used a twitter-specific tokenizer (Potts, 2011), which marked uniform resource lORs (URLs), emoti- cons, and repeated characters, and which lowercased words that began with capital letters followed by lowercase letters (but left words in all capitals).	OR	odds ratio$Odds ratio$ocator$outside region$Omission Rate$Operating Room$onset-rhyme$	2
It filters incorrect word pairs by looking into outside of synonym words and context words in the other text (we call this OR the ?	OR	odds ratio$Odds ratio$ocator$outside region$Omission Rate$Operating Room$onset-rhyme$	3
We also demonstrate that the OR model achieves performance similar to that of the syllable bigram model using the local minima strategy.	OR	odds ratio$Odds ratio$ocator$outside region$Omission Rate$Operating Room$onset-rhyme$	6
Crucially, both the OR and the syllable bigram models achieve levels of perfor-mance that surpass the monosyllabic baseline.	OR	odds ratio$Odds ratio$ocator$outside region$Omission Rate$Operating Room$onset-rhyme$	6
There are, of course, other possible  strategies of segmentation, including division at syllable  boundaries and division based on the OR  structure within the syllable (for brange, br + angel  Evaluation of these alternative methods must await fur-  ther experimentation.	OR	odds ratio$Odds ratio$ocator$outside region$Omission Rate$Operating Room$onset-rhyme$	6
This research project is funded by the DE  Research Center (DFG).	DE	Dead End$Dual Encoder$German$Document Expansion$Dutch Examples$dental$	2
Clemens LANGO  Computational Design, University of Wuppertal  Hofaue 35-39  D-42103 Wuppertal, DEy  lango@code.uni-wuppertal.de  Abstract  We understand knowledge as an infinite sequence  of associations between what we know and what  we are in the process of acquiring as knowledge.	DE	Dead End$Dual Encoder$German$Document Expansion$Dutch Examples$dental$	2
Interactive Multimedia Navigation  Prof. Dr. Dr. Mihai NADIN  Computational Design, University of Wuppertal  Hofaue 35-39  D-42103 Wuppertal, DEy  nadin @ code.uni-wuppertal.de  Dipl.	DE	Dead End$Dual Encoder$German$Document Expansion$Dutch Examples$dental$	2
c?2006 Association for Computational Linguistics The Benefit of Stochastic PP Attachment to a Rule-Based Parser Kilian A. Foth and Wolfgang Menzel Department of Informatics Hamburg University D-22527 Hamburg DEy foth|menzel@nats.informatik.uni-hamburg.de Abstract To study PP attachment disambiguation as a benchmark for empirical methods in nat- ural language processing it has often been reduced to a binary decision problem (be- tween verb or noun attachment) in a par- ticular syntactic configuration.	DE	Dead End$Dual Encoder$German$Document Expansion$Dutch Examples$dental$	2
We believe that the approach could be straightforwardly extended to other Indoeuropean languages, such as Spanish, DE or English.	DE	Dead End$Dual Encoder$German$Document Expansion$Dutch Examples$dental$	2
In recent research in the field, the main effort has been to infer semantic classes for verbs, in English (Stevenson et al, 1999) and DE (Schulte im Walde and Brew, 2002).	DE	Dead End$Dual Encoder$German$Document Expansion$Dutch Examples$dental$	2
156  Coling 2010: Poster Volume, pages 9?17, Beijing, August 2010 DE Based on WordNet for Robust IR Eneko Agirre IXA NLP Group Univ.	DE	Dead End$Dual Encoder$German$Document Expansion$Dutch Examples$dental$	3
c?2006 Association for Computational Linguistics Language Model Information Retrieval with DE Tao Tao, Xuanhui Wang, Qiaozhu Mei, ChengXiang Zhai Department of Computer Science University of Illinois at Urbana Champaign Abstract Language model information retrieval de- pends on accurate estimation of document models.	DE	Dead End$Dual Encoder$German$Document Expansion$Dutch Examples$dental$	3
This pa- per presents a novel DE method based on a WordNet-based system to find related concepts and words.	DE	Dead End$Dual Encoder$German$Document Expansion$Dutch Examples$dental$	3
2 DE Using WordNet Our key insight is to expand the document with related words according to the background infor- mation in WordNet (Fellbaum, 1998), which pro- vides generic information about general vocabu- lary terms.	DE	Dead End$Dual Encoder$German$Document Expansion$Dutch Examples$dental$	3
2 DE Retrieval Model 2.1 The KL-divergence retrieval model We first briefly review the KL-divergence retrieval model, on which we will develop the document expansion technique.	DE	Dead End$Dual Encoder$German$Document Expansion$Dutch Examples$dental$	3
2.2 SemEval-2007 Task 1: CLIR Using IR metrics, this disambiguation scheme was evaluated against another competing platform and an algorithm provided by the Task 1 (Agirre et al, Topics All Nouns 1 .393 .467 5 .397 .478 25 .387 .456 200 .359 .420 Table 1: Accuracy on disambiguating words in Sem- Cor Task PUTOP Topic Expansion 0.30 DE 0.15 English Translation 0.17 SensEval 2 0.39 SensEval 3 0.33 Table 2: Performance results on Task 1 2007) organizers.	DE	Dead End$Dual Encoder$German$Document Expansion$Dutch Examples$dental$	3
August 2010 DE Based on WordNet for Robust IR Eneko Agirre IXA NLP Group Univ.	DE	Dead End$Dual Encoder$German$Document Expansion$Dutch Examples$dental$	3
The mo- tivation for this is that the closer two tokens occur together, the more likely it is that their relatedness is not acciDE.	DE	Dead End$Dual Encoder$German$Document Expansion$Dutch Examples$dental$	5
3 Tree Search vs. Dynamic  P rogramming  Once an appropriate function for measuring simi-  larity between pairs of segments has been designed,  290  Feature Phonological Numerical  name term value  Place  Manner  High  Back  \[bilabial\]  \[labioDE\]  \[DE\]  \[alveolar\]  \[retroflex\]  \[palato-alveolar\]  \[palatal\]  \[velar\]  \[uvular\]  \[pharyngeal\]  \[glottal\]  \[stop\]  \[affricate\]  \[fricative\]  \[approximant\]  \[high vowel\]  \[mid vowel\]  \[low vowel\]  \[high\]  \[mid\]  \[low\]  \[front\]  \[central\]  \[back\]  1.0  0.95  0.9  0.85  0.8  0.75  0.7  0.6  0.5  0.3  0.1  1.0  0.9  0.8  0.6  0.4  0.2  0.0  1.0  0.5  0.0  1.	DE	Dead End$Dual Encoder$German$Document Expansion$Dutch Examples$dental$	5
InciDEly, Covington's penalties for indels can be  expressed by an affine gap function with r -- 10 and  s= 40.	DE	Dead End$Dual Encoder$German$Document Expansion$Dutch Examples$dental$	5
For ex- ample, the distance between two tokens within a paragraph probably has not such a large effect on whether their relatedness score is reliable or ac- ciDE.	DE	Dead End$Dual Encoder$German$Document Expansion$Dutch Examples$dental$	5
The fact  that Covington's distance function is not a metric is  not an acciDE oversight; rather, it reflects certain  inherent characteristics of phones.	DE	Dead End$Dual Encoder$German$Document Expansion$Dutch Examples$dental$	5
3 Tree Search vs. Dynamic  P rogramming  Once an appropriate function for measuring simi-  larity between pairs of segments has been designed,  290  Feature Phonological Numerical  name term value  Place  Manner  High  Back  \[bilabial\]  \[labioDE\]  \[DE\]  \[alveolar\]  \[retroflex\]  \[palato-alveolar\]  \[palatal\]  \[velar\]  \[uvular\]  \[pharyngeal\]  \[glottal\]  \[stop\]  \[affricate\]  \[fricative\]  \[approximant\]  \[high vowel\]  \[mid vowel\]  \[low vowel\]  \[high\]  \[mid\]  \[low\]  \[front\]  \[central\]  \[back\]  1.0  0.95  0.9  0.85  0.8  0.75  0.7  0.6  0.5  0.3  0.1  1.0  0.9  0.8  0.6  0.4  0.2  0.0  1.0	DE	Dead End$Dual Encoder$German$Document Expansion$Dutch Examples$dental$	5
TTE range (h) FIN NFI FIN+NFI 0 2.58 3.07 8.51 1?4 2.38 2.64 8.71 5?8 3.02 3.08 8.94 9?12 5.20 5.47 6.57 13?24 5.63 5.54 6.09 25?48 13.14 15.59 5.81 49?96 17.20 20.72 6.93 97?144 30.38 41.18 6.97 > 144 55.45 70.08 9.41 Table 3: MAE for the FIN, NFI, and FIN+NFI systems in different TTE ranges.	MAE	Mean Absolute Error$Mean absolute error$	0
The model thus learned is evaluated using: (a) Error metrics namely, Mean Squared Error estimate, MAE esti- mate and Mean Percentage Error. (	MAE	Mean Absolute Error$Mean absolute error$	0
18.72 18.79 18.84 20.20 20.20 20.27 20.27 Baseline Mean 27.29 27.29 27.31 27.31 25.49 25.50 25.53 25.55 26.61 26.60 26.63 26.62 Training Median 10.38 10.28 7.68 7.62 11.09 11.04 8.65 8.50 10.61 10.54 8.03 7.99 Training Mean 11.62 11.12 8.73 8.29 12.43 11.99 9.53 9.16 11.95 11.50 9.16 8.76 Coverage 31,221 31,723 32.240 32,740 18,848 19,176 19,734 20,061 52,186 52,919 53,887 54,617 Table 2: Overall MAE for each method: difference in hours between the estimated time to event and the actual time to event, computed separately for the FIN and NFI subsets, and for the combination.	MAE	Mean Absolute Error$Mean absolute error$	0
332 Table 3: MAEs for the NBER download predictions. ???	MAE	Mean Absolute Error$Mean absolute error$	1
The collection was processed as a stream, sentence by sentence, using bigram fea- 233 d 16 32 64 128 256 SLSH 0.2885 0.2112 0.1486 0.1081 0.0769 LSH 0.2892 0.2095 0.1506 0.1083 0.0755 Table 1: MAE when using signatures gener- ated online (StreamingLSH), compared to offline (LSH).	MAE	Mean Absolute Error$Mean absolute error$	1
261.6627 (2) Mean AHT 675.74 seconds Median AHT 543 seconds Mode AHT 366 seconds Standard Deviation 487.72 seconds Correlation coefficient 0.3822 MAE 320.2 seconds Root mean squared error 450.64 seconds Total Number of Instances 6175 Table 2: Data statistics and the goodness of the regression model for 6175 AHT data points.	MAE	Mean Absolute Error$Mean absolute error$	1
We use a state of the art Automatic Speech Recognition system to transcribe the calls between agents and customers, which still results in high WERs (40%) and show that even from these noisy transcrip- tions of calls we can automatically build a domain model.	WER	word error rate$position independent$Word Error Rate$word error rat$	0
A neat solution to poor sentence-level evaluation proposed by Kulesza and Shieber (2004) is to use a Support Vector Machine, using features such as WER, to estimate sentence-level translation quality.	WER	word error rate$position independent$Word Error Rate$word error rat$	0
Figure 1 shows  the document structure in CLSR test collection, two ASR  transcripts are available for this data, in this work we use  the ASRTEXT2004A field provided by IBM research with  a WER of 38%.	WER	word error rate$position independent$Word Error Rate$word error rat$	0
Current automatic speech recognition technology for telephone calls have moderate to high WERs (Padmanabhan et al, 2002).	WER	word error rate$position independent$Word Error Rate$word error rat$	0
The ASR module, based on a hybrid speech recognition system that combines Hidden Markov Models with Multi-layer Perceptrons, with an av- erage WER of 24% (Amaral et al, 2007), greatly influences the performance of the subse- quent modules.	WER	word error rate$position independent$Word Error Rate$word error rat$	0
2 Background Real-time correction must be done within difficult constraints : with typical captioning rates of 130 words per minute, and 5 to 10% WER, the user must correct between 6 and 13 errors per minute.	WER	word error rate$position independent$Word Error Rate$word error rat$	0
With one-pass prediction we decide on the prediction for each WERly of other deci- sions.	WER	word error rate$position independent$Word Error Rate$word error rat$	1
We also show that this also holds for uni- gram BLEU and the WER error rate (PER) on a slightly augmented variant of CNs which allows for edges to carry multiple symbols.	WER	word error rate$position independent$Word Error Rate$word error rat$	1
The central fixa- tion bias in scene viewing: Selecting an optimal viewing WERly of motor biases and image feature distributions.	WER	word error rate$position independent$Word Error Rate$word error rat$	1
reference errors hypothesis errors Mister#N Mrs#N be#V is#V can#V Table 3: PER errors: actual words which are partic- ipating in the WER word error rate and their corresponding POS classes An illustration of PER errors is given in Table 3.	WER	word error rate$position independent$Word Error Rate$word error rat$	1
The WER n-best list word agree- ment is the average count of n-grams that contain the word e. It is computed as: NA k (e i ) = 1 N ng N ng ?	WER	word error rate$position independent$Word Error Rate$word error rat$	1
The acoustic models are sets of context-dependent(CD),  WER phone models, which include both  intra-word and cross-word contexts.	WER	word error rate$position independent$Word Error Rate$word error rat$	1
Nonterminals 2,284 1,837 Node Array Size 224KB 204KB WER 25.05% 11.91% Recognition Time 13.8xRT 1.7xRT Ambiguity 15.4 1.9 Table 2: Comparison Results in Section 4, we believe that the amount of am- biguity can be a significant factor in recognition performance.	WER	word error rate$position independent$Word Error Rate$word error rat$	2
0 5 10 15 20 25 30 35 40 45WER600000 800000 1000000 1200000 1400000 1600000 1800000 2000000 2200000 Tok ens tc.10000tc.20000tc.50000tfcf.5tnpd.1 (b) The number of word tokens remaining after pre-processing.	WER	word error rate$position independent$Word Error Rate$word error rat$	2
Both  Word Full Simple  Backoff Context Content  POS Errors - 1573 1718  POS Error Rate - 2.69 2.94  Word Perplexity 24.8 22.6 42.4  WER 26.0 24.9 28.9  Sentence Error Rate \] 56.6 55.2 58.1  Table 1: Comparison with Word-Based Model  models were restricted to only looking at the  previous two words (and POS tags) in the con-  text, and hence are trigram models.	WER	word error rate$position independent$Word Error Rate$word error rat$	2
Figures 3(a) and 3(b) show how the various pre-processing methods affect word type and token 244 0 5 10 15 20 25 30 35 40 45WER0 10000 20000 30000 40000 50000 60000 Typ es tc.10000tc.20000tc.50000tfcf.5tnpd.1 (a) The number of word types remaining after pre-processing.	WER	word error rate$position independent$Word Error Rate$word error rat$	2
However, 104 we think the observations about the content features used in this paper were not reliable for the following two reasons: the number of training responses was limited (1000 responses), and the ASR system had a relatively high WER (39%).	WER	word error rate$position independent$Word Error Rate$word error rat$	2
First, our  Word Perplexity  WER  Sentence Error Rate  I Back?ffl Decision Tree I  Word Word Class POS  Table 2: POS, Class and Word-Based Models  word-based ecision tree model outperforms  the word backoff model, giving an absolute  word-error ate reduction of 0.5%, which was  found significant by the Wilcoxon test (Z-score  -3.26).	WER	word error rate$position independent$Word Error Rate$word error rat$	2
We use a state of the art Automatic Speech Recognition system to transcribe the calls between agents and customers, which still results in high WERes (40%) and show that even from these noisy transcrip- tions of calls we can automatically build a domain model.	WER	word error rate$position independent$Word Error Rate$word error rat$	3
A neat solution to poor sentence-level evaluation proposed by Kulesza and Shieber (2004) is to use a Support Vector Machine, using features such as WERe, to estimate sentence-level translation quality.	WER	word error rate$position independent$Word Error Rate$word error rat$	3
Figure 1 shows  the document structure in CLSR test collection, two ASR  transcripts are available for this data, in this work we use  the ASRTEXT2004A field provided by IBM research with  a WERe of 38%.	WER	word error rate$position independent$Word Error Rate$word error rat$	3
Current automatic speech recognition technology for telephone calls have moderate to high WERes (Padmanabhan et al, 2002).	WER	word error rate$position independent$Word Error Rate$word error rat$	3
The ASR module, based on a hybrid speech recognition system that combines Hidden Markov Models with Multi-layer Perceptrons, with an av- erage WERe of 24% (Amaral et al, 2007), greatly influences the performance of the subse- quent modules.	WER	word error rate$position independent$Word Error Rate$word error rat$	3
2 Background Real-time correction must be done within difficult constraints : with typical captioning rates of 130 words per minute, and 5 to 10% WERe, the user must correct between 6 and 13 errors per minute.	WER	word error rate$position independent$Word Error Rate$word error rat$	3
3.4.3 GMM Features Bin features may be generalized to multi- dimensional kernels by using a Gaussian smoothing window instead of a rectangular window.	GMM	Gaussian Mixture Model$Gaussian mixture model$	0
P (F (D, t, j)|D)P (D) where P (F (D, t, j)|D) is the value of the PDF describing D calculated in the point F (D, t, j), P (F (D, t, j)|D) is the value of the PDF describ- ing D, P (D) is the area of the distribution describ- ing D and P (D) is the area of the distribution for D. In order to estimate the parameters describing the PDF of D and D the Expectation Maximization (EM) algorithm for the GMM (Redner and Walker, 1984) is exploited.	GMM	Gaussian Mixture Model$Gaussian mixture model$	0
A GMM was trained us- ing an Expectation Maximization method with the classification of instances performed by choosing the category which maximises the probability of fitting either of the Gaussian components.	GMM	Gaussian Mixture Model$Gaussian mixture model$	0
It is difficult to select the erroneous utterances to be rejected by using a classifier that 89 distinguishes speech from noise on the basis of the GMM (Lee et al, 2004); such disfluencies and resulting utterance fragments are parts of human speech.	GMM	Gaussian Mixture Model$Gaussian mixture model$	0
c?2009 Association for Computational Linguistics On Semi-Supervised Learning of GMMs for Phonetic Classification?	GMM	Gaussian Mixture Model$Gaussian mixture model$	0
e of domain experts, who can devise pedagogically valuable reading lists that order doc- Automatic Speech Recognition (ASR) with HMMs Noisy Channel Model Viterbi Decoding for ASR Training ASR Parameters Viterbi Algorithm Dynamic ProgrammingDecoding/Search ProblemHMMs Markov Chains HMM Pronunciation Lexicon  Iterative Parameter Estimation with EM Gaussian Acoustic Model Discrete Fourier Transforms GMMs Phonemes N-gram Language Model Figure 1: A human-authored concept graph excerpt, showing possible concepts related to automatic speech recognition and their concept dependencies.	GMM	Gaussian Mixture Model$Gaussian mixture model$	0
In this work we combined textual and prosodic features, using GMMs for the extracted and non- extracted classes.	GMM	Gaussian Mixture Model$Gaussian mixture model$	1
2012), where training data for neural network model is generated by forced decoding with tradi- tional GMMs.	GMM	Gaussian Mixture Model$Gaussian mixture model$	1
We built GMMs over the locations, with 3, 5, 8, 12, 17, and 23 components.	GMM	Gaussian Mixture Model$Gaussian mixture model$	1
Variational Bayes for d-dimensional GMMs.	GMM	Gaussian Mixture Model$Gaussian mixture model$	1
In the current system, we quantize the mean and variance of each GMM dimension to 5 and 3 bits, respectively.	GMM	Gaussian Mixture Model$Gaussian mixture model$	1
We  assume a GMM for the q event vectors V1, V2, ?,	GMM	Gaussian Mixture Model$Gaussian mixture model$	1
Slavonic, Old Czech, Russian, Ukranian, Serbian, Middle  Bulgarian (Slavonic group)  Albanian  Lithuanian (old and modern), Old Prussian, Lettish (Baltic  group)  Old English, Middle English, Danish, Old Icelandic, Old Gut-  niac, Dialectal Norwegian, Swedish, Old High German, Middle  High German, German (modern), Longobard, Gothic, Burgun-  dian (Germanic group)  Ligurian  Oscan, Umbrian  Latin, VL, Medieval Latin, Italian, Dialectal Italian,  Old French, Catalan, Old Spanish, Spanish (Latin and neo-La-  tin group)  Breton, Middle Breton, Old Breton, Cymric, Middle Cymric,  Old Cymric, Old Irish, Modern Irish, Ogamic, Scottish, Gaulish,  Galatian, Ladn-Gaulish, Latin-British, Vam~elais (Celtic group)  Finnish, Vogulian  The reader will notice that not all Indo-European languages	VL	Vulgar Latin$velar$	0
Note that the marginal probability of a specific Italian word con- ditioned on its VL parent is the sum over all possible derivations that generate it.	VL	Vulgar Latin$velar$	0
It is indeed likely that these were generally eliminated in VL.	VL	Vulgar Latin$velar$	0
For example, documents such as the Appendix Probi (Baehrens, 1922) provide in- dications of orthographic confusions which resulted from the growing gap between Classical Latin and VL phonology around the 3rd and 4th cen- turies AD.	VL	Vulgar Latin$velar$	0
A phone that has two dif-  ferent places of articulation, such as labio-VL \[w\],  can be close to two phones that are distant from each  other, such as labial \[b\] and VL \[g\].	VL	Vulgar Latin$velar$	1
The estimation of the threshold is based on the distance, measured in the phonetic Feature Values Type vowel, consonant Vowel length short, long, diphthong, schwa Vowel height high, mid, low Vowel frontness front mid back Lip rounding yes, no Consonant type stop, fricative, affricative, nasal, liquid Place of articulation labial, alveolar, palatal, labio-dental, dental, VL Consonant voicing yes, no Table 1: Phone features.	VL	Vulgar Latin$velar$	1
3 Tree Search vs. Dynamic  P rogramming  Once an appropriate function for measuring simi-  larity between pairs of segments has been designed,  290  Feature Phonological Numerical  name term value  Place  Manner  High  Back  \[bilabial\]  \[labiodental\]  \[dental\]  \[alveolar\]  \[retroflex\]  \[palato-alveolar\]  \[palatal\]  \[VL\]  \[uvular\]  \[pharyngeal\]  \[glottal\]  \[stop\]  \[affricate\]  \[fricative\]  \[approximant\]  \[high vowel\]  \[mid vowel\]  \[low vowel\]  \[high\]  \[mid\]  \[low\]  \[front\]  \[central\]  \[back\]  1.0  0.95  0.9  0.85  0.8  0.75  0.7  0.6  0.5  0.3  0.1  1.0  0.9  0.8  0.6  0.4  0.2  0.0  1.0  0.5  0.0  1.0  0.5  0.0  Table 3: Multivalued features and their values.	VL	Vulgar Latin$velar$	1
the word pairs chosen in this study have connections with both ethnic and regional di- alects: consonant cluster reduction is a feature of African-American English (Green, 2002) and Te- jano and Chicano English (Bayley, 1994; Santa Ana, 1991); th-stopping (as in wit/with) is a feature of African-American English (Green, 2002) as well as several regional dialects (Gordon, 2004; Thomas, 2004); the VL nasal in doin and goin is a property of informal speech.	VL	Vulgar Latin$velar$	1
the re- placement of the VL nasal with the coronal nasal, which has been associated with informal speech in many parts of the English-speaking world.1 The final word pair know/kno does not differ in pronunciation, and is included as a control.	VL	Vulgar Latin$velar$	1
Multimodal interfaces also stand to play a critical  role in the ongoing migration of interaction onto  wireless PI, such as  PDAs and next generation phones, which have  limited screen real estate and no keyboard.	PI	portable computing devices$post inference$Proto-Iberian$	0
3.5 ILP-based Post Inference The final semantic role labeling result is gener- ated through an ILP (Integer Linear Programming) based PI method.	PI	portable computing devices$post inference$Proto-Iberian$	1
lp solve 5.5 3 is chosen as our ILP problem solver during the PI stage.	PI	portable computing devices$post inference$Proto-Iberian$	1
Be- sides adding a predicate identification and a classification stages, our semantic de- pendency parsing simplifies the traditional four stages semantic role labeling into two: a maximum entropy based argument clas- sification and an ILP-based PI.	PI	portable computing devices$post inference$Proto-Iberian$	1
Semantic role labeling is achieved us- ing maximum entropy (MaxEnt) model based semantic role classification and integer linear programming (ILP) based PI.	PI	portable computing devices$post inference$Proto-Iberian$	1
Some other works paid much attention to the robust SRL (Pradhan et al, 2005b) and PI (Pun- yakanok et al, 2004).	PI	portable computing devices$post inference$Proto-Iberian$	1
A simple PI strategy is given for comparison, where the most possible label (including the virtual label ?	PI	portable computing devices$post inference$Proto-Iberian$	1
IT WORDNET  has been coupled with a parser and a number of  experiments have been performed to individu-  ate the methodology with the best trade-off b	IT	Italian$information technology$Information Technology$	0
it  Abst rac t   We present aprototype of the IT version of  WORDNET, a general computational lexical re-  source.	IT	Italian$information technology$Information Technology$	0
Lexical Discrimination with the IT Version of WORDNET  Alessandro  Ar ta le ,  Bernardo  Magn in i  and  Car lo  S t rapparava   IRST, 1-38050 Povo TN, Italy  e-mail: {artalelmagninilstrappa}@irst.	IT	Italian$information technology$Information Technology$	0
The ability to drop argu- ments is not correlated with agreement or case features in Urdu, as has been postulated for IT, for example.	IT	Italian$information technology$Information Technology$	0
Translating IT connectives into IT Sign Lan- guage.	IT	Italian$information technology$Information Technology$	0
Collins bilingual dictionaries for English/IT,  English/French, English/Spanisla, and  English/German  5.	IT	Italian$information technology$Information Technology$	0
We prepared fifty seed terms in total: ten terms for each of five genres; natural language processing, Japanese language, IT, current topics, and persons in Japanese history.	IT	Italian$information technology$Information Technology$	1
is in Table 1; a Table 2: Experimental Result Evaluation I Evaluation II domain correct incorrect total S F A C R total natural language processing 101 (93%) 8 ( 7%) 109 6 3 14 11 8 43 Japanese language 71 (81%) 17(19%) 88 7 0 19 5 1 32 IT 113 (88%) 15 (12%) 128 10 5 27 13 0 55 current topics 106 (91%) 10 ( 9%) 116 2 0 13 19 5 39 persons in Japanese history 128 (76%) 41 (24%) 169 18 0 23 1 0 42 Total 519 (85%) 91(15%) 610 43 8 96 49 14 210 check mark ?	IT	Italian$information technology$Information Technology$	1
For example, if a text discussed inventions in IT, there could be groups of a few discourse segments each talking about inventions by specific companies.	IT	Italian$information technology$Information Technology$	1
2 Assistance in Accessing of Information  The assistive technologies played an important role  in the olden days and even today with emerging  IT it does play a significant  role.	IT	Italian$information technology$Information Technology$	1
Telephone-linked care for physical ac- tivity: a qualitative evaluation of the use patterns of an IT program for patients.	IT	Italian$information technology$Information Technology$	1
In to- tal, we created 50 Web datasets on the topics such as find a good kindergarten, purchase a used car, plan a trip to DC, how to make a cake, find a good wed- ding videographer, write a survey paper for health care systems, find the best deals for a Mother?s day gift, write a survey paper for social network, write a survey paper for EU?s finance, and write a survey paper for IT.	IT	Italian$information technology$Information Technology$	1
This research was supported by the IT Center through their grant to the first author.	IT	Italian$information technology$Information Technology$	2
In International Conference on Computer Science and IT, pages 277?281, Los Alamitos, CA, USA.	IT	Italian$information technology$Information Technology$	2
c?2006 Association for Computational Linguistics Matching Syntactic-Semantic Graphs for Semantic Relation Assignment Vivi Nastase1 and Stan Szpakowicz1,2 1 School of IT and Engineering, University of Ottawa, Ottawa, Canada 2 Institute of Computer Science, Polish Academy of Sciences, Warsaw, Poland {vnastase,szpak}@site.uottawa.ca Abstract We present a graph-matching algorithm for semantic relation assignment.	IT	Italian$information technology$Information Technology$	2
c?2010 Association for Computational Linguistics Features for Detecting Hedge Cues Nobuyuki Shimizu IT Center The University of Tokyo shimizu@r.dl.itc.u-tokyo.ac.jp Hiroshi Nakagawa IT Center The University of Tokyo n3@dl.itc.u-tokyo.ac.jp Abstract We present a sequential labeling approach to hedge cue detection submitted to the bi- ological portion of task 1 for the CoNLL- 2010 shared task.	IT	Italian$information technology$Information Technology$	2
c?2007 Association for Computational Linguistics Near-Synonym Choice in an Intelligent Thesaurus Diana Inkpen School of IT and Engineering, University of Ottawa 800 King Edward, Ottawa, ON, Canada, K1N 6N5 diana@site.uottawa.ca Abstract An intelligent thesaurus assists a writer with alternative choices of words and or- ders them by their suitability in the writ- ing context.	IT	Italian$information technology$Information Technology$	2
Transliteration Equivalence using CCA.	CCA	Canonical Correlation Analysis$canonical correlation analysis$	0
Briefly, given a multi-view data set, CCA is a tech- nique to find the projection directions in each view so that the objects when projected along these di- 408 rections are maximally aligned (Hotelling, 1936).	CCA	Canonical Correlation Analysis$canonical correlation analysis$	0
A nice geometric interpretation of these processes is proposed in (Gaussier et al, 2004), which fur- thermore introduces variants based on Fisher ker- nels, CCA and a com- bination of them, leading to an improvement of the F1-score of 2% (from 0.14 to 0.16) when con- sidering the top 20 candidates.	CCA	Canonical Correlation Analysis$canonical correlation analysis$	0
We formulate the 1257 problem of learning hash functions as an opt- mization problem whose relaxation can be solved using CCA.	CCA	Canonical Correlation Analysis$canonical correlation analysis$	0
CCA: An Overview with Application to Learning Methods.	CCA	Canonical Correlation Analysis$canonical correlation analysis$	0
Some of such embeddings are based on CCA (Hardoon et al, 2004) e.g. (Gong et al, 2014; Klein et al, 2015; Plummer et al, 2015), linear models with ranking loss (Frome et al, 2013; Karpathy and Fei-Fei, 2015; Socher et al, 2014; Weston et al, 2011) or non-linear deep learning models (Kiros et al, 2014; Mao et al, 2015; Ngiam et al, 2011).	CCA	Canonical Correlation Analysis$canonical correlation analysis$	0
Word embed- dings can be learned from large-scale unlabeled texts through context-predicting models (e.g., neu- ral network language models) or spectral methods (e.g., CCA) in an unsu- pervised setting.	CCA	Canonical Correlation Analysis$canonical correlation analysis$	1
Gener- alized CCA with missing val- ues.	CCA	Canonical Correlation Analysis$canonical correlation analysis$	1
Multi- view regression via CCA.	CCA	Canonical Correlation Analysis$canonical correlation analysis$	1
We show that it is important to scale features by their inverse variance, in a manner that is closely related to methods used in CCA.	CCA	Canonical Correlation Analysis$canonical correlation analysis$	1
A bilingual auto-encoder was proposed by Chandar et al (2014), while Faruqui and Dyer (2014) applied CCA to parallel data to improve monolingual embeddings.	CCA	Canonical Correlation Analysis$canonical correlation analysis$	1
Generalized CCA of matrices with missing rows: a simulation study.	CCA	Canonical Correlation Analysis$canonical correlation analysis$	1
recognizers are represented t n  ATNk [ I l l  f o m ,   are q u i t s  s i m p l e ,  and are not described further i n  t h i s  paper .	ATN	Augmented Transition Networ$Augmented Transition Network$augmented transition networks$	0
American Journal of Computational Linguistics, Volume 8, Number 1, January-March 1982 15  Stuart  C. Shapiro Generalized ATNk Grammars  (A  (BE  (DOG  (IS  LUCY  SAW  SAWI   SEE   SEEN  SWEET  (WAS  (YOUNG  CTGY  CTGY  CTGY  CTGY  CTGY  CTGY  CTGY  CTGY  CTGY  CTGY  CTGY  CTGY  CTGY  DET  v))  N))  v)  NPR  N)  V)  N)  V)  V)  im J   v)  ADJ   ))  ROOT .	ATN	Augmented Transition Networ$Augmented Transition Network$augmented transition networks$	0
Kaplan, R. 1973 ATNks as Psychological  Models of Sentence Comprehension.	ATN	Augmented Transition Networ$Augmented Transition Network$augmented transition networks$	0
Pereira, Fernando C. N.; and Warren, David H. D. 1980 Definite  Clause Grammars for Language Analysis--a Survey of the For-  malism and a Comparison with ATNks.	ATN	Augmented Transition Networ$Augmented Transition Network$augmented transition networks$	0
Definite Clause Grammars for Language Analysis  - A Survey of the Formalism and a Comparison  with ATNks.	ATN	Augmented Transition Networ$Augmented Transition Network$augmented transition networks$	0
A Survey of the Formalism and a Comparison  with ATNks.	ATN	Augmented Transition Networ$Augmented Transition Network$augmented transition networks$	0
recognizers are represented t n  ATN [ I l l  f o m ,   are q u i t s  s i m p l e ,  and are not described further i n  t h i s  paper .	ATN	Augmented Transition Networ$Augmented Transition Network$augmented transition networks$	1
American Journal of Computational Linguistics, Volume 8, Number 1, January-March 1982 15  Stuart  C. Shapiro Generalized ATN Grammars  (A  (BE  (DOG  (IS  LUCY  SAW  SAWI   SEE   SEEN  SWEET  (WAS  (YOUNG  CTGY  CTGY  CTGY  CTGY  CTGY  CTGY  CTGY  CTGY  CTGY  CTGY  CTGY  CTGY  CTGY  DET  v))  N))  v)  NPR  N)  V)  N)  V)  V)  im J   v)  ADJ   ))  ROOT .	ATN	Augmented Transition Networ$Augmented Transition Network$augmented transition networks$	1
Kaplan, R. 1973 ATNs as Psychological  Models of Sentence Comprehension.	ATN	Augmented Transition Networ$Augmented Transition Network$augmented transition networks$	1
Pereira, Fernando C. N.; and Warren, David H. D. 1980 Definite  Clause Grammars for Language Analysis--a Survey of the For-  malism and a Comparison with ATNs.	ATN	Augmented Transition Networ$Augmented Transition Network$augmented transition networks$	1
Definite Clause Grammars for Language Analysis  - A Survey of the Formalism and a Comparison  with ATNs.	ATN	Augmented Transition Networ$Augmented Transition Network$augmented transition networks$	1
A Survey of the Formalism and a Comparison  with ATNs.	ATN	Augmented Transition Networ$Augmented Transition Network$augmented transition networks$	1
6- F.C. PEREIRA & D.H.D. WARREN, Definite clause grammars for language analysis -  a survey of the formalism and comparison with ATN,  Artif icial Intell igence, 13(3), 1980.	ATN	Augmented Transition Networ$Augmented Transition Network$augmented transition networks$	2
Pereira, F.C.N. and Warren, D.H.D. 1980 Definite clause gram-  mars for language analysis - a survey of the formalism and a  comparison with ATN.	ATN	Augmented Transition Networ$Augmented Transition Network$augmented transition networks$	2
,by Woods in  the framework of ATN (Woods 1973).	ATN	Augmented Transition Networ$Augmented Transition Network$augmented transition networks$	2
ars, then they went over to transformational gram-  mars, and then some of them started using augmented phrase structure grammars again, (space  for moral~. Whilst we are in this careful scholarly mode, let us do the same service for computa-  tional linguistics: once upon a time computational linguists (i.e. builders of parsers) used aug-  mented phrase structure grammars, then they went over to ATN, and  then many of them started using augmented phrase structure grammars again, (space for  moral~. There are people who would have you believe in one or other of these stories (e.g.  Chomsky 1983, p65, for the first).	ATN	Augmented Transition Networ$Augmented Transition Network$augmented transition networks$	2
In an early paper by  Woods (1970), alternative algorithms that can be  used with ATN are dis-  cussed, including the bottom-up and Earley algor-  ithms.	ATN	Augmented Transition Networ$Augmented Transition Network$augmented transition networks$	2
It is important o note, however, that the  concept of ATN is a par-  ticular way to represent linguistic knowledge; it does  not require that the program using the networks  operate in top-down fashion.	ATN	Augmented Transition Networ$Augmented Transition Network$augmented transition networks$	2
Testing GC can  be done by using a test suite (cp. (	GC	grammatical coverage$generative component$Gail Collins$Generat ionCoverag$	0
We performed a number of post-hoc analyses to estimate the relative weight on fragmentation of variou s linguistic factors for which the MUC-4 version of ALEMBIC had incomplete GC .	GC	grammatical coverage$generative component$Gail Collins$Generat ionCoverag$	0
Five hundred sen-  tences, of up to ten words in length, falling  within CLARE's current core lexical (1600  root forms) and GC were  taken at random from the LOB corpus.	GC	grammatical coverage$generative component$Gail Collins$Generat ionCoverag$	0
It must be complemented with  lexical depth and GC.	GC	grammatical coverage$generative component$Gail Collins$Generat ionCoverag$	0
But translation quality rests on the  linguistic competence of the MT system which again  is based first and foremost on GC  and lexicon size.	GC	grammatical coverage$generative component$Gail Collins$Generat ionCoverag$	0
This results in high precision (96.7%), but re- call is low (82.3%) due to parse failures caused by lack of GC 2.	GC	grammatical coverage$generative component$Gail Collins$Generat ionCoverag$	0
Combining the generative syntactic model and composite language model (GEN) with equal weight yielded a devtest BLEU score of only 0.4513, while discriminatively train- ing the GC models (GLOBAL) increased the score to 0.7679.	GC	grammatical coverage$generative component$Gail Collins$Generat ionCoverag$	1
and Moulines, 2007) and incremental EM (Neal and Hinton, 1998) which they use to update the align- ment models (the GC of SMT) on the fly.	GC	grammatical coverage$generative component$Gail Collins$Generat ionCoverag$	1
The last few years have seen most work in language  processing  devoted to the  development of  i n t e g r a t e d  s y s  terns, combining  syntactic, semantic, pragmatic,  and GCs.	GC	grammatical coverage$generative component$Gail Collins$Generat ionCoverag$	1
A second step consisted in simplifying the  GC by reducing the rules in favour  of well-formedness conditions, o-called filters.	GC	grammatical coverage$generative component$Gail Collins$Generat ionCoverag$	1
1 In t roduct ion   1.1 Historic Origin  Early transformational grammar consisted of a  rather complex GC and an equally  complex and equally imperspicuous transformational  component.	GC	grammatical coverage$generative component$Gail Collins$Generat ionCoverag$	1
Generative and  analytical components can be derived from the gram-  mars: the analytical component maps a sentence of the  source language into one or more semantic D-trees;  the GC maps a semantic D-tree into  one or more sentences of the target language.	GC	grammatical coverage$generative component$Gail Collins$Generat ionCoverag$	1
ing of SLs,  notably that of the dictionary itself.	SLs	sublanguages$Sign Languages$	0
On a more theoretical level, the  automatic lexicon builder will add greatly  to our understanding of SLs,  notably that of the dictionary itself.	SLs	sublanguages$Sign Languages$	0
We demonstrated with several  examples that selectional restrictions do not  generalize across SLs, and  acquiring them by hand is often inintuitive  and very time-consuming.	SLs	sublanguages$Sign Languages$	0
The very  possibi l i ty of creating a large, general-  269  language lexicon points toward a time when  SLs will be obsolete for many of  the purposes for which they are now used;  but they will still be useful and  interesting for a long time to come, and  the automat ic  lexicon builder gives us a  new tool for analyzing them.	SLs	sublanguages$Sign Languages$	0
Two biomedical SLs: a description based on the theories of Zellig Harris.	SLs	sublanguages$Sign Languages$	0
The design of the lexicon builder is  inuended to be general enough to make it  useful for others building lexicons for  large natural language processing systems  involving different SLs.	SLs	sublanguages$Sign Languages$	0
An integrated interactive nviron-  ment has been built to experiment with these ideas,  which can also be used to define SLs and  select strategies for particular applications.	SLs	sublanguages$Sign Languages$	0
In Proceedings of the 7th Work- shop on the Representation and Processing of SLs: Corpus Mining, The 10th In- ternational Conference on Language Resources and Evaluation (LREC 2016), Portoroz, Slovenia.	SLs	sublanguages$Sign Languages$	1
HamNoSys Version 2.0: Hamburg  Notation System for SLs: An Introduc- tory Guide, volume 5 of International Studies on Sign  Language and Communication of the Deaf.	SLs	sublanguages$Sign Languages$	1
Data-Driven Machine Transla- tion for SLs.	SLs	sublanguages$Sign Languages$	1
In LREC Workshop on the Rep- resentation and Processing of SLs: Be- yond the Manual Channel.	SLs	sublanguages$Sign Languages$	1
In Proceedings of the 6thWorkshop on the Representation and Processing of SLs: Beyond the Manual Channel, The 9th International Conference on Language Resources and Evaluation (LREC 2014), Reykjavik, Iceland.	SLs	sublanguages$Sign Languages$	1
Hamburg Notation System for SLs - An Introductory Guide.	SLs	sublanguages$Sign Languages$	1
We further demonstrate how S-MART can be applied to tweet entity linking, an important and challenging task underlying many applications in- cluding product feedback (Asur and Huberman, 2010) and TDT (Math- ioudakis and Koudas, 2010).	TDT	Tactile Detection Task$topic detection and tracking$Topic detection and tracking$	1
This task is important since resolved event coreference is useful in various tasks such as TDT, information extrac- tion, question answering, textual entailment, and contradiction detection.	TDT	Tactile Detection Task$topic detection and tracking$Topic detection and tracking$	1
The well-known past experience from  IR ~ that notions of who, what, where, when, why  and how may not make a great contribution to the  TDT task (Allan and Papka,  1998) causes this fact, i.e. a topic and an event are  different from each other 1 .	TDT	Tactile Detection Task$topic detection and tracking$Topic detection and tracking$	1
2004) is centroid based multi-document sum- marizer which generates summaries using cluster  centroids produced by TDT  system.	TDT	Tactile Detection Task$topic detection and tracking$Topic detection and tracking$	1
On-line LDA: Adaptive topic models for mining text streams with applications to TDT.	TDT	Tactile Detection Task$topic detection and tracking$Topic detection and tracking$	1
1 Introduction  Our work aims to the acquisition of deep gram- matical information for nouns, because having in- formation such as countability and complementa- tion is necessary for different applications, espe- cially for deep analysis grammars, but also for  question answering, TDT,  etc.	TDT	Tactile Detection Task$topic detection and tracking$Topic detection and tracking$	1
TDT: event- based information organization.	TDT	Tactile Detection Task$topic detection and tracking$Topic detection and tracking$	2
TDT function is to  cluster the hot events and capture the rela- tionship between the relevant events based on  the collected data from websites (event also  referred as topic in this paper).	TDT	Tactile Detection Task$topic detection and tracking$Topic detection and tracking$	2
TDT pi- lot study: Final report.	TDT	Tactile Detection Task$topic detection and tracking$Topic detection and tracking$	2
TDT: event-based in- formation organization Kluwer Academic Publish- ers, pages 197?224.	TDT	Tactile Detection Task$topic detection and tracking$Topic detection and tracking$	2
TDT: event-based information organization.	TDT	Tactile Detection Task$topic detection and tracking$Topic detection and tracking$	2
VSMs These text similarity measures project texts onto high-dimensional vec- tors which are then compared.	VSM	Vector Space Model$vector space model$Vector Space Mode$	0
3.1 VSMs of Semantics In this section, we describe several methods for producing the semantic vectors associated with each event head or argument; i.e., the function sem.	VSM	Vector Space Model$vector space model$Vector Space Mode$	0
c?2008 Association for Computational Linguistics Sentiment VSM for   Lyric-based Song Sentiment Classification      Yunqing Xia Linlin Wang  Center for Speech and language Tech.	VSM	Vector Space Model$vector space model$Vector Space Mode$	0
In this article we will discuss in detail sev- eral experiments of morphological cue induction for lexical classification (C?avar et al, 2004a) and (C?avar et al, 2004b) using VSMs for category induction and subsequent rule for- mation.	VSM	Vector Space Model$vector space model$Vector Space Mode$	0
In Proceedings of EACL 2014, Workshop on Contin- uous VSMs and their Compositional- ity (CVSC).	VSM	Vector Space Model$vector space model$Vector Space Mode$	0
A Systematic Study of Semantic VSM Parameters.	VSM	Vector Space Model$vector space model$Vector Space Mode$	0
From fre- quency to meaning: VSMs of seman- tics.	VSM	Vector Space Model$vector space model$Vector Space Mode$	1
It is also done in  the VSM, so we again represent the  snippets by vectors.	VSM	Vector Space Model$vector space model$Vector Space Mode$	1
Much of the work in this study is based on that by  Bagga and Baldwin (1998), where they presented a  successful cross-document coreference resolution  algorithm to resolve ambiguities between people having  the same name using the VSM.	VSM	Vector Space Model$vector space model$Vector Space Mode$	1
Incremental vector space          Our intent with the incremental VSM  is to approximate the work reported by Bagga and  Baldwin (1998).	VSM	Vector Space Model$vector space model$Vector Space Mode$	1
To test this, we conducted a small study in which we compared the relatedness scores obtained by NGD and the semantic VSM to the human ratings compiled by Finkelstein et al (2002).	VSM	Vector Space Model$vector space model$Vector Space Mode$	1
The system then  computes the similarity of that summary with each of  the other summaries using the VSM.	VSM	Vector Space Model$vector space model$Vector Space Mode$	1
VSMls These text similarity measures project texts onto high-dimensional vec- tors which are then compared.	VSM	Vector Space Model$vector space model$Vector Space Mode$	2
3.1 VSMls of Semantics In this section, we describe several methods for producing the semantic vectors associated with each event head or argument; i.e., the function sem.	VSM	Vector Space Model$vector space model$Vector Space Mode$	2
c?2008 Association for Computational Linguistics Sentiment VSMl for   Lyric-based Song Sentiment Classification      Yunqing Xia Linlin Wang  Center for Speech and language Tech.	VSM	Vector Space Model$vector space model$Vector Space Mode$	2
In this article we will discuss in detail sev- eral experiments of morphological cue induction for lexical classification (C?avar et al, 2004a) and (C?avar et al, 2004b) using VSMls for category induction and subsequent rule for- mation.	VSM	Vector Space Model$vector space model$Vector Space Mode$	2
In Proceedings of EACL 2014, Workshop on Contin- uous VSMls and their Compositional- ity (CVSC).	VSM	Vector Space Model$vector space model$Vector Space Mode$	2
A Systematic Study of Semantic VSMl Parameters.	VSM	Vector Space Model$vector space model$Vector Space Mode$	2
On the Shortest Spanning Subtree of a Graph and the TSP.	TSP	Traveling Salesman Problem$Travelling Salesman Problem$	0
If each sen- tence in the source document set has one concept (i.e. Table 2 is a diagonal matrix), Eq.2 becomes the Prize Collecting TSP (Balas, 1989).	TSP	Traveling Salesman Problem$Travelling Salesman Problem$	0
By  viewing edge costs as log probabilities, we can cast the TSP  as one of optimizing P(e), that is, of finding the best source word order in Model 1  decoding.	TSP	Traveling Salesman Problem$Travelling Salesman Problem$	0
c?2016 Association for Computational Linguistics AMR-to-text generation as a TSP Linfeng Song1, Yue Zhang3, Xiaochang Peng1, Zhiguo Wang2 and Daniel Gildea1 1Department of Computer Science, University of Rochester, Rochester, NY 14627 2IBM T.J. Watson Research Center, Yorktown Heights, NY 10598 3Singapore University of Technology and Design Abstract The task of AMR-to-text generation is to gen- erate grammatical text that sustains the seman- tic mean	TSP	Traveling Salesman Problem$Travelling Salesman Problem$	0
If word pairs have probabilities attached  to them, then word ordering resembles the finding the least-cost circuit, also known as the  TSP.	TSP	Traveling Salesman Problem$Travelling Salesman Problem$	0
This problem can be formulated as the TSP and its variants.	TSP	Traveling Salesman Problem$Travelling Salesman Problem$	0
Finding the permutation with the high- est probability in the graph formulation is equal to finding the shortest tour in the graph or, equally, solving the TSP.	TSP	Traveling Salesman Problem$Travelling Salesman Problem$	1
Horvat and Byrne (2014) models the search for the highest prob- ability permutation of words under an n-gram model as a TSP; however, direct comparisons to existing works are not provided.	TSP	Traveling Salesman Problem$Travelling Salesman Problem$	1
TSP ?	TSP	Traveling Salesman Problem$Travelling Salesman Problem$	1
3It is worth mentioning that Cutting Plane Algorithms have been successfully applied for solving very large instances of the TSP, a problem essentially equivalent to the decoding in IBM Model 4.	TSP	Traveling Salesman Problem$Travelling Salesman Problem$	1
6 Related Research Snow et al(2008) and Sorokin and Forsyth (2008) showed that AMT use in providing non-expert annotations for NLP tasks.	AMT	Amazon?s MechanicalTurk$Amazon?s Mechanical Turk$Amazon Mechanical Turks$Amazon Mechanical Turk$	0
AMT (MTurk) is a virtual marketplace that allows anyone to create and post tasks to be completed by human workers around the globe.	AMT	Amazon?s MechanicalTurk$Amazon?s Mechanical Turk$Amazon Mechanical Turks$Amazon Mechanical Turk$	1
c?2010 Association for Computational Linguistics Using Mechanical Turk to Annotate Lexicons for Less Commonly Used Languages Ann Irvine and Alexandre Klementiev Computer Science Department Johns Hopkins University Baltimore, MD 21218 {anni,aklement}@jhu.edu Abstract In this work we present results from using AMT (MTurk) to an- notate translation lexicons between English and a large set of less commonly used lan- guages.	AMT	Amazon?s MechanicalTurk$Amazon?s Mechanical Turk$Amazon Mechanical Turks$Amazon Mechanical Turk$	1
72  Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with AMT, pages 108?113, Los Angeles, California, June 2010.	AMT	Amazon?s MechanicalTurk$Amazon?s Mechanical Turk$Amazon Mechanical Turks$Amazon Mechanical Turk$	1
In Proceedings of the NAACL HLT Workshop on Creating Speech and Language Data With AMT, pages 108?113.	AMT	Amazon?s MechanicalTurk$Amazon?s Mechanical Turk$Amazon Mechanical Turks$Amazon Mechanical Turk$	1
of Computer Science and The Center for Language and Speech Processing Johns Hopkins University Baltimore, MD 21218, USA ozaidan@cs.jhu.edu Abstract The past few years have seen an increasing interest in using AMT for purposes of collecting data and perform- ing annotation tasks.	AMT	Amazon?s MechanicalTurk$Amazon?s Mechanical Turk$Amazon Mechanical Turks$Amazon Mechanical Turk$	1
In Proceedings of the NAACL HLT Workshop on Creating Speech and Language Data With AMT, pages 208?211.	AMT	Amazon?s MechanicalTurk$Amazon?s Mechanical Turk$Amazon Mechanical Turks$Amazon Mechanical Turk$	1
ge Data with AMT, pages 108?113, Los Angeles, California, June 2010.	AMT	Amazon?s MechanicalTurk$Amazon?s Mechanical Turk$Amazon Mechanical Turks$Amazon Mechanical Turk$	1
We then describe an empirical study us- ing AMT for evaluating gener- ated referring expressions.	AMT	Amazon?s MechanicalTurk$Amazon?s Mechanical Turk$Amazon Mechanical Turks$Amazon Mechanical Turk$	2
We ask 5 in- dependent annotators on AMT to read each p and then determine whether h is true, false, or unclear given p.7 We take the majority an- swer as the true label.	AMT	Amazon?s MechanicalTurk$Amazon?s Mechanical Turk$Amazon Mechanical Turks$Amazon Mechanical Turk$	3
This evaluation is performed us- ing a set of human-corrected sentences gathered via AMT, an online service where workers are paid to perform a short task, and further filtered for correctness by an undergraduate research assistant.	AMT	Amazon?s MechanicalTurk$Amazon?s Mechanical Turk$Amazon Mechanical Turks$Amazon Mechanical Turk$	3
In order to  investigate how WSI could be accomplished using  AMT, 50 words were random- ly sampled from the 270, and their definitions were  extracted from the Longman Dictionary of Con- temporary English (LDOCE) and the Cambridge  Advanced Learner's Dictionary (CALD).	AMT	Amazon?s MechanicalTurk$Amazon?s Mechanical Turk$Amazon Mechanical Turks$Amazon Mechanical Turk$	3
We then utilized the AMT Service1.	AMT	Amazon?s MechanicalTurk$Amazon?s Mechanical Turk$Amazon Mechanical Turks$Amazon Mechanical Turk$	3
c?2010 Association for Computational Linguistics Clustering dictionary definitions using AMT   Gabriel Parent Maxine Eskenazi  Language Technologies Institute  Carnegie Mellon University  5000 Forbes Avenue  15213 Pittsburgh, USA    {gparent,max}@cs.cmu.edu        Abstract  Vocabulary tutors need word sense disambig- uation (WSD) in order to provide exercises  and assessments that match the sense of words  being taught.	AMT	Amazon?s MechanicalTurk$Amazon?s Mechanical Turk$Amazon Mechanical Turks$Amazon Mechanical Turk$	3
AMT (MTurk) has been  used for the purpose of word sense disambiguation  (Snow et al 2008).	AMT	Amazon?s MechanicalTurk$Amazon?s Mechanical Turk$Amazon Mechanical Turks$Amazon Mechanical Turk$	3
= {p1, p3, p4} 3 Matching Score 3.1 NA: Frequency-based Translation (FB) A naive solution for map translation is to use co- occurrence of multilingual tags.	NA	Naive Approach$Null adjunction$narrow$N t- Adjective$Network Analysis$	0
3 QA System Extensions for the Web 3.1 A NA to Web-based QA The simplest approach to turning InSicht into a web QA system would be to collect German web pages and work with the resulting document col- lection as described in Sect.	NA	Naive Approach$Null adjunction$narrow$N t- Adjective$Network Analysis$	0
3 A Probabilistic Model  3.1 A NA m Finite State Tagging  It is useful to note that a (W, T) pair can be represented  as a tagged sentence wl/t l ,  w2/t2, ...w,/tn where T =  tl, t2...tn is the sequence of tags denoting the semantic  type for each word in the sentence.	NA	Naive Approach$Null adjunction$narrow$N t- Adjective$Network Analysis$	0
k=1 logP (w k |w k?1 k?N+1 ) (1) 2.1 NA A naive approach to finding the permutation with the highest probability is to enumerate all permu- tations, compute their probabilities using Equa- tion 1, and choose the permutation with the highest probability as the solution.	NA	Naive Approach$Null adjunction$narrow$N t- Adjective$Network Analysis$	0
IE is a  pre-specified and autonomous task with a NA  domain of focus, where all the information of interest  is represented in the extraction template.	NA	Naive Approach$Null adjunction$narrow$N t- Adjective$Network Analysis$	2
Each arc is represented by a three segment polygon  (larger arcs are above the NAer, for readibility  reason).	NA	Naive Approach$Null adjunction$narrow$N t- Adjective$Network Analysis$	2
As it follows from Definition 3, each satura- tion of a terminal DV-structure  has the same set of nodes and a strictly NAer set of valencies.	NA	Naive Approach$Null adjunction$narrow$N t- Adjective$Network Analysis$	2
2012) 1 applied in the experiments described in the next sections, which does not enable creat- ing a word-document matrix and organizing word occurrences by documents or NAly specified topics.	NA	Naive Approach$Null adjunction$narrow$N t- Adjective$Network Analysis$	2
content are aggre- gated and serve as valid candidates for attribution, 2) if multiple characters and pronouns exist, then they are mapped (if possible) via co-reference res- olution in order to NA down the list of attri- bution candidates, and 3) the quote is attributed to the nearest quote character (or pronoun).	NA	Naive Approach$Null adjunction$narrow$N t- Adjective$Network Analysis$	2
and submitted to Google to NA the results to texts with exact phrases.	NA	Naive Approach$Null adjunction$narrow$N t- Adjective$Network Analysis$	2
3.4 NA The aim of extracting social networks from nov- els is to turn a complex object (the novel) into a schematic representation of the core structure of the novel.	NA	Naive Approach$Null adjunction$narrow$N t- Adjective$Network Analysis$	4
Using Topic  Discovery to Segment Large Communication Graphs  for Social NA, In Proceedings of the  IEEE/WIC/ACM International Conference on Web  Intelligence, 95-99.	NA	Naive Approach$Null adjunction$narrow$N t- Adjective$Network Analysis$	4
Social NA While many previous studies considered the effect of social dynamics for social media analysis, most relied on an explic- itly available social network structure or consid- ered dialogues and speech acts for which opinion holders are given (Tan et al, 2011; Hu et al, 2013; Li et al, 2014; West et al, 2014; Krishnan and Eisenstein, 2015).	NA	Naive Approach$Null adjunction$narrow$N t- Adjective$Network Analysis$	4
A  Semantic NA on the Iraq War Blogs.	NA	Naive Approach$Null adjunction$narrow$N t- Adjective$Network Analysis$	4
2 B i l ingua l  Head Transduct ion   2.1 Bilingual Head Transducers  A head transducer M is a FSMs as-  sociated with a pair of words, a source word w  and a target word v. In fact, w is taken from the  set V1 consisting of the source language vocab-  ulary augmented by the "empty word" e, and v  is taken from V~, the target language vocabulary  augmented with e. A head transducer reads from  a pair of source sequences, a left source sequence  L1 and a right source sequence	FSMs	finite state machine$Finite State Machines$	0
We learn weighted edit distance in a probabilistic FSMs (pFSM) model, where state transitions corre- spond to edit operations.	FSMs	finite state machine$Finite State Machines$	0
In (Wright et al, 1997) phrase level significance was obtained for noisy transcribed data where the phrases are clustered and combined into FSMss.	FSMs	finite state machine$Finite State Machines$	0
The jump sequence bK1 can be described by a deterministic FSMs. ?(	FSMs	finite state machine$Finite State Machines$	0
TESTS FOR UD AND UDP  By treating FSMss as encoders and decoders, tests for  UD and UDF can be converted into tests for IL and ILF (S. EVEN, 1965;  Z. KOHAVr, 1970).	FSMs	finite state machine$Finite State Machines$	0
This algorithm is based on the  state-table of the inverse of the FSMs which is taken as  the encoder device 4,7.	FSMs	finite state machine$Finite State Machines$	0
Weighted FSMs have seen a variety of use in NLP (Mohri, 1997).	FSMs	finite state machine$Finite State Machines$	1
Figure 2: Framework of speech retrieval through subword indexing 6 3.1 Subword FSMs as Speech Indices We construct a full index that can be used to search for a query within all the speech utterances ui, i ?	FSMs	finite state machine$Finite State Machines$	1
More specifically, some other previous work on Ma- chine Translation have used lattices (or more generally Weighted FSMs).	FSMs	finite state machine$Finite State Machines$	1
2 Integrating Typed Feature Structures  and FSMs  The main motivation for developing SProUT  comes from the need to have a system that (i)  allows a flexible integration of different processing  modules and (ii) to find a good trade-off between  processing efficiency and linguistic expressive- ness.	FSMs	finite state machine$Finite State Machines$	1
3 Mapping Tree Adjoining Grammar to FSMs What is crucial for being able to define a map- ping from words to application semantics is a very abstract notion of grammatical function: in devising such a mapping, we are not interested in how English realizes certain syntactic argu- ments, i.e., in the phrase structure of the verbal projection.	FSMs	finite state machine$Finite State Machines$	1
Using FSMs for  Evaluating Spoken Dialog Systems?,	FSMs	finite state machine$Finite State Machines$	1
1 context sensitive (C recursive) PSPACE  1.5 indexed  1.75 mildly context sensitive  2 context free  3 regular  Turing Machine (TM)  Linear Bounded Automata  LBA)  ested Stack Automata   NSA)  mbeded Pushdown  Automata (EPDA)  Pushdown Automata (PDA)  FSMs (FSM)  NP-Complete  n 7  n a  ' linear " '  Table 1: Models of Grammar and Computation  corrected stringset argument that Dutch licences  a"b'*c '~ constructions, which are MCS.	FSMs	finite state machine$Finite State Machines$	1
At a high level, we would like to identify query segments that corre- spond to IHs, IMs and OTHER.	OTHER	Others$other expressions$	0
Th is  0nl, it,y is rcfc:rro(I  to as t, hc backw~ird- looking ceutor ((,'b), Any  other   (mtity appear ing hi all Ill, tCl';tll('O is a. \[}.)r',var(l-lookillg  center (Cf) which niny I)ccomc a (it) later Oil ill ttic  discourse, Cfs arc ordered by grammatical flmctions  according to the, Jr degrees of salience as follows:  Topic > Subject; > Ob ject /Ob ject2   > OTHER (Oblique, Possessor, etc)  Kmneymn~ showed that  t;he zeroq)ronoun corr(>  Sllonds I,o the (3) in ;lalmnc.se.	OTHER	Others$other expressions$	0
OTHER are more  abstract, such as whether the preceding word is an  article; whether the preceding word is an adjective;  whether the preceding word is a conjunction; whether  the preceding word is a preposition.	OTHER	Others$other expressions$	0
In our model, the ordering is as follows:  'l'opic > Subject > Object > Object2 > OTHER  > Subject/Ol~ject/Ol~ject2 of subordinate clause  > OTHER ill Sllbordill~Jt;(!	OTHER	Others$other expressions$	0
OTHER like ?	OTHER	Others$other expressions$	0
The double complementation in the definitions of these conditional operators, and  also in several OTHER to be introduced later, constitutes an idiom for ex-  pressing universal quantification.	OTHER	Others$other expressions$	1
If agents can  have attitudes toward sentences of thought language, why  shouldn't hey have attitudes toward OTHER of  the same thought language?	OTHER	Others$other expressions$	1
Linguistic Expressions epistemic hedges OTHER non?phrasal lexical phrasal 1st person singular other conditionals ... ... ... We do not claim this separation of hedging markers can fully account for pragmatic and se- mantic analysis of hedging in web forums, but we are confident this classification supports reliable annotation for quantificational assessment of cer- tainty and hedging in this informal domain.	OTHER	Others$other expressions$	1
For notational convenience, we let the overbar in these extensions and in  the OTHER below stand for the complement relative to 7r* as opposed to the  more usual ~* x ~*:  _ _  m  If-P-then-S(R1,R2) = ~* - R1R2 = RIR2  m  If-S-then-P(R1, R2) = 7r* - R1R2 = RIR2  The conditions for a simple context restriction rule are then modeled by the following  relation:  Restrict(% A, p) = If- S-then-P (Tr* A, T ~r* ) n If-P-then- S ( Tr*T , p~r* )  The first component ensur	OTHER	Others$other expressions$	1
Type-based meth- ods frequently exploit the fact that idioms have 754 a number of properties which differentiate them from OTHER.	OTHER	Others$other expressions$	1
False negation cues: Some negation words may be also used in OTHER with- out constituting a negation, as in sentence (3).	OTHER	Others$other expressions$	1
Using ran- dom guesses, the BL accuracy is 0	BL	baseline$Backward Looking$baselin$bilabial$Baseline$	0
For the exact match scheme, the obtained performance is higher7 than the BL (random guess) that equals to 0.250.	BL	baseline$Backward Looking$baselin$bilabial$Baseline$	0
The BL accuracy equals 0.31 given that random guesses are used.	BL	baseline$Backward Looking$baselin$bilabial$Baseline$	0
Using random guesses, the BL precision is 0.010 and 0.333 for quote-to-speaker attribution and gender estimation, respectively.	BL	baseline$Backward Looking$baselin$bilabial$Baseline$	0
t-test wrt BL).	BL	baseline$Backward Looking$baselin$bilabial$Baseline$	0
Using ran- dom guesses, the BL accuracy is 0.33.	BL	baseline$Backward Looking$baselin$bilabial$Baseline$	0
At first glance Princi-  ple 2 is exemplified linguistically by subject-  empathy( 'ident' in her term ) sharing \[5\],  or by the combination of preference for Cb  (BL Center ) continuing and  Cf Ranking \[8\].	BL	baseline$Backward Looking$baselin$bilabial$Baseline$	1
Among them, the so-called BACKWARD-LOOKING CENTER (CB), defined as follows: BL Center (CB) CB(U i+1 ), the BACKWARD-LOOKING CENTER of utter- ance U i+1 , is the highest ranked element of CF(U i ) that is realized in U i+1 .	BL	baseline$Backward Looking$baselin$bilabial$Baseline$	1
BL Function which characterizes  how an utterance relates to the previous dis-  course.	BL	baseline$Backward Looking$baselin$bilabial$Baseline$	1
BL/Forward Looking Fea- tures.	BL	baseline$Backward Looking$baselin$bilabial$Baseline$	1
Using ran- dom guesses, the BLe accuracy is 0	BL	baseline$Backward Looking$baselin$bilabial$Baseline$	2
For the exact match scheme, the obtained performance is higher7 than the BLe (random guess) that equals to 0.250.	BL	baseline$Backward Looking$baselin$bilabial$Baseline$	2
The BLe accuracy equals 0.31 given that random guesses are used.	BL	baseline$Backward Looking$baselin$bilabial$Baseline$	2
Using random guesses, the BLe precision is 0.010 and 0.333 for quote-to-speaker attribution and gender estimation, respectively.	BL	baseline$Backward Looking$baselin$bilabial$Baseline$	2
t-test wrt BLe).	BL	baseline$Backward Looking$baselin$bilabial$Baseline$	2
Using ran- dom guesses, the BLe accuracy is 0.33.	BL	baseline$Backward Looking$baselin$bilabial$Baseline$	2
(3) T.empor~l input from the phonetic leve_l  <voiceless, 0 91A9, C>  <voiced, 91.2, 517.5, C>  <glide, 452.6, 498.2, C>  <occlusive, 0, 35.4, C>  <transient, 34.5, 641.6, C>  <noise, (:~).61, 91.16, C>  <vowellike, 94.29, 392.6, C>  <nasal, 402.9, 518.6, C>  <BL, 20.45, 93.2, C>  <tongue-retracted, 93.21, 392.6, C>  <BL, 392.62, 518.2, C >  (4) Ev?nt invent~)r3~  et: VOI (voiceless, < 0,91.19 > )  e2: VOl(voiced,<91.2,517.5>)  e~: GLt(glide, < 452.6,498.2 >)  e~: OCC(oeclusive,<0,35.4>)  es: TRA(transient,< 34.5,60.6>)  e6: NOl(noise, < 60.61,91.16 > ) eT: VOW(vowellike, <94.29,392.6 >)  es: NAS(nasal, < 402.9,518.6 > ) e~: LAB(BL, < 20.45,93.2 >)  et0: TON(retracted, < 93.21,392.6 > ) eL~: LAB(bil	BL	baseline$Backward Looking$baselin$bilabial$Baseline$	3
518.6, C>  <BL, 20.45, 93.2, C>  <tongue-retracted, 93.21, 392.6, C>  <BL, 392.62, 518.2, C >  (4) Ev?nt invent~)r3~  et: VOI (voiceless, < 0,91.19 > )  e2: VOl(voiced,<91.2,517.5>)  e~: GLt(glide, < 452.6,498.2 >)  e~: OCC(oeclusive,<0,35.4>)  es: TRA(transient,< 34.5,60.6>)  e6: NOl(noise, < 60.61,91.16 > ) eT: VOW(vowellike, <94.29,392.6 >)  es: NAS(nasal, < 402.9,518.6 > ) e~: LAB(BL, < 20.45,93.2 >)  et0: TON(retracted, < 93.21,392.6 > ) eL~: LAB(BL, < 392.62,518.2 > ) Of particular interest to the phonological parser are the  precedence r lations between those event properties of  the same type and the overlap and temlx~ral inclusion  relations between event properties of differing types.	BL	baseline$Backward Looking$baselin$bilabial$Baseline$	3
(3) T.empor~l input from the phonetic leve_l  <voiceless, 0 91A9, C>  <voiced, 91.2, 517.5, C>  <glide, 452.6, 498.2, C>  <occlusive, 0, 35.4, C>  <transient, 34.5, 641.6, C>  <noise, (:~).61, 91.16, C>  <vowellike, 94.29, 392.6, C>  <nasal, 402.9, 518.6, C>  <BL, 20.45, 93.2, C>  <tongue-retracted, 93.21, 392.6, C>  <BL, 392.62, 518.2, C >  (4) Ev?nt invent~)r3~  et: VOI (voiceless, < 0,91.19 > )  e2: VOl(voiced,<91.2,517.5>)  e~: GLt(glide, < 452.6,498.2 >)  e~: OCC(oeclusive,<0,35.4>)  es: TRA(transient,< 34.5,60.6>)  e6: NOl(noise, < 60.61,91.16 > ) eT: VOW(vowellike, <94.29,392.6 >)  es: NAS(nasal, < 402.9,518.6 > ) e~: LAB(BL, <	BL	baseline$Backward Looking$baselin$bilabial$Baseline$	3
It also shows that cer- tain German codas are assimilated by the alveolar sounds /d/ and /s/ from the original BL [m] to an apico-alveolar [n], as in Boden (E: ground, MHG: bodem) or in Besen (E: broom, MHG: be?sem, OHG: pe?samo).	BL	baseline$Backward Looking$baselin$bilabial$Baseline$	3
<BL, 392.62, 518.2, C >  (4) Ev?nt invent~)r3~  et: VOI (voiceless, < 0,91.19 > )  e2: VOl(voiced,<91.2,517.5>)  e~: GLt(glide, < 452.6,498.2 >)  e~: OCC(oeclusive,<0,35.4>)  es: TRA(transient,< 34.5,60.6>)  e6: NOl(noise, < 60.61,91.16 > ) eT: VOW(vowellike, <94.29,392.6 >)  es: NAS(nasal, < 402.9,518.6 > ) e~: LAB(BL, < 20.45,93.2 >)  et0: TON(retracted, < 93.21,392.6 > ) eL~: LAB(BL, < 392.62,518.2 > ) Of particular interest to the phonological parser are the  precedence r lations between those event properties of  the same type and the overlap and temlx~ral inclusion  relations between event properties of differing types.	BL	baseline$Backward Looking$baselin$bilabial$Baseline$	3
3 Tree Search vs. Dynamic  P rogramming  Once an appropriate function for measuring simi-  larity between pairs of segments has been designed,  290  Feature Phonological Numerical  name term value  Place  Manner  High  Back  \[BL\]  \[labiodental\]  \[dental\]  \[alveolar\]  \[retroflex\]  \[palato-alveolar\]  \[palatal\]  \[velar\]  \[uvular\]  \[pharyngeal\]  \[glottal\]  \[stop\]  \[affricate\]  \[fricative\]  \[approximant\]  \[high vowel\]  \[mid vowel\]  \[low vowel\]  \[high\]  \[mid\]  \[low\]  \[front\]  \[central\]  \[back\]  1.0  0.95  0.9  0.85  0.8  0.75  0.7  0.6  0.5  0.3  0.1  1.0  0.9  0.8  0.6  0	BL	baseline$Backward Looking$baselin$bilabial$Baseline$	3
Precision Recall F-score BL 72.66 66.17 69.26 +ALF 78.14 64.36 70.59 Table 3: Confidence-based Alignment Link Filter- ing on C-E Alignment Precision Recall F-score BL 84.43 83.64 84.04 +ALF 88.29 83.14 85.64 Table 4: Confidence-based Alignment Link Filter- ing on A-E Alignment 512 sentence pairs, and the A-E alignment test set is the 200 Arabic-English sentence pairs from NIST MT03 test set.	BL	baseline$Backward Looking$baselin$bilabial$Baseline$	4
This should be taken with a grain BL Original Query Hybrid Tot Good Top 5 Tot Good Top 5 Tot Good Top 5 Poe 12 6.5 3 10 0.5 0.5 10 5.5 2.5 Romantics 10 0 0 15 0 0 10 3 3 Witch Hunts 10 8 3 14 2 1 10 8 5 US Wars 15 12 2 0 0 0 16 13 4 Sonnets 15 10 5 10 2 0 10 8 4 Presidents 15 2 2 15 0 0 15 2 2 Epics 10 7 4 10 5 3 10 7 4 Dec of Ind 10 2 0 0 0 0 10 5.5 2 Avr.	BL	baseline$Backward Looking$baselin$bilabial$Baseline$	4
In Proceed- ings of the Fourteenth International Conference BL Original Query Hybrid FPF FPF/Tot FPF FPF/Tot FPF FPF/Tot Poe 1 .08 1 .1 1 .1 Romantics 8 .8 15 1 2 .2 Witch Hunts 2 .2 1 .07 0 0 US Wars 3 .2 3 .19 Sonnets 5 .33 8 .8 2 .2 Presidents 5 .33 10 .67 2 .13 Epics 0 0 0 0 0 0 Dec of Ind 3 .3 2 .2 28.1% 44% 12.8% Table 4: False Positives Containing Figure on Computational Linguistics, Nantes, France, July.	BL	baseline$Backward Looking$baselin$bilabial$Baseline$	4
For evaluation, 10-fold cross valida- Dataset Relaxed Exact lex pos lex pos BL 0.625 0.250 QUOTES1 0.869 0.883 0.445 0.373 QUOTES2 0.877 0.831 0.450 0.435 BOTH 0.886 0.858 0.464 0.383 Table 4: Age estimation: average accuracy.	BL	baseline$Backward Looking$baselin$bilabial$Baseline$	4
p BL 0.010 0.333 10 stories (subset of dataset) Scheme 1 0.833 0.780 0.672 0.929 Scheme 2 0.868 0.710 0.759 0.917 Scheme 3 0.835 0.710 0.759 0.917 17 stories (full dataset) Scheme 2 0.845 0.688 0.733 0.892 Table 3: Quote attribution and gender estimation.	BL	baseline$Backward Looking$baselin$bilabial$Baseline$	4
2.3 MWEss As shown in Eryigit et al. (	MWEs	Multi Word Expression$Multiword Expressions$Multiword expressions$multiword expressions$	0
Mining Complex Predicates  In Hindi Using Parallel Hindi-English Corpus, ACL- IJCNLP, Workshop on MWEs,  Singapore.	MWEs	Multi Word Expression$Multiword Expressions$Multiword expressions$multiword expressions$	0
Patterns of German Particle Verbs and their Impact on Lexical Semantics Stefan Bott Sabine Schulte im Walde Institut f?ur Maschinelle Sprachverabeitung Universit?at Stuttgart Pfaffenwaldring 5b, 70569 Stuttgart, Germany {stefan.bott,schulte}@ims.uni-stuttgart.de Abstract German particle verbs, like anblicken (to gaze at) combine a base verb (blicken) with a particle (an) to form a special kind of MWEs.	MWEs	Multi Word Expression$Multiword Expressions$Multiword expressions$multiword expressions$	0
5 83.30 81.37 AI-KU 2 78.57 85.12 83.25 76.35 83.24 81.35 Baseline 77.67 84.6 82.36 75.82 83.20 80.88 Table 9: Results on Swedish Gold Predicted Precision Recall F1 Precision Recal F1 Best System 99.41 99.38 99.39 81.68 79.97 80.81 AI-KU 1 99.41 99.38 99.39 74.47 71.51 72.96 AI-KU 2 99.38 99.36 99.37 74.34 71.51 72.89 MaltOptimizer Baseline 98.77 99.18 99.26 72.64 68.09 70.29 Table 10: Results of MWEss on French 82 4.2 Experiment II The second approach is discretizing the real valued vectors.	MWEs	Multi Word Expression$Multiword Expressions$Multiword expressions$multiword expressions$	0
One example is (Sharoff, 2004), where shallow parsing is used for the identification of preposi- tional MWEss in Russian, with the following explanation of reasons for performing some language-dependent processing: ?	MWEs	Multi Word Expression$Multiword Expressions$Multiword expressions$multiword expressions$	0
of the ACL 2003 Workshop on MWEs, 49?56.	MWEs	Multi Word Expression$Multiword Expressions$Multiword expressions$multiword expressions$	1
In Proceedings of the ACL-07 Workshop on A Broader Perspective on MWEs.	MWEs	Multi Word Expression$Multiword Expressions$Multiword expressions$multiword expressions$	1
In Proceedings of the  ACL-04 Workshop on MWEs:  Integrating Processing, p. 1?8.	MWEs	Multi Word Expression$Multiword Expressions$Multiword expressions$multiword expressions$	1
The second was a presentation on the same workshop by Aravind Joshi and Owen Rambow of their en- coding of DG in TAG, and the third was a talk by Charles Fillmore titled MWEs: An Extremist Approach.	MWEs	Multi Word Expression$Multiword Expressions$Multiword expressions$multiword expressions$	1
XDG solving is efficient at least for the smaller- scale example grammars tested so far, but these good results hinge substantially on the assump- Second ACL Workshop on MWEs: Integrating Processing, July 2004, pp.	MWEs	Multi Word Expression$Multiword Expressions$Multiword expressions$multiword expressions$	1
2009 ACL and AFNLP A Cohesion Graph Based Approach for Unsupervised Recognition of Literal and Non-literal Use of MWEs Linlin Li and Caroline Sporleder Saarland University Postfach 15 11 50 66041 Saarbr?ucken Germany {linlin,csporled}@coli.uni-saarland.de Abstract We present a graph-based model for rep- resenting the lexical cohesion of a dis- course.	MWEs	Multi Word Expression$Multiword Expressions$Multiword expressions$multiword expressions$	1
MWEs: A pain in the neck for NLP.	MWEs	Multi Word Expression$Multiword Expressions$Multiword expressions$multiword expressions$	2
MWEs: A  pain in the neck for NLP.	MWEs	Multi Word Expression$Multiword Expressions$Multiword expressions$multiword expressions$	2
2028  MWEs as dependency subgraphs Ralph Debusmann Programming Systems Lab Saarland University Postfach 15 11 50 66041 Saarbru?cken, Germany rade@ps.uni-sb.de Abstract We propose to model multiword expres- sions as dependency subgraphs, and re- alize this idea in the grammar formal- ism of Extensible Dependency Gram- mar (XDG).	MWEs	Multi Word Expression$Multiword Expressions$Multiword expressions$multiword expressions$	2
MWEs in the wild?	MWEs	Multi Word Expression$Multiword Expressions$Multiword expressions$multiword expressions$	2
MWEs: a pain in the neck for NLP.	MWEs	Multi Word Expression$Multiword Expressions$Multiword expressions$multiword expressions$	2
In Proceedings of the ACL workshop on MWEs.	MWEs	Multi Word Expression$Multiword Expressions$Multiword expressions$multiword expressions$	2
KX architecture is the same across all languages, except for the module selecting MWEs, that is based on PoS tags (this is the only language-dependent part of the system).	MWEs	Multi Word Expression$Multiword Expressions$Multiword expressions$multiword expressions$	3
20 3 KX configuration for the DEFT 2012 task As introduced before (Section 2.2), to port KX to the French language and, in particular, to adapt it to the DEFT 2012 task, the Morfette morphological analyzer (Chrupala et al, 2008) has been integrated into the system, to select as MWEs only the n-grams matching certain lexical patterns (i.e. part-of-speech).	MWEs	Multi Word Expression$Multiword Expressions$Multiword expressions$multiword expressions$	3
Alignment-based extraction of MWEs.	MWEs	Multi Word Expression$Multiword Expressions$Multiword expressions$multiword expressions$	3
We distinguish between the task of identifying candidate MWEs 2http://en.wiktionary.org Feature Description ?	MWEs	Multi Word Expression$Multiword Expressions$Multiword expressions$multiword expressions$	3
Then, from the n-gram list a sublist of MWEs (MWE) is derived, i.e. combinations of words expressing a unitary concept, for example ?	MWEs	Multi Word Expression$Multiword Expressions$Multiword expressions$multiword expressions$	3
636 bine evidence from multiple languages, we develop a novel boosting algorithm tailored to the task of ranking MWEs by their degree of id- iomaticity.	MWEs	Multi Word Expression$Multiword Expressions$Multiword expressions$multiword expressions$	3
PASSly?	PASS	passive$Passive$	0
Given a sentence that includes one or more quotes, the respective PASS characters were not considered as candidate speakers.	PASS	passive$Passive$	0
is a PASS character.	PASS	passive$Passive$	0
The PASS char- acters were identified via the following relations extracted by dependency parsing: nsubjpass (PASS nominal subject) and pobj (object of a preposition).	PASS	passive$Passive$	0
d voice: if the syntactic head of p is be and p is not ended with -ing, then p is PASS.	PASS	passive$Passive$	0
PASS-aggressive (PA) algorithm (Crammer et al 2006) was proposed for nor- mal multi-class classification and can be easily extended to structure learning (Crammer et al.,	PASS	passive$Passive$	1
(ii) PASS  E<0,1>  %Yl \[ (the (automaton))  (Ixl~*psub~ ~accept ) ) (Y ) ) \ ] ) \ ]   *en(accept) %So\[ (the(automaton))  (Ix I\[ (*psubj (x)) (s) \]) \]  / ~  *psubJ the(automaton) /  be/ acc~epted Jy thS  ~u~tomaton  where *ene E<<0,1>,<0,1,1>>,  *psubj E E<<0,O>,i>.	PASS	passive$Passive$	1
PASS-aggressive sequence labeling with discrim- inative post-editing for recognising person entities in tweets.	PASS	passive$Passive$	1
(4) PASS Mode.	PASS	passive$Passive$	1
736 PASS-Aggressive Learning We train our model with a structured version of the PASS-Aggressive (PA) algorithm (Crammer et al.,	PASS	passive$Passive$	1
1 Introduction GREs has been stud- ied for the last two decades.	GRE	Generation of referring expression$generation of referring expressions$	0
GREs: Managing structural ambiguities.	GRE	Generation of referring expression$generation of referring expressions$	0
GREs: Assessing the Incremental Algorithm.	GRE	Generation of referring expression$generation of referring expressions$	0
GREs: Assessing the incremental algorithm.	GRE	Generation of referring expression$generation of referring expressions$	0
GREs.	GRE	Generation of referring expression$generation of referring expressions$	0
7.1 Measurement Axis Descriptor GREs (noun phrases) is one of the key problems explored within the natural language generation literature.	GRE	Generation of referring expression$generation of referring expressions$	0
Com- putational GRE: A survey.	GRE	Generation of referring expression$generation of referring expressions$	1
Efficient context-sensitive GRE.	GRE	Generation of referring expression$generation of referring expressions$	1
Passonneau (1996) proposes an algorithm for the  GRE and Walker (1996a)  integrates centering into a cache model of attentional  state.	GRE	Generation of referring expression$generation of referring expressions$	1
lexicalisation, aggre- gation and GRE.	GRE	Generation of referring expression$generation of referring expressions$	1
Its applications are not limited to  the GRE.	GRE	Generation of referring expression$generation of referring expressions$	1
This paper proposes a method that allows effi-  cient GRE, through  a unification grammar, at the cost of some ini-  tial effort in tailoring the phrase-structure rules  to the current knowledge base.	GRE	Generation of referring expression$generation of referring expressions$	1
Find- ing planted partitions in nearly linear time using ar- rested SPEC.	SPEC	spectral clustering$Spectral clustering$	0
We consider two forms of SPEC: EigenCluster (Cheng et al, 2006), a method origi- nally designed to cluster snippets for search results into semantically related categories, and GSpec (Ng et al, 2001), a method that directly clusters a collo- cation graph.	SPEC	spectral clustering$Spectral clustering$	0
But instead of learning representations of pivots in source and target domains the authors used SPEC to align domain-specific and domain-independent words into a set of feature- clusters.	SPEC	spectral clustering$Spectral clustering$	0
Name disambiguation in author citations using a K-way SPEC method.	SPEC	spectral clustering$Spectral clustering$	0
Kernel k- means: SPEC and normalized cuts.	SPEC	spectral clustering$Spectral clustering$	0
This indicates that the MRW model with SPEC is more robust than that with the baseline, k-means, with respect to the differ- ent number of clusters.	SPEC	spectral clustering$Spectral clustering$	0
SPEC is a trans- formation of the original sentences into a set of or- thogonal eigenvectors.	SPEC	spectral clustering$Spectral clustering$	1
SPEC partitions objects relying on their similarity matrix.	SPEC	spectral clustering$Spectral clustering$	1
SPEC for German verbs.	SPEC	spectral clustering$Spectral clustering$	1
3.2 Classification SPEC (Shi and Malik, 2000) is used in this work, since we found it to outperform other clustering approaches such as k-means and Gaus- sian mixture models.	SPEC	spectral clustering$Spectral clustering$	1
SPEC refers to a class of techniques which rely on the eigenstructure of a similarity matrix to partition points into disjoint clusters with high intra-cluster similarity and low inter-cluster similarity.	SPEC	spectral clustering$Spectral clustering$	1
SPEC can be viewed in abstract terms as the partitioning of a graph G over a set of words W. The weights on the edges of G are the similarities Sij.	SPEC	spectral clustering$Spectral clustering$	1
In addition, tf-idf (Term-Frequency IDF) weighting was used when training LSI topic models.	IDF	Inverse Document Frequency$inverse document frequenc$	0
IDF Numeric fea- tures that compare source and target word frequen- cies.	IDF	Inverse Document Frequency$inverse document frequenc$	0
Why IDF?	IDF	Inverse Document Frequency$inverse document frequenc$	0
The weight function is a combination of similarity measure between t and si and IDF (idf) of t. The next two subsections explain the calculation of the similarity measure and the idf in detail.	IDF	Inverse Document Frequency$inverse document frequenc$	0
5.1.2 IDF If f number of documents in corpus Q contain a term t and the total number of documents in Q is N, the IDF (idf) of t is idf(t) = log N f (3) Combining the similarity measure and the idf of t in the corpus, we define the weight function ?(	IDF	Inverse Document Frequency$inverse document frequenc$	0
In order to integrate the strengths of tradi- tional IR models, the IDFy idf is considered, which measures the general im- portance of a term for predicting the content of a document.	IDF	Inverse Document Frequency$inverse document frequenc$	1
For example, word fre- quencies have typically been weighted by IDFies (tf ?	IDF	Inverse Document Frequency$inverse document frequenc$	1
We gather term frequen- cies and IDFies across the whole corpus.	IDF	Inverse Document Frequency$inverse document frequenc$	1
tf(wj , dk)idf(wj), where tf(wi, dk) is the term frequency for wi in dk and idf(wi) is the IDFy weight for wi.	IDF	Inverse Document Frequency$inverse document frequenc$	1
wh?WH idf(wh) (3) where idf(w) is the IDFy of the word w. For sake of comparison, we consider also the corresponding more classical version that does not apply the IDFy s2(T,H) = ?	IDF	Inverse Document Frequency$inverse document frequenc$	1
Since we represent word to- kens rather than word types in the cohesion graph, we do not need to model the term frequency tf separately, instead we set salience to the log value of the IDFy idf : salience(t i ) = log |D| |{d : t i ?	IDF	Inverse Document Frequency$inverse document frequenc$	1
In Table 10 we show the normalized performance  (D) of MEAD, for the six clusters at nine  CompR.	CompR	compression rates$compression rate$	0
Note that for  the largest cluster, Cluster D, MEAD outperformed  Lead at all CompR.	CompR	compression rates$compression rate$	0
Secondly, even though sentence co-selection metrics have been widely used for evaluating summaries of other genres, different CompR, different gold standards, and availability of naturally occurring competitive baselines (e.g., lead baseline in newswire summarization) make fair comparison difficult.	CompR	compression rates$compression rate$	0
In scientific ar- ticles, however, the CompR have to be much higher: Shortening a 20-page journal article to a half-page summary requires a compression to 2.5% of the original.	CompR	compression rates$compression rate$	0
As discussed above, the CompR on news texts are far lower: there are fewer sentences from which to choose, making it easier to agree on which ones to select.	CompR	compression rates$compression rate$	0
The toolkit implements multiple summariza- tion algorithms (at arbitrary CompR) such as position-based, TF*IDF, largest common subsequence, and keywords.	CompR	compression rates$compression rate$	0
The test corpus is used in the evaluation  in such a way that each cluster is summarized at 9  different CompR, thus giving nine times as  many sample points as one would expect from the  size of the corpus.	CompR	compression rates$compression rate$	0
In scientific ar- ticles, however, the CompRs have to be much higher: Shortening a 20-page journal article to a half-page summary requires a compression to 2.5% of the original.	CompR	compression rates$compression rate$	1
Figure 2: After user has added all of the documents to be summarized, they select the CompR and then submit for summarization.	CompR	compression rates$compression rate$	1
The average length of a story is 3,333 tokens and the target CompR expressed in the number of sentences is 94%.	CompR	compression rates$compression rate$	1
As discussed above, the CompRs on news texts are far lower: there are fewer sentences from which to choose, making it easier to agree on which ones to select.	CompR	compression rates$compression rate$	1
The toolkit implements multiple summariza- tion algorithms (at arbitrary CompRs) such as position-based, TF*IDF, largest common subsequence, and keywords.	CompR	compression rates$compression rate$	1
Since the abstractive summaries had already been created, summary size was de- termined by their size (which means creating sum- maries using a CompR of around 10% of the original size).	CompR	compression rates$compression rate$	1
We demonstrate that a combina- tion of instance, annotator and annotation task characteristics are important for developing an accurate estimator, and argue that both corre- lation coefficient and RMSE should be used for evaluating annotation cost estimators.	RMSE	root mean square error$root mean squared error$Root Mean Squared Error$	0
Their performances, reported in Table 4, were as- sessed using five measures: the multiple correlation ratio (R), the accuracy (acc), the adjacent accuracy 7 (adjacc), the RMSE (rmse) and the mean absolute error (mae).	RMSE	root mean square error$root mean squared error$Root Mean Squared Error$	0
rmulae 5 72.3% 0.32 SLALEX 16 68.1% 0.29 SLASYN 14 71.2% 0.28 SLALEX + SLASYN 30 82.3% 0.23 BEST10SYN 10 69.9% 0.28 All Syntactic Features 25 75.3% 0.27 BEST10LEX 10 82.4% 0.22 All Lexical Features 19 86.7% 0.20 BEST10ALL 10 89.7% 0.18 All features 46 93.3% 0.15 Table 5: Classification results for WeeBit Corpus 5.1 Evaluation Metrics We report our results in terms of classification accu- racy and RMSE.	RMSE	root mean square error$root mean squared error$Root Mean Squared Error$	0
Some ap- plications might need components with high re- call, for example; others, high precision or high F- measure or low RMSE.	RMSE	root mean square error$root mean squared error$Root Mean Squared Error$	1
Root mean squared error of the extrapolated curves at the three anchor sizes The RMSEs obtained by extrap- olating the learning curve are much lower than those obtained by prediction of translation accuracy using the monolingual corpus only (see Table 4), which is expected given that more direct evidence is avail- able in the former case .	RMSE	root mean square error$root mean squared error$Root Mean Squared Error$	1
Table 6 reports the RMSE at the three anchor sizes from the combined curves.	RMSE	root mean square error$root mean squared error$Root Mean Squared Error$	1
Initial Points 10K 75K 500K 1K-5K-10K 0.005 0.017 0.042 5K-10K-20K 0.002 0.015 0.034 1K-5K-10K-20K 0.002 0.008 0.019 Table 5: Root mean squared error of the extrapolated curves at the three anchor sizes The RMSEs obtained by extrap- olating the learning curve are much lower than those obtained by prediction of translation accuracy using the monolingual corpus only (see Table 4), which is expected given that more direct evidence is avail- able in the former case .	RMSE	root mean square error$root mean squared error$Root Mean Squared Error$	1
In Table 5, one can also see that the RMSE for the sets 1K- 5K-10K and 5K-10K-20K are quite close for anchor 9The 10K point is not an extrapolation point but lies within the range of the set of initial points.	RMSE	root mean square error$root mean squared error$Root Mean Squared Error$	1
In Table 5, one can also see that the RMSE for the sets 1K- 5K-10K and 5K-10K-20K are quite close for	RMSE	root mean square error$root mean squared error$Root Mean Squared Error$	1
par- allel corpus is available, we introduced an extrapola- tion method and a combined method yielding high- precision predictions: using models trained on up to 20K sentence pairs we can predict performance on a given test set with a RMSE in the order of 1 BLEU point at 75K sentence pairs, and in the order of 2-4 BLEU points at 500K. Consider- ing that variations in the order of 1 BLEU point on a same test dataset can be observed simply due to the instability of the standard MERT parameter tun- ing algorithm (Foster and Kuhn, 2009; Clark et al, 2011), we believe our results to be close to what can be achieve	RMSE	root mean square error$root mean squared error$Root Mean Squared Error$	1
7.2 Extrapolated Learning Curves As explained in Section 5, we evaluate the accuracy of predictions from the extrapolated curve using the RMSE (see Eq.	RMSE	root mean square error$root mean squared error$Root Mean Squared Error$	1
This setup leads to a Mean Aver- age Error of 0.62 and a RMSE of 0.78.	RMSE	root mean square error$root mean squared error$Root Mean Squared Error$	2
RMS: RMSE, MAE: Mean Absolute Error, R: Correlation Coefficient.	RMSE	root mean square error$root mean squared error$Root Mean Squared Error$	2
162  In EFF, we use this probability information to  identify the topic of the segment with the belief  that the topic is more likely to be referred to by  a pronoun.	EFF	effect$Effect$	0
Using these base features, we then evaluated the EFFs of feature combinations by repeatedly training the system and selecting feature combinations that in- creased the performance on a heldout set.	EFF	effect$Effect$	0
Although this success rate  overstates the EFF, it is a clear indication that  knowledge of a referent's gender and animatic-  ity is essential to anaphora resolution.	EFF	effect$Effect$	0
The precision score computed over all phrases  containing any of the target honorifics are 66.0%  l In EFF, this is the same as admi t t ing  that  a ref-  erent  can be in different gender  classes across different  observations.	EFF	effect$Effect$	0
Second, we analyze the EFFs of partic- ular choices we made when building our system, especially the feature combinations and learning methods.	EFF	effect$Effect$	0
he EFFs of feature combinations by repeatedly training the system and selecting feature combinations that in- creased the performance on a heldout set.	EFF	effect$Effect$	0
in EFF, this means that higher-ranked  constraints have priority in eliminating candidates.	EFF	effect$Effect$	0
EFFs of age and gender on blog- ging.	EFF	effect$Effect$	1
TP FP FN P (%) R (%) F1 (%) 1 647 79 143 89.12 81.90 85.36 2 647 80 143 89.00 81.90 85.30 1,2 647 81 143 88.87 81.90 85.24 Table 3: EFFs of removing features (1) or (2), or both Table 3 shows the effect of removing (1), (2), or both (1) and (2), showing that they overfit the training data.	EFF	effect$Effect$	1
On the EFFiveness of the Skew  Divergence for Statistical Language Analysis,  Technical Report, Department of Computer Science,  Cornell University, 2001.	EFF	effect$Effect$	1
EFF of Eliminating Feature Classes  on 10K Training Set  6 MSR Paraphrase Corpus is nearly twice that of  the 10K training set, AER performance is meas- urably degraded.	EFF	effect$Effect$	1
6.2 EFFiveness of Large Corpus  The large corpus of newspapers and the Web are  used effectively in many different cases.	EFF	effect$Effect$	1
The evaluation metrics were precision P (the number of true pos- K TP FP FN P (%) R (%) F1 (%) 10 641 80 149 88.90 81.14 84.84 20 644 79 146 89.07 81.52 85.13 30 644 80 146 88.95 81.52 85.07 40 645 81 145 88.84 81.65 85.09 50 645 80 145 88.97 81.65 85.15 Table 2: EFFs of K in Bayes Point Machines itives divided by the total number of elements la- beled as belonging to the positive class) recall R (the number of true positives divided by the to- tal number of elements that actually belong to the positive class) and their harmonic mean, the F1 score (F1 = 2PR/(P + R)).	EFF	effect$Effect$	1
The set of NEG is then extended with the unlabeled instances classified as negative by h?i.	NEG	negative instances$Negations$negation$negative$	0
In this paper, P is dynamically assigned ac- cording to different argument since different heu- ristics could produce different proportion of posi- tive and NEG used to training data.	NEG	negative instances$Negations$negation$negative$	0
It can be explained that when  using heuristics, the proportion of positive and  NEG in dataset are adjusted reasona- bly to improve the model.	NEG	negative instances$Negations$negation$negative$	0
2.3 Pruning Comparisons During Training A potential drawback of including all the negative examples as in Bengston and Roth (2008) is that the NEG far outnumber the positive ones, which is challenging for training a classifier.	NEG	negative instances$Negations$negation$negative$	0
Also a set of NEG is formed by paring the anaphor and each of the intervening candidates.	NEG	negative instances$Negations$negation$negative$	0
By contrast, Soon et al(2001) reduce the number of NEG by using only mentions between the mention and its closest coreferent pair as neg- ative examples.	NEG	negative instances$Negations$negation$negative$	0
For instance, the number of constituents labeled to  arguments (positive instances) is much less than  the number of the rest (NEG).	NEG	negative instances$Negations$negation$negative$	0
Four problems render vector space model  (VSM)-based text classification approach in- effective: 1) Many words within song lyrics  actually contribute little to sentiment; 2)  Nouns and verbs used to express sentiment are  ambiguous; 3) NEG and modifiers  around the sentiment keywords make particu- lar contributions to sentiment; 4) Song lyric is  usually very short.	NEG	negative instances$Negations$negation$negative$	1
NEG and modifiers are helpful to  determine the unique meaning of the sentiment  words within certain context window, e.g. 3 pre- ceding words and 3 succeeding words in our case.	NEG	negative instances$Negations$negation$negative$	1
(3) NEG and modifiers are included in the s- VSM model to reflect the functions of invers- ing, strengthening and weakening.	NEG	negative instances$Negations$negation$negative$	1
NEG  Because of  2  T3= G l l , l l (p lq l ) (p lq l )~ r-~q 1  410  the 2-placed FV G 2 - 11,11 can be interpreted ee   presupposition-rejecting negation.	NEG	negative instances$Negations$negation$negative$	1
System Description Precision Relevance  Without DIRT 0.6876 0.54 %  Without WordNet 0.6800 1.63 %  Without Acronyms 0.6838  1.08 %  Without BK 0.6775 2.00 %  Without NEG 0.6763 2.17 %  Without NEs 0.5758 16.71 %  Table 4: Components relevance  7 Conclusions  The system?s core algorithm is based on the tree  edit distance approach, however, focused on trans- forming the hypothesis.	NEG	negative instances$Negations$negation$negative$	1
Length of the undirected path between the two 3.3.2 NEG There are no sentences in our corpus with more than one negation.	NEG	negative instances$Negations$negation$negative$	1
In the same annotation process we have identi- fied NLP problems in these passages which must be solved to identify the facts correctly including: syn- onym and hyponym substitution, coreference reso- lution, NEG handling, and the incorporation of knowledge from within the full text and the domain.	NEG	negative instances$Negations$negation$negative$	2
We show that this protein is phosphorylated during mitosis in human cells and that this requires active cdc2-cyclin B. (Intro) Table 5: Example instances with cataphora and event anaphora 6 Negated Expressions To quantify the importance of lexical and logical NEGs we have annotated each instance involv- ing one or more negated expressions that must be resolved to derive the fact.	NEG	negative instances$Negations$negation$negative$	2
ksi azka), two neuter genders n1 (dziecko), n2 (okno), and three plurale tantum genders p1 (wujostwo), p2 (drzwi), p3 (okulary);   person: pri , sec , ter;   degree: pos , comp , sup;   aspect : imperf , perf ;   NEG: aff , neg;   accentability (Pol.:	NEG	negative instances$Negations$negation$negative$	2
Finally, for task 3 participants were asked to extract NEGs and speculations regarding events.	NEG	negative instances$Negations$negation$negative$	2
The current libraries include anal- yses of major constituent word order (SOV, SVO, etc), sentential NEG, coordination, and yes-no question formation.	NEG	negative instances$Negations$negation$negative$	2
Lexical adverbs, including manner, time, and loca- tion, and adverbs of NEG, which vary by clause type (declarative, imperative, or interrogative) ?	NEG	negative instances$Negations$negation$negative$	2
For com- puting the counts of positive and NEG words (Feature 15 and 16) we used the General Inquirer database (Stone et al.,	NEG	negative instances$Negations$negation$negative$	3
pronouns 12 count of quote tokens 13 count of 1st person plural pronouns 14 count of 2nd person singular pronouns 15 count of quote positive words 16 count of quote NEG words 17 count of nouns 18 count of verbs 19 count o	NEG	negative instances$Negations$negation$negative$	3
NEG?,	NEG	negative instances$Negations$negation$negative$	3
In order to align the existing annotations to our three-class scheme the following mapping5 was adopted: (i) AN, DI, FE, SA were mapped to NEG affect, (ii) NE was mapped to neutral affect, and (iii) HA was mapped to positive affect.	NEG	negative instances$Negations$negation$negative$	3
pronouns 12 count of quote tokens 13 count of 1st person plural pronouns 14 count of 2nd person singular pronouns 15 count of quote positive words 16 count of quote NEG words 17 count of nouns 18 count of verbs 19 count of adjectives 20 count of adverbs 21 up to 3-grams extracted from quote Table 1: Common feature set.	NEG	negative instances$Negations$negation$negative$	3
NEG surprise? (	NEG	negative instances$Negations$negation$negative$	3
4 count of 1st person singular pronouns 5 count of NEG particles 6 count of numbers 7 count of prepositions 8 count of pronouns 9 count of ?	NEG	negative instances$Negations$negation$negative$	3
A main characteristic of question answering in restricted domains is the integration of domain-specific information that is either developed for question answering or that has been developed for other PRPs.	PRP	purpose$personal pro-$personal pronouns$phonological recognition problem$	0
As in the previous two tasks 10FCV was applied for evaluation PRPs.	PRP	purpose$personal pro-$personal pronouns$phonological recognition problem$	0
Whereas structured knowledge-based QA systems are well adapted to applications managing complex queries in a very structured information environment, the kind of research developed in TREC, CLEF, and NTCIR is probably better suited to broad-PRP generic applications dealing with simple factual ques- tions such as World Wide Web?based question answering.	PRP	purpose$personal pro-$personal pronouns$phonological recognition problem$	0
One of the PRPs of the paper is to test whether this hypothesis is right.	PRP	purpose$personal pro-$personal pronouns$phonological recognition problem$	0
WordNet-based approaches are unsuitable for our PRPs as they only model so-called ?	PRP	purpose$personal pro-$personal pronouns$phonological recognition problem$	0
Alternatively, having made a selection of texts, one may opt to be presented with a word cloud of its most salient terms, for exploratory PRPs.	PRP	purpose$personal pro-$personal pronouns$phonological recognition problem$	0
To do this, we treat  the omitted NP as an anaphor which, like Sidner's  treatment of full definite NP's and PRP  nouns, co-specifies an element recorded by the fo-  cusing algorithm.	PRP	purpose$personal pro-$personal pronouns$phonological recognition problem$	1
Pronouns,  Types and  Tokens  Incidence of PRP nouns, number of pronouns  per noun phrase, types and  tokens.	PRP	purpose$personal pro-$personal pronouns$phonological recognition problem$	1
These features have similar weights to the character n-gram features and for the most part seem to represent ungrammatical constructions (e.g., the first feature indicates that a PRP noun followed by an uninflected verb predicts Chi- nese).	PRP	purpose$personal pro-$personal pronouns$phonological recognition problem$	1
Impersonal verbs with obligatory short ac- cusative personal pronoun, short dative per- sonal pronoun or short dative PRP noun + se (e.g., marzi me ?	PRP	purpose$personal pro-$personal pronouns$phonological recognition problem$	1
  In transitive VPCs unstressed PRP nouns must precede the particle (e.g. They ate it up but not *They ate up it).	PRP	purpose$personal pro-$personal pronouns$phonological recognition problem$	1
Based on the correctness of scratch pad annotations aggregated over several translation ex- ercises, the system gives feedback in the form of a simple message, such as King Alfred is pleased with your work on strong nouns and PRP nouns, or King Alfred suggests that you should re- view weak verbs.	PRP	purpose$personal pro-$personal pronouns$phonological recognition problem$	1
13ul PRP, in par-  ticular 1st and 2nd person pronouns, also show  a much higher frequency, and this can hardly be  attrihutcd to subject matter, rather to different  communicative functi(ms of feuillet(mistie writ-  ing and say economic news.	PRP	purpose$personal pro-$personal pronouns$phonological recognition problem$	2
From both corpora, in order to use  "'semantically relevant" tokens for the HMM bigrams,  we retained all nouns, verbs, adverbs, and adjectives and  deleted all function words except prepositions, commas,  final stops, PRP and interrogative adverbs.	PRP	purpose$personal pro-$personal pronouns$phonological recognition problem$	2
Verbs whose subjects are common nouns account for 57.8% of all verbs that have subjects (verbs with different types of subjects, most of which are PRP, are not considered here, since these subjects are not part of the noun classifier).	PRP	purpose$personal pro-$personal pronouns$phonological recognition problem$	2
Two stage classifier with First-person pro- nouns + Seed words/trigrams (FP+SE2): A 5 PRP, 3rd person singular words, family words, human words, sexual words, etc 1991 Method Acc G F 1 M F 1 H F 1 Avg F 1 LDA 49.2 0.00 0.65 0.05 0.23 MedLDA 43.3 0.41 0.52 0.09 0.34 LIWC 49.2 0.34 0.61 0.18 0.38 BOW+ 54.1 0.50 0.59 0.15 0.41 SEED 54.4 0.52 0.60 0.14 0.42 ASUM 56.6 0.32 0.70 0.38 0.47 SDTM?	PRP	purpose$personal pro-$personal pronouns$phonological recognition problem$	2
Noun phrases are proper nouns and PRP.	PRP	purpose$personal pro-$personal pronouns$phonological recognition problem$	2
PRP?i.e.,	PRP	purpose$personal pro-$personal pronouns$phonological recognition problem$	2
Note that this model is DIR: Each target word (observation) can be aligned to at most one source word (hidden state), whereas a source word could be used multiple times.	DIR	directional$direction$	0
However, we show that it is much better to train two DIR models concurrently, coupling their posterior distributions over alignments to approximately agree.	DIR	directional$direction$	0
We propose two such constraints: (i) bijectivity: one word should not translate to many words; and (ii) symmetry: DIR alignments should agree.	DIR	directional$direction$	0
There are several problems with the model that arise from its DIRity, however.	DIR	directional$direction$	0
This leads to the common practice of post-processing heuristics for intersecting DIR alignments to produce nearly bijective and symmetric results (Koehn, Och, and Marcu 2003).	DIR	directional$direction$	0
Let the DIR models be defined as: ??	DIR	directional$direction$	0
3.4 Symmetry Constraints The DIR nature of the generative models used to recover word alignments con- flicts with their interpretation as translations.	DIR	directional$direction$	0
Define z to range over the union of all possible 491 Computational Linguistics Volume 36, Number 3 DIR alignments ??	DIR	directional$direction$	0
CO-   Note that a(w, w') has DIR (from w to w'), so  that a(w, w') may not be equal to a(w', w):  a(films, theatre) = 0.178988 ,  o ( theat re ,  films) ---- 0.068927.	DIR	directional$direction$	1
The START multimedia information system: Current technology and future DIRs.	DIR	directional$direction$	1
We minimize E and find optimum locations for points separating each segment us- ing Powell?s conjugate DIR method, deter- mined the most effective for this task.	DIR	directional$direction$	1
The present paper at-  tempts to be a small step in this DIR.	DIR	directional$direction$	1
DIR?.	DIR	directional$direction$	1
We replace this radical to a placeholder and generate a candidate rule with the corresponding DIR by the radical position in this character.	DIR	directional$direction$	1
References:  I. "A Theory of Linguistic MDls MEANING -- TEXT", Moscow,  1972 (in Russian).	MD	Mode$Maureeen Dowd$Rieser$minimum divergence$Mutual dependency$Modal$	0
2 A Probabi l i s t ic  MD l   There are many factors, both syntactic and se-  mantic, upon which a pronoun resolution sys-  tem relies. (	MD	Mode$Maureeen Dowd$Rieser$minimum divergence$Mutual dependency$Modal$	0
to apply to either a structure y or its parts r. The Markov assumption for factoring labels lets us use the Viterbi algorithm (much like a Hidden Markov MDl) in order to find y = argmaxy? (	MD	Mode$Maureeen Dowd$Rieser$minimum divergence$Mutual dependency$Modal$	0
Frey B.J. 1998, Graphical MDls for Machine  Learning and Digital Communication,  Cambridge, MA, MIT Press  Gildea D., Jurafsky D. 2002, Automatic labeling of  semantic roles, Computational Linguistics,  28(3):245-288.	MD	Mode$Maureeen Dowd$Rieser$minimum divergence$Mutual dependency$Modal$	0
MDrate performance was achieved for the QUOTES3 and QUOTES4 datasets, 0.426 and 0.411, respectively.	MD	Mode$Maureeen Dowd$Rieser$minimum divergence$Mutual dependency$Modal$	0
In this case  Probability  MDl  P(dH)  P(plwa)  P(w lh, t,l)  P(alm.)	MD	Mode$Maureeen Dowd$Rieser$minimum divergence$Mutual dependency$Modal$	0
Simulation-based RL allows to explore unseen actions which are not in the data, and thus less initial data is needed (MD and Lemon, 2008b).	MD	Mode$Maureeen Dowd$Rieser$minimum divergence$Mutual dependency$Modal$	2
Verena MD and Oliver Lemon.	MD	Mode$Maureeen Dowd$Rieser$minimum divergence$Mutual dependency$Modal$	2
689 We are currently collecting data in targeted Wizard-of-Oz experiments, to derive a fully data- driven training environment and test the learnt policy with real users, following (MD and Lemon, 2008b).	MD	Mode$Maureeen Dowd$Rieser$minimum divergence$Mutual dependency$Modal$	2
For example, presenting differing numbers of at- tributes to the user, and making the user more or less likely to choose an item, as shown by (MD and Lemon, 2008b) for multimodal interaction.	MD	Mode$Maureeen Dowd$Rieser$minimum divergence$Mutual dependency$Modal$	2
In extending this previous work we treat NLG as a statistical sequential planning problem, anal- ogously to current statistical approaches to Dia- logue Management (DM), e.g. (Singh et al, 2002; Henderson et al, 2008; MD and Lemon, 2008a) and ?	MD	Mode$Maureeen Dowd$Rieser$minimum divergence$Mutual dependency$Modal$	2
c?2009 Association for Computational Linguistics Natural Language Generation as Planning Under Uncertainty for Spoken Dialogue Systems Verena MD School of Informatics University of Edinburgh vrieser@inf.ed.ac.uk Oliver Lemon School of Informatics University of Edinburgh olemon@inf.ed.ac.uk Abstract We present and evaluate a new model for Natural Language Generation (NLG) in Spoken Dialogue Systems, based on statis- tical planning, given noisy feedback from the current generation context (e.g. a user and a surface realiser).	MD	Mode$Maureeen Dowd$Rieser$minimum divergence$Mutual dependency$Modal$	2
A maximum entropy/MD translation model.	MD	Mode$Maureeen Dowd$Rieser$minimum divergence$Mutual dependency$Modal$	3
If we wish to find the estimate which has MD to an infinite distribution P (T ), we use the same for- mula, but the counts become expected counts: P (X ?	MD	Mode$Maureeen Dowd$Rieser$minimum divergence$Mutual dependency$Modal$	3
In alternative, if q 0 is not uniform then p is called a MD model (according to (Berger and Printz, 1998)).	MD	Mode$Maureeen Dowd$Rieser$minimum divergence$Mutual dependency$Modal$	3
While a uni- form prior on the set of futures results in a max- imum entropy model, choosing other priors out- put a MD models.	MD	Mode$Maureeen Dowd$Rieser$minimum divergence$Mutual dependency$Modal$	3
A comparison of criteria for maximum entropy / MD feature selec- tion.	MD	Mode$Maureeen Dowd$Rieser$minimum divergence$Mutual dependency$Modal$	3
Incorporating position informa- tion into a maximum entropy/MD translation model.	MD	Mode$Maureeen Dowd$Rieser$minimum divergence$Mutual dependency$Modal$	3
MD.	MD	Mode$Maureeen Dowd$Rieser$minimum divergence$Mutual dependency$Modal$	4
And  there  are  three  types  o f  un i f i ca -   t ions  as  fo l lows ;   *MD: compos i t ion  o f  t ran -   s i t i ve  verb  w i th  d i rec t  ob jec t ,  and  com-   pos i t ion  o f  d i rec t  ob jec t  w i th  ind i rec t   ob jec t .	MD	Mode$Maureeen Dowd$Rieser$minimum divergence$Mutual dependency$Modal$	4
3.6 MD Elements of a mutual dependency set are mutually confirming.	MD	Mode$Maureeen Dowd$Rieser$minimum divergence$Mutual dependency$Modal$	4
MDity and Time features have been added in order to implement fusion strategies at dialogue level.	MD	Mode$Maureeen Dowd$Rieser$minimum divergence$Mutual dependency$Modal$	5
3 Improving Multi-MD Representations Figure 1 illustrates how our system computes multi-modal semantic representations.	MD	Mode$Maureeen Dowd$Rieser$minimum divergence$Mutual dependency$Modal$	5
Improving Multi-MD Representa- tions Using Image Dispersion: Why Less is Some- times More.	MD	Mode$Maureeen Dowd$Rieser$minimum divergence$Mutual dependency$Modal$	5
c?2014 Association for Computational Linguistics Learning Image Embeddings using Convolutional Neural Networks for Improved Multi-MD Semantics Douwe Kiela ?	MD	Mode$Maureeen Dowd$Rieser$minimum divergence$Mutual dependency$Modal$	5
2 Related work 2.1 Multi-MD Distributional Semantics Multi-modal models are motivated by parallels with human concept acquisition.	MD	Mode$Maureeen Dowd$Rieser$minimum divergence$Mutual dependency$Modal$	5
The codes characterize some  inherent features (such as "MD"), control proper?	MD	Mode$Maureeen Dowd$Rieser$minimum divergence$Mutual dependency$Modal$	5
Reranking and SELF for parser adaptation.	SELF	self-training$talking to oneself$	0
Of course, the SELF strategy is orthogonal to the improvements we have made.	SELF	self-training$talking to oneself$	0
In this paper we use SELF	SELF	self-training$talking to oneself$	0
They have success- fully played in three types of constraints for our experiments: PR penalty (Our model), decoding constraints in SELF (STS) and virtual evi- dences (VES).	SELF	self-training$talking to oneself$	0
SELF?)	SELF	self-training$talking to oneself$	0
This is the first time that SELF with small labeled datasets is applied suc- cessfully to these tasks.	SELF	self-training$talking to oneself$	0
Verbal imagery is think-  ing ~In words or SELF, while visual imagery is thinking  in p~ ctures.	SELF	self-training$talking to oneself$	1
In Proceedings of the 17th International Congress on Computer Assisted RAD Surgery (CARS), pages 299?304, London, UK, June.	RAD	Radiology and$Radiology$	0
1The captions were extracted from RAD Ra- diographics c?	RAD	Radiology and$Radiology$	0
For British-English a laboratory version of the  interactive mode is available for applications in RAD  Pathology.	RAD	Radiology and$Radiology$	0
c?2007 Association for Computational Linguistics Machine Learning Based Semantic Inference: Experiments and Ob- servations at RTE-3  Baoli Li1, Joseph Irwin1, Ernest V. Garcia2, and Ashwin Ram1    1 College of Computing  Georgia Institute of Technology  Atlanta, GA 30332, USA  baoli@gatech.edu  gtg519g@mail.gatech.edu  ashwin@cc.gatech.edu    2 Department of RAD  School of Medicine, Emory University  Atlanta, GA 30322, USA  Ernest.Garcia@emoryhealthcare.org    Abstract  Textual Entailment Recognition is a se- mantic inference task that is required in  many natural language processing (NLP)  applications.	RAD	Radiology and$Radiology$	1
A Machine Learning Approach for Identi- fying Anatomical Locations of Actionable Findings in RAD Reports.	RAD	Radiology and$Radiology$	1
Professional Lan- guage in Swedish RAD Reports ?	RAD	Radiology and$Radiology$	1
A Novel Hybrid  Approach to Automated Negation Detection in Clin- ical RAD Reports.	RAD	Radiology and$Radiology$	1
The British Journal of RAD, 73(873):999?1001.	RAD	Radiology and$Radiology$	1
Improved Identification of Noun Phrases in Clinical RAD Reports Using a High-Performance Statistical Natural Language Parser Augmented with the UMLS Specialist Lex- icon.	RAD	Radiology and$Radiology$	1
La- tent semantic WSI and disambigua- tion.	WSI	word sense induction$Word Sense Induction$	0
Samuel Brody  et al (2009) adopt a novel Bayesian approach  and formalize the WSI problem  in a generative model.	WSI	word sense induction$Word Sense Induction$	0
The goal of this paper is to inves- tigate different options available to crowdsource a  clustering task and evaluate their efficiency in the  concrete application of WSI.	WSI	word sense induction$Word Sense Induction$	0
Semeval-2007 task 02: evaluating WSI and discrim- ination systems.	WSI	word sense induction$Word Sense Induction$	0
In work on WSD and other tasks related to pol- ysemy, such as WSI, sense alter- nations are treated as word-specific.	WSI	word sense induction$Word Sense Induction$	0
The results from this tech- nique on our WSI problem are  shown in the next section.	WSI	word sense induction$Word Sense Induction$	0
Figure 2: The general WSI Model: models extract distributional data from contexts and induce senses by clustering the extracted informa- tion.	WSI	word sense induction$Word Sense Induction$	1
pervised systems known as Ensemble Clustering to unsupervised NLP systems and focus on the fully unsupervised task of WSI.	WSI	word sense induction$Word Sense Induction$	1
We propose applying a new and more gen- eral framework for combining unsupervised systems known as Ensemble Clustering to unsupervised NLP systems and focus on the fully unsupervised task of WSI.	WSI	word sense induction$Word Sense Induction$	1
We propose evaluating various unsupervised en- sembles when applied to the unsupervised task of WSI with a framework for combining diverse feature spaces and cluster- ing algorithms.	WSI	word sense induction$Word Sense Induction$	1
The task of WSI extends the problem of Word Sense Disambiguation by simply assuming that a model must first learn and define a sense inventory before disambiguating multi-sense words.	WSI	word sense induction$Word Sense Induction$	1
Since WSI is fundamentally a clustering problem, with many vari- ations, it serves well as a NLP case study for Ensem- ble Clustering.	WSI	word sense induction$Word Sense Induction$	1
3 WSI Models WSI models define word senses in terms of the distributional hypothesis, whereby the meaning of a word can be defined by the surround- ing	WSI	word sense induction$Word Sense Induction$	1
Since WSI is fundamentally a clustering problem, with many vari- ations, it serves well as a NLP case study	WSI	word sense induction$Word Sense Induction$	1
c?2012 Association for Computational Linguistics Evaluating Unsupervised Ensembles when applied to WSI Keith Stevens1,2 1University of California Los Angeles; Los Angeles , California, USA 2Lawrence Livermore National Lab; Livermore, California, USA?	WSI	word sense induction$Word Sense Induction$	1
In the present settings, only 42 of the 95 French source words remained, 38 of which kept exactly one En- glish candidate; among these, 27 are the ET, and 1 is an adjective derived from the ET (estomac/gastric).	ET	expected translation$element$	0
For the evaluation, we computed the rank of the ET of each test word and syn- thesized them as a percentile rank distribution.	ET	expected translation$element$	0
Instead of re- ranking the translations provided by the component systems, we search for the hypothesis with the min- imum ET error among all the pos- sible finite-length strings in the target language.	ET	expected translation$element$	0
For instance, for the French word chirurgie whose ET is surgery, we have as top ranked words pain, breast, desmoplasia, pro- cedure, metastatic..., and for m?decine (medicine), we have information, clinician, article, medical.... For common words like, e.g., analyse/analysis and sang/blood, we have girdle, sample, statistic... for analysis and output, collection, calorimetry... for blood as best ranked translations.	ET	expected translation$element$	0
The ET  quality score for the phrase pair (e,f) is defined as  )|,()|,(),( ** effBleufeeBleufeB +=  (8)  where *e is the human translation of the source  phrase f, and *f is the human translation of the  target phrase e. These human translations are  obtained from hand alignment of some parallel  sentences.	ET	expected translation$element$	0
We then pro- duced a ranked list of the top translational equiv- alents and tested whether the ET can be differentiated from other well-known domain words.	ET	expected translation$element$	0
Coor- dinating ETs are commata and coordinating conjunctions.	ET	expected translation$element$	1
Hence  P(pla, W) = P(plw,~)  Since I~" is a vector, we need to normal-  ize P(ff'lh, t,l, a) to obtain the probability of  each ET in the vector.	ET	expected translation$element$	1
The representation for sentence 1 states that the first ET of the 5-gram (-3; third word to the left of the adjective) is empty (because the second ET is a phrase boundary marker), that the sec- ond ET is a clause delimiter (conjunction that), the third one (-1; word preceding the adjective) is a definite determiner, and the fourth one (+1; word following the adjective) is a common noun.	ET	expected translation$element$	1
The evaluation metrics were precision P (the number of true pos- K TP FP FN P (%) R (%) F1 (%) 10 641 80 149 88.90 81.14 84.84 20 644 79 146 89.07 81.52 85.13 30 644 80 146 88.95 81.52 85.07 40 645 81 145 88.84 81.65 85.09 50 645 80 145 88.97 81.65 85.15 Table 2: Effects of K in Bayes Point Machines itives divided by the total number of ETs la- beled as belonging to the positive class) recall R (the number of true positives divided by the to- tal number of ETs that actually belong to the positive class) and their harmonic mean, the F1 score (F1 = 2PR/(P + R)).	ET	expected translation$element$	1
5.4 Global and Local Reordering Model In order to show the advantages of explicitly mod- eling global phrase reordering, we implemented a different reordering model where the reordering pattern is classified into three values: monotone adjacent, RA and neutral.	RA	reverse adjacent$recognition accuracy$	0
On two corpora, namely, Medstract and the  new 100-Medline corpus, 100% RA was achieved.	RA	reverse adjacent$recognition accuracy$	1
All three methods produce similar speech RA.	RA	reverse adjacent$recognition accuracy$	1
The use of MFCDCN and SCA  improves RA by 35.0 percent overall, and the  ratio of overaU error rates for the C2 and P0 conditions i  1.49, as  in Spoke 5.	RA	reverse adjacent$recognition accuracy$	1
Mildly and moderately dysarthric speakers can attain a RA of 80% in dictation systems, breath exercises and phonation training improve performance (Young and Mihai- lidis, 2010).	RA	reverse adjacent$recognition accuracy$	1
2 we describe these compensation procedures in detail,  and we examine their effect on RA in Secs.	RA	reverse adjacent$recognition accuracy$	1
PERFORMANCE OF ALGORITHMS IN  DEVELOPMENTAL TESTING  In this and the following section we describe the results of a series  of experiments that compare the RA of the vari-  ous algorithms described in Sec.	RA	reverse adjacent$recognition accuracy$	1
By  comparing the C2 and C3 results using the CLSTLK microphone  with the P0 and S1 results using the Audio-Technica mic, we note  that very little degradation is observed in the 20-dB condition but  that RA is quite low for the 0-riB condition, even  when the signal is compensated.	RA	reverse adjacent$recognition accuracy$	1
By collaps- ing MG and reverse gap into neutral, it can be thought of as a local reordering model sim- ilar to the block orientation bigram (Tillmann and Zhang, 2005).	MG	monotone gap$Minimalist Grammar$metagrammar$	0
Since non-local reorderings such as MG and reverse gap are more frequent in Japanese to English translations, they are worth modeling ex- plicitly in this reordering model.	MG	monotone gap$Minimalist Grammar$metagrammar$	0
c?2013 Association for Computational Linguistics Distributions on MG Derivations Tim Hunter Department of Linguistics Cornell University 159 Central Ave.,	MG	monotone gap$Minimalist Grammar$metagrammar$	1
On Formal Properties of MGs.	MG	monotone gap$Minimalist Grammar$metagrammar$	1
Pittsburgh, PA, 15213 cdyer@cs.cmu.edu Abstract We present three ways of inducing proba- bility distributions on derivation trees pro- duced by MGs, and gi	MG	monotone gap$Minimalist Grammar$metagrammar$	1
MGs are feature-driven, meaning features of lexical items determine which operations can occur and when.	MG	monotone gap$Minimalist Grammar$metagrammar$	1
4 MGs If correct, the ERH would explain the increasing difficulty across the AH in terms of greater or lesser uncertainty about intermediate parser states.	MG	monotone gap$Minimalist Grammar$metagrammar$	1
A MG is a five- tuple G = ??,	MG	monotone gap$Minimalist Grammar$metagrammar$	1
Pittsburgh, PA, 15213 cdyer@cs.cmu.edu Abstract We present three ways of inducing proba- bility distributions on derivation trees pro- duced by MGs, and give their maximum likelihood estimators.	MG	monotone gap$Minimalist Grammar$metagrammar$	1
nguistics Distributions on MG Derivations Tim Hunter Department of Linguistics Cornell University 159 Central Ave.,	MG	monotone gap$Minimalist Grammar$metagrammar$	1
purpose  of enhancing the recall rate, FASTR includes a  MG used to generate term variant rules from  term rules.	MG	monotone gap$Minimalist Grammar$metagrammar$	2
The MG goes multilin- gual: A cross-linguistic look at the V2-phenomenon.	MG	monotone gap$Minimalist Grammar$metagrammar$	2
In this experiment, he MG consists of  positive paradigmatic metarules (e.g. (11)) and filtering  negative metarules rejecting the spurious variations  extracted by the positive ones (e.g. (12)).	MG	monotone gap$Minimalist Grammar$metagrammar$	2
These for- malisms take the MG approach, where the basic units are tree descriptions (i.e., formulas denoting sets of trees) rather than trees.	MG	monotone gap$Minimalist Grammar$metagrammar$	2
With the purpose  of enhancing the recall rate, FASTR includes a  MG used to generate term variant rules from  term rules.	MG	monotone gap$Minimalist Grammar$metagrammar$	2
(2) Rule : NI ---> (N2 ---> N3 N4) N 5  <N I label> = 'XRD'  <N I metaLabel> = 'XX'  <N I lexicalization> = 'Ns'  <N 3 lenuna> = 'X'  <N 3 inflection> = 7  <N 4 lemma> = 'ray'  <N 4 inflection> = I  <Ns lemma> = 'diffraction'  <N5 inflection> = I.  The third level of the formalism consists of a  MG.	MG	monotone gap$Minimalist Grammar$metagrammar$	2
The formalism of FASTR is organized into three  levels : a single word lexicon, a terminological grammar  and a MG for term variations.	MG	monotone gap$Minimalist Grammar$metagrammar$	2
This has implications for role text containing appositives, the second situation in which a comma appears within a single RS.	RS	role span$resource scarce$Reference Standard$Resource Situation$relevance score$Red State$	0
Semantic roles                                                    6 http://nlp.cs.berkeley.edu/Main.html  799 can cover sentential constituents of arbitrary  length, and simply using word alignments for  projection is likely to result in wrong RSs.	RS	role span$resource scarce$Reference Standard$Resource Situation$relevance score$Red State$	0
Note that there is no obvious random baseline for the complex task of predicting RSs and their labels, however.	RS	role span$resource scarce$Reference Standard$Resource Situation$relevance score$Red State$	0
We observe that if a RS does include a predicate, resulting questions are often ungrammatical due to the con- jugation of that predicate.	RS	role span$resource scarce$Reference Standard$Resource Situation$relevance score$Red State$	0
SVM classifiers were trained 2 with the LIBLINEAR library (Fan et al, 2008) and learned to predict the frame name, RSs, and role labels.	RS	role span$resource scarce$Reference Standard$Resource Situation$relevance score$Red State$	0
We believe that our current work reduces the barrier for semantic MT evaluation for RS languages sufficiently so that semantic MT evaluation can be applied to most other languages.	RS	role span$resource scarce$Reference Standard$Resource Situation$relevance score$Red State$	1
The re- quirement of a large training corpus renders these  algorithms unsuitable for RS languag- es.	RS	role span$resource scarce$Reference Standard$Resource Situation$relevance score$Red State$	1
Introduction Development of language applications for local languages in Africa requires innovative approaches since many of these languages are RS.	RS	role span$resource scarce$Reference Standard$Resource Situation$relevance score$Red State$	1
duate School,  Harbin Institute of Technology, Shenzhen 518055  2Department Of Computing, the Hong Kong Polytechnic University  guilin.nlp@gmail.com, xuruifeng@hitsz.edu.cn, csluqin@comp.polyu.edu.hk, xujun@hitsz.edu.cn,  csjxu@comp.polyu.edu.hk,{bliu,wangxl}@insun.hit.edu.cn    Abstract  Transfer learning has been used in opin- ion analysis to make use of available lan- guage resources for other RS  languages.	RS	role span$resource scarce$Reference Standard$Resource Situation$relevance score$Red State$	1
How- ever, since our work focuses on RS lan- guages we did not want to incur the additional cost of using a development set.	RS	role span$resource scarce$Reference Standard$Resource Situation$relevance score$Red State$	1
In this paper we study the challenges of  named entity recognition for RS  languages among South Asian languages.	RS	role span$resource scarce$Reference Standard$Resource Situation$relevance score$Red State$	1
RS.	RS	role span$resource scarce$Reference Standard$Resource Situation$relevance score$Red State$	2
where qtf is the frequency of term t in the query q, and w(t,d)  is the RS of a document d for the query term t,  given by:  ) 5.0 1log() )1( 1(),( 2 + +??	RS	role span$resource scarce$Reference Standard$Resource Situation$relevance score$Red State$	4
Following (Zhai, 2008), the RS between question q and user u can be estimated by the negative Kullback-Leibler divergence between ?	RS	role span$resource scarce$Reference Standard$Resource Situation$relevance score$Red State$	4
Statisti- cal methods for calculating the RS of each fragment can be categorized into sev- eral classes: cue-based (Edmundson, 1969), key- word- or frequency-based (Luhn, 1958; Edmund- son, 1969; Neto et al, 2000; Steinberger and Jezek, 2004; Kallel et al, 2004; Vanderwende et al.,	RS	role span$resource scarce$Reference Standard$Resource Situation$relevance score$Red State$	4
The RS for a sentence is calculated as a sum of its domination scores over all paths.	RS	role span$resource scarce$Reference Standard$Resource Situation$relevance score$Red State$	4
Using the In_expC2 model, the RS of a  document d for a query q is given by the formula:                    (3) ?	RS	role span$resource scarce$Reference Standard$Resource Situation$relevance score$Red State$	4
interests, which is helpful to derive RSs.	RS	role span$resource scarce$Reference Standard$Resource Situation$relevance score$Red State$	4
1% 59.5% 36.3%     + Proper Nouns 36.5% 56.8% 44.4%  Named Entities 48.4% 49.1% 48.7%  All Combined 21.1% 65.0% 31.9%  Manual Scoring 67.0% 75.0% 70.8%      Single word SVM 19.0% 30.0% 23.3%  + Stemming 22.0% 30.2% 25.5%     + Proper Nouns 46.3% 54.0% 49.9%  Named Entities 60.1% 41.5% 49.1%  All Combined 20.3% 65.7% 31.0%  Manual Scoring 47.0% 62.0% 53.5%  Figure 1: Results on Daily Kos (top) and RS  (bottom) data.	RS	role span$resource scarce$Reference Standard$Resource Situation$relevance score$Red State$	5
dailykos.com) and RS  (www.redstate.com).	RS	role span$resource scarce$Reference Standard$Resource Situation$relevance score$Red State$	5
RS is a conserva- tive political blog whereas Daily Kos is a liberal  political blog.	RS	role span$resource scarce$Reference Standard$Resource Situation$relevance score$Red State$	5
We collected a  total of 100,000 blog posts from Daily Kos and  70,000 blog posts from RS and a total of  787,780 tags across both blogs (an average of 4.63  tags per post).	RS	role span$resource scarce$Reference Standard$Resource Situation$relevance score$Red State$	5
5 Results and Evaluation  For evaluating our methods, we used 2,681 posts  from Daily Kos and 571 posts from RS.	RS	role span$resource scarce$Reference Standard$Resource Situation$relevance score$Red State$	5
We collected a  total of 100,000 blog posts from Daily Kos and  70,000 blog posts from RS and a to	RS	role span$resource scarce$Reference Standard$Resource Situation$relevance score$Red State$	5
We collected a  total of 100,000 blog posts from Daily Kos and  70,000 blog posts from RS and a total of  787,780 tags across bot	RS	role span$resource scarce$Reference Standard$Resource Situation$relevance score$Red State$	5
3 Data  We collected data from two major political blogs,  Daily Kos (www.dailykos.com) and RS  (www.redstate.com).	RS	role span$resource scarce$Reference Standard$Resource Situation$relevance score$Red State$	5
Maximum entropy classifier is thus closely related to logistic regression model.1560 PF: - Position from the beginning of listing - Position to the end of listing Orthographic Features: - Identity of the current word - Current word contains a digit - Current word contains only digits - Current word is capitalized - Current word begins with a capitalized letter followed by all non-cap letters.	PF	Position Features$prediction and feedbac$Predicate Form$predicted frequency$predicate frame$	0
pos(wi)] / |s|  3.4 Sentence PF  In our study, another type of position features,  which model sentence position information, is  defined for comparison with the word position  features.	PF	Position Features$prediction and feedbac$Predicate Form$predicted frequency$predicate frame$	0
3.2 Word PF  With the above model, word position features  are defined to represent the word position  information and are then incorporated into the  model.	PF	Position Features$prediction and feedbac$Predicate Form$predicted frequency$predicate frame$	0
3.3 Incorporating the PF   To incorporate the position features into the  word-based summarization model, we use them  to adjust the importance of the word appearance.	PF	Position Features$prediction and feedbac$Predicate Form$predicted frequency$predicate frame$	0
+b 3 ) Embeddings PF     &       Path Features Input Features Position features Path features Softmax  Layer concatenate Dropout  Layer ?	PF	Position Features$prediction and feedbac$Predicate Form$predicted frequency$predicate frame$	0
3.5.3 Relative PF  We define three types of position features  which depict the relative structures between  the two entities, including Nested, Adjacent  and Separated.	PF	Position Features$prediction and feedbac$Predicate Form$predicted frequency$predicate frame$	0
To c a p t u r e  t h e  dependence o f  syntax on semantic con ten t  a n d   b o c j a l  c o n t e x t ,  t h e  sentence  gene ra to r  uses f u n c t i o n - l i k e  grammar  rules o f  t h e  form  ( R u l  erame C a t  Variables PFs ) .	PF	Position Features$prediction and feedbac$Predicate Form$predicted frequency$predicate frame$	2
In general, a predicate is determined 85 Name Note Baseline Features PF and POS of the predi- cate Noun Form and POS of the head- word of the candidate phrase Particle Form and POS of the particle of the candidate phrase Path Dependency relation between the predicate and the candi- date phrase Passive Passive auxiliary verbs that the predicate contains PhPosit Relative phrase position be- tween the predicate and the candidate phrase SentPosit Relative s	PF	Position Features$prediction and feedbac$Predicate Form$predicted frequency$predicate frame$	2
of distinct aspect terms identified by the method, ordered by decreasing PF.	PF	Position Features$prediction and feedbac$Predicate Form$predicted frequency$predicate frame$	3
Syntactic features (14): part-of-speech tags (POS) of: first phrase word, last phrase word, 229 word immediately before phrase and word im- mediately after phrase; syntactic paths from word to verb: all paths, only paths for words before verb and only paths for words after verb; phrase label, label of phrase parent, subcate- gorisation of verb parent, PF from PropBank, voice, head preposition for preposi- tional phrases and same parents flag.	PF	Position Features$prediction and feedbac$Predicate Form$predicted frequency$predicate frame$	4
In the end, neither the ar- gument lemma, nor the PF improved the performance.	PF	Position Features$prediction and feedbac$Predicate Form$predicted frequency$predicate frame$	4
We tried to represent the chain using various subsets of the following elements: the argument lemma and part-of-speech, the PF and part- of-speech, the parts-of-speech and syntactic de- pendencies of the intermediate words linking the argument to the predicate.	PF	Position Features$prediction and feedbac$Predicate Form$predicted frequency$predicate frame$	4
In order to allow access to addi- tional useful information, such as subsumption,  property inheritance, PFs from other  sources, links to instances, and so on, our goal is to  link the senses to an ontology.	PF	Position Features$prediction and feedbac$Predicate Form$predicted frequency$predicate frame$	4
4.3 Data Distribution The total amount of data prepared for the 14 verbs are divided into three non-overlapping sets in a bal- anced form in terms of both the number of the target PFs, and the relevant instances of each frame.	PF	Position Features$prediction and feedbac$Predicate Form$predicted frequency$predicate frame$	4
Parsing consists of four steps: predicate sense disambiguation, argument identification, argument classification and PF constraint satis- faction.	PF	Position Features$prediction and feedbac$Predicate Form$predicted frequency$predicate frame$	4
39 That-COMP are optional words that in- troduce sentential complements in English.	COMP	complementizers$compressor$Completion$compound$competition$competitio$	0
Label numRef numSys numCorr F1 CP-NoneL 1723 1724 1715 0.995 IP-NoneL 3874 3875 3844 0.992 VP-NoneR 660 633 597 0.923 IP-NoneM 440 432 408 0.936 VP-NoneL 135 107 105 0.868 Table 7: 5 most frequent labels carrying pseudo tags and their performances COMP for subordinate clauses.	COMP	complementizers$compressor$Completion$compound$competition$competitio$	0
This implies that deep annotations as, for instance, have been derived so far from PennTreeBank/PropBank, in which ei- ther all syntactic nodes of the annotation are kept (as in (Bohnet et al, 2010)) or only certain syntac- tic nodes are removed (as THAT COMP and TO infinitives in the shared task 2011 on sur- face realization (Belz et al, 2011)) still fall short of a genuine semantic annotation.	COMP	complementizers$compressor$Completion$compound$competition$competitio$	0
For example, there are no empty COMP annotated in the CTB while English does not allow dropped pronouns.	COMP	complementizers$compressor$Completion$compound$competition$competitio$	0
The poss ib le  COMP are*  ]CHAT Mother said t h a t  Mike should move,  FORT0 Mother told Mike t o  move.	COMP	complementizers$compressor$Completion$compound$competition$competitio$	0
We extract HLDS- based quasi logical form graphs from the CCG- bank and semantically empty function words such as COMP, infinitival-to, expletive subjects, and case-marking prepositions are adjusted to reflect their purely syntactic status.	COMP	complementizers$compressor$Completion$compound$competition$competitio$	0
4 Evaluation We compared the Gibbs sampling COMP (GS) against a version of maximum a posteriori EM (with Dirichlet parameter greater than 1) and a discriminative STSG based on SVM training (Cohn and Lapata, 2008) (SVM).	COMP	complementizers$compressor$Completion$compound$competition$competitio$	1
At the same time, the satisfaction mean score is 3.23 over 5, whereas the same users attribute to human COMPs a satisfaction mean score of 3.7, really not so much more.	COMP	complementizers$compressor$Completion$compound$competition$competitio$	1
EFFECTS OF THE STRUCTURE OF TASK-ORIENTED  DIALOGUES  Task-oriented conversations have a specific goal to be  achieved: the performance of a task (e.g., the air  COMP assembly in Grosz (1977)).	COMP	complementizers$compressor$Completion$compound$competition$competitio$	1
For example, Grosz collected  and studied dialogues in which an expert guides an  apprentice in the assembly of an air COMP.	COMP	complementizers$compressor$Completion$compound$competition$competitio$	1
3 COLIN?s COMP: System architecture and implementation 3.1 Architecture We assume we have a raw text as an input, which may be the output of an extract summarizer, and we have to produce a compressed version of it, by reducing as many sentences as we can, without deleting a single one.	COMP	complementizers$compressor$Completion$compound$competition$competitio$	1
157 "heart attack" compressed encoding RBM trained encoder original vector Figure 2: Using a RBM trained COMP to generate a compressed encoding of ?	COMP	complementizers$compressor$Completion$compound$competition$competitio$	1
, i , - , - , l )  t\[,\] --* t\[rlr\]  COMP of the material below the root node ~r of an initial tree  allows for the completion of the node at which substitution occurred.	COMP	complementizers$compressor$Completion$compound$competition$competitio$	2
4.1 COMP Time Figure 3 shows the time that it took for our HITs for 37 languages to be completed on MTurk.	COMP	complementizers$compressor$Completion$compound$competition$competitio$	2
COMP rate is calculated as the number of completed calls divided by the total num- ber of calls.	COMP	complementizers$compressor$Completion$compound$competition$competitio$	2
COMP of items (moving of the dot from left to right over a nonterminal)  breaks up into several cases, depending on which production type is being completed.	COMP	complementizers$compressor$Completion$compound$competition$competitio$	2
c?2009 Association for Computational Linguistics Cross-lingual Alignment and COMP of Wikipedia Templates Gosse Bouma Information Science University of Groningen g.bouma@rug.nl Sergio Duarte Information Science University of Groningen sergio.duarte@gmail.com Zahurul Islam Information Science University of Groningen zaisdb@gmail.com Abstract For many languages, the size of Wikipedia is an order of magnitude smaller than the En- glish Wikipedia.	COMP	complementizers$compressor$Completion$compound$competition$competitio$	2
4.4 Selection of Feature Templates of the Graph-based COMP Model The graph-based completion model re-scores the beam incrementally and leads to a higher accuracy.	COMP	complementizers$compressor$Completion$compound$competition$competitio$	2
He observes that this approach suffers from an acute data sparseness problem and goes on to obtain counts for candidate COMPs through web searches, thus achieving a translation accu- racy of 86?87%.	COMP	complementizers$compressor$Completion$compound$competition$competitio$	3
Then we investi- gate the generality of the web-based approach by apply- ing it to a range of analysis and generations tasks, involv- ing both syntactic and semantic knowledge: (c) ordering of prenominal adjectives, (d) COMP noun bracketing, (e) COMP noun interpretation, and (f) noun count- ability detection.	COMP	complementizers$compressor$Completion$compound$competition$competitio$	3
Ad-adjectival adjectives are special forms of adjectives used in COMPs like angielsko- polski ?	COMP	complementizers$compressor$Completion$compound$competition$competitio$	3
on the assump- tion that the COMP exerts similar attractions as the base noun.	COMP	complementizers$compressor$Completion$compound$competition$competitio$	3
Termhood  is a numeric estimation of the degree to which a  given linguistic unit (a multiword COMP) is  related to a domain-specific concept.	COMP	complementizers$compressor$Completion$compound$competition$competitio$	3
Grefenstette translates COMPs from German and Spanish into English, and uses BNC frequencies as a filter for candidate translations.	COMP	complementizers$compressor$Completion$compound$competition$competitio$	3
We will call the set of composable fragments at a certain step in the stochastic process the COMP set at that step.	COMP	complementizers$compressor$Completion$compound$competition$competitio$	4
f'?CS  P(f') P( f)  (3) CP(f | CS)  = Bod & Kaplan give three definitions of increasing complexity for the COMP set: the first definition groups all fra	COMP	complementizers$compressor$Completion$compound$competition$competitio$	4
i CP(f i | CSi) where the COMP probability  CP(f | CS) is expressed in terms of fragment probabilities P( f): ?	COMP	complementizers$compressor$Completion$compound$competition$competitio$	4
f'?CS  P(f') P( f)  (3) CP(f | CS)  = Bod & Kaplan give three definitions of increasing complexity for the COMP set: the first definition groups all fragments that only satisfy the Category-matching condition of the composition operation; the second definition groups all fragments which satisfy both	COMP	complementizers$compressor$Completion$compound$competition$competitio$	4
ote the probability of choosing a fragment f from a COMP set CS containing f, then the probability of a derivation D = < f1, f2 ... fk> is (2) P(< f1, f2 ... fk>)  =  ?	COMP	complementizers$compressor$Completion$compound$competition$competitio$	4
The required efforts discouraged us from building a middle ontology between the BioNLP and the PIKB data models, especially given the time lim- itations for the present task COMP.	COMP	complementizers$compressor$Completion$compound$competition$competitio$	4
Let CP( f | CS) denote the probability of choosing a fragment f from a COMP set CS containing f, then the probability of a derivation D = < f1, f2 ... fk> is (2) P(< f1, f2 ... fk>)  =  ?	COMP	complementizers$compressor$Completion$compound$competition$competitio$	4
f'?CS  P(f') P( f)  (3) CP(f | CS)  = Bod & Kaplan give three definitions of increasing complexity for the COMP set: the first definition groups all fragments that only satisfy the Category-matching condition of the composition operation; the second definition groups all fragments which satisfy both Category- matching and Uniqueness; and the third definition groups all fragments which satisfy Category- matching, Uniqueness and Coherence.	COMP	complementizers$compressor$Completion$compound$competition$competitio$	4
As noted by Lewis and Sparck-Jones (1996), the results of large-scale document-retrieval COMPs such as the Text Retrieval Conference (TREC, 2000) do not necessarily reflect the experience many users have with retrieval systems.	COMP	complementizers$compressor$Completion$compound$competition$competitio$	4
We will call the set of composable fragments at a certain step in the stochastic process the COMPn set at that step.	COMP	complementizers$compressor$Completion$compound$competition$competitio$	5
f'?CS  P(f') P( f)  (3) CP(f | CS)  = Bod & Kaplan give three definitions of increasing complexity for the COMPn set: the first definition groups all fra	COMP	complementizers$compressor$Completion$compound$competition$competitio$	5
i CP(f i | CSi) where the COMPn probability  CP(f | CS) is expressed in terms of fragment probabilities P( f): ?	COMP	complementizers$compressor$Completion$compound$competition$competitio$	5
f'?CS  P(f') P( f)  (3) CP(f | CS)  = Bod & Kaplan give three definitions of increasing complexity for the COMPn set: the first definition groups all fragments that only satisfy the Category-matching condition of the composition operation; the second definition groups all fragments which satisfy both	COMP	complementizers$compressor$Completion$compound$competition$competitio$	5
ote the probability of choosing a fragment f from a COMPn set CS containing f, then the probability of a derivation D = < f1, f2 ... fk> is (2) P(< f1, f2 ... fk>)  =  ?	COMP	complementizers$compressor$Completion$compound$competition$competitio$	5
The required efforts discouraged us from building a middle ontology between the BioNLP and the PIKB data models, especially given the time lim- itations for the present task COMPn.	COMP	complementizers$compressor$Completion$compound$competition$competitio$	5
Let CP( f | CS) denote the probability of choosing a fragment f from a COMPn set CS containing f, then the probability of a derivation D = < f1, f2 ... fk> is (2) P(< f1, f2 ... fk>)  =  ?	COMP	complementizers$compressor$Completion$compound$competition$competitio$	5
f'?CS  P(f') P( f)  (3) CP(f | CS)  = Bod & Kaplan give three definitions of increasing complexity for the COMPn set: the first definition groups all fragments that only satisfy the Category-matching condition of the composition operation; the second definition groups all fragments which satisfy both Category- matching and Uniqueness; and the third definition groups all fragments which satisfy Category- matching, Uniqueness and Coherence.	COMP	complementizers$compressor$Completion$compound$competition$competitio$	5
As noted by Lewis and Sparck-Jones (1996), the results of large-scale document-retrieval COMPns such as the Text Retrieval Conference (TREC, 2000) do not necessarily reflect the experience many users have with retrieval systems.	COMP	complementizers$compressor$Completion$compound$competition$competitio$	5
2 TSGs TSGs (Joshi and Schabes, 1997) generalize context-free grammars by allow- ing nonterminals to rewrite as tree fragments of ar- bitrary size, instead of as only a sequence of one or 217 .S .NP .VP .VBD .said .NP .SBAR .. Figure 1: A Tree Substitution Grammar fragment.	TSG	Tree substitution grammar$tree substitution grammar$	0
4 As an example,  4 The tree substitution grammar is lexicalized inthe sense that each of the trees has an associated anchor,  442  van Noord Efficient Head-Corner Parsing  nt5:I nt0:a ntl:4 nt2:3  / \  r  ntO man home  nt3:6 / \   at nt2  nt4:5 nt6:l  /\ /\  4 nt3 nt5 7 /\ /\  nt0 man see nt4  nt6:2 / \   1 nt3 / \   nt5 7 / \   see nt l   Figure 5  TSG that derives each of the two derivation trees of the sentence I see a  man at home, for the grammar of Billot and Lang (1989).	TSG	Tree substitution grammar$tree substitution grammar$	0
The best accuracy is achieved by excluding fea- 137 Feature Accuracy (1) Word 1,2-gram 0.8075 (2) POS 2,3-gram 0.5555 (3) POS,Function 2,3-gram 0.7080 (4) Chracter 2,3-gram 0.6678 (5) Dependency 0.7236 (6) TSG 0.6455 (7) 1 + 2 0.7825 (8) 1 + 3 0.7913 (9) 1 + 4 0.7953 (10) 1 + 5 0.8020 (11) 1 + 6 0.7999 (12) 1 + 2 + 3 0.7849 (13) 1 + 2 + 3 + 4 0.8000 (14) 1 + 2 + 3 + 4 + 5 0.8097 (15) ALL 0.8088 Table 5: 10-fold cross validation results for each feature tures whose NLF is 1 or 11.	TSG	Tree substitution grammar$tree substitution grammar$	0
In TSGs, for instance, there may be many ways of combining elementary trees to pro- duce the same output tree; in machine translation, many different elementary phrases or elementary tree pairs might produce the same output string.	TSG	Tree substitution grammar$tree substitution grammar$	1
In- cremental TSG for pars- ing and word prediction.	TSG	Tree substitution grammar$tree substitution grammar$	1
P ) be a TSG (regular tree grammar) in normal form that recognizesL (i.e.	TSG	Tree substitution grammar$tree substitution grammar$	1
Like description TSGs, but unlike multi-component TAGs, it allows trees to be partitioned into any desired set of contiguous components during composition, 2.	TSG	Tree substitution grammar$tree substitution grammar$	1
Like multi-component TAGs, but unlike descrip- tion TSGs, it allows the specication of particular insertion sites within elementary trees, and 3.	TSG	Tree substitution grammar$tree substitution grammar$	1
Bayesian learning of a TSG.	TSG	Tree substitution grammar$tree substitution grammar$	1
Since the new SVM QP is convex, it is no harder to op- timize than the standard SVM objective.	QP	PROBING QUESTION$quadratic program$Question Processing$quadratic programming$	1
2.2.1 Optimization as a Binary SVM We could solve the optimization problem in (4) directly using a QPming solver.	QP	PROBING QUESTION$quadratic program$Question Processing$quadratic programming$	1
Building on this efficient framework, we incorporate variance regularization into the SVM?s QP.	QP	PROBING QUESTION$quadratic program$Question Processing$quadratic programming$	1
2 Three Multi-Class SVM Models We describe three max-margin multi-class classi- fiers and their corresponding QPs.	QP	PROBING QUESTION$quadratic program$Question Processing$quadratic programming$	1
For VAR-SVM, we solve the primal form of the QP directly in CPLEX (2005), a general optimization package.	QP	PROBING QUESTION$quadratic program$Question Processing$quadratic programming$	1
We refer to a CS-SVM trained using the variance minimization QP as the VAR-SVM.	QP	PROBING QUESTION$quadratic program$Question Processing$quadratic programming$	1
IE Index Case  Insensitive Corpus Multi-level  TemplateQuestion Keyword  IndexKeyword indexing InfoXtract Asking PointIdentification Feature RankingSnippet Retrieval Snippet-levelAnswer QP Text Processing Answer Ranking Case Restoration InfoXtract   Figure 2:  Architecture of QA Based on NLP/IE    Snippet Retrieval    Snippet retrieval generates the top n (we chose  200) most relevant sentence-level candidate  answer snippets based on the question processing  results.	QP	PROBING QUESTION$quadratic program$Question Processing$quadratic programming$	2
First, the keywords extracted by the QP module can be enhanced with concepts from the topic signatures to produce a ranked list of paragraphs, resulting from the Pas- sage Retrieval Module.	QP	PROBING QUESTION$quadratic program$Question Processing$quadratic programming$	2
If the QP component detects an  Asking Point CE Link, the system first attempts to  retrieve snippets that contain the corresponding CE  relationship.	QP	PROBING QUESTION$quadratic program$Question Processing$quadratic programming$	2
QP: The query is classified and the two top expected answer types are estimated; it is then submitted to the underlying search engine; ?	QP	PROBING QUESTION$quadratic program$Question Processing$quadratic programming$	2
This system consists of three components:  (i) QP, (ii) Text Processing, and  (iii) Answer Ranking.	QP	PROBING QUESTION$quadratic program$Question Processing$quadratic programming$	2
i  Figure 2: Textract/QA 1.0 Prototype Architecture  The general algorithm for question  answering is as follows:  168  Process Question  Shallow parse question  Determine Asking Point  Question expansion (using word lists)  Process Documents  Tokenization, POS tagging, NE Indexing  Shallow Parsing (not yet utilized)  Text Matcher  Intersect search engine results with NE  rank answers  2.1 QP  The QP results are a list of  keywords plus the information for asking point.	QP	PROBING QUESTION$quadratic program$Question Processing$quadratic programming$	2
A QP pro- cedure.	QP	PROBING QUESTION$quadratic program$Question Processing$quadratic programming$	3
N = length(S); m = length(f); F = [f;D]; pc = min(pc,(D-Ds)/N); % Adjust pc, if Ds is close to D. % Solve a QP problem to find an initial 350 Li and Church Sketch for Estimating Associations % guess of the MLE that minimizes the 2-norm with respect to % the MF estimation and satisfies the constraints.	QP	PROBING QUESTION$quadratic program$Question Processing$quadratic programming$	3
2.2.1 Optimization as a Binary SVM We could solve the optimization problem in (4) directly using a QP solver.	QP	PROBING QUESTION$quadratic program$Question Processing$quadratic programming$	3
SVM is trained by solving a  dual QP problem.	QP	PROBING QUESTION$quadratic program$Question Processing$quadratic programming$	3
An algorithm for QP.	QP	PROBING QUESTION$quadratic program$Question Processing$quadratic programming$	3
It is formu- lated as a QP problem: min ?	QP	PROBING QUESTION$quadratic program$Question Processing$quadratic programming$	3
The experiment has been carried out on 60 sentences  with 1201 different lectures, and formed by using seven  verbs (wr~te, eat, smell, corrode, buy, receive, assocza~e)  coupled with fifty common ouns and two PNs.	PN	proper noun$proper names$POS ngrams$proper name$Personal Name$preceding noun phrase$	0
For nouns, number and gender information is needed, as well as infor- mation as to whether it is a common or PN.	PN	proper noun$proper names$POS ngrams$proper name$Personal Name$preceding noun phrase$	0
NNP Noun phrases (e.g., PNs) ex- tracted from comment texts.	PN	proper noun$proper names$POS ngrams$proper name$Personal Name$preceding noun phrase$	0
Noun phrases are PNs and personal pronouns.	PN	proper noun$proper names$POS ngrams$proper name$Personal Name$preceding noun phrase$	0
PN?	PN	proper noun$proper names$POS ngrams$proper name$Personal Name$preceding noun phrase$	0
The Urdu morphol- ogy provides the following analysis for the PN nAdyA. (3) nAdyA +Noun +Name +Fem The tags provide the information that it is a noun, in particular a type of PN (Name), and is fem- inine.	PN	proper noun$proper names$POS ngrams$proper name$Personal Name$preceding noun phrase$	0
An extreme alternative is to have a first pass  where only referring expressions which look like  anaphors are marked up, such as pronouns, def-  inite NPs and reduced forms of PN.	PN	proper noun$proper names$POS ngrams$proper name$Personal Name$preceding noun phrase$	1
We used two approaches for identifying story characters motivated by (Elson and McKeown, 2010): 1) named entity recognition was used for identifying PN, e.g., ?	PN	proper noun$proper names$POS ngrams$proper name$Personal Name$preceding noun phrase$	1
2In case of PN, there exist many dedicated algo- rithms and systems for finding them in texts, often developed within the Message Understanding Conference series.	PN	proper noun$proper names$POS ngrams$proper name$Personal Name$preceding noun phrase$	1
2) a set of part-of-speech patterns was used for the extraction of human and non-human characters that were not represented by PN, e.g., ?	PN	proper noun$proper names$POS ngrams$proper name$Personal Name$preceding noun phrase$	1
2 A Flexemic Tagset for Polish The tagset presented in this section is based on the following design assumptions:   what is being tagged is a single orthographic word or, in some well-defined cases, a part thereof; multi-word constructions, even those sometimes considered to be morphological formations (so-called analytic forms) or dic- tionary entries (PN), should be considered by a different level of process- ing;2 cf.	PN	proper noun$proper names$POS ngrams$proper name$Personal Name$preceding noun phrase$	1
The analysis also explains why PN can  never be modified restrictively.	PN	proper noun$proper names$POS ngrams$proper name$Personal Name$preceding noun phrase$	1
Utterance-length number of words in the sentence Part-of-speech (compare) POS-auto POS tag of the misrecognized word au- tomatically assigned on a transcript POS-guess POS tag of the misrecognized word guessed by a user PN PN all bigrams and trigrams of POS tags in a sentence Syntactic Dependency Dep-tag dependency tag of the misrecognized word automatically assigned on a tran- script Dep-pair dependency tags of all (parent, child) pairs in the sentence Parent-POS POS tag of the syntactic parent of the misrecognized word Semantic Sem-role semantic role of the misrecognized word Sem-presence all se	PN	proper noun$proper names$POS ngrams$proper name$Personal Name$preceding noun phrase$	2
Overall, the most discriminating features for both INFORM and CONVENTIONAL are mostly word ngrams, while those for REQUEST-ACTION and REQUEST-INFORMATION are mostly PN.	PN	proper noun$proper names$POS ngrams$proper name$Personal Name$preceding noun phrase$	2
semble tags PDS ART PDS PDS ART POS context (CRF) ADV:PROAV:VAFIN:APPR, PROAV:VAFIN:APPR:PDS, ... POS context (tree) PROAV:VAFIN:APPR, VAFIN:APPR:ART, ... word form with POS context (CRF) PROAV:VAFIN:APPR:der, VAFIN:APPR:der:APPR, ... word form with POS context (CRF:tree) PROAV:VAFIN:APPR:der, ..., der:APPR:ART:NN, ... extended features I: universal POS universal ensemble tags P D P P D universal PN P:D, P:P, P:P, ..., P:P:P:D, P:D:P:P:D universal POS context (CRF) ADV:P:VF:ADP, P:VF:ADP:P, ... word form with universal POS context (CRF) P:VF:ADP:der, VF:ADP:der:ADP, ADP:der:ADP:D, ... word form with universal POS context (CRF:tree) VF:VF:ADP:ADP:der, ADP:ADP:der:ADP:ADP, ... extended features II: brown clusters brown cluster for word form 110111011111 brown cluster with universal P	PN	proper noun$proper names$POS ngrams$proper name$Personal Name$preceding noun phrase$	2
BOWn=word ngrams; CHAR3=char trigrams; POSn=PN; DEP/DEPL=syntactic dependecies.	PN	proper noun$proper names$POS ngrams$proper name$Personal Name$preceding noun phrase$	2
0.727 +0.1% less PN 72.8% ?	PN	proper noun$proper names$POS ngrams$proper name$Personal Name$preceding noun phrase$	2
Utterance-length number of words in the sentence Part-of-speech (compare) POS-auto POS tag of the misrecognized word au- tomatically assigned on a transcript POS-guess POS tag of the misrecognized word guessed by a user PN PN all bigrams and trigrams of POS tags in a sentence Syntactic Dependency Dep-tag dependency tag of the misrecognized word automatically assigned on a tran- script Dep-pair dependency tags of all (parent, child) pairs in the sentence Parent-POS POS tag of the syntactic parent of the misrecognized word Semantic Sem-role semantic role of the misrecognized word Sem-presence all semantic role	PN	proper noun$proper names$POS ngrams$proper name$Personal Name$preceding noun phrase$	2
An extreme alternative is to have a first pass  where only referring expressions which look like  anaphors are marked up, such as pronouns, def-  inite NPs and reduced forms of PNs.	PN	proper noun$proper names$POS ngrams$proper name$Personal Name$preceding noun phrase$	3
tag as it is a part of a PN?	PN	proper noun$proper names$POS ngrams$proper name$Personal Name$preceding noun phrase$	3
We used two approaches for identifying story characters motivated by (Elson and McKeown, 2010): 1) named entity recognition was used for identifying PNs, e.g., ?	PN	proper noun$proper names$POS ngrams$proper name$Personal Name$preceding noun phrase$	3
2In case of PNs, there exist many dedicated algo- rithms and systems for finding them in texts, often developed within the Message Understanding Conference series.	PN	proper noun$proper names$POS ngrams$proper name$Personal Name$preceding noun phrase$	3
2) a set of part-of-speech patterns was used for the extraction of human and non-human characters that were not represented by PNs, e.g., ?	PN	proper noun$proper names$POS ngrams$proper name$Personal Name$preceding noun phrase$	3
2 A Flexemic Tagset for Polish The tagset presented in this section is based on the following design assumptions:   what is being tagged is a single orthographic word or, in some well-defined cases, a part thereof; multi-word constructions, even those sometimes considered to be morphological formations (so-called analytic forms) or dic- tionary entries (PNs), should be considered by a different level of process- ing;2 cf.	PN	proper noun$proper names$POS ngrams$proper name$Personal Name$preceding noun phrase$	3
c?2007 Association for Computational Linguistics Towards Robust Unsupervised PN Disambiguation  Ying Chen  Center for Spoken Language Research University of Colorado at Boulder  yc@colorado.edu  James Martin  Department of Computer Science  University of Colorado at Boulder  James.Martin@colorado.edu      Abstract  The increasing use of large open-domain  document sources is exacerbating the  problem of ambiguity in named entities.	PN	proper noun$proper names$POS ngrams$proper name$Personal Name$preceding noun phrase$	4
Unsuper- vised PN Disambiguation In Proceedings  of the seventh conference on Natural language learn- ing at HLT-NAACL, pages 33-40.	PN	proper noun$proper names$POS ngrams$proper name$Personal Name$preceding noun phrase$	4
PolyUHK:A Robust Information Extraction  System for Web PNs.	PN	proper noun$proper names$POS ngrams$proper name$Personal Name$preceding noun phrase$	4
                  Figure 8: MSR test error curve for TuneUp  272    DLUT: Chinese PN Disambiguation with Rich  Features  Dongliang Wang  Department of Computer Science  and Engineering, Dalian University  of Technology  wdl129@163.com  Degen Huang  Department of Computer Science  and Engineering, Dalian University  of Technology  huangdg@dlut.edu.cn    Abstract  In this paper we describe a person clus- tering system for a given document set  and report the results we	PN	proper noun$proper names$POS ngrams$proper name$Personal Name$preceding noun phrase$	4
A Study of PN Disam- biguation.	PN	proper noun$proper names$POS ngrams$proper name$Personal Name$preceding noun phrase$	4
Ex- tracting PNs from Email: Applying  Named Entity Recognition to Informal Text, Proc.	PN	proper noun$proper names$POS ngrams$proper name$Personal Name$preceding noun phrase$	4
Lexical Token n-gram Token n-grams in a 2 word window around the preposition POS n-gram POS n-grams in a 2 word window around the preposition HEAD PREC VP The head verb in the preceding verb phrase HEAD PREC NP The head noun in the PN HEAD FOLLOW NP The head noun in the following noun phrase Parsing HEAD Head of the preposition HEAD POS POS of the head COMP Complement of the preposition COMPLEMENT POS POS of the complement HEAD RELATION Prep-Head relation name COMPLEMENT RELATION Prep-Comp relation name Phrase Structure PARENT TAG TAG of the preposition?s parent GRANDPARENT TAG TAG of the preposition?s gra	PN	proper noun$proper names$POS ngrams$proper name$Personal Name$preceding noun phrase$	5
Nasukawa lso finds  (similarly to (\[Mitkov 93\])) that the frequency of  PNs with the same lemma as  the candidate noun phrase may be an indication  for preference.	PN	proper noun$proper names$POS ngrams$proper name$Personal Name$preceding noun phrase$	5
<- N18  5.7 Conjunction Construction  In Persian, there is a construction to modify the  PN with an adjective clause  which we have named the Conjunction construc- tion.	PN	proper noun$proper names$POS ngrams$proper name$Personal Name$preceding noun phrase$	5
Each noun phrase is  compared to all PNs.	PN	proper noun$proper names$POS ngrams$proper name$Personal Name$preceding noun phrase$	5
Earlier work \[11 \] on PP-attachment for verb phrases (whether  the PP attaches to the PN or to the verb  phrase) used statistics on co-occurences of two bigrams: the  main verb (V) and preposition (P) bigram and the main noun  in the object noun phrase (N1) and preposition bigram.	PN	proper noun$proper names$POS ngrams$proper name$Personal Name$preceding noun phrase$	5
For  each noun phrase NPj encountered, consider each  PN NPi.	PN	proper noun$proper names$POS ngrams$proper name$Personal Name$preceding noun phrase$	5
Mod- eling interestingness with DNNs.	DNN	deep neural network$Deep Neural Network$	0
In the future, we would like to investigate the advantages and disadvantages between tree- based models and other non-linear models such as DNNs or recurrent neural net- works.	DNN	deep neural network$Deep Neural Network$	0
The in- teraction function could be an inner product, a bi- linear operation, or a nonlinear function such as a DNN.	DNN	deep neural network$Deep Neural Network$	0
Some recent work on relation classification has focused on the use of DNNs with the aim of reducing the number of handcrafted fea- tures (Socher et al, 2012; Zeng et al, 2014; Yu et al.,	DNN	deep neural network$Deep Neural Network$	0
The DRRN uses separate DNNs to map state and action text strings into embedding vectors, from which ?	DNN	deep neural network$Deep Neural Network$	0
Context-dependent pre-trained DNNs for large-vocabulary speech recognition.	DNN	deep neural network$Deep Neural Network$	0
We consequently introduce one-class classification problem and develop a One-Class DNN.	DNN	deep neural network$Deep Neural Network$	1
2016 Association for Computational Linguistics Bi-Transferring DNNs for Domain Adaptation Guangyou Zhou 1 , Zhiwen Xie 1 , Jimmy Xiangji Huang 2 , and Tingting He 1 1 School of Computer, Central China Normal University, Wuhan 430079, China 2 School of Information Technology, York University, Toronto, Canada {gyzhou,xiezhiwen,tthe}@mail.ccnu.edu.cn jhuang@yorku.ca Abstract Sentiment classification aims to automati- cally predict sentiment polar	DNN	deep neural network$Deep Neural Network$	1
Relation Classification via  Convolutional DNN.	DNN	deep neural network$Deep Neural Network$	1
Joint Opinion Relation Detection Using One-Class DNN Liheng Xu, Kang Liu and Jun Zhao National Laboratory of Pattern Recognition Institute of Automation, Chinese Academy of Sciences, Beijing, 100190, China {lhxu, kliu, jzhao}@nlpr.ia.ac.cn Abstract Detecting opinion relation is a crucial step for fine-gained opinion summarization.	DNN	deep neural network$Deep Neural Network$	1
DNN Approach for the Dialog State Tracking Challenge.	DNN	deep neural network$Deep Neural Network$	1
ConjVs in consecutive sequence  of Complex Predicates (CPs) have  been identified.	ConjVs	conjunct verbs$junct verbs$	0
System  achieves F-Scores of 75.73%, and  77.92% for compound verbs and  89.90% and 89.66% for ConjVs  respectively on two types of Bengali  corpus.	ConjVs	conjunct verbs$junct verbs$	0
rty        Sivaji Bandyopadhyay  Department of Computer Science and Engineering  Jadavpur University  dipankar.dipnil2005@gmail.com,  santanupersonal1@gmail.com,  tapabratamondal@gmail.com, its_tanmoy@yahoo.co.in,  sivaji_cse_ju@yahho.com      Abstract  This paper presents the automatic ex- traction of Complex Predicates (CPs)  in Bengali with a special focus on  compound verbs (Verb + Verb) and  ConjVs (Noun /Adjective +  Verb).	ConjVs	conjunct verbs$junct verbs$	0
1 Introduction  Complex Predicates (CPs) contain [verb] +  verb (compound verbs) or [noun/  adjective/adverb] +verb (ConjVs)  combinations in South Asian languages (Hook,  1974).	ConjVs	conjunct verbs$junct verbs$	0
Compound verbs  follow the pattern of verb-verb (V-V) combination  while ConjVs are formed with either noun- verb (N-V) or adjective-verb (A-V) combinations.	ConjVs	conjunct verbs$junct verbs$	0
Jadavpur University  dipankar.dipnil2005@gmail.com,  santanupersonal1@gmail.com,  tapabratamondal@gmail.com, its_tanmoy@yahoo.co.in,  sivaji_cse_ju@yahho.com      Abstract  This paper presents the automatic ex- traction of Complex Predicates (CPs)  in Bengali with a special focus on  compound verbs (Verb + Verb) and  ConjVs (Noun /Adjective +  Verb).	ConjVs	conjunct verbs$junct verbs$	0
They are used to handle special constructions like ConjVs (ex:- prashna kiyaa (question did)), coordinating conjunc- tions and ellipses.	ConjVs	conjunct verbs$junct verbs$	0
Lexical scopes of compound and  ConjVs in consecutive sequence  of Complex Predicates (CPs) have  been identified.	ConjVs	conjunct verbs$junct verbs$	0
u@yahho.com      Abstract  This paper presents the automatic ex- traction of Complex Predicates (CPs)  in Bengali with a special focus on  compound verbs (Verb + Verb) and  ConjVs (Noun /Adjective +  Verb).	ConjVs	conjunct verbs$junct verbs$	0
The lexical patterns of com- pound and ConjVs are extracted  based on the information of shallow  morphology and available seed lists of  verbs.	ConjVs	conjunct verbs$junct verbs$	0
ahho.com      Abstract  This paper presents the automatic ex- traction of Complex Predicates (CPs)  in Bengali with a special focus on  compound verbs (Verb + Verb) and  conConjVs (Noun /Adjective +  Verb).	ConjVs	conjunct verbs$junct verbs$	1
System  achieves F-Scores of 75.73%, and  77.92% for compound verbs and  89.90% and 89.66% for conConjVs  respectively on two types of Bengali  corpus.	ConjVs	conjunct verbs$junct verbs$	1
onConjVs in consecutive sequence  of Complex Predicates (CPs) have  been identified.	ConjVs	conjunct verbs$junct verbs$	1
1 Introduction  Complex Predicates (CPs) contain [verb] +  verb (compound verbs) or [noun/  adjective/adverb] +verb (conConjVs)  combinations in South Asian languages (Hook,  1974).	ConjVs	conjunct verbs$junct verbs$	1
Compound verbs  follow the pattern of verb-verb (V-V) combination  while conConjVs are formed with either noun- verb (N-V) or adjective-verb (A-V) combinations.	ConjVs	conjunct verbs$junct verbs$	1
They are used to handle special constructions like conConjVs (ex:- prashna kiyaa (question did)), coordinating conjunc- tions and ellipses.	ConjVs	conjunct verbs$junct verbs$	1
Sivaji Bandyopadhyay  Department of Computer Science and Engineering  Jadavpur University  dipankar.dipnil2005@gmail.com,  santanupersonal1@gmail.com,  tapabratamondal@gmail.com, its_tanmoy@yahoo.co.in,  sivaji_cse_ju@yahho.com      Abstract  This paper presents the automatic ex- traction of Complex Predicates (CPs)  in Bengali with a special focus on  compound verbs (Verb + Verb) and  conConjVs (Noun /Adjective +  Verb).	ConjVs	conjunct verbs$junct verbs$	1
Lexical scopes of compound and  conConjVs in consecutive sequence  of Complex Predicates (CPs) have  been identified.	ConjVs	conjunct verbs$junct verbs$	1
avpur University  dipankar.dipnil2005@gmail.com,  santanupersonal1@gmail.com,  tapabratamondal@gmail.com, its_tanmoy@yahoo.co.in,  sivaji_cse_ju@yahho.com      Abstract  This paper presents the automatic ex- traction of Complex Predicates (CPs)  in Bengali with a special focus on  compound verbs (Verb + Verb) and  conConjVs (Noun /Adjective +  Verb).	ConjVs	conjunct verbs$junct verbs$	1
The lexical patterns of com- pound and conConjVs are extracted  based on the information of shallow  morphology and available seed lists of  verbs.	ConjVs	conjunct verbs$junct verbs$	1
Our motivation that within a search set, reviews tend to resemble one another rather than differ is reminiscent of intuitions underlying the use of PRF (PF) in IR (Ruthven and Lalmas, 2003, Section 3.5).	PRF	pseudo relevance feedback$Pseudo Relevance Feedback$	0
Explicit and PRF (RF) techniques (Ruthven and Lalmas, 2003; Baeza- Yates and Ribeiro-Neto, 1999; Manning et al, 2008) are more related to Bobo in the sense that they do not build long-term profiles.	PRF	pseudo relevance feedback$Pseudo Relevance Feedback$	0
4.2 Quality of Seeds As in PRF, quality of seeds plays an critical role in search performance.	PRF	pseudo relevance feedback$Pseudo Relevance Feedback$	0
This formula has been proposed in the setting of PRF, where expansion terms are chosen based on the top documents re- trieved for the original query.	PRF	pseudo relevance feedback$Pseudo Relevance Feedback$	0
Ex- pansion terms are extracted from these definition clusters using PRF: we first retrieve the definition clusters which are most re- lated to the user query, and then extract the most relevant terms from these definition clusters to ex- pand the query.	PRF	pseudo relevance feedback$Pseudo Relevance Feedback$	0
We apply the technique of PRF to obtain expansion terms from definition clusters.	PRF	pseudo relevance feedback$Pseudo Relevance Feedback$	0
I e -~ ~3~ \[o:m\]  Itnow it looks as if they they both  .32 ~ iml l l lh~ l l l~   131 AR,,,R~VIil IJ3 \[~\] I l lthink that we ,muslr l ' l  wor ry  too +,+much A,~m~UT TH'ISl  134 Ilwe we limake it ~+perfectly ~lear that :,papers must be in on the ~first of  : .MXYI  '~-*  135 l~:ml  L~6 ,\[mlltlh~\]I,  31   Coling 2010: Poster Volume, pages 54?62, Beijing, August 2010 Query Expansion based on PRF from Definition Clusters Delphine Bernhard LIMSI-CNRS Delphine.Bernhard@limsi.fr Abstract Query expansion consists in extending user queries with related terms in order to solve the lexical gap problem in Infor- mation Retrieval and Question Answer- ing.	PRF	pseudo relevance feedback$Pseudo Relevance Feedback$	1
Inspired by the successful use of PRF (Tao and Zhai, 2006) techniques in Information Retrieval and the cosine similarity measure (Salton, 1989) in Data Mining, we design a novel learning model which takes the multistream prediction feedback that is initially re- turned from seed samples 1 and uses a mean cosine similarity measure to calculate the distance between the new instance and prominent seed data point	PRF	pseudo relevance feedback$Pseudo Relevance Feedback$	1
Collins and Koo Discriminative Reranking for NLP   PRF Method Based on Taylor Expansion of Re- trieval Function in NTCIR-3 Patent Retrieval Task   Kazuaki KISHIDA  Faculty of Cultural Information Resources  Surugadai University  698 Azu, Hanno, Saitama 357-8555 JAPAN  kishida@surugadai.ac.jp      Abstract  Pseudo relevance feedback is empirically  known as a useful method for enhancing  retrieval performance.	PRF	pseudo relevance feedback$Pseudo Relevance Feedback$	1
For example, for  the PRF problem, Qin  et al (2008) defined Equation 2 as the similari- ties between any two documents in their CRF- based model.	PRF	pseudo relevance feedback$Pseudo Relevance Feedback$	1
We adapted the classic PRF  algorithm (Qiu, 1993), which has been so far applied  only to document retrieval tasks, to similarity  computation in a straightforward way and also tried  several variations of if (not described here due to  lack of space).	PRF	pseudo relevance feedback$Pseudo Relevance Feedback$	1
epartment of Computer Science, National Tsing-Hua University,   2Institute of Information Science, Academia Sinica,   3Department of Computer Science & Engineering, Yuan Ze University  hongjie@iis.sinica.edu.tw  s951416@mail.yzu.edu.tw   thtsai@saturn.yzu.edu.tw  hsu@iis.sinica.edu.tw  223 though one of the documents is not as relevant  to the given query as the other; this problem is  similar to PRF (Kwok,  1984).	PRF	pseudo relevance feedback$Pseudo Relevance Feedback$	1
with a CM. ?	CM	case marker$cM$	0
wa, CM) and ??? (	CM	case marker$cM$	0
Furthermore, Arabic distinguishes between definite and indefinite CMs.	CM	case marker$cM$	0
For the gold input, apart from LEMMA that pro- vides around 0.7 points, the most useful feature is 4NORK2, NOR1 and NMG are auxiliaries CMs.	CM	case marker$cM$	0
5.3 Case markers Turkish, being a fairly scrambling language, uses CMs to denote the syntactic functions of nouns and noun groups.	CM	case marker$cM$	0
Chinese language does not have inflection, con- jugation, or CMs (Li and Thompson, 1989).	CM	case marker$cM$	0
(< synt~_~c-t~escriptioh> <semandCMgscfi'ption> ) ) ) )  A morphological descriodon contaM~ both contextual and  context-free inf6i'mafion.	CM	case marker$cM$	1
CM,wtM ) (1) We applied Bayes?	CM	case marker$cM$	1
lhe stru~ure ofthese atoms is presented belo~  (<root> (<lemma>  (< para~CMesc0"pfi'on-sclector >  < fiaorph~logic .-~:xi. "	CM	case marker$cM$	1
The atoms generated in  this situation have the foll6wing structure:  (UNKNOWN < unknown-word>  ( < ixm~'ble-root > < morphologiCMe.scri'pdon >'~*)  The unknown word is associated with aql legal segmentations  and for each of them the morphological inf6i'rnation deduced  from the identified en "dings i  prOcidex\[  Lexical synthesis i the reverse of lexical analysk The process  interface nsures conversion of a morpho-lexical tom sefluence  into a word _Sg:luence.	CM	case marker$cM$	1
The development set with no  morphologiCM processing was used for these tests.	CM	case marker$cM$	1
Note also that these lexi-  cal rules can be interpreted statically as well as dy-  namiCMly.	CM	case marker$cM$	1
We use the MIRA and Support Vector Machines for experiments on Arabic, Dari, English, and Pashto, and provide a detailed analysis of our results.	MIRA	Margin-Infused Relaxed Algorithm$Margin Infused Relaxed Algorithm$McDonald and Pereira$Margin infused relaxed algorithm$	0
MIRA for Moses.	MIRA	Margin-Infused Relaxed Algorithm$Margin Infused Relaxed Algorithm$McDonald and Pereira$Margin infused relaxed algorithm$	1
In order to incorporate second-order features (specifically, sibling features), McDonald et al pro- posed a dependency parser based on the Eisner algo- rithm (MIRA, 2006).	MIRA	Margin-Infused Relaxed Algorithm$Margin Infused Relaxed Algorithm$McDonald and Pereira$Margin infused relaxed algorithm$	2
Unlike the training procedure employed by McDonald et al (2005b) and MIRA (2006), we provide positive and negative examples in the training data.	MIRA	Margin-Infused Relaxed Algorithm$Margin Infused Relaxed Algorithm$McDonald and Pereira$Margin infused relaxed algorithm$	2
2005a) 90.9 MIRA (2006) 91.5 Huang and Sagae (2010) 92.1 Koo and Collins (2010) 93.04 Zhang and Nivre (2011) 92.9 Martins et al. (	MIRA	Margin-Infused Relaxed Algorithm$Margin Infused Relaxed Algorithm$McDonald and Pereira$Margin infused relaxed algorithm$	2
We adopt the second-order graph- based model of MIRA (2006), which casts the problem as finding an optimal tree from a fully-connect directed graph and factors the score of a dependency tree into scores of pairs of sibling dependencies.	MIRA	Margin-Infused Relaxed Algorithm$Margin Infused Relaxed Algorithm$McDonald and Pereira$Margin infused relaxed algorithm$	2
We ex- perimented with popular feature sets previously used for named entity (McCallum and Li, 2003) and gene (MIRA, 2005) recognition including orthographic, part-of-speech (POS), shallow parsing and gazetteers.	MIRA	Margin-Infused Relaxed Algorithm$Margin Infused Relaxed Algorithm$McDonald and Pereira$Margin infused relaxed algorithm$	2
However, online learning for NLP typically involves expensive inference on each example for 10 or more passes over millions of examples, which of- ten makes training too slow in practice; for example systems such as the popular (2nd-order) MST parser (MIRA, 2006) usually require the order of days to train on the Treebank on a com- modity machine (McDonald et al 2010).	MIRA	Margin-Infused Relaxed Algorithm$Margin Infused Relaxed Algorithm$McDonald and Pereira$Margin infused relaxed algorithm$	2
MIRA for moses.	MIRA	Margin-Infused Relaxed Algorithm$Margin Infused Relaxed Algorithm$McDonald and Pereira$Margin infused relaxed algorithm$	3
verb part- of-speech; and INC aspect.	INC	incompletive$incorrect$	0
Since Identifinder occasionally tags an  entity INCly, we manually went through each  selection to filter out entities that were not people?s  names.	INC	incompletive$incorrect$	1
Two patterns are grammatically INC due to gender dis- agreement, namely wydawa?a sie?	INC	incompletive$incorrect$	1
Fur- thermore, from a half to two thirds of the INC responses were specifically about the subject the query wanted to exclude, displaying little or no understanding of excision alternative phrases.	INC	incompletive$incorrect$	1
Consider again the  three high-salience words to which our unsuper-  vised learning program assigned INC gen-  der: "husband", "wife", and "years."	INC	incompletive$incorrect$	1
in the gold standard, 1285 are attached correctly by the parser, while 607 receive an INC regent.	INC	incompletive$incorrect$	1
981 correct parsed unparsed average INC ambiguity Existing 50% 8% 42% 10.62 vocab w/added 76% 8% 14% 12.56 vocab Table 1: Grammar performance on held-out data evaluation was carried out only on the remaining 72 sentences.	INC	incompletive$incorrect$	1
On the other hand, they could prove to be invaluable when training a NER tag- ger for UGC, which is known to be noisy and fragmented.	UGC	User Generated Content$User-generated content$	0
The French Social Media Bank: a Treebank of Noisy UGC.	UGC	User Generated Content$User-generated content$	0
impressme : The Language of Motivation in UGC.	UGC	User Generated Content$User-generated content$	0
A Hierarchical Entity-based Approach to Structuralize UGC in Social Media: A Case of Yahoo!	UGC	User Generated Content$User-generated content$	0
Mining Refinements to Online Instructions from UGC Gregory Druck Yahoo!	UGC	User Generated Content$User-generated content$	0
UGC has been investigated in the context of machine translation in recent work dealing specifically with spelling correc- tion (Bertoldi et al.,	UGC	User Generated Content$User-generated content$	1
UGC on the social media rep- resents the views of the users and hence, may be opinion-bearing.	UGC	User Generated Content$User-generated content$	1
1 Introduction UGC ?	UGC	User Generated Content$User-generated content$	1
2 Related work 2.1 Text normalization UGC on the web is notoriously low-quality, containing slang, abbreviations, inconsis- tent grammar and spelling.	UGC	User Generated Content$User-generated content$	1
2014 Association for Computational Linguistics Mining Lexical Variants from Microblogs: An Unsupervised Multilingual Approach Alejandro Mosquera University of Alicante San Vicente del Raspeig s/n - 03690 Alicante, Spain amosquera@dlsi.ua.es Paloma Moreda University of Alicante San Vicente del Raspeig s/n - 03690 Alicante, Spain moreda@dlsi.ua.es Abstract UGC has become a re- current resource for NLP tools and ap- plications, hence many efforts have been made lately in order to handle the noise present in short social media texts.	UGC	User Generated Content$User-generated content$	1
Preliminary experiments with the HAC algorithm showed that the number of clusters used did not have a big impact on translation quality, 1 so we will only present results that use the same num- ber of clusters as in the manual partitions (10 for Arabic and 17 for Chinese).	HAC	hierarchical agglomerative clustering$Hierarchical Agglomerative Clustering$	0
Compared with other possible clustering approaches, such as HAC (Kartsaklis et al.,	HAC	hierarchical agglomerative clustering$Hierarchical Agglomerative Clustering$	0
Bottom-up clustering is also  called HAC,  which is more popular than top-down clustering.	HAC	hierarchical agglomerative clustering$Hierarchical Agglomerative Clustering$	0
In this paper the following clustering methods  are used: a genetic algorithm with integers, vec- tor quantization networks trained by a genetic  algorithm, HAC  with different metrics.	HAC	hierarchical agglomerative clustering$Hierarchical Agglomerative Clustering$	0
A den- dogram of the similarity between the classifiers is shown in Figure 2, derived using maximum linkage HAC.	HAC	hierarchical agglomerative clustering$Hierarchical Agglomerative Clustering$	0
attack bang hit knock walkout find attack - 4 18 7 0 0 bang - 38 43 2 0 0 hit - 44 2 29 knock - 2 0 walkout - 0 find - We used the CLUTO clustering toolkit (Karypis, 2002) to induce a HAC on the vectors for Ws.	HAC	hierarchical agglomerative clustering$Hierarchical Agglomerative Clustering$	0
And we try to  use HAC al- gorithm to help raise the recall score.	HAC	hierarchical agglomerative clustering$Hierarchical Agglomerative Clustering$	1
HAC can be applied to both feature vectors and collocation graphs.	HAC	hierarchical agglomerative clustering$Hierarchical Agglomerative Clustering$	1
2 Combining Clustering and Machine Learning We use HAC (see e.g. Everitt (1993)) as our clustering method.	HAC	hierarchical agglomerative clustering$Hierarchical Agglomerative Clustering$	1
We follow (Jur- gens and Stevens, 2010a) and cluster the final cen- troids with HAC, with the average link criteria as suggested by (Ped- ersen and Bruce, 1997).	HAC	hierarchical agglomerative clustering$Hierarchical Agglomerative Clustering$	1
Yaari Y. (1997) Segmentation of Expository Text by  HAC.	HAC	hierarchical agglomerative clustering$Hierarchical Agglomerative Clustering$	1
4.3 HAC  In this work we consider hierarchical agglomera- tive binary clustering where we set each utter- ance to one subclass and then we consequently  group classes into pairs until there is only one  ).	HAC	hierarchical agglomerative clustering$Hierarchical Agglomerative Clustering$	1
Unsupervised learning of SO from a hundred- billion-word corpus (technical report erc-1094).	SO	semantic orientation$Sequence Ontology$source order$	0
Various approaches have been adopted in subjec- tivity detection, SO detection,  review classification and review mining.	SO	semantic orientation$Sequence Ontology$source order$	0
They use the two terms excellent and poor as seed  terms to determine the SO of  other terms.	SO	semantic orientation$Sequence Ontology$source order$	0
SO applied to unsupervised classifi- cation of reviews.	SO	semantic orientation$Sequence Ontology$source order$	0
Deter- mining the SO of terms through  gloss classification.	SO	semantic orientation$Sequence Ontology$source order$	0
Latent variable models for SOs of phrases.	SO	semantic orientation$Sequence Ontology$source order$	0
Acknowledgments We gratefully acknowledge Mike Bada?s help in loading the SO into Prote?ge?.	SO	semantic orientation$Sequence Ontology$source order$	1
For example, if a string in text refers to something in the domain of the Gene Ontology, we take it as belonging to a Gene Ontology seman- tic class (using the name of the ontology only for convenience); if a string in text refers to something belonging to the domain of the SO, we take it as belonging to a SO se- mantic class.	SO	semantic orientation$Sequence Ontology$source order$	1
We loaded the ConceptMapper with dictionar- ies derived from several ontologies, including the Gene Ontology Cellular Component branch, Cell Type Ontology, BRENDA Tissue Ontology, and the SO.	SO	semantic orientation$Sequence Ontology$source order$	1
SO ?	SO	semantic orientation$Sequence Ontology$source order$	1
The best decomposition order varies from language to language: right-to-left in SO is best for Chinese-English, right-to-left in target order is best for German-English and left-to-right or right- to-left in target order are best in English-Bulgarian.	SO	semantic orientation$Sequence Ontology$source order$	2
get side, for X , the order of the two children follows the SO, while for Y , the or- der follows the inverse.	SO	semantic orientation$Sequence Ontology$source order$	2
We compare all four decomposition orders (SO left-to-right and right-to-left, and target order left-to-right and right- to-left).	SO	semantic orientation$Sequence Ontology$source order$	2
Likewise SO based mod- els provide distributions over source sentences and unordered target sides of MTUs.	SO	semantic orientation$Sequence Ontology$source order$	2
The first clause (monotone, reverse) in- dicates whether the target language translation order follows the SO; the second (adjacent, gap) indicates whether the source phrases are adjacent or separated by an intervening phrase on the target side.	SO	semantic orientation$Sequence Ontology$source order$	2
We get around this problem by splitting the  knowledge SOing into two or more orderings that are  partial.	SO	semantic orientation$Sequence Ontology$source order$	2
2 Polarization across Dimensions IP models describe probabilistic rela- tionships between observed responses (votes) on a set of items (bills) by a set of responders (legis- lators) who are characterized by continuous latent traits (Fox, 2010).	IP	Ideal point$intellectual property$independent phrase$I Pi$	0
IP models are often created using Bayesian techniques over large amounts of roll-call data (Clin- ton et al, 2004; Jackman, 2001).	IP	Ideal point$intellectual property$independent phrase$I Pi$	0
IP models assume all legisla- tors and bills can be plotted as single points in one- dimensional ?	IP	Ideal point$intellectual property$independent phrase$I Pi$	0
Using music copyright as the paral- lel, consider the situation where an AAC user, Alice say, imports text from a novel under copyright (me- chanical rights) and adapts it by adding other text and other copyright material in order to create her own monologue (IP rights).	IP	Ideal point$intellectual property$independent phrase$I Pi$	1
The other topic, named C2, talks about IP and contains 10 pairs of paper drafts.	IP	Ideal point$intellectual property$independent phrase$I Pi$	1
Intellectual Property Rights  Since lexicons, unlike text and speech databases, are  likely to be incorporated (perhaps in derived form) in  commercial HLT products, IP rights  come to center stage.	IP	Ideal point$intellectual property$independent phrase$I Pi$	1
Moreover, the increased creation of novel utterances and wider opportunity to relay such utterances potentially in- crease IP issues.	IP	Ideal point$intellectual property$independent phrase$I Pi$	1
150 (a) first draft (b) final draft (c) Revision detection using Hashemi?s approach Figure 1: Fragments of a paper in corpus C2 discussing IP, (c) is Hashemi?s work, green for recognized modifications, blue for insertions and red for deletion For sentence alignment, each sentence in the fi- nal draft is assigned the index of its aligned sen- tence in the original draft.	IP	Ideal point$intellectual property$independent phrase$I Pi$	1
We know that resources themselves are not openly available, but at least the metadata description should inform the community about their existence, about IP rights and modes of usage.	IP	Ideal point$intellectual property$independent phrase$I Pi$	1
3.4 Independent Phrase Chunking There is no special rule for IP chunking.	IP	Ideal point$intellectual property$independent phrase$I Pi$	2
It can be done only through knowledge base that stores the cases where IPs take place.	IP	Ideal point$intellectual property$independent phrase$I Pi$	2
This result can be attributed to the fact that there are too small number of exceptions for adverb phrases and IPs.	IP	Ideal point$intellectual property$independent phrase$I Pi$	2
However, phrase-based models cannot effectively conduct long-distance reordering because they are based purely on statis- tics of syntax-IPs.	IP	Ideal point$intellectual property$independent phrase$I Pi$	2
This ap- proach is a bit like our change regions combined with Moses?s region-IP pairs.	IP	Ideal point$intellectual property$independent phrase$I Pi$	2
These results were obtained from a  corpus of spoken utterances many of which con-  tained several IPs and sentences.	IP	Ideal point$intellectual property$independent phrase$I Pi$	2
Since the word w has exactly k different analyses:  k E~ 1P(Ai w) Ei=IP = = \[ = 1.	IP	Ideal point$intellectual property$independent phrase$I Pi$	3
3.1 Context of current part-of-speech and  current word  Here, we assume:  e(t i I G~) = I P(ti I p~wi)  \[ P(tl IP)  where  piwi ~ dp  PiWi ~ dp  ~={piwi,piwi3C}+{pi,pi3C } and piwi is a  part-of-speech and word pair existing in the  training data C.   In this case, the current part-of-speech and  word pair is also used as a lexical entry to  determine the current structural chunk tag and  we have a total of about 49563 lexical  entries(\[ ?	IP	Ideal point$intellectual property$independent phrase$I Pi$	3
dIPttsburgh); the variable ?	IP	Ideal point$intellectual property$independent phrase$I Pi$	3
We estimated the probabilities  P(c IP) and P(c) similarly to Resnik (1993) by us-  ing relative frequencies from the BNC, together with  WordNet (Miller et al, 1990) as a source of taxo-  nomic semantic lass information.	IP	Ideal point$intellectual property$independent phrase$I Pi$	3
e(c IP)" log P(c IP_______~)  rli P(c)  (4) rli=~-~P(clpi).logP(Cplc;i)  C  In the case of adjective-noun combinations, the se-  lectional association measures the semantic fit of an  adjective and each of the semantic lasses of the  nouns it co-occurs with.	IP	Ideal point$intellectual property$independent phrase$I Pi$	3
We estimated the probabilities  P(c IP) and P(c) similarly to Resnik (1993) by us-  ing relative frequencies from the BNC, together with  W	IP	Ideal point$intellectual property$independent phrase$I Pi$	3
We estimated the probabilities  P(c IP) and P(c) similarly to Resnik (1993) by us-  ing relative frequencies from the BNC, together with  WordNet (Miller	IP	Ideal point$intellectual property$independent phrase$I Pi$	3
TAL et langues anciennes, 50(2):47?71.	TAL	Traitement automatique des langues$Tree Adjoining Lan-  gages$Traitement Automatique des Langue$	0
TAL et langues an- ciennes, 50(2):201?235, October.	TAL	Traitement automatique des langues$Tree Adjoining Lan-  gages$Traitement Automatique des Langue$	0
Essai de definition", Actes du colloque  "TAL naturelles  et syst~mes documentaires ", Clermont-Ferrand  \[Milner 1989\] Milner Jean-Claude (1989),  "Introduction ~t une science du langage", Scull,  Paris  \[Monteil 1990\] Monteil Marie Gaelle,  P~not Nadine (1990), "Indexation  Automatique, fonctionnement - Principes  gtntraux", Note interne HN46464, EDF,  Direction des Etudes et Recherches, Service  IPN, C	TAL	Traitement automatique des langues$Tree Adjoining Lan-  gages$Traitement Automatique des Langue$	0
TAL, 36(1- 2):23?35.	TAL	Traitement automatique des langues$Tree Adjoining Lan-  gages$Traitement Automatique des Langue$	0
TAL, 44(2):81?105.	TAL	Traitement automatique des langues$Tree Adjoining Lan-  gages$Traitement Automatique des Langue$	0
TAL 44(2):81-105.	TAL	Traitement automatique des langues$Tree Adjoining Lan-  gages$Traitement Automatique des Langue$	0
Approche Probabiliste de l'Analyse  Syntaxique", TALs, vol.	TAL	Traitement automatique des langues$Tree Adjoining Lan-  gages$Traitement Automatique des Langue$	2
Actes de la 12?me Conf?rence annuelle  sur le TALs Natu- relles, 451-456.	TAL	Traitement automatique des langues$Tree Adjoining Lan-  gages$Traitement Automatique des Langue$	2
In TALs.	TAL	Traitement automatique des langues$Tree Adjoining Lan-  gages$Traitement Automatique des Langue$	2
le 5 f?vrier 2012 sur les principales listes de diffusion dans les domaines des sciences de l?information (ASIS-L), de la fouille de textes (TextAnalytics, KDnuggets), des humanit?s num?riques (DH, Humanist), du TALs et de la linguistique de corpus (Corpora, LN, etc.).	TAL	Traitement automatique des langues$Tree Adjoining Lan-  gages$Traitement Automatique des Langue$	2
Diego Molla, Rolf Schwitter, Michael Hess, Rachel Fournier, 2000, Extrans, an Answer Extraction System, TALs, Hermes Science Publication, 41-2, 495-522.	TAL	Traitement automatique des langues$Tree Adjoining Lan-  gages$Traitement Automatique des Langue$	2
TALs, 46(1):115?140.	TAL	Traitement automatique des langues$Tree Adjoining Lan-  gages$Traitement Automatique des Langue$	2
Fast Training of SV  Machines Using Sequential Minimal Optimization.	SV	Support Vector$Swedish$shared variables$	0
Learning to Classify Text  Using SV Machines: Methods, Theory,  and Algorithms.	SV	Support Vector$Swedish$shared variables$	0
931  SV Machines for Paraphrase Identification   and Corpus Construction  Chris Brockett and William B. Dolan  Natural Language Processing Group  Microsoft Research  One Microsoft Way, Redmond, WA 98502, U.S.A.  {chrisbkt, billdol}@microsoft.com  Abstract  The lack of readily-available large cor- pora of aligned monolingual sentence  pairs is a major obstacle to the devel- opment of Statist	SV	Support Vector$Swedish$shared variables$	0
Learning with Kernels: SV Machines,  Regularization, Optimization, and Beyond.	SV	Support Vector$Swedish$shared variables$	0
Advances in  Kernel Methods: SV Learning.	SV	Support Vector$Swedish$shared variables$	0
In this  paper, we describe the use of annotated  datasets and SV Machines  to induce larger monolingual para- phrase corpora from a comparable cor- pus of news clusters found on the  World Wide Web.	SV	Support Vector$Swedish$shared variables$	0
5.9 SV As happened with Polish, the results for SV (Nivre et al 2006b) are not as good as we could ex- pect; however we believe that the information shown in this paper is useful because MaltOptimizer detects which features are able to outperform the best model found so far and the model trained with MaltParser in default settings by a bit less than 2 points in the predicted scenario and mor	SV	Support Vector$Swedish$shared variables$	1
5.9 SV As happened with Polish, the results for SV (Nivre et al 2006b) are not as good as we could ex- pect; however we believe that the information shown in this paper is useful because MaltOptimizer detects which features are able to outperform the best model found so far and the model trained with MaltParser in default settings by a bit less than 2 points in the predicted scenario and more than 2 points in the gold scenario.	SV	Support Vector$Swedish$shared variables$	1
5.8 Polish Polish (S?widzin?ski and Wolin?ski, 2010) is one of the two languages (with SV) in which our model performs with the worst results.	SV	Support Vector$Swedish$shared variables$	1
85.19 86.30 78.16 79.86 84.93 85.71 German 79.90 81.09 84.85 87.70 7.80 87.32 90.40 76.64 79.98 83.59 86.96 Hebrew 76.78 76.80 79.37 80.17 3.39 79.83 79.83 76.61 76.61 80.03 80.03 Hungarian 70.37 71.11 71.98 81.91 11.54 80.69 80.74 71.27 72.34 82.37 83.14 Korean 87.22 87.22 87.22 88.94 1.72 86.52 90.20 81.69 88.43 83.74 89.39 Polish 75.52 75.58 79.28 80.27 4.75 81.58 81.91 76.64 77.70 79.79 80.49 SV 76.75 76.75 78.91 79.76 3.01 74.85 74.85 75.73 75.73 77.67 77.67 Table 1: Labeled attachment score per phase compared to default settings for all training sets from the Shared Task on PMRLs in the gold scenario on the held-out test set for optimization.	SV	Support Vector$Swedish$shared variables$	1
77.65 79.33 76.54 77.98 77.56 79.00 German 78.69 79.87 82.58 83.97 5.28 83.39 86.63 74.81 77.81 79.22 82.75 Hebrew 76.29 76.31 79.01 79.67 3.38 73.40 73.40 69.97 69.97 73.01 73.01 Hungarian 68.26 69.12 69.96 78.71 10.45 76.82 77.62 69.08 70.15 79.00 79.63 Korean 80.08 80.08 80.08 81.63 1.55 77.96 83.02 74.87 82.06 75.90 82.65 Polish 74.43 74.49 76.93 78.41 3.98 80.61 80.83 75.29 75.63 79.50 80.49 SV 74.53 74.53 76.51 77.66 3.13 72.90 72.90 73.21 73.21 75.82 75.82 Table 2: Labeled attachment score per phase compared to default settings for all training sets from the Shared Task on PMRLs in the predicted scenario on the held-out test set for optimization.	SV	Support Vector$Swedish$shared variables$	1
For the Advanced search option we fully acknowledge to emulate the elegant interface to CQL- query building as provided by the SV Spr?akbanken 10 .	SV	Support Vector$Swedish$shared variables$	1
By means of SV, the partial  SemSpec is linked to the denotation.	SV	Support Vector$Swedish$shared variables$	2
At the grammatical level (i.e. leaving aside prag-  matic onsiderations),the translation of an InL' formu-  la to a scoped logical formula can be determined by the  specific scoping operator involved (indicated in the  sub-formula) nd by its relation to its semantic argu-  ment (indicated by SV).	SV	Support Vector$Swedish$shared variables$	2
All the necessary  connections between phrases are made at the com-  position level when lexical entries are instantiated,  through the SV of the sigma projec-  tions.	SV	Support Vector$Swedish$shared variables$	2
matching the remaining characters in the  word against he surface part of the spelling  pattern, thereby, through SV,  instantiating the characters for the lexical  part to provide a possible root spelling;  ?	SV	Support Vector$Swedish$shared variables$	2
As a consequence of this mathematical formulation, the  metarules are expressed as couples of  monoadic  predicates with SV, For example, the  metanfle of coordination (3) is described by formula (7).	SV	Support Vector$Swedish$shared variables$	2
This is because : (i) in InL', the scope of seeping  operators i left undefined ; (ii) SV ex-  press the relation between determiner and restrictor,  and between seeping operators and their semantic  arguments ; (iii) the grammar places constants (i.e.  proper names) in the specified place of the argumental  list of the predicate.	SV	Support Vector$Swedish$shared variables$	2
chalte  is the FV with inflection (-?	FV	Full Verb$filling status$	0
4.2 FV Information  Three more disambiguation strategies condi-  tion the choice of tense on the full verb in  a CVP, viz.	FV	Full Verb$filling status$	0
uthe is a FV  with inflection -e ?	FV	Full Verb$filling status$	0
Figure 1: Example of an agenda graph for building  guidance domain  Feature Types Features #Size  Word-level   features  unigram 175  bigram 573  trigram 1034  Utterance-level   features  dialog act (DA) 9  main goal (MG) 16  slot FV 8  system act (SA) 26  Discourse-level   features  previous DA 10  previous MG 17  previous SA 27  Table 1: List of feature sets  90 For a set of N dialog examples X={xi|i=1,..,N}, the  binary feature vectors are represented by using a set of  features from the dialog corpus.	FV	Full Verb$filling status$	1
A  user's query may be expressed in a controlled lan-  guage (e.g., a boolean expression of keywords) or,  more desirably, aNL, such as English.	NL	natural anguage$Dutch$Natural Language$nonlem$nonliteral$natura l  language$Northern Light$natural language$	0
The environ-  ment can facilitate xploration of macro debug-  ging techniques and has relevance for studies  of translation from readable diagram programs  to NL instructions.	NL	natural anguage$Dutch$Natural Language$nonlem$nonliteral$natura l  language$Northern Light$natural language$	0
Recent develop-  ments in NL text retrieval.	NL	natural anguage$Dutch$Natural Language$nonlem$nonliteral$natura l  language$Northern Light$natural language$	0
For example, the  lexical atoms extracted by this process from the  CACM corpus (about 1 MB) include "operating  system", "data structure", "decision table", "data  base", "real time", "NL", "on line",  "least squares", "numerical integration", and "fi-  nite state automaton", among others.	NL	natural anguage$Dutch$Natural Language$nonlem$nonliteral$natura l  language$Northern Light$natural language$	0
Massively parallel parsing: A strongly inter-  active model of NL interpretation.	NL	natural anguage$Dutch$Natural Language$nonlem$nonliteral$natura l  language$Northern Light$natural language$	0
This environment is based on a  set of NL processing compo-  nents, at the morphologic, syntactic and  semantic levels.	NL	natural anguage$Dutch$Natural Language$nonlem$nonliteral$natura l  language$Northern Light$natural language$	0
1 Introduction Concerted efforts over the past years 1 in the NL language area in Europe, the Netherlands and the northern half of Belgium, Flanders, have yielded a corpus of over 500 million words of richly linguis- tically annotated contemporary written NL, called SoNaR (Oostdijk et al.,	NL	natural anguage$Dutch$Natural Language$nonlem$nonliteral$natura l  language$Northern Light$natural language$	1
Licence details: http://creativecommons.org/licenses/by/4.0/ 1 Funded in large part by the NL Language Union in the STEVIN programme described in the Open Access book ?	NL	natural anguage$Dutch$Natural Language$nonlem$nonliteral$natura l  language$Northern Light$natural language$	1
Essential Speech and Language Technology for NL?	NL	natural anguage$Dutch$Natural Language$nonlem$nonliteral$natura l  language$Northern Light$natural language$	1
2 SoNaR The SoNaR project developed a large scale reference corpus for contemporary, written NL.	NL	natural anguage$Dutch$Natural Language$nonlem$nonliteral$natura l  language$Northern Light$natural language$	1
OpenSoNaR: user-driven development of the SoNaR corpus interfaces Martin Reynaert TiCC / Tilburg University CLST / Radboud Universiteit Nijmegen reynaert@uvt.nl Matje van de Camp De Taalmonsters matje@taalmonsters.nl Menno van Zaanen TiCC / Tilburg University mvzaanen@uvt.nl Abstract OpenSoNaR is an online system that allows for analyzing and searching the large scale NL reference corpus SoNaR. Due to the size of the corpus, accessing the information contained in the dataset has proven to be difficult for less technically inclined researchers.	NL	natural anguage$Dutch$Natural Language$nonlem$nonliteral$natura l  language$Northern Light$natural language$	1
This balanced corpus consists of about 540 million tokens of NL across a wide range of text types, such as books, magazine articles, reports, subtitles, but also data from the ?	NL	natural anguage$Dutch$Natural Language$nonlem$nonliteral$natura l  language$Northern Light$natural language$	1
In J. Vicedo, P. Martnez- Barco, R. Muoz, and M. Saiz Noeda, editors, Ad- vances in NL Processing, volume 3230 of Lecture Notes in Computer Science, pages 82?90.	NL	natural anguage$Dutch$Natural Language$nonlem$nonliteral$natura l  language$Northern Light$natural language$	2
NL Engineer- ing, 6 (1):15 ?	NL	natural anguage$Dutch$Natural Language$nonlem$nonliteral$natura l  language$Northern Light$natural language$	2
He  has worked on Machine Learning in the context of NL Processing and  has published papers in several conferences.	NL	natural anguage$Dutch$Natural Language$nonlem$nonliteral$natura l  language$Northern Light$natural language$	2
Managing fieldwork data with Toolbox and the NL Toolkit.	NL	natural anguage$Dutch$Natural Language$nonlem$nonliteral$natura l  language$Northern Light$natural language$	2
Proceedings of the Fourteenth Conference on Computational NL Learning: Shared Task, pages 138?143, Uppsala, Sweden, 15-16 July 2010.	NL	natural anguage$Dutch$Natural Language$nonlem$nonliteral$natura l  language$Northern Light$natural language$	2
Head-Driven Statistical Models for NL Parsing.	NL	natural anguage$Dutch$Natural Language$nonlem$nonliteral$natura l  language$Northern Light$natural language$	2
The lemmatized 1http://domino.watson.ibm.com/library/CyberDig.nsf (key- word=RC22176) 2http://www.cs.cmu.edu/?alavie/METEOR 182 Table 1: Translation results as increasing amount of training data in IWSLT06 CSTAR track System AER BLEU METEOR 50K NL 0.217 0.158 0.427 lemma 0.199 0.167 0.431 100K NL 0.178 0.182 0.457 lemma 0.177 0.188 0.463 300K NL 0.150 0.223 0.501 lemma 0.132 0.217 0.505 400K NL 0.136 0.231 0.509 lemma 0.102 0.224 0.507 500K NL 0.119 0.235 0.519 lemma 0.104 0.241 0.522 600K NL 0.095 0.238 0.535 lemma 0.069 0.248 0.536 Table 2: Statistical significance test in terms of BLEU: sys1=non-lemma, sys2=lem	NL	natural anguage$Dutch$Natural Language$nonlem$nonliteral$natura l  language$Northern Light$natural language$	3
matized 1http://domino.watson.ibm.com/library/CyberDig.nsf (key- word=RC22176) 2http://www.cs.cmu.edu/?alavie/METEOR 182 Table 1: Translation results as increasing amount of training data in IWSLT06 CSTAR track System AER BLEU METEOR 50K NL 0.217 0.158 0.427 lemma 0.199 0.167 0.431 100K NL 0.178 0.182 0.457 lemma 0.177 0.188 0.463 300K NL 0.150 0.223 0.501 lemma 0.132 0.217 0.505 400K NL 0.136 0.231 0.509 lemma 0.102 0.224 0.507 500K NL 0.119 0.235 0.519 lemma 0.104 0.241 0.522 600K NL 0.095 0.238 0.535 lemma 0.069 0.248 0.536 Table 2: Statistical significance test in terms of BLEU: sys1=non-lemma, sys2=lemma Data size Diff(sys1-sys2) 50K -0.092 [-0.0176,-0.0012] 100K -0.006 [-0.0155,0.0039] 300K 0.0057 [-0.0046,0.0161] 400K 0.0074 [-0.0023,0.0174] 500K -0.0054 [-0.	NL	natural anguage$Dutch$Natural Language$nonlem$nonliteral$natura l  language$Northern Light$natural language$	3
The lemmatized 1http://domino.watson.ibm.com/library/CyberDig.nsf (key- word=RC22176) 2http://www.cs.cmu.edu/?alavie/METEOR 182 Table 1: Translation results as increasing amount of training data in IWSLT06 CSTAR track System AER BLEU METEOR 50K NL 0.217 0.158 0.427 lemma 0.199 0.167 0.431 100K NL 0.178 0.182 0.457 lemma 0.177 0.188 0.463 300K NL 0.150 0.223 0.501 lemma 0.132 0.217 0.505 400K NL 0.136 0.231 0.509 lemma 0.102 0.224 0.507 500K NL 0.119 0.235 0.519 lemma 0.104 0.241 0.522 600K NL 0.095 0.238 0.535 lemma 0.069 0.248 0.536 Table 2: Statistical significance test in terms of BLEU: sys1=non-lemma, sys2=lemma Data size Diff(sys1-sys2) 50K -0.092 [-0.0176,-0.00	NL	natural anguage$Dutch$Natural Language$nonlem$nonliteral$natura l  language$Northern Light$natural language$	3
e/METEOR 182 Table 1: Translation results as increasing amount of training data in IWSLT06 CSTAR track System AER BLEU METEOR 50K NL 0.217 0.158 0.427 lemma 0.199 0.167 0.431 100K NL 0.178 0.182 0.457 lemma 0.177 0.188 0.463 300K NL 0.150 0.223 0.501 lemma 0.132 0.217 0.505 400K NL 0.136 0.231 0.509 lemma 0.102 0.224 0.507 500K NL 0.119 0.235 0.519 lemma 0.104 0.241 0.522 600K NL 0.095 0.238 0.535 lemma 0.069 0.248 0.536 Table 2: Statistical significance test in terms of BLEU: sys1=non-lemma, sys2=lemma Data size Diff(sys1-sys2) 50K -0.092 [-0.0176,-0.0012] 100K -0.006 [-0.0155,0.0039] 300K 0.0057 [-0.0046,0.0161] 400K 0.0074 [-0.0023,0.0174] 500K -0.0054 [-0.0139,0.0035] 600K -0.0103 [-0.0201,-0.0006] translations did not outperform the non-lemmatized ones uniforml	NL	natural anguage$Dutch$Natural Language$nonlem$nonliteral$natura l  language$Northern Light$natural language$	3
The lemmatized 1http://domino.watson.ibm.com/library/CyberDig.nsf (key- word=RC22176) 2http://www.cs.cmu.edu/?alavie/METEOR 182 Table 1: Translation results as increasing amount of training data in IWSLT06 CSTAR track System AER BLEU METEOR 50K NL 0.217 0.158 0.427 lemma 0.199 0.167 0.431 100K NL 0.178 0.182 0.457 lemma 0.177 0.188 0.463 300K NL 0.150 0.223 0.501 lemma 0.132 0.217 0.505 400K NL 0.136 0.231 0.509 lemma 0.102 0.224 0.507 500K NL 0.119 0.235 0.519 lemma 0.104 0.241 0.522 600K NL 0.095 0.238 0.535 lemma 0.069 0.248 0.536 Table 2: Statistical significance test in terms of BLEU: sys1=non-lemma, sys2=lemma Data size Diff(sys1-sys2) 50K -0.092 [-0.0176,-0.0012] 100K -0.006 [-0.0155,0.0039] 300K 0.0057 [-0.0046,	NL	natural anguage$Dutch$Natural Language$nonlem$nonliteral$natura l  language$Northern Light$natural language$	3
.nsf (key- word=RC22176) 2http://www.cs.cmu.edu/?alavie/METEOR 182 Table 1: Translation results as increasing amount of training data in IWSLT06 CSTAR track System AER BLEU METEOR 50K NL 0.217 0.158 0.427 lemma 0.199 0.167 0.431 100K NL 0.178 0.182 0.457 lemma 0.177 0.188 0.463 300K NL 0.150 0.223 0.501 lemma 0.132 0.217 0.505 400K NL 0.136 0.231 0.509 lemma 0.102 0.224 0.507 500K NL 0.119 0.235 0.519 lemma 0.104 0.241 0.522 600K NL 0.095 0.238 0.535 lemma 0.069 0.248 0.536 Table 2: Statistical significance test in terms of BLEU: sys1=non-lemma, sys2=lemma Data size Diff(sys1-sys2) 50K -0.092 [-0.0176,-0.0012] 100K -0.006 [-0.0155,0.0039] 300K 0.0057 [-0.0046,0.0161] 400K 0.0074 [-0.0023,0.0174] 500K -0.0054 [-0.0139,0.0035] 600K -0.0103 [-0.0201,-0.0006] translatio	NL	natural anguage$Dutch$Natural Language$nonlem$nonliteral$natura l  language$Northern Light$natural language$	3
The term 'metaphor' is often used to refer to NL comparisons that  are novel and vivid and that convey ideas that might otherwise be difficult to express (Ortony, 1975).	NL	natural anguage$Dutch$Natural Language$nonlem$nonliteral$natura l  language$Northern Light$natural language$	4
Most would agree that metaphors are NL similarity comparisons (though not everyone  would agree on how literality should be defined), and that they are typically used for expressive-affective as  opposed to explanatory-predictive purposes.	NL	natural anguage$Dutch$Natural Language$nonlem$nonliteral$natura l  language$Northern Light$natural language$	4
A clustering approach for the nearly unsupervised recognition of NL language.	NL	natural anguage$Dutch$Natural Language$nonlem$nonliteral$natura l  language$Northern Light$natural language$	4
Gedigian et alused PropBank annotation (arguments and their semantic pour *NL cluster* wsj04:7878 N As manufacturers get bigger, they are likely to pour more money into the battle for shelf space, raising the ante for new players.	NL	natural anguage$Dutch$Natural Language$nonlem$nonliteral$natura l  language$Northern Light$natural language$	4
Yet, what is still assumed, rather than demonstrated, is that NL  uses of language are sometimes necessary for accomplishing such goals, rather than merely convenient or  elegant ways of doing so.	NL	natural anguage$Dutch$Natural Language$nonlem$nonliteral$natura l  language$Northern Light$natural language$	4
A clustering ap- proach for the nearly unsupervised recognition of NL language.	NL	natural anguage$Dutch$Natural Language$nonlem$nonliteral$natura l  language$Northern Light$natural language$	4
Ring s t ruc tures  a re  a very  f lex ib le  med ium,   su i tab le  fo r  the  organ izat ion  of data on which  NL  operates  and fo rmal i zed  enough to be amenab le  to man ipu la t ion  by  computer  p rograms.	NL	natural anguage$Dutch$Natural Language$nonlem$nonliteral$natura l  language$Northern Light$natural language$	5
By des ign ing  the log ica l  par t  su f f i c ient ly  genera l ,   one can use i t  even fo r  var ious  NLs,  i f  one  combln~s it with suitable information parts.	NL	natural anguage$Dutch$Natural Language$nonlem$nonliteral$natura l  language$Northern Light$natural language$	5
The defect in the ex is tent  theor ies  is the lack  of explanat ion of the mechan ism for adjust ing to tile  real world the formal  symbol ic  sys tems used in the  theories;  the only tMng they explain is the relation  between NL and the formal  system.	NL	natural anguage$Dutch$Natural Language$nonlem$nonliteral$natura l  language$Northern Light$natural language$	5
Natura l   language encompasses  a mul t i tude  of fo rmal  languages  and it is the  complex i t ies  of the memory  s t ruc tures  on which  NL  can  and does  operate  that  account  for the complex i t ies ,  f lex ib i l i ty   and r i chness  of NL.	NL	natural anguage$Dutch$Natural Language$nonlem$nonliteral$natura l  language$Northern Light$natural language$	5
These  la t te r  give r i se  to the  notor ious  prob lem of ambigu i t ies  in NL ana lys i s .	NL	natural anguage$Dutch$Natural Language$nonlem$nonliteral$natura l  language$Northern Light$natural language$	5
Tk, with k on the order 104, similar in granularity to the NL topic hierarchy (Kornai et al2003) and reserve T0 to topicless texts or ?	NL	natural anguage$Dutch$Natural Language$nonlem$nonliteral$natura l  language$Northern Light$natural language$	6
Through a  series of experiments, we found that we could  download highly useful information from Web  search engines such as Google, Yahoo, and  NL by searching ambiguous location  names in the Gazetteer.	NL	natural anguage$Dutch$Natural Language$nonlem$nonliteral$natura l  language$Northern Light$natural language$	6
He has developed several machine learning based NL  processing systems that are widely used in the computational linguistics community and  in industry and has presented invited talks and tutorials in several major conferences.	NL	natural anguage$Dutch$Natural Language$nonlem$nonliteral$natura l  language$Northern Light$natural language$	7
2 Motivation PP attachment disambiguation has often been studied as a benchmark test for empirical meth- ods in NL processing.	NL	natural anguage$Dutch$Natural Language$nonlem$nonliteral$natura l  language$Northern Light$natural language$	7
As for documents in NL, the amount of source code on Internet is increasing; facilitating the re-use of all or part of previously implemented programs.1 If no reference to the original work is included, pla- giarism would be committed.	NL	natural anguage$Dutch$Natural Language$nonlem$nonliteral$natura l  language$Northern Light$natural language$	7
2002), an open source NL engineering  system.	NL	natural anguage$Dutch$Natural Language$nonlem$nonliteral$natura l  language$Northern Light$natural language$	7
Roth has published broadly in machine learning, NL processing,  knowledge representation and reasoning and received several paper, teaching and  research awards.	NL	natural anguage$Dutch$Natural Language$nonlem$nonliteral$natura l  language$Northern Light$natural language$	7
Applied to the subtask of syntax analysis, the di- chotomy manifests itself in the existence of learnt and handwritten grammars of NLs.	NL	natural anguage$Dutch$Natural Language$nonlem$nonliteral$natura l  language$Northern Light$natural language$	7
He published several papers in NL  processing, machine learning and semantic interpretation.	NL	natural anguage$Dutch$Natural Language$nonlem$nonliteral$natura l  language$Northern Light$natural language$	7
Machine learning approaches using DTs  proposed so far have focused on preference selection  criteria directly derived from the decision tree re-  sults.	DTs	decision trees$Discourse Trees$	0
In  the case of DTs, we do have to provide in-  formation about possible antecedent indicators (syn-  tactic, semantic, and pragmatic features) contained  in the corpus, but the relevance of features for the  resolution task is extracted automatically from the  training data.	DTs	decision trees$Discourse Trees$	0
(Magerman 95; Jelinek et al 94) describe a  history-based approach which uses DTs to  estimate 7a(T\[S).	DTs	decision trees$Discourse Trees$	0
How-  ever, DTs are characterized by an indepen-  dent learning of specific features, i.e., relations be-  tween single attributes cannot be obtained automat-  ically.	DTs	decision trees$Discourse Trees$	0
Supervised training methods already applied to PP attachment range from stochastic maximum likelihood (Collins and Brooks, 1995) or maxi- mum entropy models (Ratnaparkhi et al, 1994) to the induction of transformation rules (Brill and Resnik, 1994), DTs (Stetina and Nagao, 1997) and connectionist models (Sopena et al, 1998).	DTs	decision trees$Discourse Trees$	0
Machine learning approaches using DTs  proposed so far have focused on preference selection  criteria directly deriv	DTs	decision trees$Discourse Trees$	0
Their Machine  Learning-based Resolver (MLR) is trained us-  ing DTs with 1971 anaphoras (exclud-  ing those referring to multiple discontinuous an-  tecedents) and they report an average success  rate of 74.8%.	DTs	decision trees$Discourse Trees$	0
5 Visualization of DTs We accompany the above Scala library with a web- based visualization tool that runs the two parsers in parallel and visualizes the two outputs for the same text side by side.	DTs	decision trees$Discourse Trees$	1
Experiments in Constructing a Corpus of DTs.	DTs	decision trees$Discourse Trees$	1
37  Experiments in Constructing a Corpus of DTs  Daniel Marcu  Information Sciences Institute and  Department of Computer Science  University of Southern California  4676 Admiralty Way, Suite 1001  Marina del Rey, CA 90292  marcu @isi.	DTs	decision trees$Discourse Trees$	1
DTs Are Good Indicators  of Importance in Text.	DTs	decision trees$Discourse Trees$	1
DTs Are Good In- dicators of Importance in Text.	DTs	decision trees$Discourse Trees$	1
3.1 Generating DTs In Rhetorical Structure Theory, discourse analysis involves two subtasks: (i) discourse segmentation, or breaking the text into a sequence of EDUs, and (ii) discourse parsing, or the task of linking the units (EDUs and larger discourse units) into la- beled discourse trees.	DTs	decision trees$Discourse Trees$	1
This accounts for the fact that the English PP[in] can have the anchored interpre- tation in a clause with a future auxiliary, such as (12) and (39), or in a clause with a futurate present tense, such as we are leaving in a minute, but not in a clause with a VBD, such as (2), or in a clause with a non-futurate present tense, such as (11).	VBD	past tense verb$verb past tense$	0
For instance, many surnames  are at the same time nouns or plural nouns in  English and thus in both variants can be fol-  lowed by a VBD.	VBD	past tense verb$verb past tense$	0
Three  features had high positive loadings on this dimen- sion:  Frequency of past perfect aspect verbs, fre- quency of VBDs and frequency of 3rd  person singular pronouns.	VBD	past tense verb$verb past tense$	0
A VBD situates the phrase in 2003 differently than one in the future.	VBD	past tense verb$verb past tense$	0
The need for the category of agglutination is a result of the way VBD forms are seg- mented (cf. (	VBD	past tense verb$verb past tense$	0
For example, the triple \[1\] 'be'  \[2\] adverb \[3\] past-tense-verb has been assigned a  scaling factor which downgrades a sequence contain-  ing this triple compared with a competing sequence  of \[1\] 'be' \[2\] adverb \[3\]-past-participle/adjective, on  the basis that after a form of 'be', past participles and  adjectives are more likely than a VBD  (Marshall (1983), p. 146).	VBD	past tense verb$verb past tense$	0
In some cases, such as VBDs or noun plurals, morphological distinctions found in Czech are also found in English.	VBD	past tense verb$verb past tense$	1
Main verb tenses are indefinitely referential, cre-  ating a new temporal entity under constraints  imposed by its type (i.e., past, present, or fu-  ture) in relation to a discourse reference time 2 tR.  For instance, a main VBD introduces a new temporal entity t under the constraint prior-  to(t, tR).	VBD	past tense verb$verb past tense$	1
This allows to generate out-of-vocabulary (OOV) words and phrases, which are not only recogni- tion errors, but also plausible variants of different source phrases that can be translated to one tar- get phrase, e.g., VBD forms or function words.	VBD	past tense verb$verb past tense$	1
{ assert : proposition presup : proposition* 3.2 Alternative Sets The concept of alternative sets VBZ an impor- tant role in the semantics of alternative phrases.	VBZ	third-person singular present verb.$verb present 3rd person$plays$NNS$	2
Table 1 disVBZ the distribution of entities versus  their occurrences in our corpus.	VBZ	third-person singular present verb.$verb present 3rd person$plays$NNS$	2
The space reconst ructor  VBZ the role of recon-   strucUng the semant ic  space so that the user  can be  satisfied.	VBZ	third-person singular present verb.$verb present 3rd person$plays$NNS$	2
His research work studies  the role that an external context, such as the real world or a simulated world, VBZ in  semantic interpretation and learning protocols.	VBZ	third-person singular present verb.$verb present 3rd person$plays$NNS$	2
and Vicedo Question Answering in Restricted Domains: An Overview it should be noted that some words would still have several senses available and therefore word-sense disambiguation still VBZ a role.	VBZ	third-person singular present verb.$verb present 3rd person$plays$NNS$	2
When a function is pointed, the plain text section disVBZ the source code.	VBZ	third-person singular present verb.$verb present 3rd person$plays$NNS$	2
P7-X4---------- RV--7---------- z??ska?n??m (getting) NVBZ7-----A---- NVBZ7-----A---- telefonn??ch (phone) AAFP2----1A---- AAFP2----1A---- linek (lines) NNFP2-----A---- NNFP2-----A---- Figure 1: Annotation error: P7-X4----------, should have been: RV--7---------- strong advantage.	VBZ	third-person singular present verb.$verb present 3rd person$plays$NNS$	3
The used patterns are: 1) (DT|CD) (NN|VBZ), 2) DT JJ (NN|VBZ), 3) NN POS (NN|VBZ), and 4) PRP$ JJ (NN|VBZ).	VBZ	third-person singular present verb.$verb present 3rd person$plays$NNS$	3
The training set consists of 1674 documents from newswire, MZ articles, broadcast news, broad- cast conversations and webpages, and the develop- ment set consists of 202 documents from the same source.	MZ	magazine$Magazine$	0
IEEE Circuits and systems MZ, 6(3):21?45.	MZ	magazine$Magazine$	0
rt meadows pearland laval safeway osu turnt lds hayward westbank harkins parker jammin poutine huskies stillwater angeles temple cal bayou camelback blake mayne boul everett topeka usc murray jose houma mesa cherry katy est seatac sooners chargers menudito swaaaaggg lawd gilbert siiiiim jamming je ducks straighht oc mormon folsom gtf pima coors tsu sherbrooke victoria kc compton gateway roseville MZ dbacks englewood marcos pas beaverton manhattan meadowview megaplex juiced gumbo mcdowell pikes laredo fkn hella boomer rancho lake vallejo buku devils rockies texas centre sounders sooner ventura Table 6: Top 20 features selected for various regions using logistic regression on TWUS with a uniform 5 ?	MZ	magazine$Magazine$	0
The dataset consists of 2083 documents from a much larger va- riety of genres, such as conversations, MZs, web text, etc.	MZ	magazine$Magazine$	0
The texts that the templates were filled from were newspaper and technical MZ articles concerned either with joint business ventures or microelectronics fabrication technology.	MZ	magazine$Magazine$	0
This balanced corpus consists of about 540 million tokens of Dutch across a wide range of text types, such as books, MZ articles, reports, subtitles, but also data from the ?	MZ	magazine$Magazine$	0
AI MZ, 19(4):25?49.	MZ	magazine$Magazine$	1
Even for the `naturally' multi- agent task of soccer commentary, the systems de- scribed in the recent AI MZ special issue on RoboCup (Andre et al, 2000) are all single-agent.	MZ	magazine$Magazine$	1
Technology and Society MZ, IEEE, 31(4):73?80.	MZ	magazine$Magazine$	1
AI MZ, 21(1):57{66, Spring.	MZ	magazine$Magazine$	1
AI MZ, pages 73{85, Spring.	MZ	magazine$Magazine$	1
by Euromoney and Banker?s MZ.	MZ	magazine$Magazine$	1
When human annotators were not sure, they used <OPTIONAL PSB=...> where PSB is a list of possible NE classes.	PSB	POSSIBILITY$Pacific Symposium on Biocomputing$	0
iNVESTIGATING THE PSB OF A HICROPROCESSOR-BASED  MACIIINE TRANSLATTON SYSTEM  Haro ld  L. Somers  Centre fo r  Computat iona l  L ingu is t i cs   Un ivers i ty  of  Hanchester  Ins t i tu te  o f  Sc ience and Technology  PO Box RR, Manchester  H60 tqO, England  ABSTRACT  This  paper descr ibes  an on-go in~ research   pro jec t  be ing car r ied  out  by s ta f f  and s tudents  ac  the Centre fo r  Compu	PSB	POSSIBILITY$Pacific Symposium on Biocomputing$	0
26 SYNONYMY 0 0 0 0 0 27 ANTONYMY 0 0 0 0 0 28 PROBABILITY 0 0 0 0 0 29 PSB 0 0 0 0 0 30 CERTAINTY 0 0 0 0 0 31 THEME 6.51 1.75 3.30 6.26 9.75 ?	PSB	POSSIBILITY$Pacific Symposium on Biocomputing$	0
26 SYNONYMY a word/concept that means the same or nearly the same as another word/concept; (NAME) (Marry is called Minnie); (Sowa 1994) 27 ANTONYMY a word/concept that is the opposite of another word/concept; (empty is the opposite of full); (Sowa 1994) 28 PROBABILITY OF the quality/state of being probable; likelihood EXISTENCE (There is little chance of rain tonight); (Sowa 1994) 29 PSB the state/condition of being possible; (I might go to Opera tonight); (Sowa 1994) 30 CERTAINTY the state/condition of being certain or without doubt; (He definitely left the house this morning); 31 THEME an entity that is changed/involved by the action/event denoted by the predicate; (music lover; John opened the door.); (	PSB	POSSIBILITY$Pacific Symposium on Biocomputing$	0
nt to CLAUSE, A doesn't want  CLAUSE(1), B appeals to A to realize CLAUSE),  (NECESSITY-WITH-SOURCE-ENVIROI~ENTAL-CIRCUMSTANCES: A is in-  different to CLAUSE, One realizes CLAUSE(1) if A doesn't  realize CLAUSE, A doesn't want CLAUSE(1)),  (NECESSITY-WITH-SOME-SOURCE: At least one type of necessity  'is given),  (PO~IBILITY-WITH-SOURCE-AGENT: Inner circumstances of A are  complete for CLAUSE),  (PSB-WITH-SOURCE-NON-AGENT: B is superior to A, B  agrees to realize CLAUSE, B realizes CLAUSE(1) if A real-  izes CLAUSE and B doesn't agree to realize CLAUSE, A  doesn't want CLAUSE(I ) ),  ( PSB-WITH-S OURCE-ENVIRONMENTAL -CIRCUMSTANCES : Environ-  mental circumstances are complete for CLAUSE),  (PSB-WITH-ALL-SOURCES: All types of possibilities are  - 238 -  given),  (WIL	PSB	POSSIBILITY$Pacific Symposium on Biocomputing$	0
of the PSB, volume 11, pages 100?111, Maui, HI.	PSB	POSSIBILITY$Pacific Symposium on Biocomputing$	1
PSB, pages 652?663.	PSB	POSSIBILITY$Pacific Symposium on Biocomputing$	1
In PSB, pages 451?	PSB	POSSIBILITY$Pacific Symposium on Biocomputing$	1
PSB.	PSB	POSSIBILITY$Pacific Symposium on Biocomputing$	1
In PSB.	PSB	POSSIBILITY$Pacific Symposium on Biocomputing$	1
In Proceedings of the Second PASCAL Chal- lenges Workshop on RTE.	RTE	Recognizing Textual Entailment$Recognizing Textual En-tailment$recognizing textual entailment$	0
intino 31 Torino,10121, Italy kouylekov@celi.it Luca Dini Celi S.R.L. via San Quintino 31 Torino,10121, Italy dini@celi.it Alessio Bosca Celi S.R.L. via San Quintino 31 Torino,10121, Italy alessio.bosca@celi.it Marco Trevisan Celi S.R.L. via San Quintino 31 Torino, Italy trevisan@celi.it Abstract This paper presents CELI?s participation in the SemEval The Joint Student Response Anal- ysis and 8th RTE Challenge (Task7) and Cross-lingual Textual Entailment for Content Synchronization task (Task 8).	RTE	Recognizing Textual Entailment$Recognizing Textual En-tailment$recognizing textual entailment$	0
An Open-Source Package for RTE.	RTE	Recognizing Textual Entailment$Recognizing Textual En-tailment$recognizing textual entailment$	0
The PASCAL RTE Challenge.	RTE	Recognizing Textual Entailment$Recognizing Textual En-tailment$recognizing textual entailment$	0
With the introduction of the MSR alignment cor- pus (Brockett, 2007) developed from the second RTE challenge data (Bar- Haim et al.,	RTE	Recognizing Textual Entailment$Recognizing Textual En-tailment$recognizing textual entailment$	0
3 RTE Recent work in computational seman- tics (Haghighi et al, 2005; Hickl et al, 2006b; MacCartney et al, 2006) has demonstrated the viability of supervised machine learning-based approaches to the recognition of textual en- tailment (TE).	RTE	Recognizing Textual Entailment$Recognizing Textual En-tailment$recognizing textual entailment$	0
In Pro-ceedings of the PASCAL RTE Challenge Workshop.	RTE	Recognizing Textual Entailment$Recognizing Textual En-tailment$recognizing textual entailment$	1
The fifth pascal RTE chal- lenge.	RTE	Recognizing Textual Entailment$Recognizing Textual En-tailment$recognizing textual entailment$	2
The PASCAL RTE challenge.	RTE	Recognizing Textual Entailment$Recognizing Textual En-tailment$recognizing textual entailment$	2
contains two major types of experimental setups: (i) those for an intrinsic eval- uation allow to evaluate the system performance in an isolated setting by comparing the system results with a human gold standard, and (ii) those for an extrinsic evaluation allow to evaluate the system with respect to a particular task at hand, where text similarity is a means for solving a concrete prob- lem, e.g. RTE.	RTE	Recognizing Textual Entailment$Recognizing Textual En-tailment$recognizing textual entailment$	2
Biutee: A modular open-source system for RTE.	RTE	Recognizing Textual Entailment$Recognizing Textual En-tailment$recognizing textual entailment$	2
Tree edit models for RTEs, paraphrases, and answers to questions.	RTE	Recognizing Textual Entailment$Recognizing Textual En-tailment$recognizing textual entailment$	2
Extrinsic Evaluation Our framework includes two setups for an extrinsic evaluation: detecting text reuse, and RTE.	RTE	Recognizing Textual Entailment$Recognizing Textual En-tailment$recognizing textual entailment$	2
to be carried out by 7 users by interacting with an IM platform, which they were told to be the system interface.	IM	instant messaging$Interregnum$Input Manager$	0
Automatic IM dialogue using statistical models and dialogue acts.	IM	instant messaging$Interregnum$Input Manager$	0
They were all told to use a web application that describes the Desert Scenario (see Section 3) and proposes to un- dertake two IM chats with two human users5.	IM	instant messaging$Interregnum$Input Manager$	0
1 Introduction The increasing prevalence of informal text from which a dialog structure can be reconstructed (e.g., email or IM), raises new challenges if we are to help users make sense of this cacophony.	IM	instant messaging$Interregnum$Input Manager$	0
Arabizi is a non-standard romanization of Arabic  script that is widely adopted for communication  over the Internet (World Wide Web, email) or  for sending messages (IM and  mobile phone text messaging) when the actual  Arabic script alphabet is either unavailable for  technical reasons or otherwise more difficult to  use.	IM	instant messaging$Interregnum$Input Manager$	0
85  Analysis of Intention in Dialogues Using Category Trees and  Its Application to Advertisement Recommendation  Hung-Chi Huang Ming-Shun Lin Hsin-Hsi Chen  Department of Computer Science and Information Engineering   National Taiwan University   Taipei, Taiwan   {hchuang, mslin}@nlg.csie.ntu.edu.tw; hhchen@ntu.edu.tw       Abstract  We propose an intention analysis system  for IM applications.	IM	instant messaging$Interregnum$Input Manager$	0
IMs are generated as follows.	IM	instant messaging$Interregnum$Input Manager$	1
IM to Denver ? ?? ?	IM	instant messaging$Interregnum$Input Manager$	1
Reparandum IM ? ?? ?	IM	instant messaging$Interregnum$Input Manager$	1
Because the paraphrase grammar is de- signed to directly relate surface language to dialogue moves, dialogue moves are generated directly, skip- ping the IM processing.	IM	instant messaging$Interregnum$Input Manager$	2
By contrast, DSMs are trained on large, domain-general corpora.	DSMs	distributional semantic models$Distributional Semantic Models$	0
This result reinforces the importance of the contextualization procedure for DSMs.	DSMs	distributional semantic models$Distributional Semantic Models$	0
There are a number of potential advantages that DSMs offer.	DSMs	distributional semantic models$Distributional Semantic Models$	0
The method is based on DSMs by effectively treating abbreviations and their corresponding def- inition as synonymous, at least in the sense of shar- ing distributional properties.	DSMs	distributional semantic models$Distributional Semantic Models$	0
A large scale evaluation of DSMs: Pa- rameters, interactions and model selection.	DSMs	distributional semantic models$Distributional Semantic Models$	0
4 Experiments We trained the DSMs us- ing the Annotated Gigaword corpus (Napoles et al.,	DSMs	distributional semantic models$Distributional Semantic Models$	0
c?2010 Association for Computational Linguistics DSMs Stefan Evert, University of Osnabr?ck 1.	DSMs	distributional semantic models$Distributional Semantic Models$	1
Contrasting Syntagmatic and Paradigmatic Relations: Insights from DSMs Gabriella Lapesa 3,1 1 Universit?at Osnabr?uck Institut f?ur Kognitionswissenschaft glapesa@uos.de Stefan Evert 2 2 FAU Erlangen-N?urnberg Professur f?ur Korpuslinguistik stefan.evert@fau.de Sabine Schulte im Walde 3 3 Universit?at Stuttgart Institut f?ur Maschinelle Sprachverarbeitung schulte@ims.uni-stuttgart.de Abstract This paper presents a large-scale evalua- ti	DSMs	distributional semantic models$Distributional Semantic Models$	1
UNIBA: Combining DSMs and Word Sense Disambiguation for Textual Similarity Pierpaolo Basile and Annalina Caputo and Giovanni Semeraro Department of Computer Science University of Bari Aldo Moro Via, E. Orabona, 4 - 70125 Bari (Italy) {firstname.surname}@uniba.it Abstract This paper describes the UNIBA team participation in the Cross-Level Semantic Similarity task at SemEval 2014.	DSMs	distributional semantic models$Distributional Semantic Models$	1
4 Conclusions and Future Work Our contribution is in the use of complementary features in order to learn the function of STS, a part of the challenge of building Compositional DSMs.	DSMs	distributional semantic models$Distributional Semantic Models$	1
2.6 Compositional features In DSMs, given the vector representations of two words, it is always possible to compute their similarity as the cosine of the angle between them.	DSMs	distributional semantic models$Distributional Semantic Models$	1
2.1 Distributional Semantics Level DSMs (DSM) are an easy way for building geometrical spaces of con- cepts, also known as Semantic (or Word) Spaces, by skimming through huge corpora of text in or- der to learn the context of word usage.	DSMs	distributional semantic models$Distributional Semantic Models$	1
Difference between the number of PP/ARGP/ADVP/CONJP phrases in the source and target; ?	ARG	Attribute Relation Graph$NP/VP/ADJ$Arguments$	1
difference between the number of PP/ARGP/ADVP/CONJP phrases in the source and target; ?	ARG	Attribute Relation Graph$NP/VP/ADJ$Arguments$	1
ARG Y (a branching5) and Z (a set of edges) are constraints on the edges that can be part of the solution, A. Edges in Y are required to be in the solution and edges in 5A branching is a subgraph that contains no cycles and no more than one edge directed into each node.	ARG	Attribute Relation Graph$NP/VP/ADJ$Arguments$	2
ARG are the complements selected by the  head 4.	ARG	Attribute Relation Graph$NP/VP/ADJ$Arguments$	2
In this paper, we present DAVID (Detector of ARG of Verbs with Incompatible Denota- tions), a resource-based system for detecting pref- erence violations.	ARG	Attribute Relation Graph$NP/VP/ADJ$Arguments$	2
ARG are assigned to clusters based on their inferred canonical function.	ARG	Attribute Relation Graph$NP/VP/ADJ$Arguments$	2
ARG that can be filled multiple times marked with ?	ARG	Attribute Relation Graph$NP/VP/ADJ$Arguments$	2
ARG of all but 2 of the 7 available mappings were edited, either to add missing arguments or to correct nonsensi- cal ones.	ARG	Attribute Relation Graph$NP/VP/ADJ$Arguments$	2
Binot and K. Jensen A Semantic Expert  Using an On-line Standard Dictionary  Proceedings of the IJCAI Milano, 1987  \[41 K. Dahlgren and J. McDoweU KT in  Knowledge Reimesentation Proceedings of the  Coling-86 1986  151 Heidorn G.E. "Augmented Phrase Structure  Grammar" in "Theoretical Issues in Natural  Language Processing" N ash- Webber and  Schank ,eds, ACL 1975  161 J. Katz, P. Postal An Integrated Theory of  Linguistic Descriptions Cambridge, M.LT.	KT	Kind Types$Knowledge Type$	0
Dahlgren, K. and McDowell, J. 1986a KT in Knowledge  Representation.	KT	Kind Types$Knowledge Type$	0
Dalflgren, K., and J. McDowell (1986) KT  in Knowledge Representation.	KT	Kind Types$Knowledge Type$	0
1986 KT in Knowledge  Representation.	KT	Kind Types$Knowledge Type$	0
Although the KT value forms the  basis for this, it is not in itself sufficient.	KT	Kind Types$Knowledge Type$	1
2 Data, Tool and KTs Interoperability in building pipelined NLP applications is intended ensure the exchange of information between the different NLP tools.	KT	Kind Types$Knowledge Type$	1
These were selected because 0099 contained multiple tie up relationships, and the other contained a single tie up relationship wit h 	 KT Words (Stems) Tokens Compounds Idioms Verb categories Nominal categories Grammar Arcs Grammar States Concepts Semantic Mappings Domain Template Core System 14,81 6 18,928 343 88 16 404 273 90 386 0 0 New/ Mod for MUC-5 387 81 1 110 5 0 0 9 5 1 147 1 1 204 some complex coreference phenomena.	KT	Kind Types$Knowledge Type$	1
Dimension Cohen?s Kappa  KT 0.9017  Certainty Level 0.9329  Polarity 0.9059  Manner 0.8944  Source 0.9520  Table 3.	KT	Kind Types$Knowledge Type$	1
Events with  the KT of Observation could corre- spond to new knowledge, but only if they repre- sent observations from the current study, rather  than observations cited from elsewhere.	KT	Kind Types$Knowledge Type$	1
9 .Acknowledgements   Thi~ report is based upon work supported by  the Defense Advanced DARPA  under Grant N00014-9O-J-1851 from the Office  of Naval Research and by the National Science  l:oun(lation under Grant 11H-89-02304.	DARPA	Research Projects Agency$Defense Advanced Research Projects Agency$	0
400-81-0030 of the National Institute of Education and by the Advanced DARPA of the  Department of Defense under Contract No.	DARPA	Research Projects Agency$Defense Advanced Research Projects Agency$	0
Defense Advanced DARPA,  February.	DARPA	Research Projects Agency$Defense Advanced Research Projects Agency$	0
This  work  was  sponsored by the Defense Advanced DARPA and was monitored by the Space and Naval  Warfare Systems Command under  contract  N000-39-86-C-0307  193  The 'stack' (which is far from being a stack in the computer science sense of a f irst- in-f irst-out  list) is a list of partial transcriptions, ordered by the PTE.	DARPA	Research Projects Agency$Defense Advanced Research Projects Agency$	0
Acknowledgements    We gratefully acknowledge the support of the Na- tional Science Foundation Grant NSF-0715078,  Consistent Criteria for Word Sense Disambigua- tion, and the GALE program of the Defense Ad- vanced DARPA, Contract No.	DARPA	Research Projects Agency$Defense Advanced Research Projects Agency$	0
Acknowledgements  This research was sponsored in part by US West and in part  by the Defense Advanced DARPA (DOD),  Axpa Order No.	DARPA	Research Projects Agency$Defense Advanced Research Projects Agency$	0
9 .Acknowledgements   Thi~ report is based upon work supported by  the DARPA  under Grant N00014-9O-J-1851 from the Office  of Naval Research and by the National Science  l:oun(lation under Grant 11H-89-02304.	DARPA	Research Projects Agency$Defense Advanced Research Projects Agency$	1
Acknowledgments  This material is based upon work supported by the  DARPA un- der Contract No.	DARPA	Research Projects Agency$Defense Advanced Research Projects Agency$	1
DARPA,  February.	DARPA	Research Projects Agency$Defense Advanced Research Projects Agency$	1
This  work  was  sponsored by the DARPA and was monitored by the Space and Naval  Warfare Systems Command under  contract  N000-39-86-C-0307  193  The 'stack' (which is far from being a stack in the computer science sense of a f irst- in-f irst-out  list) is a list of partial transcriptions, ordered by the PTE.	DARPA	Research Projects Agency$Defense Advanced Research Projects Agency$	1
Acknowledgments This research was supported in part by the GALE pro- gram of the DARPA, Contract No.	DARPA	Research Projects Agency$Defense Advanced Research Projects Agency$	1
Acknowledgements  This research was sponsored in part by US West and in part  by the DARPA (DOD),  Axpa Order No.	DARPA	Research Projects Agency$Defense Advanced Research Projects Agency$	1
NP repetition is  one simple way of approximately identifying the  TOP.	TOP	topic$topic marker$token on top$	0
162  In effect, we use this probability information to  identify the TOP of the segment with the belief  that the TOP is more likely to be referred to by  a pronoun.	TOP	topic$topic marker$token on top$	0
The more accurately the TOP of a seg-  ment can be identified, the higher the success  rate we expect an anaphora resolution system  can achieve.	TOP	topic$topic marker$token on top$	0
The idea is similar to that used in  the centering approach (Brennan et al, 1987)  where a continued TOP is the highest-ranked  candidate for pronominalization.	TOP	topic$topic marker$token on top$	0
References  by pronouns are closely related to the TOP or  the center of the discourse.	TOP	topic$topic marker$token on top$	0
The more accurately the TOP of a seg-  ment can be identified, the higher the success  rate we expect an anaphora re	TOP	topic$topic marker$token on top$	0
The more fre-  quently an entity is repeated, the more likely it  is to be the TOP of the story and thus to be  a candidate for pronominalization.	TOP	topic$topic marker$token on top$	0
l emma exp lanat ion   o  suushi*  wa  suru   J  f  mo  (  )  nado  nai  aru   kara  koto  dewa  nen  hi  no  comma  period  numerals  TOP  'do'  right angular parenthesis  left angular parenthesis  TOP  left parenthesis  right parenthesis  'so forth'  dash  negative auxiliary  'exist ' , 'be'   ' from'  nominalizer  TOP  'year'   'day'   possessive particle  sible to think of a complex clue in terms of its com-  ponent clues for which a sentence is marked.	TOP	topic$topic marker$token on top$	1
3.1 Extraction of Topic Part A phrase whose head word is marked with a TOP wa is extracted as a topic.	TOP	topic$topic marker$token on top$	1
In Japanese,  the antecedent part can be syntactically deter-  mined, so far as the topic phrase is expressed with  the TOP.	TOP	topic$topic marker$token on top$	1
For fear of noise, we only harvested PASs that have just a predicate and an argument for ga (nominative) with its topic (an NP) explicitly marked by a TOP wa.	TOP	topic$topic marker$token on top$	1
7The names Top and Next refer to the TOP of the stack S and the first token in the remaining input I, respectively.	TOP	topic$topic marker$token on top$	2
In the arc-eager approach introduced by Nivre et al (2006) the possible actions are as follows, with s0 being the TOP of the stack and n0 being the next token in the buffer: ?	TOP	topic$topic marker$token on top$	2
Given an arbitrary configuration of the parser, there are four possible transitions to the next configuration (where t is the TOP of the stack, n is the next input token, w is any word, and r, r? ?	TOP	topic$topic marker$token on top$	2
that the Single head constraint is satisfied, while the Reduce transition can only be ap- plied if the TOP of the stack already has a head.	TOP	topic$topic marker$token on top$	2
In particular, many features involve properties of the two target tokens, the TOP of the stack ?	TOP	topic$topic marker$token on top$	2
with or with- out adding an arc from the TOP of the stack to the token pushed ?	TOP	topic$topic marker$token on top$	2
By collating by referent and ABSing away  to the gender classes of pronouns, rather than  individual pronouns, we have the relative fre-  quencies with which a given referent is referred  to by pronouns of each gender class.	ABS	abstract$abstractio$abstraction$	0
After several  inquiries, the learner will "own" several such  associative maps and will be able to continue the  learning process at the level of generality or  ABSion at which new knowledge can be  generated.	ABS	abstract$abstractio$abstraction$	0
The data source used by the system is the set of MEDLINE ABSs, a large bibliographic database that is accessed on-line via PubMed.	ABS	abstract$abstractio$abstraction$	0
This would be com-  patible in the longer term with extending the  domain of the task to cover ABS objects  such as events, when they are not described  using an NP.	ABS	abstract$abstractio$abstraction$	0
In particular, only Noun Phrases were anno-  tated (thereby circumventing problems of null  anaphora, summation, ABSion, etc.,	ABS	abstract$abstractio$abstraction$	0
Demner-Fushman and Lin (2005) operationalize knowledge extraction for populat- ing a database with PICO (Population, Intervention, Comparison, and Outcome) ele- ments from medical ABSs obtained from MEDLINE.	ABS	abstract$abstractio$abstraction$	0
STG's add another ABSn to parsing  schemata, namely on the grammar side.	ABS	abstract$abstractio$abstraction$	1
After several  inquiries, the learner will "own" several such  associative maps and will be able to continue the  learning process at the level of generality or  ABSn at which new knowledge can be  generated.	ABS	abstract$abstractio$abstraction$	1
STG's constitute a level of ABSn be-  tween grammars and parsing schemata because  they can be used to encode various classes of  grammars, whereas the mechanism for recog-  nizing admissible sequences of subconstituents  by a parsing algorithm is built into the gram-  mar.	ABS	abstract$abstractio$abstraction$	1
Un- like the thread extension, no additional ABSn will be needed.	ABS	abstract$abstractio$abstraction$	1
A parsing schema abstracts  from unimportant algorithmic details and thus,  like STG's, represents a well-defined level of  ABSn between grammars and parsers.	ABS	abstract$abstractio$abstraction$	1
In particular, only Noun Phrases were anno-  tated (thereby circumventing problems of null  anaphora, summation, ABSn, etc.,	ABS	abstract$abstractio$abstraction$	1
SllCtl an ABSn is useflll beCallSe  it; allows to study l)rot)erties of parsing algo-  rithms, and to compare different parsing algo-  rithms, independently of tile prot)erties of an  mtderlying rammar formalism.	ABS	abstract$abstractio$abstraction$	1
STG's add another ABS to parsing  schemata, namely on the grammar side.	ABS	abstract$abstractio$abstraction$	2
After several  inquiries, the learner will "own" several such  associative maps and will be able to continue the  learning process at the level of generality or  ABS at which new knowledge can be  generated.	ABS	abstract$abstractio$abstraction$	2
STG's constitute a level of ABS be-  tween grammars and parsing schemata because  they can be used to encode various classes of  grammars, whereas the mechanism for recog-  nizing admissible sequences of subconstituents  by a parsing algorithm is built into the gram-  mar.	ABS	abstract$abstractio$abstraction$	2
Un- like the thread extension, no additional ABS will be needed.	ABS	abstract$abstractio$abstraction$	2
A parsing schema abstracts  from unimportant algorithmic details and thus,  like STG's, represents a well-defined level of  ABS between grammars and parsers.	ABS	abstract$abstractio$abstraction$	2
In particular, only Noun Phrases were anno-  tated (thereby circumventing problems of null  anaphora, summation, ABS, etc.,	ABS	abstract$abstractio$abstraction$	2
SllCtl an ABS is useflll beCallSe  it; allows to study l)rot)erties of parsing algo-  rithms, and to compare different parsing algo-  rithms, independently of tile prot)erties of an  mtderlying rammar formalism.	ABS	abstract$abstractio$abstraction$	2
1997)  and later Harabagiu and Maiorano (HM) (2000)  investigated the acquisition of the lexical concept  SPA using WordNet and have applied their methods  to the Information Extraction task.	SPA	space$Speech Act$	0
In all the experiments, we clustered the whole set of 2283 adjectives, as the set of objects alters the vector SPA and thus the classification results.	SPA	space$Speech Act$	0
V (D) is the version SPA, which is the set of weightswi that classify the training data correctly, and |V (D)| is the size of the version SPA.	SPA	space$Speech Act$	0
As increasing the number of perceptrons re- sults in more thorough exploration of the version SPA V (D), we expect that the performance of the classifier would improve as K increases.	SPA	space$Speech Act$	0
In practice, to explore the version SPA of weights consistent with the training data, BPM trains a few different perceptrons (Collins, 2002) by shuffling the samples.	SPA	space$Speech Act$	0
A Plan-Based Approach to  SPA Recognition.	SPA	space$Speech Act$	1
ference NE Aliasing Concept Textual Input 1 Textual Input 2 Lexical Alignment Paraphrase Acquisition Alignment Module WWW Training Corpora  Classifier YES NO Features Alignment Dependency Features Paraphrase Features Semantic/ Pragmatic Features Feature Extraction Classification Module Lexico?Semantic PoS/ NER Synonyms/ Antonyms Normalization Syntactic Semantic Temporal Parsing Modality Detection SPA Recognition Pragmatics Factivity Detection Belief Recognition Preprocessing Figure 4: Textual Entailment Architecture.	SPA	space$Speech Act$	1
Syntax and Seman-  tics, Volume 3: SPAs, Academic Press, pp.	SPA	space$Speech Act$	1
Syntax and  70  Semantics III: SPAs, (pp.	SPA	space$Speech Act$	1
SPAs.	SPA	space$Speech Act$	1
192  An ascription-based approach to SPAs  Abstract:  The two principal areas of natural language processing  research in pragmatics are belief modelling and speech  act processing.	SPA	space$Speech Act$	1
Our decision to run MBC for MUC-4 was largely motivated by use of the AT metric as the official scorin g metric for MUC-4 .	AT	All Templates$Article$attach$answer type$Activity Tree$	0
If we had generated a summary score report based on only two templates instead of three, our AT precision would have been 94 .	AT	All Templates$Article$attach$answer type$Activity Tree$	0
erp-total 230 87 3 	 2 	 3 0 	 2 79 	 222 	 351 2 	 4 	 9 1 phys-tgt-total 195 68 0 	 0 	 3 0 	 0 65 	 192 	 728 0 	 0 	 9 6 hum-tgt-total 449 63 20 	 4 	 2 0 	 4 37 	 423 	 772 5 	 35 	 5 9 Matched/Missing 1259 71 39 	 13 	 11 1 	 7 8 	 1196 	 898 4 	 64 	 1 1 Matched/Spurious 113 527 39 	 13 	 26 1 	 7 464 	 50 	 1277 40 	 9 	 8 8 Matched Only 113 71 39 	 13 	 11 1 	 7 8 	 50 	 69 40 	 64 	 1 1 AT 1259 527 39 	 13 	 26 1 	 7 464 	 1196 	 2106 4 	 9 	 8 8 Set Fills Only 593 36 26 	 4 	 4 0 	 2 2 	 559 	 418 5 	 78 	 6 String Fills Only 337 9 5 	 2 	 2 0 	 2 0 	 328 	 257 2 	 67 	 0 Table 2.	AT	All Templates$Article$attach$answer type$Activity Tree$	0
With the third template averaged in, our AT precision drops to 76 .	AT	All Templates$Article$attach$answer type$Activity Tree$	0
Because AT is maximally sensitive to all types of precision loss, it is generall y advantageous to minimize spurious templates for this metric .	AT	All Templates$Article$attach$answer type$Activity Tree$	0
529 1189 160 63 24 0 23 942 282 718 36 16 79 perp-total 249 687 39 19 41 0 4 588 150 631 19 7 86 phys-tgt-total 255 280 26 12 28 1 10 214 189 1788 12 11 76 hum-tgt-total 594 236 82 42 28 1 38 84 442 2038 17 44 36 Matched/Missing 1627 614 307 136 121 2 75 50 1063 1203 23 61 8 Matched/Spurious 971 2392 307 136 121 2 75 1828 407 4601 39 16 7 6 Matched Only 971 614 307 136 121 2 75 50 407 629 39 61 8 AT 1627 2392 307 136 121 2 75 1828 1063 5175 23 16 7 6 Set Fills Only 778 333 177 35 74 0 7 47 492 538 25 58 1 4 String Fills Only 419 105 50 30 22 1 30 3 317 353 16 62 3 Table 3 .	AT	All Templates$Article$attach$answer type$Activity Tree$	0
Introduction to the ATs in this Special Section Demner-Fushman and Lin?s article (Answering clinical questions with knowledge-based and statistical techniques) extends previous work by the authors (Demner-Fushman and Lin 2005) on a QA system in the medical domain.	AT	All Templates$Article$attach$answer type$Activity Tree$	1
Generating Summaries of Multiple News ATs.	AT	All Templates$Article$attach$answer type$Activity Tree$	1
Another example:  "President and wife came to capital"  (ATs and pronouns ere dropped to reflect Russian),  This phrase is processed as  "President roof-country with xhis wife came to capital Eof-  country".	AT	All Templates$Article$attach$answer type$Activity Tree$	1
Auto- matic Paraphrase Acquisition from News ATs.	AT	All Templates$Article$attach$answer type$Activity Tree$	1
AT 2.	AT	All Templates$Article$attach$answer type$Activity Tree$	1
ATs are in English and come from a variety of sources.	AT	All Templates$Article$attach$answer type$Activity Tree$	1
It is use- ful for low-level tasks such as parsing (e.g. for PP-ATment ambiguity within NPs), but also for tasks oriented to semantics, such as the extraction of relationships between individuals or concepts.	AT	All Templates$Article$attach$answer type$Activity Tree$	2
Unlike in typical HPSG approaches, the informa- tion about the realized arguments is still exposed in the COMPS and SUBJ lists of this constituent.10 This makes the necessary information available to separately-ATing modifiers (such as ngara- ganaguja (?	AT	All Templates$Article$attach$answer type$Activity Tree$	2
The procedure is not based on the deno-  tative meaning of a word, but only on the connota-  tive emotions ATed to the word; it is difficult to  choose the relevant dimensions, i.e. the dimensions  required for the sufficient semantic space.	AT	All Templates$Article$attach$answer type$Activity Tree$	2
g.de Abstract To study PP ATment disambiguation as a benchmark for empirical methods in nat- ural language processing it has often been reduced to a binary decision problem (be- tween verb or noun ATment) in a par- ticular syntactic configuration.	AT	All Templates$Article$attach$answer type$Activity Tree$	2
c?2006 Association for Computational Linguistics The Benefit of Stochastic PP Attachment to a Rule-Based Parser Kilian A. Foth and Wolfgang Menzel Department of Informatics Hamburg University D-22527 Hamburg Germany foth|menzel@nats.informatik.uni-hamburg.de Abstract To study PP ATment disambiguation as a benchmark for empirical methods in nat- ural language processing it has often been reduced to a binary decision problem (be- tween verb or noun ATment) in a par- ticular syntactic configuration.	AT	All Templates$Article$attach$answer type$Activity Tree$	2
We combine the ATment predictions mad	AT	All Templates$Article$attach$answer type$Activity Tree$	2
We combine the ATment predictions made by a simple model of lexical attraction with a full-fledged parser of German to de- termine the actual benefit of the subtask to parsing.	AT	All Templates$Article$attach$answer type$Activity Tree$	2
We show that the combination of data-driven and rule-based components can reduce the number of all parsing errors by 14% and raise the ATment accuracy for dependency parsing of German to an unprecedented 92%.	AT	All Templates$Article$attach$answer type$Activity Tree$	2
nefit of Stochastic PP Attachment to a Rule-Based Parser Kilian A. Foth and Wolfgang Menzel Department of Informatics Hamburg University D-22527 Hamburg Germany foth|menzel@nats.informatik.uni-hamburg.de Abstract To study PP ATment disambiguation as a benchmark for empirical methods in nat- ural language processing it has often been reduced to a binary decision problem (be- tween verb or noun ATment) in a par- ticular syntactic configuration.	AT	All Templates$Article$attach$answer type$Activity Tree$	2
are: the AT (ANY),  the restriction (sport), the question target (Jenni- fer Capriati), and the relation (play).	AT	All Templates$Article$attach$answer type$Activity Tree$	3
Where Q and T are sets of the bag-of-words  for the question relation and the triple relation  respectively, Lin(a,b) is a measure for the seman- tic similarity between a and b based on WordNet  (Lin, 1998), and L(x) is the number of elements  in the set x.  The Answer Extraction: this component first  filters out the triples mismatching the expected  AT.	AT	All Templates$Article$attach$answer type$Activity Tree$	3
Fi- nally, when the question word is the WH-word, we check if the paired word belongs to some phrase that has the correct AT using simple rules, such as ?	AT	All Templates$Article$attach$answer type$Activity Tree$	3
However, adding more information like named entity match- ing and AT verification does not seem to help much (Line #5 vs. #4).	AT	All Templates$Article$attach$answer type$Activity Tree$	3
Another interesting finding we have is that while the latent structured model, LCLR, performs better than the other two unstructured models, the difference diminishes after more in- formation, including the enhanced lexical seman- tic knowledge and AT verification, has been incorporated.	AT	All Templates$Article$attach$answer type$Activity Tree$	3
However, the questions in the reading  comprehension don?t limit the ATs to  person and organization, even if the question is  ?	AT	All Templates$Article$attach$answer type$Activity Tree$	3
Currently, we reject all but the parsed S fragments?and NP fragments when expected 8Command-and-control applications have also made use of an AT, which represents activities being carried out by the dialogue-enabled device (Gruenstein, 2002); however, this application currently makes no use of this.	AT	All Templates$Article$attach$answer type$Activity Tree$	4
Items which the system will consider for genera- tion are placed (either directly by the robot, or indi- rectly by the AT) on the ?	AT	All Templates$Article$attach$answer type$Activity Tree$	4
For example, the Dialogue Move Tree can update Salience List, System Agenda, Pend- ing List, and AT, while the AT can update only the System Agenda and send ex- ecution requests to the robot, and it can query the Activity Model (when adding nodes).	AT	All Templates$Article$attach$answer type$Activity Tree$	4
ive]) * Root (1) Root o Command (0) command([go],[param_list([pp_loc(to,arg([np(det([def],the),[n(tower, sg)])]))])]) [[dmtask0] current] + Report report(inform,agent([np([n(uav,sg)])]),curr_activity([command ([take_off])]))[] o Report report(inform,agent([np([n(uav,sg)])]),confirm_activity([command([go], [param_list([pp_loc(to,arg([np(det([def],the),[n(tower,sg)], )]))])])])) [[dmtask0] current] AT * root o [dmtask0] current relation = SEQuential command = go pp = pp_loc(to,Args) np = np(det([def],the),[n(tower,sg)]) + [sim3] current relation = none command = take_off pp = null, np = null Salience List (least salient -- most salient) * [np(det([def],the),[n(tower,sg)])] (speech) * [np(det([def],the),[n(tower,sg)])] (speech) Figure 6: Attachment in the Dialogue Move Classes DMT	AT	All Templates$Article$attach$answer type$Activity Tree$	4
AT?	AT	All Templates$Article$attach$answer type$Activity Tree$	4
which is a shared representation of current and planned activities and their execution status, in- volving temporal and hierarchical ordering (in fact, one can think of the AT as a Hierarchical Task Network for the device).	AT	All Templates$Article$attach$answer type$Activity Tree$	4
Other examples also have a similar significance: (3) for the pair  ACT-Objective, (4) for Manner-Directional.	ACT	Actor$arboreal context tree$active$accumulated tag counts$	0
83  Computational Linguistics Volume 21, Number 1  and Sgall, 1994), as well as investigations with native speakers of English, we hypoth-  esize that the SO of some of the main kinds of complementations i  English has the  following shape: 4  Time - ACT - Addressee - Objective - Origin - Effect - Manner - Directional.	ACT	Actor$arboreal context tree$active$accumulated tag counts$	0
A1: Jack Lemmon won the Academy Award for Best ACT for Save the Tiger (1973).	ACT	Actor$arboreal context tree$active$accumulated tag counts$	0
These include, on the one hand, inner participants or arguments,  such as ACT, Addressee, Objective, and, on the other hand, free modifications, uch  as Locative, Means, Manner, Cause, several temporal and directional modifications,  those of Condition, Regard, Accompaniment, etc.	ACT	Actor$arboreal context tree$active$accumulated tag counts$	0
Its lexical scope can be enlarged easily, if the added lexical items are accompanied by  appropriate grammatical data, especially by valency (case) frames specifying the optional and  obligatory arguments (ACT, Addressee, Objective, Origin, and Effect, with verbs).	ACT	Actor$arboreal context tree$active$accumulated tag counts$	0
Pret t (yesterday)Time  Most of our symbols (for Indefinite, Preterite, ACT, Addressee, Objective, Di-  rectional) should be self-explanatory; Gener(al Relationship) is the free modification  typical for an adjectival modifier of a noun.	ACT	Actor$arboreal context tree$active$accumulated tag counts$	0
A stochastic parser based on an SLM with ACTs.	ACT	Actor$arboreal context tree$active$accumulated tag counts$	1
InterACT Multimedia Navigation  Until now, problems of navigation have in the main  been reduced to the research of interface.	ACT	Actor$arboreal context tree$active$accumulated tag counts$	2
In recent ime,  with the emergence of channels and ACT  desktops, the dynamic haracteristics ofnavigation  tools have again changed.	ACT	Actor$arboreal context tree$active$accumulated tag counts$	2
However, these forms are  interACT in a limited sense only.	ACT	Actor$arboreal context tree$active$accumulated tag counts$	2
The three aspects  that make up the object of our research are 1) the  expression of knowledge through interACT  multimedia; 2) navigation tools corresponding to  the expression of knowledge through interACT  multimedia; nd 3) mechanisms for updating both  the body of knowledge represented through  interACT multimedia methods and the  appropriate navigation tools.	ACT	Actor$arboreal context tree$active$accumulated tag counts$	2
In recent ime,  with the emergence of channels and ACT  desktops, the dynamic haracteristics ofnavigation  tools have again cha	ACT	Actor$arboreal context tree$active$accumulated tag counts$	2
InterACT Multimedia Navigation  Prof. Dr. Dr. Mihai NADIN  Computational Design, University of Wuppertal  Hofaue 35-39  D-42103 Wuppertal, Germany  nadin @ code.uni-wuppertal.de  Dipl.	ACT	Actor$arboreal context tree$active$accumulated tag counts$	2
Associations are almost  always multimedial, i.e., we associate texts,  sounds, pictures, movement e c.  The structure of an interACT, multimedia  encyclopedia that is based on associations includes  1) a knowledge space/domain,  2) an associative search procedure,  3) a function for storing associative traces.	ACT	Actor$arboreal context tree$active$accumulated tag counts$	2
3 Representations We survey the following distributional represen- tations: (i) count vectors reduced by a Singular Value Decomposition (SVD), (ii) word clusters in- duced using the likelihood of a class-based language model, (iii) distributed embeddings trained using a neural network and (iv) ACT, a task-specific representation obtained from an auto- matically tagged corpus.	ACT	Actor$arboreal context tree$active$accumulated tag counts$	3
SemLink maps Arg0 to the Agent of carve-21.2-2 and Arg1 to the PAT.	PAT	Patient$pattern$	0
1996-1997 ABPI Compendium of  PAT Information Leaflets.	PAT	Patient$pattern$	0
2.2 Semantic Role Labeling  Semantic role labeling is the task of identifying  semantic roles such as Agent, PAT, Speaker,  or Topic, in a sentence.	PAT	Patient$pattern$	0
In our running example, VerbNet specifies +int control and +concrete for the Agent and PAT of carve-21.2-2, respectively.	PAT	Patient$pattern$	0
Depression as a Risk Factor for Poor Prog- nosis Among PATs With Acute Coronary Syn- drome: Systematic Review and Recommendations A Scientific Statement From the American Heart Association.	PAT	Patient$pattern$	0
Using SemLink, map each PropBank argu- ment name to the corresponding VerbNet the- matic roles in these entries (Agent, PAT, etc.).	PAT	Patient$pattern$	0
This likely stems from the variance of  verbs with respect o the thematic roles they allow (e.g.,  Agent, Instrument, PAT, etc.)	PAT	Patient$pattern$	0
PATs with depression often have common symptoms of low energy, reduced or intensified psychomotor movements, low concentration, in- decisiveness, and thoughts of death, as well as related symptoms such as fatigue, insomnia, and weight gain.	PAT	Patient$pattern$	0
The PAT is applied at the sentence level.	PAT	Patient$pattern$	1
The used PATs are: 1) (DT|CD) (NN|NNS), 2) DT JJ (NN|NNS), 3) NN POS (NN|NNS), and 4) PRP$ JJ (NN|NNS).	PAT	Patient$pattern$	1
These POS-based PATs are quite generic, al- lowing for the creation of large sets of characters.	PAT	Patient$pattern$	1
using syntactic PATs.	PAT	Patient$pattern$	1
2) a set of part-of-speech PATs was used for the extraction of human and non-human characters that were not represented by proper names, e.g., ?	PAT	Patient$pattern$	1
A hybrid ap- proach is adopted, where PAT-based and statistical methods are used along with utilization of external knowledge sources.	PAT	Patient$pattern$	1
The identification of quotes in the story is based on a simple PAT-based ap- proach: the quote boundaries are signified by the respective symbols, e.g., ?	PAT	Patient$pattern$	1
The output of grammatical modules is fed then onto the BMe which activates an algorithm for anaphoric binding.	BM	Binding Modul$Binding Module$	0
The output of grammatical modules is fed then onto the BM which activates an algorithm for anaphoric binding.	BM	Binding Modul$Binding Module$	1
This work was funded by the Ad- vanced Research and Development Activity's  Advanced Question AQUAINT  Program, National Science Foundation award  IIS-0325646 and a Stanford Graduate Fellow- ship.	AQUAINT	Answering for Intelligence$Advanced Question Answering for Intelligence$	0
8 Acknowledgements This work was supported in part by the Advanced Research and Development Activity?s Advanced Question AQUAINT Program.	AQUAINT	Answering for Intelligence$Advanced Question Answering for Intelligence$	0
This work was funded by the Ad- vanced Research and Development Activity's  AQUAINT  Program, National Science Foundation award  IIS-0325646 and a Stanford Graduate Fellow- ship.	AQUAINT	Answering for Intelligence$Advanced Question Answering for Intelligence$	1
Graphical  representations can range from simple diagrams  47  to ANIMd sequences.	ANIM	animate$animal$	0
Being ANIM, man performs either thematic  role well, allowing the main clause reading to remain  *I thank Christy Doran, Jason Eisner, Jeff Reynar, and  John Trueswell for valuable comments.	ANIM	animate$animal$	0
5 Unsuperv ised  Learn ing  o f  Gender   In fo rmat ion   The importance of gender information as re-  vealed in the previous experiments caused us to  consider automatic methods for estimating the  probability that nouns occurring in a large cor-  pus of English text deonote inANIM, mascu-  line or feminine things.	ANIM	animate$animal$	0
What follows is the complete list of morpholog- ical categories assumed in the proposed tagset:   number: sg , pl ;   case: nom , acc , gen , dat , inst , loc , voc;   gender: masculine personal m1 (facet), mas- culine ANIM m2 (ko?n), masculine inani- mate m3 (st?), feminine f (kobieta, zyrafa, 3Segmentation, as understood in the present context, is discussed at length in (Przepi?rkowski and Wolin?ski, 2003).	ANIM	animate$animal$	0
Agent SteaJ Object  Apple:'  Figure 8: Graphs from the sentence "Un avocat vole  une pomme"  ample, "Steal" needs an ANIMd agent (Figure 9),  therefore graphs with the "Avocado" concept can be  removed from the selection.	ANIM	animate$animal$	0
Since van,  which is inANIM, makes a good Theme but a poor  Agent for recognized, the past participial analysis in  2) is reinforced and the main clause (past tense) sup-  pressed.	ANIM	animate$animal$	0
a subset of the STORIES dataset that included 10 stories, the following schemes were used for filter- ing of candidate speakers: (i) Scheme 1: all speak- ers linked with speech verbs, (ii) Scheme 2: speak- ers, who are persons or ANIMs or spiritual entities according to their first WordNet sense, linked with speech verbs , and (iii) Scheme 3: as Scheme 2, 5SU+/?	ANIM	animate$animal$	1
refers to the co- occurrence pattern between a verb and a noun 33 [Sentence pattern] <word1> ga <word2> wo taberu (eat) [Sense relation] agent object [Case particle] ga (nominative) wo (accusative) [Sense identifier] 30f6b0 (human);30f6bf (ANIM) 30f6bf(ANIM);30f6ca(plants); 30f6e5(parts of plants); 3f9639(food and drink); 3f963a(feed) Figure 1: An example of a verb ?	ANIM	animate$animal$	1
A character was retained if any of its hypernyms was found to fall into certain types of WordNet concepts: person, ANIM, plant, artifact, spiritual being, physical entity.	ANIM	animate$animal$	1
Characters in chil- dren?s stories can either be human or non-human entities, i.e., ANIMs and non-living objects, ex- hibiting anthropomorphic traits.	ANIM	animate$animal$	1
There is no doubt that I  claim that all cats are ugly ANIMs.	ANIM	animate$animal$	1
Automatically ac- quiring fine-grained IS distinctions in German.	IS	INTERRUPTION SITE$information status$in ter face  St ructure$is$information structure$Iterative Sealing$	1
It con- sists of 50 texts taken from the WSJ portion of the OntoNotes corpus (Weischedel et al 2011) with al- most 11,000 NPs annotated for IS including 663 bridging NPs and their antecedents.	IS	INTERRUPTION SITE$information status$in ter face  St ructure$is$information structure$Iterative Sealing$	1
2For German surface realization, Cahill and Riester (2009) show that incorporating IS features based on the linguistics literature improves performance on realization ranking.	IS	INTERRUPTION SITE$information status$in ter face  St ructure$is$information structure$Iterative Sealing$	1
Learning IS of discourse entities.	IS	INTERRUPTION SITE$information status$in ter face  St ructure$is$information structure$Iterative Sealing$	1
Learning the fine- grained IS of discourse entities.	IS	INTERRUPTION SITE$information status$in ter face  St ructure$is$information structure$Iterative Sealing$	1
This is a minimal assignment in the sense that we do not project informativity; instead, we only set informa- tivity for those discourse referents whose realization shows explicit clues as to their IS.	IS	INTERRUPTION SITE$information status$in ter face  St ructure$is$information structure$Iterative Sealing$	1
1 In t roduct ion   We present a statIStical method for determin-  ing pronoun anaphora.	IS	INTERRUPTION SITE$information status$in ter face  St ructure$is$information structure$Iterative Sealing$	3
We incorpo-  rate multiple anaphora resolution factors into  a statIStical framework - -  specifically the dIS-  tance between the pronoun and the proposed  antecedent, gender/number/animaticity of the  proposed antecedent, governing head informa-  tion and noun phrase repetition.	IS	INTERRUPTION SITE$information status$in ter face  St ructure$is$information structure$Iterative Sealing$	3
We present some experiments il-  lustrating the accuracy of the method and note  that with thIS information added, our pronoun  resolution method achieves 84.2% accuracy.	IS	INTERRUPTION SITE$information status$in ter face  St ructure$is$information structure$Iterative Sealing$	3
We incorpo-  rate multiple anaphora resolution factors into  a statIStical framework - -  specifically the dIS-  tance between the pronoun and the proposed  antecedent, gender/number/animaticity of the  proposed antecedent, governing head informa-  tion and noun phrase repetition	IS	INTERRUPTION SITE$information status$in ter face  St ructure$is$information structure$Iterative Sealing$	3
A StatIStical Approach to Anaphora Resolution  Niyu  Ge, John  Hale and Eugene Charn iak   Dept.	IS	INTERRUPTION SITE$information status$in ter face  St ructure$is$information structure$Iterative Sealing$	3
brown, edu  Abst ract   ThIS paper presents an algorithm for identi-  fying pronominal anaphora and two experi-  ments based upon thIS algorithm.	IS	INTERRUPTION SITE$information status$in ter face  St ructure$is$information structure$Iterative Sealing$	3
We incorpo-  rate multiple anaphora resolution factors into  a statIStical framework - -  specif	IS	INTERRUPTION SITE$information status$in ter face  St ructure$is$information structure$Iterative Sealing$	3
ThIS program differs  from earlier work in its almost complete lack of  hand-craft	IS	INTERRUPTION SITE$information status$in ter face  St ructure$is$information structure$Iterative Sealing$	3
The second  experiment investigates a method for unsuper-  vISed learning of gender/number/animaticity  information.	IS	INTERRUPTION SITE$information status$in ter face  St ructure$is$information structure$Iterative Sealing$	3
Our system uses three visual properties to predict IS; we se- lect properties that are known from previous work to help predict whether a landmark will be men- tioned.	IS	INTERRUPTION SITE$information status$in ter face  St ructure$is$information structure$Iterative Sealing$	4
Complex descriptions, however, have a non-trivial IS?	IS	INTERRUPTION SITE$information status$in ter face  St ructure$is$information structure$Iterative Sealing$	4
Freer-word-order languages such as Ger- man also have predictable ISs which have been employed in surface realization systems, but these require a different structural analysis than in English (Zarrie?	IS	INTERRUPTION SITE$information status$in ter face  St ructure$is$information structure$Iterative Sealing$	4
Finally, it makes a the- oretical contribution: By linking the ISs observed in the data to the existing re- 520 search on salience and IS, we show that visually prominent objects are treated as part of common ground despite the lack of pre- vious mention.	IS	INTERRUPTION SITE$information status$in ter face  St ructure$is$information structure$Iterative Sealing$	4
2013), also studying the Wally corpus, demonstrates that visual features affect determiner choice for NPs, but do not study IS.	IS	INTERRUPTION SITE$information status$in ter face  St ructure$is$information structure$Iterative Sealing$	4
Although these aspects of DBG are important in processing written and even transcribed voice messages i n the Air Force and Army messages to which the system has been applied, the MUC-5 application does no t depend heavily on outside information, and is not concerned with evaluation of the information received , and is characterized by more variability in IS across texts .	IS	INTERRUPTION SITE$information status$in ter face  St ructure$is$information structure$Iterative Sealing$	4
J N. and Ratcliff, D., "Generalized IS  for Log-Linear Models", The Annals of Mathematical Statis-  tics, VoL 43, pp 1470-1480,1972.	IS	INTERRUPTION SITE$information status$in ter face  St ructure$is$information structure$Iterative Sealing$	5
We use a set of 30 word pairs from a study carried out by (MC, 1991).	MC	Miller and Charles$misclassified$Major Claim$	0
Additionally, numerous studies (Carnine et al, 1984; MC, 1991; McDonald and Ramscar, 2001) have shown that context plays a vital role in defining the mean- ings of words. (	MC	Miller and Charles$misclassified$Major Claim$	0
the MC study as well as the Rubenstein and Goodenough research.	MC	Miller and Charles$misclassified$Major Claim$	0
MC, 1991) suggest that the cognitive representation of a word is an abstrac- tion derived from its contexts (encountered by the person).	MC	Miller and Charles$misclassified$Major Claim$	0
Using the distributional hypothesis (Harris, 1951), and operationalizing similarity of terms (MC, 1991), it became possible to compute term similarities for a large vocabulary (Ruge, 1992).	MC	Miller and Charles$misclassified$Major Claim$	0
They rank MC (1991)?s set (henceforth ?	MC	Miller and Charles$misclassified$Major Claim$	0
Thus a leaf with the label  inexpens ive  has the total of 4 cases, one of which  is MC.	MC	Miller and Charles$misclassified$Major Claim$	1
The best combination re- sults in 74% of the clusters having no MC  definitions.	MC	Miller and Charles$misclassified$Major Claim$	1
If those MC definitions end  up being used to represent possible sense labels in  WSD, wrong labels might decrease the quality of  the disambiguation stage.	MC	Miller and Charles$misclassified$Major Claim$	1
(4/1), indicates the number of MC cases.	MC	Miller and Charles$misclassified$Major Claim$	1
Test (656 cases):  T F ~ f i e d   293 62 \[Class : T  68 233 IClass : F Errors : 130 (19.8%)  1  Figure 2 An Example of Decision Trees  The two numbers in the brackets denote the  number of cases covered by the branch and the  number of cases being MC respectively:  The results of our experiment will be elaborated  on in future, when we shall also explore the  application of machine learning techniques to  recognizing rhetorical relations on the basis of  discourse markers, and extracting important  sentences from Chinese text.	MC	Miller and Charles$misclassified$Major Claim$	1
They indicate the number of  cases that reach a particular leaf and also the num-  ber of MC cases.	MC	Miller and Charles$misclassified$Major Claim$	1
For instance, the idiom break the ice in Example 7 could be MC as lit- eral due to there being a high relatedness score be- tween ice and snow.	MC	Miller and Charles$misclassified$Major Claim$	1
Figure 2: Typology of Argumentative Structure: Examples of (i) Tree h>1 ; (ii) Chain; (iii) Tree h=1 Feature Group Id Argumentation Feature Description 1 # of Claims AC 2 # of Premises 3,4 # and fraction of sentences containing argument components 5, 6 # and % of supported Claims AR 7, 8 # and % of dangling Claims 9 # of Claims supporting MC 10, 11 # of total Attacks and Attacks against MC 12 # of Argument Chains TS 13 # of Argument Tree h=1 14 # of Argument Tree h>1 Table 1: Argumentation Features or claims.	MC	Miller and Charles$misclassified$Major Claim$	2
MC Claim Premise None MC .675 .132 .148 .045 Claim .025 .552 .338 .086 Premise .014 .163 .754 .069 None .012 .123 .204 .660 Table 3: Confusion probability matrix for argument component annotations (Category ?	MC	Miller and Charles$misclassified$Major Claim$	2
where the root is a claim and the leaves are premises 550 Figure 2: Typology of Argumentative Structure: Examples of (i) Tree h>1 ; (ii) Chain; (iii) Tree h=1 Feature Group Id Argumentation Feature Description 1 # of Claims AC 2 # of Premises 3,4 # and fraction of sentences containing argument components 5, 6 # and % of supported Claims AR 7, 8 # and % of dangling Claims 9 # of Claims supporting MC 10, 11 # of total Attacks and Attacks against MC 12 # of Argument Chains TS 13 # of Argument Tree h=1 14 # of Argument Tree h>1 Table 1: Argumentation Features or claims.	MC	Miller and Charles$misclassified$Major Claim$	2
COR feedback and persistent learning for information extraction.	COR	CORROLA TION$Corrective$Coreference$  Roles Model$	1
3.2479 1.1383 1.0366 3.1029 1.1298 1.0247 0 0.02 0.04 0.06 0.08 0.10.838 0.84 0.842 0.844 0.846 0.848 0.85 0.852 COR Step O?Lo ss     d=110d=220 (a) O-Loss 0 0.02 0.04 0.06 0.08 0.12.15 2.2 2.25 2.3 2.35 2.4 COR Step S?Lo ss     d=110d=220 (b) S-Loss 0 0.02 0.04 0.06 0.08 0.1 1.02 1.025 1.03 1.035 1.04 1.045 1.05 COR Step H?Lo ss     d=110d=220 (c) H-Loss Figure 2: Impact of COR Step ?	COR	CORROLA TION$Corrective$Coreference$  Roles Model$	1
0.8591 0.8428 S-Loss 8.5516 2.8921 2.3190 7.8623 2.8449 2.2812 H-Loss 3.2479 1.1383 1.0366 3.1029 1.1298 1.0247 0 0.02 0.04 0.06 0.08 0.10.838 0.84 0.842 0.844 0.846 0.848 0.85 0.852 COR Step O?Lo ss     d=110d=220 (a) O-Loss 0 0.02 0.04 0.06 0.08 0.12.15 2.2 2.25 2.3 2.35 2.4 COR Step S?Lo ss     d=110d=220 (b) S-Loss 0 0.02 0.04 0.06 0.08 0.1 1.02 1.025 1.03 1.035 1.04 1.045 1.05 COR Step H?Lo ss     d=110d=220 (c) H-Loss Figure 2: Impact of COR Step ?	COR	CORROLA TION$Corrective$Coreference$  Roles Model$	1
view.com/ 409 Table 1: Performance Comparisons (A Smaller Loss Value Means a Better Performance) Metrics Dimensinality=110 Dimensinality=220H-RLS HL-flat HL-SOT H-RLS HL-flat HL-SOT O-Loss 0.9812 0.8772 0.8443 0.9783 0.8591 0.8428 S-Loss 8.5516 2.8921 2.3190 7.8623 2.8449 2.2812 H-Loss 3.2479 1.1383 1.0366 3.1029 1.1298 1.0247 0 0.02 0.04 0.06 0.08 0.10.838 0.84 0.842 0.844 0.846 0.848 0.85 0.852 COR Step O?Lo ss     d=110d=220 (a) O-Loss 0 0.02 0.04 0.06 0.08 0.12.15 2.2 2.25 2.3 2.35 2.4 COR Step S?Lo ss     d=110d=220 (b) S-Loss 0 0.02 0.04 0.06 0.08 0.1 1.02 1.025 1.03 1.035 1.04 1.045 1.05 COR Step H?Lo ss     d=110d=220 (c) H-Loss Figure 2: Impact of COR Step ?	COR	CORROLA TION$Corrective$Coreference$  Roles Model$	1
cs Dimensinality=110 Dimensinality=220H-RLS HL-flat HL-SOT H-RLS HL-flat HL-SOT O-Loss 0.9812 0.8772 0.8443 0.9783 0.8591 0.8428 S-Loss 8.5516 2.8921 2.3190 7.8623 2.8449 2.2812 H-Loss 3.2479 1.1383 1.0366 3.1029 1.1298 1.0247 0 0.02 0.04 0.06 0.08 0.10.838 0.84 0.842 0.844 0.846 0.848 0.85 0.852 COR Step O?Lo ss     d=110d=220 (a) O-Loss 0 0.02 0.04 0.06 0.08 0.12.15 2.2 2.25 2.3 2.35 2.4 COR Step S?Lo ss     d=110d=220 (b) S-Loss 0 0.02 0.04 0.06 0.08 0.1 1.02 1.025 1.03 1.035 1.04 1.045 1.05 COR Step H?Lo ss     d=110d=220 (c) H-Loss Figure 2: Impact of COR Step ?	COR	CORROLA TION$Corrective$Coreference$  Roles Model$	1
4.4 Impact of COR Step ?	COR	CORROLA TION$Corrective$Coreference$  Roles Model$	1
COR analysis refers to the process of  determining whether or not two mentions of entities  refer to the same person (Kibble and Deemter, 2000).	COR	CORROLA TION$Corrective$Coreference$  Roles Model$	2
Algorithms for  Scoring COR Chains.	COR	CORROLA TION$Corrective$Coreference$  Roles Model$	2
COR analysis attempts to decide whether  John Smith and Mr. Smith refer to the same person, and  whether John is also the same person.	COR	CORROLA TION$Corrective$Coreference$  Roles Model$	2
COR as the  Foundations for Link Analysis Over Free Text  Databases.	COR	CORROLA TION$Corrective$Coreference$  Roles Model$	2
In Proceedings of the  Linguistic COR Workshop at The First  International Conference on Language Resources and  Evaluation (LREC'98), pp563-566, 1998.	COR	CORROLA TION$Corrective$Coreference$  Roles Model$	2
A Methodology for  Cross-Document COR.	COR	CORROLA TION$Corrective$Coreference$  Roles Model$	2
239   Cross-Document COR on a Large Scale Corpus        Abstract       In this paper, we will compare and evaluate the  effectiveness of different statistical methods in the  task of cross-document coreference resolution.	COR	CORROLA TION$Corrective$Coreference$  Roles Model$	2
Introduction       COR analysis refers to the process of  determining whether or not two mentions of entities  refer to the same person (Kibble and Deemter, 2000).	COR	CORROLA TION$Corrective$Coreference$  Roles Model$	2
c?2014 Association for Computational Linguistics Linguistic and AF for Automatic Identification of Autism Spectrum Disorders in Children?s Narrative Hiroki Tanaka, Sakriani Sakti, Graham Neubig, Tomoki Toda, Satoshi Nakamura Graduate School of Information Science, Nara Institute of Science and Technology {hiroki-tan, ssakti, neubig, tomoki, s-nakamura}@is.naist.jp Abstract Autism spectrum disorders are develop- mental disorders characterised as	AF	Acoustic Features$adjusted frequency$	0
c?2009 Association for Computational Linguistics On NoMatchs, NoInputs and BargeIns: Do Non-AF Support Anger Detection?	AF	Acoustic Features$adjusted frequency$	0
4 AF: The Speech   Recognition Process  In the last two decades ignificant advances have  been made in the field of automatic speech  recognition (SR), both in commercial and re-  search domains.	AF	Acoustic Features$adjusted frequency$	0
Clas- sifying Subject Ratings of Emotional Speech Using AF.	AF	Acoustic Features$adjusted frequency$	0
75    Automatic Prosodic Labeling with Conditional Random Fields and Rich AF Gina-Anne Levow University of Chicago Department of Computer Science 1100 E. 58th St. Chicago, IL 60637 USA levow@cs.uchicago.edu Abstract Many acoustic approaches to prosodic la- beling in English have employed only lo- cal classifiers, although text-based classifi- cation has employed some sequential mod- els.	AF	Acoustic Features$adjusted frequency$	0
AF  -mean confidence, pmisrecs%l, pmisrecs%2, pmis-  recs%3, pmisrecs%4  ?	AF	Acoustic Features$adjusted frequency$	0
Figure 1: Overall ratio of L?SBarT verbs, presence in SBV and symbol coverage 5.2 Nouns We found that 24 % of the noun lemmas in LBL and SBV lacked symbol coverage, and that there was a wide range in AF, varying from 232.84 down to 1.06.	AF	Acoustic Features$adjusted frequency$	1
Without making any formal categorization, it is clear that the words with highest AF are abstract words, such as sam- band ?	AF	Acoustic Features$adjusted frequency$	1
A heuristic AF estimate is  proposed that, at least for novel-sized texts, is considerably more accurate.	AF	Acoustic Features$adjusted frequency$	1
which had an AF of 105.71 in SBV and a relative frequency of 1.03 ?	AF	Acoustic Features$adjusted frequency$	1
We use a t-score derived from the adjusted frequen-  cies in our corpus to decide whether the prepositional  phrase into Afganistan is attached to the verb (root)  send/V or to the noun (root) soldier/N. In our cor-  pus, soldier/N has an AF of 1488.5, and  send/V has an AF of 1706.5; soldier/N  occurred in 32 distinct preposition contexts, and send/V  in 60 distinct preposition contexts; f(send/V into) = 84,  f(soldier/N into) = 1.5.	AF	Acoustic Features$adjusted frequency$	1
A phrase-based alignment model for NLI.	NLI	natural language inference$native language identification$Natural Language Inference$	0
Ambiguity in word meaning In order for our system to be able to make correct NLI, it must be able to handle paraphrasing and deal with hypernymy.	NLI	natural language inference$native language identification$Natural Language Inference$	0
A large annotated cor- pus for learning NLI.	NLI	natural language inference$native language identification$Natural Language Inference$	0
We re- lease our implementation as the first open-source monolingual aligner, which we hope to be of ben- efit to other researchers in the rapidly expanding area of NLI.	NLI	natural language inference$native language identification$Natural Language Inference$	0
Robust, lexical- ized NLI.	NLI	natural language inference$native language identification$Natural Language Inference$	1
Exploiting parse structures for NLI.	NLI	natural language inference$native language identification$Natural Language Inference$	1
Contrastive analysis and NLI.	NLI	natural language inference$native language identification$Natural Language Inference$	1
A report on the first NLI shared task.	NLI	natural language inference$native language identification$Natural Language Inference$	1
Wilks, Y. (1975a) A Preferential Pattern-Seeking Semantics for NLI.	NLI	natural language inference$native language identification$Natural Language Inference$	2
NLI.	NLI	natural language inference$native language identification$Natural Language Inference$	2
32   Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 521?528 Manchester, August 2008 Modeling Semantic Containment and Exclusion in NLI Bill MacCartney Stanford University wcmac@cs.stanford.edu Christopher D. Manning Stanford University manning@cs.stanford.edu Abstract We propose an approach to natural lan- guage inference based on a model of nat- ural logic, which identifies valid infer- ences by their lexical and syntactic fea- tures, without full semantic interpretation.	NLI	natural language inference$native language identification$Natural Language Inference$	2
WilLS, Y.A. (1975) "A Preferential Pattern-Seeking Semantics for  NLI."	NLI	natural language inference$native language identification$Natural Language Inference$	2
Wilks, Y., A Preferential Pattern-Seeking Semantics  for NLI.	NLI	natural language inference$native language identification$Natural Language Inference$	2
Wilks, Y.A. 1975 A Preferential Pattern-Seeking Semantics for  NLI.	NLI	natural language inference$native language identification$Natural Language Inference$	2
2.2 GLMs We follow the framework of Collins (2002; 2004), recently applied to language modeling in Roark et al. (	GLM	Global Linear Model$Generalised Linear Model$generalised linear models$	0
2.2 A GLM The model used for parsing with this approach is a global linear model.	GLM	Global Linear Model$Generalised Linear Model$generalised linear models$	0
Biconvex functions and possible ap- plications have been well studied in the optimi- sation literature (Quesada and Grossmann, 1995; 4Note that other loss functions could be used here, such as logistic loss for classification, or more generally bilinear variations of GLMs (Nelder and Wed- derburn, 1972).	GLM	Global Linear Model$Generalised Linear Model$generalised linear models$	1
Yogatama et al(2011) proposed a regulariser for GLM that encourages local temporal smoothness.	GLM	Global Linear Model$Generalised Linear Model$generalised linear models$	2
2 3 Models A common approach to regression arises through the application of GLM.	GLM	Global Linear Model$Generalised Linear Model$generalised linear models$	2
Rosti et al (2007) look at sentence-level com- binations (as well as word- and phrase-level), using reranking of n-best lists and confidence scores derived from GLM with probabilistic features from n-best lists.	GLM	Global Linear Model$Generalised Linear Model$generalised linear models$	2
This was partly done by Maeda et al 1988  by means of DRT.	DRT	Discourse Representation Theory$discourse representation theory$	0
Examples are Quasi Logical Forms (Al- shawi, 1990), Dynamic Predicate Logic (Groe- nendijk and Stokhof, 1991), and Underspecified DRT (Reyle, 1993).	DRT	Discourse Representation Theory$discourse representation theory$	0
I. (1998): The dynamic potential of topic and focus: A Praguian approach to DRT.	DRT	Discourse Representation Theory$discourse representation theory$	0
In Groenindijk and Stokhof, editors, Studies  in DRT and the The-  ory of Generalized Quantifiers.	DRT	Discourse Representation Theory$discourse representation theory$	0
For the semantic analysis, a version  of DRT is used which  can express underspecification a d take composi-  tionality into account.	DRT	Discourse Representation Theory$discourse representation theory$	0
Studies in DRT and the  Theory of Generalized Quantifiers, Foris Publica-  tions.	DRT	Discourse Representation Theory$discourse representation theory$	0
Our treatment of discourse refer- ents and accessibility domains is similar to that of DRT (Kamp and Reyle, 1993).	DRT	Discourse Representation Theory$discourse representation theory$	1
\[18\] Schubert, L. K. and Pelletier, F. J. "Generically speak-  ing, or, using DRT to in-  terpret generics."	DRT	Discourse Representation Theory$discourse representation theory$	1
An underspacified seg-  meated DRT (US-  DR'I').	DRT	Discourse Representation Theory$discourse representation theory$	1
From discourse to logic: Introduction to modeltheoretic semantics of natural language, formal logic and DRT.	DRT	Discourse Representation Theory$discourse representation theory$	1
Seg- mented DRT: Dynamic semantics with discourse structure.	DRT	Discourse Representation Theory$discourse representation theory$	1
In terms of DRT, e  is tim discourse referent of which the logical form is a  description.	DRT	Discourse Representation Theory$discourse representation theory$	1
One possibility is that higher ranked retrieved documents are more likely to contain biographical facts, while in later documents it is more likely that AA instances are in fact false positives.	AA	automatically annotated training$an adverb$	0
Bacchiani and Roark (2003) obtained positive results in unsupervised domain adaptation of language models by using a speech recognition system with an out-of-domain language model to produce an AA cor-pus that is used to adapt the language model us-ing a maximum a posteriori (MAP) adaptation strategy.	AA	automatically annotated training$an adverb$	0
They AA data for the test target Donald Trump, thus converting the task into weakly supervised seen target stance detection.	AA	automatically annotated training$an adverb$	0
In (1), so is usually classified as AA  within a sentence, but in (2) so is recognized as  marking a change in message thrust at the  discourse level.	AA	automatically annotated training$an adverb$	1
till), when is AA of time and no nominal element fol- lows.	AA	automatically annotated training$an adverb$	1
This may be done by inserting AA into  the sentence and asking the user whether the meaning  remains unchanged.	AA	automatically annotated training$an adverb$	1
The combination of AA with an adjec-  tive, past participle, or progressive verb is given  score 0.	AA	automatically annotated training$an adverb$	1
Our concern in this  project is to identify so in the discourse sense as in  (2) in contrast to so used as AA in the  sentential sense as in (1).	AA	automatically annotated training$an adverb$	1
where the wildcard can be either an ad- jective or AA.	AA	automatically annotated training$an adverb$	1
The 1 Portions of this article are taken from the paper "MUC-6 : A Brief History", in COLING-96, Proc .	MUC-6	Message Understanding Conference-6$Message Understanding Conference 6$	0
MUC-6: A Brief History.	MUC-6	Message Understanding Conference-6$Message Understanding Conference 6$	0
We have annotated two data sets, one from the Brown corpus and one based on data from the MUC-6 (MUC6).	MUC-6	Message Understanding Conference-6$Message Understanding Conference 6$	1
MUC-6.	MUC-6	Message Understanding Conference-6$Message Understanding Conference 6$	1
c?2010 Association for Computational Linguistics Preferences versus Adaptation during REG Martijn Goudbeek University of Tilburg Tilburg, The Netherlands m.b.goudbeek@uvt.nl Emiel Krahmer University of Tilburg Tilburg, The Netherlands e.j.krahmer@uvt.nl Abstract Current REG algorithms rely on domain dependent pref- erences for both content selection and lin- guistic realization.	REG	Referring Expression Generation$Regression$	0
A Two-tier User Simulation Model for Reinforcement Learning of Adaptive REG Poli- cies.	REG	Referring Expression Generation$Regression$	0
CoNLL 2008: Proceedings of the 12th Conference on Computational Natural Language Learning, pages 151?158 Manchester, August 2008 Trainable Speaker-Based REG Giuseppe Di Fabbrizio and Amanda J. Stent and Srinivas Bangalore AT&T Labs - Research, Inc. 180 Park Avenue Florham Park, NJ 07932, USA {pino,stent,srini}@research.att.com Abstract Previous work in referring expression gen- eration has explored general purpose tech- niques for attribute selection and surface realization.	REG	Referring Expression Generation$Regression$	0
c?2012 Association for Computational Linguistics Learning Preferences for REG: Effects of Domain, Language and Algorithm Ruud Koolen Tilburg University P.O. Box 90135 5000 LE Tilburg The Netherlands r.m.f.koolen@uvt.nl Emiel Krahmer Tilburg University P.O. Box 90135 5000 LE Tilburg The Netherlands e.j.krahmer@uvt.nl Marie?t Theune University of Twente P.O. Box 217 7500 AE Enschede The Netherlands m.theune@utwente.nl Abstract One important sub	REG	Referring Expression Generation$Regression$	0
REG We dis- tinguish three types of referring expressions: common names, familiar names and descriptions.	REG	Referring Expression Generation$Regression$	0
N Table 3: Syllable Organization for the       Logistic REG Model 4.1 Logistic REG Model for Combining Syllables The model to combine syllables is built upon Binary Logistic REG whose answers are either combine or not combine.	REG	Referring Expression Generation$Regression$	1
1095   Combining Prediction by Partial Matching and Logistic REG                for Thai Word Segmentation Ohm Sornil Department of Computer Science National Institute of Development Administration, Bangkok, Thailand osornil@as.nida.ac.th Paweena Chaiwanarom National Statistical Office Bangkok, Thailand paweena@nso.go.th Abstract Word segmentation is an important part of many applications, including information retrieval, information filtering, docum	REG	Referring Expression Generation$Regression$	1
10 0 10 2000.05 0.10.15 0.20.25 0.30.35 Density Amplitude REG    Empirical DistributionSemiparametric FitGamma Fit Figure 1: Empirical distributions of saccade amplitudes in training data for first individual, with fitted Gamma distribu- tions and semiparametric distribution fits.	REG	Referring Expression Generation$Regression$	1
REG modeling.	REG	Referring Expression Generation$Regression$	1
CNs are well analyzed as  noun bigram.	CNs	Compound nouns$common ouns$	0
CNs evaluated:  We prepared two kinds of data: compound nouns that  included correct homophones (correct homophone data  sets) and compound nouns that included wrong  homophones (wrong homophone data sets).	CNs	Compound nouns$common ouns$	0
2 Background 2.1 Compound Noun Interpretation CNs were seminally and thoroughly analysed by Levi (1978), who hand?constructs a nine?way set of semantic relations that she identi- fies as broadly defining the observed relationships between the compound head and modifier.	CNs	Compound nouns$common ouns$	0
1 Introduction CNs are a class of multiword expres- sion (MWE) that have been of interest in recent computational linguistic work, as any task with a lexical semantic dimension (like machine transla- tion or information extraction) must take into ac- count their semantic markedness.	CNs	Compound nouns$common ouns$	0
CNs were reduced to their base nouns, so that ?	CNs	Compound nouns$common ouns$	0
The semantic restriction dictionary:  CNs including all homophones in table 1,  were collected from newspaper a ticles over a 90 day  period, and the semantic restriction dictionary was made  based on the semantic restrictions between the  homophones and the adjoining words in compound  nouns.	CNs	Compound nouns$common ouns$	0
We will have, in particular, the categories  ELECN of regular language CNs,  RREI of regular elation instructions,  EDE$ of regular definition sentences.	CNs	Compound nouns$common ouns$	1
For instance, the tag fam-  ily "MONEY" contains CNs, proper  nouns, adjectives, and adverbs, the semantic  component of whose tags within the ATR Gen-  eral English Tagset, is "money": 500-stock, De-  posit, TOLL-FREE, inexpensively, etc.	CNs	Compound nouns$common ouns$	1
We tried  a total of 5 x 5 x 3 x 3 = 225 group settings for the  four variables (front, rear, during weights and  linking length settings) for each of the three  (CNs, proper nouns and pronoun forms)  term types.	CNs	Compound nouns$common ouns$	1
Investigation was  restricted to the translation of content words:  CNs, verbs, adjectives and adverbs.	CNs	Compound nouns$common ouns$	1
We find that Bayes Point Machines have a good trade-off between perfor- mance and training speed, justifying our repeated usage of BPM in the GA for feature selection.	GA	genetic algorithm$gravitationally$	0
As the train- ing time for BPM is better than CRF, our choice of BPM helped us to run the GA re- peatedly as well.	GA	genetic algorithm$gravitationally$	0
4.2 Combinations of Base Features In order to discover combinations of base features, we implemented a GA (Goldberg, 1989).	GA	genetic algorithm$gravitationally$	0
Since these fea- tures seem unintuitive to the authors, it is likely that they would not have been found without the GA we employed.	GA	genetic algorithm$gravitationally$	0
Moreover, the resolution is implemented with a GA on  its feature selection.	GA	genetic algorithm$gravitationally$	0
Surprisingly, our GA removed features F 10 and F 11, the last two/three let- ters in a token.	GA	genetic algorithm$gravitationally$	0
Exact answer Star, large celestial body composed of GA contained hot gases emitting electromagnetic radiation, especially light, as a result of nuclear reactions inside the star.	GA	genetic algorithm$gravitationally$	1
73chs)  (iv) ~ (in fact), tl l~l~ ~ ~ (authors) ~ r U k'b ~" (it) ~E.9"X: (using), ~ll)'J~ff~l~J~r~: (GA interacting) 3F.~.  "j" ~5 (governing)J ~{tgto (astronomical) ~-gw'~ (about the motion), ?	GA	genetic algorithm$gravitationally$	1
2 The Rule-based Component 2.1 Formal MNS Taken strictly formally, the rule-based component has the form of a restarting automaton with dele- tion (Pla?tek et al, 1995), that is, each rule can be thought of as a finite-state automaton starting from the beginning of the sentence and passing to the right until it finds an input configuration on which it can operate by deletion of some parts of the input.	MNS	Means$MEANS$	0
They compared K-MNS clustering  with Spectral Clustering.	MNS	Means$MEANS$	0
Random K-MNS local  global   CSPA  global   centroid  GS #1 0,387 0,586 0,737 0,741 0,741  GS #2 0,415 0,613 0,765 0,777 0,777  GS #3 0,385 0,609 0,794 0,805 0,809  GS #4 0,399 0,606 0,768 0,776 0,776  Avg.	MNS	Means$MEANS$	0
Assumption 1 (Linear, Rank m, MNS) E[z i |pix(zi),x] = A (z i |z pix(z i ) ,x)pix(zi) ?	MNS	Means$MEANS$	0
This will be of interest not only in deter- mining expected agreement, but also in terms of   -5 0 5 10 15 20 25 30 0 20 40 60 80 100 120 140 160 180 MNS of Annotated Durations N um be r o f A nn ot at ed  D ur at io ns   Figure 2: Distribution of MNS of Annotated  Durations.	MNS	Means$MEANS$	0
These relations specify the role of a concept with  respect o an action (John (AGNT) eats), to a function  (building for (MNS) residence) or to an event (a  delay for (CAUSE) a traffic jam).	MNS	Means$MEANS$	1
\ [ I IUMAN\]  (o I~J!- --lTVO P \]  253  b.  e.  (MNS)-- > \[brain\]  (PURPOSE)-- > \[AIM'\]  while for book would be:  (MNS)<--\[ACT OF COMMUNICATION\]  (OBJ) < --\[MOVE_POSITION\]  Complement.	MNS	Means$MEANS$	1
MNS (MNS) Profits increase investments  3.	MNS	Means$MEANS$	1
--(MNS) < --\[RESIDENCE\]  were I~IIII.I)ING represents the species, or  supertype, and (MNS)<--\[RESIDENCE\] the  differentia.	MNS	Means$MEANS$	1
7 Conclusions We introduced models of MTUs for phrasal systems, and showed that they make a substantial and statistically significant improvement on three distinct language-pairs.	MTUs	Minimal Translation Units$minimal translation units$	0
Model With MTUs, But Decode With Phrases.	MTUs	Minimal Translation Units$minimal translation units$	0
c?2013 Association for Computational Linguistics Model With MTUs, But Decode With Phrases Nadir Durrani?	MTUs	Minimal Translation Units$minimal translation units$	0
Can Markov Models Over MTUs Help Phrase- Based SMT?	MTUs	Minimal Translation Units$minimal translation units$	0
Can Markov Models Over MTUs Help Phrase-Based SMT?	MTUs	Minimal Translation Units$minimal translation units$	0
Task-Specific Alignment Evaluation In this section we evaluate the alignments resulting from using the proposed constraints in two different tasks: Statistical machine translation where alignments are used to restrict the number of possible MTUs; and syntax transfer, where alignments are used to decide how to transfer dependency links.	MTUs	Minimal Translation Units$minimal translation units$	1
Word alignments are used primarily for extracting MTUs for machine translation (MT) (e.g., phrases [Koehn, Och, and Marcu 2003] and rules [Galley et al 2004; Chiang et al 2005]) as well as for ?	MTUs	Minimal Translation Units$minimal translation units$	1
Model with MTUs, but de- code with phrases.	MTUs	Minimal Translation Units$minimal translation units$	1
Therefore, training an n-gram model over MTUs turns out to be a simple and clean choice: the resulting segmen- tation is unique, and the distribution is smooth.	MTUs	Minimal Translation Units$minimal translation units$	1
2013) also present a Markov model based on MPs (they call MTUs) and fur- ther define operation sequence over MPs which are taken as the events in the Markov model.	MTUs	Minimal Translation Units$minimal translation units$	1
The 982 N Matrix types 891 ordinary 390 pos disjunctions 591 Wambaya-specific types 911 Phrase structure rules 83 Lexical rules 161 Lexical entries 1528 Table 2: Size of Wambaya grammar Matrix core types w/ POS types Directly used 132 34% 136 15% Indirectly used 98 25% 584 66% TL types used 230 59% 720 81% Types unused 160 41% 171 19% Types modified 16 4% 16 2% TL 390 100% 891 100% Table 3: Matrix core types used in Wambaya grammar Wambaya grammar includes 891 types defined in the Matrix core type hierarchy.	TL	Total$tweet length$target language$Timeline$target  language$text length$	0
Physics EE TL Stud Tut Stud Tut in addition to 0 0 0 3 3 besides 1 3 0 2 6 another 56 124 10 38 228 especially 0 1 0 0 1 except 0 17 1 1 19 other 107 484 36 59 686 in particular 0 6 0 0 6 such 2 18 3 10 33 unlike 0 0 0 1 1 TL 166 653 50 114 983 Alternative Phrases per Dialog Dialogs 203 203 66 66 269 AP/dialog 0.82 3.22 0.76 1.73 3.65 Alternative Phrases in Queries per Dialog Query APs 51 261 16 86 414 per dialog 0.25 1.29 0.24 1.3 1.54 Table 1: Frequency of Alternative Phrases in Di- alog 5 Evaluation 5.1 Frequency of Alternative Phrases First, it is useful to determine how many queries contain alternative phras	TL	Total$tweet length$target language$Timeline$target  language$text length$	0
5 TL number of pages indexed The total num- ber of the web pages indexed by a search engine varies across time and the numbers provided are somewhat unreliable.	TL	Total$tweet length$target language$Timeline$target  language$text length$	0
This amounts to a per- 47 Task 1 Task 2 R P F R P F Loc 37.9 88.0 53.0 32.8 76.0 45.8 Bind 23.1 48.2 31.2 22.4 47.0 30.3 Expr 63.0 75.1 68.5 63.0 75.1 68.5 Trans 16.8 29.9 21.5 16.8 29.9 21.5 Cata 64.3 81.8 72.0 64.3 81.8 72.0 Phos 78.5 77.4 77.9 69.1 70.1 69.6 TL 48.3 68.9 56.8 46.8 67.0 55.1 Reg 23.7 40.8 30.0 22.3 38.5 28.2 Pos 26.8 42.8 32.9 26.7 42.3 32.7 Neg 27.2 40.2 32.4 26.1 38.6 31.2 TL 26.3 41.8 32.3 25.8 40.8 31.6 TL 36.9 55.6 44.4 35.9 54.1 43.1 Table 3: (R)ecall, (P)recision, and (F)-Score for task 1 and 2 in terms of event types.	TL	Total$tweet length$target language$Timeline$target  language$text length$	0
Physics EE TL Stud Tut Stud Tut in addition to 0 0 0 3 3 besides 1 3 0 2 6 another 56 124 10 38 228 especially 0 1 0 0 1 except 0 17 1 1 19 other 107 484 36 59 686 in particular 0 6 0 0 6 such 2 18 3 10 33 unlike 0 0 0 1 1 TL 166 653 50 114 983 Alternative Phrases per Dialog Dialogs 203 203 66 66 269 AP/dialog 0.82 3.22 0.76 1.73 3.65 Alternative Phrases in Queries per Dialog Query APs 51 261 16 86 414	TL	Total$tweet length$target language$Timeline$target  language$text length$	0
Surface features include TL in words (Tsur et al.,	TL	Total$tweet length$target language$Timeline$target  language$text length$	1
2013) for details of the RepLab series 73 These studies combine Twitter-specific and textual features such as retweet counts, TLs and hashtag frequency, together with sentence-length, character n-grams and punctuation counts.	TL	Total$tweet length$target language$Timeline$target  language$text length$	1
Context Similarity: f2(mi, ei) = coocurence number TL (3) where: coccurence number is the the number of the words that occur in both the tweet containing mi and the Wikipedia page of ei; TL denotes the number of tokens of the tweet containing mention mi.	TL	Total$tweet length$target language$Timeline$target  language$text length$	1
The regression models use both binary presence-of feature classes (quotation; past, present tense; 16 types of discourse relations; 10 NE types; 3 hashtag positions) as well as normalized numeric features (TL, hashtag count, sentence sim- ilarity, 3 sentiment polarity strengths).	TL	Total$tweet length$target language$Timeline$target  language$text length$	1
They found similar language usage trends for both genders, with increasing word and TL with age, and an increasing tendency to write more grammatically correct, standardized 844 text.	TL	Total$tweet length$target language$Timeline$target  language$text length$	1
2008) used the sentence-pair confidence scores estimated with source and TL mod- els to weight phrase translation pairs.	TL	Total$tweet length$target language$Timeline$target  language$text length$	2
alignment, the correspondence between words in source and TLs.	TL	Total$tweet length$target language$Timeline$target  language$text length$	2
Background A word alignment for a parallel sentence pair represents the correspondence between words in a source language and their translations in a TL (Brown et al 1993b).	TL	Total$tweet length$target language$Timeline$target  language$text length$	2
Each observation corresponds to a word in the TL xi.	TL	Total$tweet length$target language$Timeline$target  language$text length$	2
This system uses a word-aligned corpus and a parser for a resource-rich language (source language) in order to create a parser for a resource-poor language (TL).	TL	Total$tweet length$target language$Timeline$target  language$text length$	2
A rare word in the source language links to many words in the TL that we would ideally like to see unaligned, or aligned to other words in the sentence.	TL	Total$tweet length$target language$Timeline$target  language$text length$	2
For each such edge, if both end points are aligned to words in the TL, then the edge is transferred.	TL	Total$tweet length$target language$Timeline$target  language$text length$	2
using the proposed method is available at http://mednlp.jp/influ/.    Figure 8: The TL of Influenza Epidemics in Fukushima.	TL	Total$tweet length$target language$Timeline$target  language$text length$	3
TL visualization in XOpin    The comparison view (Figure 4) allows the user  to compare side by side different product features  in a collection of texts.	TL	Total$tweet length$target language$Timeline$target  language$text length$	3
Detections, Bounds, and TLs: UMass and TDT-3.	TL	Total$tweet length$target language$Timeline$target  language$text length$	3
TL: A dynamic hierarchical Dirichlet process model for re- covering birth/death and evolution of topics in text stream.	TL	Total$tweet length$target language$Timeline$target  language$text length$	3
12) TLss We estimate timeliness using the time- based language models ?	TL	Total$tweet length$target language$Timeline$target  language$text length$	3
c?2012 Association for Computational Linguistics Extracting Narrative TLs as Temporal Dependency Structures Oleksandr Kolomiyets KU Leuven Celestijnenlaan 200A B-3001 Heverlee, Belgium Oleksandr.	TL	Total$tweet length$target language$Timeline$target  language$text length$	3
A post-edlted system  res igns  i t se l f  f rom the start  to inadequacy,   bu i ld ing  in the requ i rement  for (more or less)  radical human revis ion of its output, so that it  might  bet ter  be ca l led  pro - t rans la t ion  than  t rans la t ion  proper;  wh i le  many cur rent  pro-  ed i t ing  sys tems,  a l though o f fe r ing  fu l l y   automat ic  p roduct ion  of TL text  f rom source language text, requ i re  a human  contribution in the pre-input stage, control l ing  and res t r i c t ing  that  source  text,  wh ich   qua l i ta t ive ly  far exceeds the demands of on- l ine  interaction (fn 3).	TL	Total$tweet length$target language$Timeline$target  language$text length$	4
But progress  has  been  slower  on  translation models  that  are  able  to  learn  the  rela- tionship  between  the  grammars  of  both the source and TL.	TL	Total$tweet length$target language$Timeline$target  language$text length$	4
We used only the target side of the bilingual corpus for the TL  model,  rather  than  the  larger supplied  language  model.	TL	Total$tweet length$target language$Timeline$target  language$text length$	4
Malay has  very simple and regular question structure, the  question words appear at the front of question sen- tences (in the same way as the TL) and  do not take any other function in the language (un- like the English ?	TL	Total$tweet length$target language$Timeline$target  language$text length$	4
With source-language LUs replaced by unique  mul t i i i ngua l -d ic t ionary  addresses, th is  canonical   representat ion  is the In ter l ingua  which is passed  for  synthesis  in to  the TL(s~.  C. Synthesis  Assuming the analys is  has been cor rec t ly   per formed,  synthesis  is a relatively straight-  forward determin is t i c  process.	TL	Total$tweet length$target language$Timeline$target  language$text length$	4
In this work we treat the process of translit- eration as a process of direct  transduction from  sequences of tokens in the source language to  sequences of tokens in the target language with  no modeling of the phonetics of either source or  TL (Knight and Graehl, 1997).	TL	Total$tweet length$target language$Timeline$target  language$text length$	4
In the example from Table 1, conTL is captured by the row  marginals, e.g. the conTL for Context1 is  3, which means that	TL	Total$tweet length$target language$Timeline$target  language$text length$	5
Thus, a conTL could be very small compared to the size of the feature set.	TL	Total$tweet length$target language$Timeline$target  language$text length$	5
To simu- late the structure of the observed data, the fol- lowing features are to be emulated: (a) ConTL is the number of features  that can occur in a context.	TL	Total$tweet length$target language$Timeline$target  language$text length$	5
nTL is the number of features  that can occur in a context.	TL	Total$tweet length$target language$Timeline$target  language$text length$	5
In the example from Table 1, conTL is captured by the row  marginals, e.g. the conTL for Context1 is  3, which means that overall there are only three features for that context.	TL	Total$tweet length$target language$Timeline$target  language$text length$	5
(b) Sparsity is a consequence of relatively small conTL.	TL	Total$tweet length$target language$Timeline$target  language$text length$	5
Addi- tionally, conTL is influenced by the fea- ture selection method ?	TL	Total$tweet length$target language$Timeline$target  language$text length$	5
LSH provides a way of dealing with this problem: instead of retaining the explicit sparse high-dimensional ~ci, we use a ran- dom projection h(?)	LSH	Locality sensitive hashing$Locality Sensitive Hashing$	0
LSH: A comparison of hash function types and querying mechanisms.	LSH	Locality sensitive hashing$Locality Sensitive Hashing$	0
c?2011 Association for Computational Linguistics Efficient Online LSH via Reservoir Counting Benjamin Van Durme HLTCOE Johns Hopkins University Ashwin Lall Mathematics and Computer Science Denison University Abstract We describe a novel mechanism called Reser- voir Counting for application in online Local- ity Sensitive Hashing.	LSH	Locality sensitive hashing$Locality Sensitive Hashing$	1
s. 51 LSH, at similar levels of accu- racy, by replacing explicit 32-bit counting variables with approximate counters of smaller size.	LSH	Locality sensitive hashing$Locality Sensitive Hashing$	1
With regard to context representation, it is also intriguing to explore other dimensionality reduc- tion methods (such as LSH or Random Indexing) and to compare them to the SVD-based model.	LSH	Locality sensitive hashing$Locality Sensitive Hashing$	1
3 Randomized Model Now situated within a streaming context we exact space savings through approximation, extending the approach of Van Durme and Lall (2011), there con- cerned with online LSH, here initially concerned with taking averages.	LSH	Locality sensitive hashing$Locality Sensitive Hashing$	1
3.3 LSH As described in Section 3.2, we find paraphrases of a phrase pi by finding its nearest neighbors based on cosine similarity between the feature vector of pi and other phrases.	LSH	Locality sensitive hashing$Locality Sensitive Hashing$	1
8 add d to inverted index 9 end 2.2 LSH The problem of finding the nearest neighbor to a given query has been intensively studied, but as the dimensionality of the data increases none of the cur- rent solutions provide much improvement over a sim- ple linear search (Datar et al, 2004).	LSH	Locality sensitive hashing$Locality Sensitive Hashing$	1
Thus  P(sp, dola, M) = P(sp, d.la)  Then we combine sp and de into one vari-  able dIt, Hobbs distance, since the Hobbs  algorithm takes both the SYN and dis-  tance into account.	SYN	SYNONYMY$syntax$synonyms$SYNTAX$	1
We now turn to briefly present the SYN of adjectives in Catalan and discuss the parameters in more detail.	SYN	SYNONYMY$syntax$synonyms$SYNTAX$	1
However, any SYN-only pronoun resolution  strategy will be wrong some of the time - these  methods know nothing about discourse bound-  aries, intentions, or real-world knowledge.	SYN	SYNONYMY$syntax$synonyms$SYNTAX$	1
Shared story reading with parents or teachers helps children to learn about vocabulary, SYN and phonology, and to develop narrative comprehension and awareness of the concepts of print, all of which are linked to developing reading and writing skills (National Early Literacy Panel 2008).	SYN	SYNONYMY$syntax$synonyms$SYNTAX$	1
We also note that the very simple model that  ignores SYN and takes the last mentioned  noun-phrase as the referent performs quite a  bit worse, about 43% correct.	SYN	SYNONYMY$syntax$synonyms$SYNTAX$	1
This indicates  that SYN does play a very important role in  anaphora resolution.	SYN	SYNONYMY$syntax$synonyms$SYNTAX$	1
Yu et al (2002) showed that the Introduction defines the majority of SYN, while Schuemie et al (2004) and Shah et al. (	SYN	SYNONYMY$syntax$synonyms$SYNTAX$	2
The specific ontological in- formation extracted is the type hierarchy and sets of SYN (AKA, or ?	SYN	SYNONYMY$syntax$synonyms$SYNTAX$	2
Further, it identi- fies a range of NLP tools required, including: identifying SYN, and resolving coref- erence and negated expressions.	SYN	SYNONYMY$syntax$synonyms$SYNTAX$	2
We believe it would be quite difficult  to achieve the same accuracy by compiled  knowledge, such as a dictionary of verbs, anto- nyms, SYN, and relation words, and a the- saurus.	SYN	SYNONYMY$syntax$synonyms$SYNTAX$	2
The questions about SYN ask relations  of priority/inferiority between words and choos- ing the word in a different group.	SYN	SYNONYMY$syntax$synonyms$SYNTAX$	2
5.1 Synonym Facts The frequent use of SYN, abbreviations and acronyms in biomedical text is a common source of ambiguity that is often hard to resolve (Sehgal et al, 2004).	SYN	SYNONYMY$syntax$synonyms$SYNTAX$	2
We assume this occurs be- cause Spanish is one of the six official United Na- tions languages, and the GT engine is using the United Nations parallel corpus to train their translation engine, therefore implying that a better quality translation is achieved as compared to the one available for Romanian.	GT	Google translation$Google Translate$glottal$Generalized Transformation$	0
For  the NEs not covered by our dictionary, we  use GT service as a back-up.	GT	Google translation$Google Translate$glottal$Generalized Transformation$	0
In our  experiments we identify SL (Chinese) NEs                                                    2 We also try online GT service, and the  performance was roughly the same.	GT	Google translation$Google Translate$glottal$Generalized Transformation$	0
The number of languages currently covered by the GT system (41 language) is smaller than the number of languages in which there exist Wikipedia articles (265 languages).	GT	Google translation$Google Translate$glottal$Generalized Transformation$	0
However, we believe that using for cross-lingual analysis de- scriptions only in those languages that can be han- dled by the GT system does not af- fect the generality of our conclusions.	GT	Google translation$Google Translate$glottal$Generalized Transformation$	0
However, we searched for 20 of the 57 English words for which the workers agreed upon a manually entered Russian translation in Google translate, and we found that the Russian translation was the top GT for only 11 of the 20 English words.	GT	Google translation$Google Translate$glottal$Generalized Transformation$	0
2 We also try online GT service, and the  performance was roughly the same.	GT	Google translation$Google Translate$glottal$Generalized Transformation$	0
All the sentences were automatically  translated into Chinese sentences by using the  GT service.	GT	Google translation$Google Translate$glottal$Generalized Transformation$	1
Finally, the sentences in the English summary  are translated into the corresponding Chinese  sentences by using GT, and the  Chinese summary is formed.	GT	Google translation$Google Translate$glottal$Generalized Transformation$	1
GT  is one of the state-of-the-art commercial machine  translation systems used today.	GT	Google translation$Google Translate$glottal$Generalized Transformation$	1
MT features are  not used because GT is used as a black box.	GT	Google translation$Google Translate$glottal$Generalized Transformation$	1
by using GT is ?????	GT	Google translation$Google Translate$glottal$Generalized Transformation$	1
In this study, we adopt GT1 for  English-to-Chinese translation.	GT	Google translation$Google Translate$glottal$Generalized Transformation$	1
We can easily set  up o as an allophone Of u before k. Only the case of  GT stop needs to be considered.	GT	Google translation$Google Translate$glottal$Generalized Transformation$	2
The lack of articulatory control also leads to various involuntary non-speech sounds including velopharyngeal or GT noise (Rosen and Yampolsky, 2000).	GT	Google translation$Google Translate$glottal$Generalized Transformation$	2
So we revise the  form, replacing $V with just the vowels in question,  and replacing the $C df the coda with apostrophe (for  GT stop).	GT	Google translation$Google Translate$glottal$Generalized Transformation$	2
For example, there are many  possible places of articulation, which form a near-  continuum ranging from \[labial\] to \[GT\].	GT	Google translation$Google Translate$glottal$Generalized Transformation$	2
3 Tree Search vs. Dynamic  P rogramming  Once an appropriate function for measuring simi-  larity between pairs of segments has been designed,  290  Feature Phonological Numerical  name term value  Place  Manner  High  Back  \[bilabial\]  \[labiodental\]  \[dental\]  \[alveolar\]  \[retroflex\]  \[palato-alveolar\]  \[palatal\]  \[velar\]  \[uvular\]  \[pharyngeal\]  \[GT\]  \[stop\]  \[affricate\]  \[fricative\]  \[approximant\]  \[high vowel\]  \[mid vowel\]  \[low vowel\]  \[high\]  \[mid\]  \[low\]  \[front\]  \[central\]  \[back\]  1.0  0.95  0.9  0.85  0.8  0.75  0.7  0.6  0.5  0.3  0.1  1.0  0.9  0.8  0.6  0.4  0.2  0.0  1.0  0.5  0.0  1.0  0.5  0.0  Table 3: Multivalued features and their values.	GT	Google translation$Google Translate$glottal$Generalized Transformation$	2
Suppose we wish to identify the minimal pairs for o /u   discussed above, but without having to specify GT  stop in the query, as shown in Figure 3.	GT	Google translation$Google Translate$glottal$Generalized Transformation$	2
The 4Substitution connects the root node of one elementary tree in an empty slot in another elementary tree, similarly to a GT of Chomsky (1955/75) or Chomsky (1995), Ch.3.	GT	Google translation$Google Translate$glottal$Generalized Transformation$	3
Some of the cases can be explained by differences in where an attribute is marked: For example, for definiteness the performance is 1% from English to BG, as BG marks definiteness on nouns and adjectives rather than on determiners.	BG	Bulgarian$Background$background$	0
Figure 10 shows our results for transferring from English to BG (En?Bg) and from English to Spanish (En?Es).	BG	Bulgarian$Background$background$	0
However, there are also some cases of unrelated source languages performing best: Using Danish as source language gives the highest performing models for both BG and Czech.	BG	Bulgarian$Background$background$	0
In our experiments we learn taggers for a set of 11 European lan- guages that have both UD training data with mor- phological features, and parallel data in Europarl: BG, Czech, Danish, Dutch, Finnish, Ital- ian, Polish, Portuguese, Slovene, Spanish and Swedish.	BG	Bulgarian$Background$background$	0
We generated supervised parses using the first-order model from the MST parser (McDonald, Crammer, and Pereira 2005) trained on the Penn Treebank for English and the CoNLL X parses for BG and Spanish.	BG	Bulgarian$Background$background$	0
One admirable standardization effort in the field of Slavic part of speech (POS) tagging has been the Multext-East project (Erjavec, 2001), one of whose aims was to construct mutually compati- ble tagsets for 8 European languages, including 4 Slavic languages (originally BG, Czech and Slovene, later extended to Croatian); additionally, a Multext-East-style tagset for Russian was con- structed at the University of T?bingen (http: //www.sfb441.uni-tuebingen.de/c1/ tagset.html).	BG	Bulgarian$Background$background$	0
To bet- ter analyze the influence of the background in- formation, all automatic summarization methods are based on the up-to-date LSA method previ- ously described: one taking as input only the news story to be summarized (Simple) and used as base- line; other taking as input only the selected back- ground information (BG only); and, the last one, using both the news story and the back- ground information (BG + News).	BG	Bulgarian$Background$background$	1
BG  Semantic Extraction has become a strong  research focus in the last few years.	BG	Bulgarian$Background$background$	1
2 BG  Two broad approaches have dominated the lit- erature on constructing paraphrase corpora.	BG	Bulgarian$Background$background$	1
the influence of the background in- formation, all automatic summarization methods are based on the up-to-date LSA method previ- ously described: one taking as input only the news story to be summarized (Simple) and used as base- line; other taking as input only the selected back- ground information (BG only); and, the last one, using both the news story and the back- ground information (BG + News).	BG	Bulgarian$Background$background$	1
2 BG 2.1 The LinGO Grammar Matrix The LinGO Grammar Matrix is situated theoreti- cally within Head-Driven Phrase Structure Gram- mar (HPSG; Pollard and Sag, 1994), a lexicalist, constraint-based framework.	BG	Bulgarian$Background$background$	1
BG A word alignment for a parallel sentence pair represents the correspondence between words in a source language and their translations in a target language (Brown et al 1993b).	BG	Bulgarian$Background$background$	1
Instead of using the traditional output of the ASR module, we use the phonetic transliteration of the output and compare it to the phonetic transliteration of solid BG infor- mation.	BG	Bulgarian$Background$background$	2
We use broadcast news as a case study and news stories from online newspapers provide the BG information.	BG	Bulgarian$Background$background$	2
In this work, we explore the possibilities offered by pho- netic information to select the BG information and conduct a perceptual evaluation to better assess the relevance of the inclusion of that infor	BG	Bulgarian$Background$background$	2
2 provides BG on the Grammar Ma- trix and Wambaya, and situates the project with re- spect to related work. ?	BG	Bulgarian$Background$background$	2
We propose the inclusion of related, solid BG information to cope with the difficulties of summarizing spoken language and the use of multi-document summarization techniques in single document speech- to-text summarization.	BG	Bulgarian$Background$background$	2
In this work, we explore the possibilities offered by pho- netic information to select the BG information and conduct a perceptual evaluation to better assess the relevance of the inclusion of that information.	BG	Bulgarian$Background$background$	2
Furthermore, we build on the conjecture that this BG infor- mation is often used by humans to overcome per- ception difficulties.	BG	Bulgarian$Background$background$	2
Aspects  of metaphor constitution, of the role of visual  language, as well as of the understanding of feedback mechanisms, and the role of back  channels were pushed into the BG.	BG	Bulgarian$Background$background$	2
On Re- verse Feature Engineering of STKs.	STK	Syntactic Tree Kernel$Syntactic Tree Kernels$	0
2.2 STK Syntactic tree kernels were first introduced by Collins and Duffy (2001) and were also used by 401 SNP NP DT A NN cat PP IN with NP DT a JJ red NN collar VP AUX was VP VBD chased ADVP NP CD two NNS days RB ago PP IN by NP DT a JJ fat NN dog Figure 3: Syntactic parse tree of the sentence shown in Figure 1 (b).	STK	Syntactic Tree Kernel$Syntactic Tree Kernels$	0
c?2010 Association for Computational Linguistics On Reverse Feature Engineering of STKs Daniele Pighin FBK-irst, DISI, University of Trento Via di Sommarive, 14 I-38123 Povo (TN) Italy daniele.pighin@gmail.com Alessandro Moschitti DISI, University of Trento Via di Sommarive, 14 I-38123 Povo (TN) Italy moschitti@disi.unitn.it Abstract In this paper, we provide a theoretical framework for feature selection in tree ker- nel spaces based on gradient-vector com- pon	STK	Syntactic Tree Kernel$Syntactic Tree Kernels$	0
STK (K1), also known as a subset tree kernel (Collins and Duffy, 2002), maps ob- jects in the space of all possible tree fragments constrained by the rule that the sibling nodes cannot be separated from their parents.	STK	Syntactic Tree Kernel$Syntactic Tree Kernels$	0
The most general kind of kernels used in NLP are string kernels, e.g. (Shawe-Taylor and Cristianini, 2004), the STKs (Collins and Duffy, 2002) and the Partial Tree Kernels (Moschitti, 2006a).	STK	Syntactic Tree Kernel$Syntactic Tree Kernels$	0
We consider three systems which are reported to perform efficiently and effectively on processing syntactic trees using three proposed ap- proaches STK (Moschitti, 2006), Syntactic Generalization (Galitsky, 2013) and Dis- tributed Tree Kernel (Zanzotto and Dell?Arciprete, 2012).	STK	Syntactic Tree Kernel$Syntactic Tree Kernels$	0
On Re- verse Feature Engineering of STK.	STK	Syntactic Tree Kernel$Syntactic Tree Kernels$	1
2 Semantic STK In kernel-based methods, both learning and classi- fication only depend on the inner product between instances.	STK	Syntactic Tree Kernel$Syntactic Tree Kernels$	1
The most general kind of kernels used in NLP are string kernels, e.g. (Shawe-Taylor and Cristianini, 2004), the STK (Collins and Duffy, 2002) and the Partial Tree Kernels (Moschitti, 2006a).	STK	Syntactic Tree Kernel$Syntactic Tree Kernels$	1
c?2010 Association for Computational Linguistics On Reverse Feature Engineering of STK Daniele Pighin FBK-irst, DISI, University of Trento Via di Sommarive, 14 I-38123 Povo (TN) Italy daniele.pighin@gmail.com Alessandro Moschitti DISI, University of Trento Via di Sommarive, 14 I-38123 Povo (TN) Italy moschitti@disi.unitn.it Abstract In this paper, we provide a theoretical framework for feature selection in tree ker- nel spaces based on gradient-vector com- pon	STK	Syntactic Tree Kernel$Syntactic Tree Kernels$	1
SWNt demonstrates quite an unsatisfactory performance, while SentiStrength, being very preci	SWN	SentiWordNe$SentiWordNet$	0
SWNt (Esuli and Sebastiani, 2006), SO- CAL (Taboada et al2010) and SentiStrength (Thelwall et al2012).	SWN	SentiWordNe$SentiWordNet$	0
SWNt demonstrates quite an unsatisfactory performance, while SentiStrength, being very precise, has an insufficient scope and, therefore, finds no sentiment in a substantial number of documents.	SWN	SentiWordNe$SentiWordNet$	0
Since SWNt entries are associated with word senses and because we don?t perform word sense disambiguation, the SWNt po- larity of the most dominant word sense is used for words in the comment section.	SWN	SentiWordNe$SentiWordNet$	0
First, we evaluate two methods to automatically determine the comment polarity: SWNt (Baccianella and Sebastiani, 2010) a general purpose resource that assigns sen- timent scores to entries in WordNet, and an auto- 12 mated corpus-specific technique based on pointwise mutual information.	SWN	SentiWordNe$SentiWordNet$	0
It incorporates a rich fea- ture set, relying on the usage of SWNt (Esuli et al, 2010) and further orthological, morphological and syntactic features.	SWN	SentiWordNe$SentiWordNet$	0
3.1 SWNt In the first stage of the study, we use SentiWord- Net (Baccianella and Sebastiani, 2010) which as- sociates a large number of words in WordNet with a positive, negative and objective score (summing up to 1).	SWN	SentiWordNe$SentiWordNet$	0
SWN demonstrates quite an unsatisfactory performance, while SentiStrength, being very preci	SWN	SentiWordNe$SentiWordNet$	1
SWN (Esuli and Sebastiani, 2006), SO- CAL (Taboada et al2010) and SentiStrength (Thelwall et al2012).	SWN	SentiWordNe$SentiWordNet$	1
SWN demonstrates quite an unsatisfactory performance, while SentiStrength, being very precise, has an insufficient scope and, therefore, finds no sentiment in a substantial number of documents.	SWN	SentiWordNe$SentiWordNet$	1
Since SWN entries are associated with word senses and because we don?t perform word sense disambiguation, the SWN po- larity of the most dominant word sense is used for words in the comment section.	SWN	SentiWordNe$SentiWordNet$	1
First, we evaluate two methods to automatically determine the comment polarity: SWN (Baccianella and Sebastiani, 2010) a general purpose resource that assigns sen- timent scores to entries in WordNet, and an auto- 12 mated corpus-specific technique based on pointwise mutual information.	SWN	SentiWordNe$SentiWordNet$	1
It incorporates a rich fea- ture set, relying on the usage of SWN (Esuli et al, 2010) and further orthological, morphological and syntactic features.	SWN	SentiWordNe$SentiWordNet$	1
3.1 SWN In the first stage of the study, we use SentiWord- Net (Baccianella and Sebastiani, 2010) which as- sociates a large number of words in WordNet with a positive, negative and objective score (summing up to 1).	SWN	SentiWordNe$SentiWordNet$	1
Thus, in many cases two Ns are given, with the larger number the numbe r of templates scored and the smaller number the number of individual template scores used in estimating the variance, calculatin g the standard error and CI, and performing statistical tests .	CI	confidence intervals$confidence Interval$Campus Indoor$conventional implicatures$	0
The probabilities are accompanied by 95% CI, which are computed from the standard deviation of the binomial distribution.	CI	confidence intervals$confidence Interval$Campus Indoor$conventional implicatures$	0
la ck lef left jus just wit with goin going doin doing kno know 60 62 64 66 68 70 72 74 %  w hi te lef left jus just wit with goin going doin doing kno know 14 16 18 20 22 24 %  h isp an ic lef left jus just wit with goin going doin doing kno know 2000 4000 6000 8000 10000 12000 14000 16000 po p.  d en sit y Figure 1: Average demographics of the counties in which users of each term live, with 95% CI 16 sonant cluster reduction examples are indeed pre- ferred by authors from densely-populated (urban) counties with more African Americans, although these counties tend to prefer all of the non-standard variants, including the control pair kno/know.	CI	confidence intervals$confidence Interval$Campus Indoor$conventional implicatures$	0
.038 Table 1: Classification results from a 10-fold cross-validation experiment on ETP-gold with 95% CI.	CI	confidence intervals$confidence Interval$Campus Indoor$conventional implicatures$	0
The two systems were also re- ported to have overlapping CI in the shared task.	CI	confidence intervals$confidence Interval$Campus Indoor$conventional implicatures$	0
coefficients with their 95% CI.	CI	confidence intervals$confidence Interval$Campus Indoor$conventional implicatures$	0
Our good advice went unheeded for a long time but in recent work by Christopher Potts (2004) we see an attempt to build the sort of two-dimensional semantics Stanley and I sketched out that separates CI from truth-conditional aspects of meaning.	CI	confidence intervals$confidence Interval$Campus Indoor$conventional implicatures$	3
In contrast, CI arise from the meaning of the uttered sentence and the maxims of communication, without any influence from the interactional situation.	CI	confidence intervals$confidence Interval$Campus Indoor$conventional implicatures$	3
By adding the extra utterance to the initial  theory (7), uttered(went(ail(boys),theatre)), one  would obtain one optimistic model schema in which  the CI have been cancelled  (see figure 5).	CI	confidence intervals$confidence Interval$Campus Indoor$conventional implicatures$	3
4 An alternative might be to retreat from the notion  of meaning postulates per se, and view them instead  as some form of CI which are  "usually" or "often" true.	CI	confidence intervals$confidence Interval$Campus Indoor$conventional implicatures$	3
2.3 Conversational Implicatures5 Authors can be held responsible for more than just assertions and CI.	CI	confidence intervals$confidence Interval$Campus Indoor$conventional implicatures$	3
Conventional impli- catures are a rich source of information for IE tasks because the material presented in them is supposed 4For more on CI, see e.g. Karttunen and Peters (1979) and Potts (2005) to be non-controversial.	CI	confidence intervals$confidence Interval$Campus Indoor$conventional implicatures$	3
Second, we use two graph-based summarization approaches, Generalized CWS and Page- Rank, to extract sentences as summaries.	CWS	ClueWordSummarizer$Chinese word segmentation$	0
We apply two summarization algorithms, Generalized CWS and Page-Rank to rank nodes in the sentence quotation graph and to select the corresponding most highly ranked sen- tences as the summary.	CWS	ClueWordSummarizer$Chinese word segmentation$	0
tagging approach has been widely used in CWS recently (Xue and Shen, 2003; Peng and McCal- lum, 2004; Tseng et al, 2005).	CWS	ClueWordSummarizer$Chinese word segmentation$	1
The EM-like method for learning dependency  relations described in Section 3.3 has also been  applied to other tasks such as hidden Markov model  training (Rabiner, 1989), syntactic relation learning  (Yuret, 1998), and CWS  (Gao et al, 2002a).	CWS	ClueWordSummarizer$Chinese word segmentation$	1
Section 5 describes current state- of-the-art methods for CWS.	CWS	ClueWordSummarizer$Chinese word segmentation$	1
Integrated CWS in statistical machine translation.	CWS	ClueWordSummarizer$Chinese word segmentation$	1
Word lattice reranking for CWS and part-of-speech tagging.	CWS	ClueWordSummarizer$Chinese word segmentation$	1
Do we need CWS for statistical machine translation?	CWS	ClueWordSummarizer$Chinese word segmentation$	1
INESC-ID / Instituto Superior T?enico, Lisboa, Portugal nlp2ct.samuel@gmail.com, {lidiasc, derekfw}@umac.mo, isabel.trancoso@inesc-id.pt,tianliang0123@gmail.com Abstract This study investigates on building a better CWS mod- el for statistical machine translation.	CWS	ClueWordSummarizer$Chinese word segmentation$	1
= argmax c?C p(c|x) The maxent classifiers are implemented with the toolkit of Zhang Le (2004), and the parameters of the model are estimated using Generalized IIS (Darroch and Ratcli, 1972).	IIS	Iterative Scaling$improved iterative scaling$	0
Generalised IIS (GIS) is used to esti- mate the values of the weights and we use a Gaus- sian prior over the weights (Chen and Rosenfeld, 1999) which allows many rare, but informative, features to be used without overfitting.	IIS	Iterative Scaling$improved iterative scaling$	0
Generalized  IIS for Log-linear Models.	IIS	Iterative Scaling$improved iterative scaling$	0
We have also experimented with Non-negative Matrix Factorization (NMF) (Lee and Seung 1999), Probabilistic Latent Semantic Analysis (PLSA) (Hofmann 1999), Kernel Principal Com- ponents Analysis (KPCA) (Scholkopf, Smola, and Muller 1997), and IIS (IS) (Ando 2000).	IIS	Iterative Scaling$improved iterative scaling$	0
The classifier.maxent module defines the maximum entropy model for text classification, and implements two algorithms for training the model: Generalized IIS and Improved IIS.	IIS	Iterative Scaling$improved iterative scaling$	0
Maximum-likelihood parameter values can be estimated using Generalized IIS (Ratnaparkhi 96), or gradient descent methods.	IIS	Iterative Scaling$improved iterative scaling$	0
The IIS algorithm: A gentle introduction.	IIS	Iterative Scaling$improved iterative scaling$	1
for example, generalized or IIS (Berger, Della Pietra, and 4 It might seem to be a restriction to have the hyperplane passing through the origin of the space.	IIS	Iterative Scaling$improved iterative scaling$	1
Once the set of features functions are selected, algorithm such as IIS (Berger et al, 1996) or sequential conditional generalized iterative scal- ing (Goodman, 2002) can be used to find the optimal parameter values of {?	IIS	Iterative Scaling$improved iterative scaling$	1
, An are Lagrange multipliers  that impose the constraints corresponding to the  chosen features f l ,  ..-,fn- The term Z(x) normal-  izes the probabilities by summing over all possible  outcomes y. Berger et al (1996) demonstrate that  the optimal values for the Ai's can be obtained by  maximizing the likelihood of the training data with  respect o the model, which can be performed using  their IIS algorithm.	IIS	Iterative Scaling$improved iterative scaling$	1
cessing, two aspects of  CRFs have been investigated sufficiently: one is to  apply it to new tasks, such as named entity recog- nition (McCallum and Li, 2003; Li and McCallum,  2003; Settles, 2004), part-of-speech tagging (Laf- ferty et al, 2001), shallow parsing (Sha and Perei- ra, 2003), and language modeling (Roark et al,  2004); the other is to exploit new training methods  for CRFs, such as IIS (Laf- ferty et al, 2001), L-BFGS (McCallum, 2003) and  gradient tree boosting (Dietterich et al, 2004).	IIS	Iterative Scaling$improved iterative scaling$	1
The IIS technique  (Della Pietra et al, 1997) was used to train the  parameters in the ME model.	IIS	Iterative Scaling$improved iterative scaling$	1
ACM CSUR, 34(1):1?47.	CSUR	computing surveys$Computing Surveys$	0
ACM CSUR, 24(4).	CSUR	computing surveys$Computing Surveys$	0
Neural CSUR, 3:157?195.	CSUR	computing surveys$Computing Surveys$	0
ACM CSUR, 24(4):377?439.	CSUR	computing surveys$Computing Surveys$	1
ACM  CSUR, 12, 213-253.	CSUR	computing surveys$Computing Surveys$	1
ACM CSUR, 41:10:1?10:69, February.	CSUR	computing surveys$Computing Surveys$	1
Kukich, Karen, 1992 Techniques for automatically cor- recting words in text, CSUR, 24:4, pp.	CSUR	computing surveys$Computing Surveys$	1
CSUR , Vol, 4 , no.	CSUR	computing surveys$Computing Surveys$	1
ACM CSUR, 41(2):1?69.	CSUR	computing surveys$Computing Surveys$	1
2.3 LB As a simple baseline, we also evaluated a method that labels words as shell if they appear frequently in persuasive writing?specifically, in the set of 100,000 unannotated essays described in ?	LB	Lexical Baseline$ListenerBot$LazyBoost$likely beneficial$	0
2.1 Raw LB The raw lexical baseline is a simple system that only relies on polarity lexicons and takes the aver- age valence of all the words.	LB	Lexical Baseline$ListenerBot$LazyBoost$likely beneficial$	0
Majority Label LB Table 2 Features Reduced Training Training Set CV 54.6 59.7 77.1  Unseen Answers 51.1 56.1 75.5  Unseen Questions 58.4 63.4 61.7 66.5 Unseen Modules 53.4 62.9 61.4 68.8 Table 3.	LB	Lexical Baseline$ListenerBot$LazyBoost$likely beneficial$	0
This provides indirect empirical support for our ear- lier hypothesis that the med class can benefit from 1073 Original Nissim Baseline Baseline+LB+Both R P F R P F R P F R P F old 91.5 94.1 92.8 91.2 85.8 88.5 88.7 91.7 90.2 93.0 95.2 94.1 med 87.6 68.1 76.6 84.7 62.7 72.1 92.5 63.2 75.1 89.1 70.9 79.0 new 22.3 56.3 32.0 30.2 66.4 41.5 32.1 68.3 43.7 34.4 71.5 46.5 Accuracy 79.5 74.1 76.3 82.2 Table 3: Per-class performance of four information-status classifiers.	LB	Lexical Baseline$ListenerBot$LazyBoost$likely beneficial$	0
The state space S of LB	LB	Lexical Baseline$ListenerBot$LazyBoost$likely beneficial$	1
However, LB does not produce utterances.	LB	Lexical Baseline$ListenerBot$LazyBoost$likely beneficial$	1
This allows LB to track its beliefs about the location of the card and to incor- porate linguistic advice.	LB	Lexical Baseline$ListenerBot$LazyBoost$likely beneficial$	1
The state space S of LB consists of the location of the player p and the location of the card c. As discussed above in Section 3.3, we cl	LB	Lexical Baseline$ListenerBot$LazyBoost$likely beneficial$	1
region r}| 4 LB We first introduce LB, an agent that does not take into account the actions or beliefs of its partner.	LB	Lexical Baseline$ListenerBot$LazyBoost$likely beneficial$	1
LB decides what actions to take using a Partially Observable Markov Decision Pro- cess (POMDP).	LB	Lexical Baseline$ListenerBot$LazyBoost$likely beneficial$	1
Us- ing this task and a model of the meaning of spatial language, we next discuss two agents that play the game: LB (Section 4) makes decisions us- ing a single-agent POMDP that does not take into account the beliefs or actions of its partner, whereas DialogBot (Section 5) maintains a model of its part- ner?s beliefs.	LB	Lexical Baseline$ListenerBot$LazyBoost$likely beneficial$	1
Regarding the M L algorithms tested, the  contribution of this work consist of empiri-  cally demonstrating that the LBing al-  gorithm outperforms other three state-of-the-  art supervised ML methods for WSD.	LB	Lexical Baseline$ListenerBot$LazyBoost$likely beneficial$	2
177  Naive Bayes  Exemplar Based  Snow  LBing  58  56 1  54  ~?52  o  50  44  4O  58  56  Af~  52  ~o  ~ 48  46   58  56 '  54  o 52  50  46  62  60 '  58   ~o  48  46   4.4  Test on B corpus  (la)  . . . . .	LB	Lexical Baseline$ListenerBot$LazyBoost$likely beneficial$	2
Extensively evaluate LBing on the  WSD task.	LB	Lexical Baseline$ListenerBot$LazyBoost$likely beneficial$	2
LBing (Escudero et al, 2000a), is a  simple modification of the AdaBoost.	LB	Lexical Baseline$ListenerBot$LazyBoost$likely beneficial$	2
This observation  allows both to identify the noisy exam-  ples and use LBing as a way to  improve data quality.	LB	Lexical Baseline$ListenerBot$LazyBoost$likely beneficial$	2
8.64?1.04  64.26?2.07  69.33?2.92  66.20?2.12  48.61?0.96  48.22?3 06  48.46?1.21  61.38?2.08  64.32?3.27  62.50?1.47  48.87?1 68  48.22?1.90  48.62?1.09  63.19?1.65  68.51?2.45  65.22?1.50  B-B A-B B-A  48.61?0.96 48.99 48.99  48.22?3.06 48.22 48.22  48.46?1.21 48.70 48.70  60.65?1.01 53.45 55.27  63.49?2.27 60.44 62.55  61.74?1.18 56.12 58.05  Table 3: Accuracy results (5= standard eviation) of LBing on the sense-balanced corpora  Furthermore, these results are in contradic-  tion with the idea of "robust broad-coverage  WSD" introduced by (Ng, 1997b), in which a  supervised system trained on a large enough  corpora (say a thousand examples per word)  ~hould provide accurate disambiguation on  any corpora (or, at least significantly better  than MFS).	LB	Lexical Baseline$ListenerBot$LazyBoost$likely beneficial$	2
ng the M L algorithms tested, the  contribution of this work consist of empiri-  cally demonstrating that the LBing al-  gorithm outperforms other three state-of-the-  art supervised ML methods for WSD.	LB	Lexical Baseline$ListenerBot$LazyBoost$likely beneficial$	2
For each disease, CE classifies drugs into one of six categories: beneficial, LB, trade- offs (i.e., may have adverse side effects), un- known, unLB, and harmful.	LB	Lexical Baseline$ListenerBot$LazyBoost$likely beneficial$	3
In the first set of judgments, the asses- sor determined which of the six categories (ben- eficial, LB, tradeoffs, unknown, un- LB, harmful) the system answer be- longed to, based on the CE recommendations.	LB	Lexical Baseline$ListenerBot$LazyBoost$likely beneficial$	3
AFP, English Service   ?	AFP	Agence France-Presse$Agence France Presse$	0
We formalize this as a supervised clas- 1 https://code.google.com/p/word2vec 2 LDC2012T21, AFP 2010 3 https://code.google.com/p/cistern sification task and apply SVMs (Chang and Lin, 2011).	AFP	Agence France-Presse$Agence France Presse$	0
The human summaries are not threaded; they are flat, roughly daily news summaries published by AFP and found in the Gigaword corpus, distinguished by their ?	AFP	Agence France-Presse$Agence France Presse$	0
4.3.1 Automatically-Parsed Corpus The text corpus we use consists of 125 mil- lion words from the L?Est Republicain newspa- per5, 125 million words of dispatches from the AFP, and 225 million words from a French Wikipedia backup dump6.	AFP	Agence France-Presse$Agence France Presse$	0
Good examples are the multilingual news feeds produced by news agencies such as AFP, Xinhua News, Reuters, CNN, BBC, etc.	AFP	Agence France-Presse$Agence France Presse$	1
These datasets are 2004-2007 newswire feeds col- lected from different news agencies and news pa- pers, such as AFP, Xinhua, Al- Hayat, Al-Asharq Al-Awsat, Al-Quds Al-Arabi, An-Nahar, Al-Ahram and As-Sabah.	AFP	Agence France-Presse$Agence France Presse$	1
The final  round of Spanish retrieval took place in TREC-5,  again with 25 new topics and also with additional  text (1994 newswire from AFP, in-  cluding 308 megabytes or 173,950 documents).	AFP	Agence France-Presse$Agence France Presse$	1
Chinese  English  French  Japanese  Portuguese  Spanish  Xinhua  Wall Street Journal  Le Monde  Kyodo  Radiobras  AFP  China  USA  France  Japan  Brazil  France  Language  Chinese  English  French  Japanese  Portuguese  Spanish  NE TIM NUM ENA  4454 17.2 0 1.8 0 80.9 0  2242 10.7% 9.5% 79.8%  2321 18.6% 3.0% 78.4%  2146 26.4% 4.0% 69.6%  3839 17.7% 12.1% 70.3%  3579 24.6% 3.0% 72.5%  Table 1: Corpora sources.	AFP	Agence France-Presse$Agence France Presse$	1
They have been published by the English and French editors of AFP, and report on the same event, an epidemic of cholera in Pyongyang.	AFP	Agence France-Presse$Agence France Presse$	1
LDC2003T06, roughly 166K words of  written Modern Standard Arabic newswire from  the AFP corpus; and (2) Arabic  Treebank: Part 2 v 2.0, LDC Catalog No.	AFP	Agence France-Presse$Agence France Presse$	1
442  Proceedings of the 7th Workshop on Syntax, Semantics and Structure in SSST, pages 19?28, Atlanta, Georgia, 13 June 2013.	SSST	Statistical Translation$Syntax and Structure in Statistical Translation$	0
A Syntax- Based SSST Model.	SSST	Statistical Translation$Syntax and Structure in Statistical Translation$	0
Information Retrieval as SSST.	SSST	Statistical Translation$Syntax and Structure in Statistical Translation$	0
Headline Generation Based on SSST.	SSST	Statistical Translation$Syntax and Structure in Statistical Translation$	0
of the Third Workshop on Syntax and Structure in SSST, pages 51?59.	SSST	Statistical Translation$Syntax and Structure in Statistical Translation$	0
HMM-based  Word Alignment In SSST.	SSST	Statistical Translation$Syntax and Structure in Statistical Translation$	0
In HLT-NAACL Workshop on SSST, pages 1-8.	SSST	Statistical Translation$Syntax and Structure in Statistical Translation$	1
In Proceedings of the 4th Workshop on  SSST,  pages 43?51.	SSST	Statistical Translation$Syntax and Structure in Statistical Translation$	1
In Proceedings of the ACL-HLT Second Workshop on SSST, pages 10?18.	SSST	Statistical Translation$Syntax and Structure in Statistical Translation$	1
In Proceedings of the NAACL-HLT 2007/AMTA Workshop on SSST, pages 96?102.	SSST	Statistical Translation$Syntax and Structure in Statistical Translation$	1
In Proceedings of the HLT-NAACL Workshop on SSST, pages 51-59.	SSST	Statistical Translation$Syntax and Structure in Statistical Translation$	1
In NAACL-HLT/AMTA Workshop on SSST, pages 103?	SSST	Statistical Translation$Syntax and Structure in Statistical Translation$	1
The learning algorithnl fol-  lows the essence of the MDL principle to search for  the optimal segmentation f an utterance that has  the maximal DLG (and there-  fore approaches the minimum description length  of the utterance).	DLG	description length gain$Description Length Gain$	0
Exper- iments show that DLG outperforms other measures because of its strength for identifying short words.	DLG	description length gain$Description Length Gain$	0
Unsupervised learning of word boundary with DLG.	DLG	description length gain$Description Length Gain$	0
Furthermore, DLG offers a unified statistical account of an MWE as a linguistically motivated structure that can com- press relevant corpus data.	DLG	description length gain$Description Length Gain$	0
Conc lus ions  and  Future  Work   We have presented an unsupervised learning algo-  rithm for lexical acquisition based on the goodness  measure DLG formulated follow-  ing information theory.	DLG	description length gain$Description Length Gain$	0
275   Unsupervised Learning of Word Boundary with  DLG  Chunyu K i t  t$  Dept.	DLG	description length gain$Description Length Gain$	1
occupation; NP1 = n, NC t] Parenthesis6 (e1 pare) NP1 (NP2) [NP1 = t, NC n] Also-known-as7 (e1/2 aka) NP1, (also) known as NP2 [NP1 = t ?	NC	NP2 =$nonconvertible$noun compound$	0
us used for the TREC QA Track, available from the Linguistic Data Consortium Name Pattern Bindings Copular1 (e1 is) NP1 be NP2 [NP1 = t, NC n] Become2 (e1 beca) NP1 become NP2 [NP1 = t, NC n] Verb3 (e1 verb): NP1 v NP2 [where v ?	NC	NP2 =$nonconvertible$noun compound$	0
n] Also-called8 (e2 also) NP1, (also) called NP2 [NP1 = n, NC t] Or9 (e1 or) NP1, or NP2 [NP1 = t, NC n] Like10 (e2 like) NP1 (such as|like) NP2 [NP1 = n, NC t] Relative cl	NC	NP2 =$nonconvertible$noun compound$	0
n] Also-called8 (e2 also) NP1, (also) called NP2 [NP1 = n, NC t] Or9 (e1 or) NP1, or NP2 [NP1 = t, NC n] Like10 (e2 like) NP1 (such as|like) NP2 [NP1 = n, NC t] Relative clause11 (e1 wdt) NP (which|that) VP [NP = t, VP = n] 1In order to filter out spuriou	NC	NP2 =$nonconvertible$noun compound$	0
n] Also-called8 (e2 also) NP1, (also) called NP2 [NP1 = n, NC t] Or9 (e1 or) NP1, or NP2 [NP1 = t, NC n] Like10 (e2 lik	NC	NP2 =$nonconvertible$noun compound$	0
n, NC t ?	NC	NP2 =$nonconvertible$noun compound$	0
nguistic Data Consortium Name Pattern Bindings Copular1 (e1 is) NP1 be NP2 [NP1 = t, NC n] Become2 (e1 beca) NP1 become NP2 [NP1 = t, NC n] Verb3 (e1 verb): NP1 v NP2 [where v ?	NC	NP2 =$nonconvertible$noun compound$	0
However, there were several instances where the target term was not correctly extracted from 1official corpus used for the TREC QA Track, available from the Linguistic Data Consortium Name Pattern Bindings Copular1 (e1 is) NP1 be NP2 [NP1 = t, NC n] Become2 (e1 beca) NP1 become NP2 [NP1 = t, NC n] Verb3 (e1 verb): NP1 v NP2 [where v ?	NC	NP2 =$nonconvertible$noun compound$	0
n] Also-called8 (e2 also) NP1, (also) called NP2 [NP1 = n, NC t] Or9 (e1 or) NP1, or NP2 [NP1 = t, NC n] Like10 (e2 like) NP1 (such as|like) NP2 [NP1 = n, NC t] Relative clause11 (e1 wdt) NP (which|that) VP [NP = t, VP = n] 1In order to filter out spurious nuggets (e.g., progressive tense), our system disc	NC	NP2 =$nonconvertible$noun compound$	0
ral instances where the target term was not correctly extracted from 1official corpus used for the TREC QA Track, available from the Linguistic Data Consortium Name Pattern Bindings Copular1 (e1 is) NP1 be NP2 [NP1 = t, NC n] Become2 (e1 beca) NP1 become NP2 [NP1 = t, NC n] Verb3 (e1 verb): NP1 v NP2 [where v ?	NC	NP2 =$nonconvertible$noun compound$	0
biography-verb; NP1 = t, NC n] Appositive4 (e1/2 appo) NP1, NP2 [NP1 = t ?	NC	NP2 =$nonconvertible$noun compound$	0
Lexical atoms may be found among proper  names, idioms, and many noun-NCs.	NC	NP2 =$nonconvertible$noun compound$	2
roll) and NCs (dry ice).	NC	NP2 =$nonconvertible$noun compound$	2
ng measure 0.16 student loan entity 0.16 theater orchestra entity 0.17 sunday restrictions abstraction 0.20 yesterday afternoon measure 0.20 relations agency abstraction 0.21 crime novelist entity 0.21 office buildings structure 0.21 Table 5: Best and worst scoring NCs with their Least Common Subsumer and Spear- man ?	NC	NP2 =$nonconvertible$noun compound$	2
Our approach to this task is to de- velop a machine learning classifier which determines for each verb pair describing a NC which verb should be ranked higher.	NC	NP2 =$nonconvertible$noun compound$	2
correlation good results on the task of ranking verbs para- phrasing NCs.	NC	NP2 =$nonconvertible$noun compound$	2
Split- ting NCs via monolingual and bilingual paraphrasing: A study on Japanese Katakana words.	NC	NP2 =$nonconvertible$noun compound$	2
Corpus statistics meet the NC: Some empirical results.	NC	NP2 =$nonconvertible$noun compound$	2
In Proceedings of the IEEE Work- shop on SLT, pages 79?84.	SLT	Spoken Language Technology$Spoken language translation$	0
Proceedings of the IEEE/ACL 2006 Workshop on SLT, 134?137.	SLT	Spoken Language Technology$Spoken language translation$	0
Proceedings of the IEEE/ACL 2006 Workshop on SLT.	SLT	Spoken Language Technology$Spoken language translation$	0
In Proceedings of the IEEE / ACL 2006 Workshop on SLT.	SLT	Spoken Language Technology$Spoken language translation$	0
of the 2008 SLT Workshop, Goa.	SLT	Spoken Language Technology$Spoken language translation$	0
SLT re-  quires (1) an accurate translation and (2) a real-  time response.	SLT	Spoken Language Technology$Spoken language translation$	1
1 Introduction SLT technologies attempt to bridge the language barriers between people with different native languages who each want to engage in conversation by using their mother- tongue.	SLT	Spoken Language Technology$Spoken language translation$	1
2 Dialect Translation SLT technologies attempt to bridge the language barriers between people with different native languages who each want to engage in conversation by using their mother-tongue.	SLT	Spoken Language Technology$Spoken language translation$	1
For instance, the Latin-derived Noun  ADJPe "Bacillus subtilis" has a  structure inverse to the canonical English noun  phrase (Adjective Noun).	ADJP	Adjective phras$adjective phrase$	0
The phrase recognition rules are to be applied  in the following order:  (VPH) Verb phrases  (APH)  Adverb  phrases  (JPH) ADJPes  (NPH)  Noun phrases   (PPH) Prepositional phrases  29--  The typical features of this system are: taking  tone units as the basis of grammat ica l  analysis,  choosing a general-purpose dictionary for word   class tagging, mak ing  extensive use of phrase  structure rules which are applied in a certain  order and cyclically, and partly adopting an  interactive mode of analysis.	ADJP	Adjective phras$adjective phrase$	0
4.1 AP  Segmentat ion   ADJPes are marked by a replacement  transducer which inserts the \[AP and AP\]  bound-  aries around any word sequence that matches the  regular expression (RE):  \[ (ADVP) ADJ ( COMMA \[ (ADVP) ADJ  COMMA \]+ ) ( COORD (ADVP) ADJ ) \]  ADVP stands for adverb phrase and is defined as:  \[ ADV+ \[\[COORD\[COMMA\] DV?\]* \]  4.2 NP  Segmentat ion   Unlike APs, NPs are marked in two steps	ADJP	Adjective phras$adjective phrase$	0
k et al1985) and (Semmelmeyer and Bolander 1992)): (1) Compound Nominals consisting of two consecutive nouns (eg night club - a TEMPORAL relation - indicat- ing that club functions at night), (2) Adjective Noun con- structions where the adjectival modifier is derived from a noun (eg musical clock - a MAKE/PRODUCE relation), (3) Genitives (eg the door of the car - a PART-WHOLE rela- tion), and (4) ADJPes (cf. (	ADJP	Adjective phras$adjective phrase$	0
ADJPes indicating measurement (as in "a 10 ft pole" or "a pole 10  f t  long") are converted to modifiers where the measured quantity is made explicit, e.g.,  (LENGTH 10 FT).	ADJP	Adjective phras$adjective phrase$	0
By  and  large, English phrase structure typically has  the head to the right, as in  Verb phrases: will be DOING  Noun phrases: the nice little DOG  ADJPes: stunningly BEAUTIFUL   Assuming  that a good number  of the tone units  consist of, at least, g rammat ica l  phrases, the  nucleus will occur within the phrase and, more   often than not, within the head of the phrase.	ADJP	Adjective phras$adjective phrase$	0
SO(a)= -SO(a) Where C(a) denotes the category of DSAAs; C(n) denotes the sentiment expectation of nouns; SO(a) is the SO of DSAAs in a give noun- ADJP.	ADJP	Adjective phras$adjective phrase$	1
|price is low Table 1: The SO of DSAAs in noun-ADJPs In previous research, the SO of nouns is classified into three categories: positive, negative and neutral.	ADJP	Adjective phras$adjective phrase$	1
Inflection We introduced tag suffixes for inflec- tion as clues to identify the attachment position of the verb and ADJPs, because Japanese verbs and adjectives have inflections, which depends 110 (no label) base form cont continuative form attr attributive form neg negative form hyp hypothetical form imp imperative form stem stem Table 2: Inflection tag suffixes on their modifying words and phrases (e.g. noun and verb phrases).	ADJP	Adjective phras$adjective phrase$	1
qing|light} 3.2 Sentiment Expectation of Noun The SO of most DSAAs can be determined by target nouns in noun-ADJPs, as shown in Table 1.	ADJP	Adjective phras$adjective phrase$	1
(a) The phrase which is an ADJP and  modifies "each", appositive  to the preceding "statements",  (b) The phrase which is a past participle phrase  and modifies "names".	ADJP	Adjective phras$adjective phrase$	1
For example, a noun phrase (the string) such as [adjective+noun] can be represented as NP(AP,noun) where AP refers to a chart of general ADJPs, possibly containing adverbs as in ?	ADJP	Adjective phras$adjective phrase$	1
Wilensky, R., Mayfield, J., Chin, D., Luria, M., Martin,  J. and Wu, D. The Berkeley UC Project.	UC	UNIX Consultant$User Centered$	0
Wilensky, R., Mayfield, L, Chin, D., Lm'ia, M., Martin,  L and Wu, D. The Berkeley UC Project.	UC	UNIX Consultant$User Centered$	0
Higher-level plan-  ning in the UC, for modularity, is  performed by a separable planning component.	UC	UNIX Consultant$User Centered$	0
WILENSKY ,R. (1984): Talking To UNIX In English:  An Overview Of An Online UC.	UC	UNIX Consultant$User Centered$	0
Experience with the UC has  suggested that the interaction of specialized and general  linguistic knowledge is important for a natural anguage  interface.	UC	UNIX Consultant$User Centered$	0
0362-613X/88/010035-84503.00  Computational Linguistics, Volume 14, Number 4, December 1988 35  Robert Wilensky, David N. Chin, Marc Luria, ,lames Martin, James Mayfield, and Dekai Wu The Berkeley UC Project  progress on fundamental issues that comprise the cen-  tral goals of AI researchers.	UC	UNIX Consultant$User Centered$	0
In  Donald A. Norman and Stephen W. Draper, edi-  tors, UC System Design: new Perspec-  tives on Human-Computer Interaction, chapter 5,  pages 87-124.	UC	UNIX Consultant$User Centered$	1
The involvement ofusers since  the very beginning of the system design (i.e. the  adoption of a UC approach) can greatly  enhance the effectiveness of a LEADS: user needs  can pervade the design and implementation f all  the basic functionalities of the tool.	UC	UNIX Consultant$User Centered$	1
UC System Design: new Perspectives  on Human-Computer Interaction.	UC	UNIX Consultant$User Centered$	1
In: Norman, D.A,, and Drap-  er, S.W. (eds): UC System Design: New Per-  spectives on Human-Computer Interaction.	UC	UNIX Consultant$User Centered$	1
UC System Design; New Perspectives on Human-Computer Interaction.	UC	UNIX Consultant$User Centered$	1
PPf ) 1 2 (3) 656 Table 1: Size of parallel corpora English Chinese English Chinese In-domain parallel corpus 40 K 40 K 320 K 301 K BTEC Out-of-domain parallel corpus 2.5 M 2.5 M 62 M 54 M LDC corpus (LDC 2002T01, LDC2003T17, LDC2004T07, LDC2004T08, LDC2005T06 and LDC2005T10) # of sentences # of words Explanation 4.	BTEC	Basic Travel Expressions Corpus$Basic Travel Expression Corpu$	0
The IWSLT09 data set is comprised of short sentences (with an average of 9.5 words per sentence) from a particular domain (the C-STAR project?s BTECs).	BTEC	Basic Travel Expressions Corpus$Basic Travel Expression Corpu$	1
554 Training Test Japanese Korean Japanese Korean # of sentences 162,320 10,150 # of total morphemes 1,153,954 1,179,753 74,366 76,540 # of bunsetsu/eojeol 448,438 587,503 28,882 38,386 vocabulary size 15,682 15,726 5,144 4,594 Table 2: Statistics of BTECs PER mWER BLEU NIST WBIBM 0.3415 / 0.3318 0.3668 / 0.3591 0.5747 / 0.5837 6.9075 / 7.1110 WBLMC 0.2667 / 0.2666 0.2998 / 0.2994 0.5681 / 0.5690 9.0149 / 9.0360 CBIBM 0.2677 / 0.2383 0.2992 / 0.2700 0.6347 / 0.6741 8.0900 / 8.6981 CBLMC 0.1954 / 0.1896 0.2176 / 0.2129 0.7060 / 0.7166 9.9167 / 10.027 Table 3: Evaluation Results of Translation Systems: without BiVN/with	BTEC	Basic Travel Expressions Corpus$Basic Travel Expression Corpu$	1
During the decoding process, when a pair of chunks appeared in the first stage, the score is boosted by using this formula in the log domain, log Ptm(J|E) + log Plm(E) Table 1: BTECs Japanese English # of sentences 171,894 # of words 1,181,188 1,009,065 vocabulary size 20472 16232 # of singletons 82,06 5,854 3-gram perplexity 23.7 35.8 + weight ?	BTEC	Basic Travel Expressions Corpus$Basic Travel Expression Corpu$	1
Statistics of BTECs Chinese English Japanese Korean # of sentences 167,163 # of words(morph) 1,006,838 1,128,151 1,226,774 1,313,407 Vocabulary size(S) 17,472 11,737 19,485 17,600 Vocabulary size(B) 17,472 9172 15,939 15,410 Vocabulary size(SB) 17,472 13,385 20,197 18,259 Vocabulary size(SP) 18,505 13,467 20,118 20,249 Vocabulary size(SBP(L)) 18,505 14,408 20,444 20,369(26,668) # of sin	BTEC	Basic Travel Expressions Corpus$Basic Travel Expression Corpu$	1
Three types of attribution are possible in our sys- tem: 1) ESA mention of speakers, e.g., ?	ESA	explicit$Explicit Semantic Analysis$	0
In these chains of utterances, the speaker is not ESAly mentioned because the author relies on the shared understanding with the reader that adja- cent pieces of quoted speech are not independent (Zhang et al.,	ESA	explicit$Explicit Semantic Analysis$	0
Several syntac- tic patterns were applied to associate quotes with ESA mention of speakers in their vicinity to characters from the pruned list of story charac- ters.	ESA	explicit$Explicit Semantic Analysis$	0
Hatzivassiloglou and McKeown, 1997)  for another application in which no ESA  indicators are available in the stream).	ESA	explicit$Explicit Semantic Analysis$	0
They are not ESA  about conlparisc.n of lists of marks of tlncqual englh except m the binary  casK, hi that case, lheir delinitiolls \]lave the Sllllle consequences ;i those  described here.	ESA	explicit$Explicit Semantic Analysis$	0
In the first type of attri- bution, the speaker is ESAly mentioned in the vicinity of the quote.	ESA	explicit$Explicit Semantic Analysis$	0
S-Space Package Even though no designated text similarity library, the S-Space Package (Jur- gens and Stevens, 2010)8 contains some text sim- ilarity measures such as Latent Semantic Analysis (LSA) and ESA (see Sec- tion 3.2).	ESA	explicit$Explicit Semantic Analysis$	1
These measures include simple distances like Levenshtein edit distance, cosine, Named En- tities overlap and more complex distances like ESA, WordNet-based similarity, IR-based similarity, and a similar- ity measure based on syntactic dependencies.	ESA	explicit$Explicit Semantic Analysis$	1
ESA (Gabrilovich and Markovitch, 2007) constructs the vector space on corpora where the documents are assumed to de- scribe natural concepts such as cat or dog.	ESA	explicit$Explicit Semantic Analysis$	1
One of the most successful systems in *SEM 2012 STS, (Ba?r et al 2012), managed to grade pairs of sentences accurately by combining focused mea- sures, either simple ones based on surface features (ie n-grams), more elaborate ones based on lexical semantics, or measures requiring external corpora such as ESA, into a robust measure by using a log-linear regression model.	ESA	explicit$Explicit Semantic Analysis$	1
Com- puting Semantic Relatedness using Wikipedia-based ESA.	ESA	explicit$Explicit Semantic Analysis$	1
Computing Semantic Relatedness using Wikipedia- based ESA.	ESA	explicit$Explicit Semantic Analysis$	1
MEAD is significantly different from previous work  on multi-document summarization \[Radev &  McKeown, 1998; Carbonell and Goldstein, 1998;  Mani and Bloedorn, 1999; MeKeown et aI., 1999\],  21  which use techniques such as graph matching,  MMR, or language generation.	MMR	maximal marginal relevance$Maximal Marginal Relevance$	0
773 2.1 Meeting Summarization Among early work on meeting summarization, Waibel et al (1998) implemented a modified version of the MMR algorithm (Car- bonell and Goldstein, 1998) applied to speech tran- scripts, presenting the user with the n best sentences in a meeting browser interface.	MMR	maximal marginal relevance$Maximal Marginal Relevance$	1
A future improvement will be to use a reorder- ing approach like MMR A r t i c l e G o l d M e a d ?	MMR	maximal marginal relevance$Maximal Marginal Relevance$	1
Sentence extractive methods comprehend, es- sentially, methods like LSA (Gong and Liu, 2001), MMR (Carbonell and Goldstein, 1998), and feature-based meth- ods (Edmundson, 1969).	MMR	maximal marginal relevance$Maximal Marginal Relevance$	1
c?2010 Association for Computational Linguistics Putting the User in the Loop: Interactive MMR for Query-Focused Summarization Jimmy Lin, Nitin Madnani, and Bonnie J. Dorr University of Maryland College Park, MD 20742, USA jimmylin@umd.edu, {nmadnani,bonnie}@umiacs.umd.edu Abstract This work represents an initial attempt to move beyond ?	MMR	maximal marginal relevance$Maximal Marginal Relevance$	1
Our experiments with the NTCIR ACLIA ques- tion answering test collections show that our method achieves a pyramid F3-score of up to 0.313, a 36% improvement over a baseline us- ing MMR.	MMR	maximal marginal relevance$Maximal Marginal Relevance$	1
In 2004, Conroy (Conroy, 2004) tested MMR (Goldstein et al.,	MMR	maximal marginal relevance$Maximal Marginal Relevance$	1
Initial trends in enrolment and completion of MOOCs.	MOOCs	massive open online courses$Massive Open Online Courses$	0
Linguistic reflections of student engagement in MOOCs.	MOOCs	massive open online courses$Massive Open Online Courses$	0
turn on, tune in, drop out: Anticipating student dropouts in MOOCs.	MOOCs	massive open online courses$Massive Open Online Courses$	0
Together we stand, Together we fall, Together we win: Dynamic team formation in MOOCs?	MOOCs	massive open online courses$Massive Open Online Courses$	0
Predicting student reten- tion in MOOCs using hidden markov models.	MOOCs	massive open online courses$Massive Open Online Courses$	0
In Proceedings of the 1st Workshop on MOOCs at the 16th Annual Conference on Artificial Intelligence in Education, Memphis, TN.	MOOCs	massive open online courses$Massive Open Online Courses$	1
Turn on, Tune in, Drop out: Anticipating  student dropouts in MOOCs,  in NIPS Data-Driven Education Workshop.	MOOCs	massive open online courses$Massive Open Online Courses$	1
Turn on, Tune in, Drop out: Anticipating student dropouts in MOOCs?	MOOCs	massive open online courses$Massive Open Online Courses$	1
Linguistic Reflections of Student Engagement in  MOOCs.	MOOCs	massive open online courses$Massive Open Online Courses$	1
Stu- dents are taking MOOCs as well as online tutorials and paid online courses.	MOOCs	massive open online courses$Massive Open Online Courses$	1
TT The port scanner is a utility to scan a system to get the status of the TCP.	TT	THEME-TOOL$target time$Tree Transducer$TextTiling$trigger term$Tongue Tip$	0
This can be explained as the effect of their specifications; the three best-ranked relations are well-defined by human standards, while the TT relation is more ambiguous.	TT	THEME-TOOL$target time$Tree Transducer$TextTiling$trigger term$Tongue Tip$	0
From this table it can be observed that the PRODUCT-PRODUCER, INSTRUMENT-AGENCY, and CAUSE-EFFECT rela- tions were detected with a relatively very high per- formance score, whereas the TT relation classification yielded a relatively small score.	TT	THEME-TOOL$target time$Tree Transducer$TextTiling$trigger term$Tongue Tip$	0
In particular, the models pre- sented here perform relatively badly on the ORIGIN-ENTITY and TT relations, while scoring better than all SemEval entrants on INSTRUMENT-AGENCY and PRODUCT- PRODUCER.	TT	THEME-TOOL$target time$Tree Transducer$TextTiling$trigger term$Tongue Tip$	0
R1 R2 R3 R4 R5 R6 R7 before 682 1200 913 898 861 849 677 after 13 19 10 15 15 8 16 Table 4: The number of features before and af- ter Weka selection, for each semantic relation dataset: R1 CAUSE-EFFECT, R2 INSTRUMENT- AGENCY, R3 PRODUCT-PRODUCER, R4 ORIGIN- ENTITY, R5 TT, R6 PART-WHOLE, and R7 CONTENT-CONTAINER.	TT	THEME-TOOL$target time$Tree Transducer$TextTiling$trigger term$Tongue Tip$	0
In this paper, we  approach the temporal correspondence prob- lem in which, given an input term (e.g., iPod)  and the TT (e.g. 1980s), the task is to  find the counterpart of the query that existed  in the TT.	TT	THEME-TOOL$target time$Tree Transducer$TextTiling$trigger term$Tongue Tip$	1
Figure 3 shows a sample of the interaction be- tween reference times and TTs.	TT	THEME-TOOL$target time$Tree Transducer$TextTiling$trigger term$Tongue Tip$	1
Here the second and the  fourth patches make corresponding temporal  expressions be treated as non-TTs that  need not be processed.	TT	THEME-TOOL$target time$Tree Transducer$TextTiling$trigger term$Tongue Tip$	1
In particular, for an input pair of a  term (e.g., iPod) and the TT (e.g. 1980s),  we find the corresponding term that existed in the  TT (walkman).	TT	THEME-TOOL$target time$Tree Transducer$TextTiling$trigger term$Tongue Tip$	1
Interaction between reference times  and TTs  In Figure 3, we notice that different classes of  time dynamically and automatically choose ref- erences based on their respective classes rather  than do it using the fixed value or the inconside- rate rule under the static mechanism.	TT	THEME-TOOL$target time$Tree Transducer$TextTiling$trigger term$Tongue Tip$	1
4 Extended-LHS TTs (xR) Section 1 informally described the root-to-frontier trans- ducer class R. We saw that R allows, by use of states, finite lookahead and arbitrary rearrangement of non- sibling input subtrees removed by a finite distance.	TT	THEME-TOOL$target time$Tree Transducer$TextTiling$trigger term$Tongue Tip$	2
c?2010 Association for Computational Linguistics A TT Model for Synchronous Tree-Adjoining Grammars Andreas Maletti Universitat Rovira i Virgili Avinguda de Catalunya 25, 43002 Tarragona, Spain.	TT	THEME-TOOL$target time$Tree Transducer$TextTiling$trigger term$Tongue Tip$	2
6  Training TTs Jonathan Graehl Information Sciences Institute University of Southern California 4676 Admiralty Way Marina del Rey, CA 90292 graehl@isi.edu Kevin Knight Information Sciences Institute University of Southern California 4676 Admiralty Way Marina del Rey, CA 90292 knight@isi.edu Abstract Many probabilistic models for natural language are now written in terms of hierarchical tree stru	TT	THEME-TOOL$target time$Tree Transducer$TextTiling$trigger term$Tongue Tip$	2
c?2010 Association for Computational Linguistics Parsing and Translation Algorithms Based on Weighted Extended TTs Andreas Maletti?	TT	THEME-TOOL$target time$Tree Transducer$TextTiling$trigger term$Tongue Tip$	2
An Overview of Probabilistic  TTs for Natural Language Processing.	TT	THEME-TOOL$target time$Tree Transducer$TextTiling$trigger term$Tongue Tip$	2
c?2010 Association for Computational Linguistics Efficient Inference Through Cascades of Weighted TTs Jonathan May and Kevin Knight Information Sciences Institute University of Southern California Marina del Rey, CA 90292 {jonmay,knight}@isi.edu Heiko Vogler Technische Universita?t Dresden Institut fu?r Theoretische Informatik 01062 Dresden, Germany heiko.vogler@tu-dresden.de Abstract Weighted tree transducers have been pro- posed as useful formal models for rep- resenting syntact	TT	THEME-TOOL$target time$Tree Transducer$TextTiling$trigger term$Tongue Tip$	2
TT: A quantitative ap-  proach to discourse segmentation.	TT	THEME-TOOL$target time$Tree Transducer$TextTiling$trigger term$Tongue Tip$	3
54% 45% 52% 53%  H94(~,a/ 0.67s 0.52s 0.66s 0.88s  H94(c,~) 0.68s 0.52s 0.67s 0.92s  H94(j,~) 3.77s 2.21s 3.69s 5.07s  Table 3: The error rate and speed performance of  TT.	TT	THEME-TOOL$target time$Tree Transducer$TextTiling$trigger term$Tongue Tip$	3
We implemented a variant of Hearst?s [1997] TT algorithm.)	TT	THEME-TOOL$target time$Tree Transducer$TextTiling$trigger term$Tongue Tip$	3
4.3 Experiment 2 - TextTil ing  We compare three versions of the TT algo-  rithm (Hearst, 1994).	TT	THEME-TOOL$target time$Tree Transducer$TextTiling$trigger term$Tongue Tip$	3
TT: Segmenting text into multi-paragraph subtopic passages.	TT	THEME-TOOL$target time$Tree Transducer$TextTiling$trigger term$Tongue Tip$	3
TT: segmenting text into multi-paragraph subtopic passages.	TT	THEME-TOOL$target time$Tree Transducer$TextTiling$trigger term$Tongue Tip$	3
It illustrates that the event TT ?	TT	THEME-TOOL$target time$Tree Transducer$TextTiling$trigger term$Tongue Tip$	4
Note that all event TTs are placeholders for alternatives (see text): ?	TT	THEME-TOOL$target time$Tree Transducer$TextTiling$trigger term$Tongue Tip$	4
Events are extracted using a newly developed query lan- guage with traverses the BioLG linkages be- tween TTs, arguments, and events.	TT	THEME-TOOL$target time$Tree Transducer$TextTiling$trigger term$Tongue Tip$	4
Essentially, we automatically extract all shortest link paths that connect event TTs to themes, themes to sites, themes to locations, and so on.	TT	THEME-TOOL$target time$Tree Transducer$TextTiling$trigger term$Tongue Tip$	4
All in all, we extracted 1845 different link paths from the training data (2197 from training plus devel- opment) that connect two constituents each (event TT to protein, or protein to site, for in- stance), corresponding to as many PTQL queries.	TT	THEME-TOOL$target time$Tree Transducer$TextTiling$trigger term$Tongue Tip$	4
Per link path type, the increase rate ranged from only 9% (localization: theme to at- loc) over 11-15% for basic events (gene-expression or transcription TT to theme) to almost 27% (regulation: theme to site).	TT	THEME-TOOL$target time$Tree Transducer$TextTiling$trigger term$Tongue Tip$	4
A Prototype  System that uses a Mobile Phone to Sup- port Personal NARRe for Children  with Complex Communication   Rolf Black1, Annalu Waller1, Ehud Reiter2, Nava  Tintarev2, Joseph Reddington2   (1University of Dundee, 2University of Aberdeen)  We will show a sensor based mobile phone proto- type that supports personal narrative for children  with complex communication needs.	NARR	Narrativ$Narrative$	0
NARRe order in the  generation of indirect replies is an area we are cur-  rently investigating also; for related research, see  section 3.)	NARR	Narrativ$Narrative$	0
NARRe/story  structure, pausing and American Sign Language.	NARR	Narrativ$Narrative$	0
Topics were  structured using the standard TREC format of Title,  Description and NARRe fields.	NARR	Narrativ$Narrative$	0
We obtained the following values:  c=0.75 for queries using the Title only, c=1 for queries  using the Title and Description fields, and c=1 for queries  using the Title, Description, and NARRe fields.	NARR	Narrativ$Narrative$	0
Dyer, M. 1983 In-Depth Understanding: A Computer Model of  NARRe Comprehension, MIT Press, Cambridge, MA.	NARR	Narrativ$Narrative$	0
A Prototype  System that uses a Mobile Phone to Sup- port Personal NARR for Children  with Complex Communication   Rolf Black1, Annalu Waller1, Ehud Reiter2, Nava  Tintarev2, Joseph Reddington2   (1University of Dundee, 2University of Aberdeen)  We will show a sensor based mobile phone proto- type that supports personal narrative for children  with complex communication needs.	NARR	Narrativ$Narrative$	1
NARR order in the  generation of indirect replies is an area we are cur-  rently investigating also; for related research, see  section 3.)	NARR	Narrativ$Narrative$	1
NARR/story  structure, pausing and American Sign Language.	NARR	Narrativ$Narrative$	1
Topics were  structured using the standard TREC format of Title,  Description and NARR fields.	NARR	Narrativ$Narrative$	1
We obtained the following values:  c=0.75 for queries using the Title only, c=1 for queries  using the Title and Description fields, and c=1 for queries  using the Title, Description, and NARR fields.	NARR	Narrativ$Narrative$	1
Dyer, M. 1983 In-Depth Understanding: A Computer Model of  NARR Comprehension, MIT Press, Cambridge, MA.	NARR	Narrativ$Narrative$	1
Acknowledgments This work has been funded by the DFG within the CRC 673 and the CITEC Excellence Center.	CITEC	Cognitive Interaction Technology$Center of Excellence$	0
Semantic Computing Group CITEC ?	CITEC	Cognitive Interaction Technology$Center of Excellence$	0
c?2013 Association for Computational Linguistics Bidirectional Inter-dependencies of Subjective Expressions and Targets and their Value for a Joint Model Roman Klinger and Philipp Cimiano Semantic Computing Group CITEC ?	CITEC	Cognitive Interaction Technology$Center of Excellence$	0
c?2014 Association for Computational Linguistics An Impact Analysis of Features in a Classification Approach to Irony Detection in Product Reviews Konstantin Buschmeier, Philipp Cimiano and Roman Klinger Semantic Computing Group CITEC ?	CITEC	Cognitive Interaction Technology$Center of Excellence$	0
Acknowledgments This research was supported by the Human Language Technology CITEC, by the DARPA GALE program under Contract No.	CITEC	Cognitive Interaction Technology$Center of Excellence$	1
c?2009 ACL and AFNLP Using Word-Sense Disambiguation Methods to Classify Web Queries by Intent Emily Pitler Computer and Information Science University of Pennsylvania Philadelphia, PA 19104, USA epitler@seas.upenn.edu Ken Church Johns Hopkins University Human Language Technology CITEC Baltimore, MD 21211 Kenneth.Church@jhu.edu Abstract Three methods are proposed to classify queries by intent (CQI), e.g., navigational, informational, commercial, etc.	CITEC	Cognitive Interaction Technology$Center of Excellence$	1
Acknowledgments This research was supported by the Human Lan- guage Technology CITEC, by gifts from Google and Microsoft, and by the DARPA GALE program under Contract No.	CITEC	Cognitive Interaction Technology$Center of Excellence$	1
, Benjamin Van Durme Human Language Technology CITEC Johns Hopkins University, Baltimore, MD USA ?	CITEC	Cognitive Interaction Technology$Center of Excellence$	1
Human Language Technology CITEC Johns Hopkins University Baltimore, MD 21211 USA bloodgood@jhu.edu K. Vijay-Shanker Computer and Information Sciences Department University of Delaware Newark, DE 19716 USA vijay@cis.udel.edu Abstract Actively sampled data can have very different characteristics than passively sampled data.	CITEC	Cognitive Interaction Technology$Center of Excellence$	1
European CITEC, CZ.1.05/1.1.00/02.0090.	CITEC	Cognitive Interaction Technology$Center of Excellence$	1
Table 1: Visual DG defines eight re- lations between pairs of annotated regions.	DG	Dependency Grammar$Dependency grammar$Dependency Grammars$dependency gramma$	0
cent Advances in DG.	DG	Dependency Grammar$Dependency grammar$Dependency Grammars$dependency gramma$	0
Weighted Constraint DG (Schro?der, 2002) models syntax structure as la- belled dependency trees as shown in the exam- ple.	DG	Dependency Grammar$Dependency grammar$Dependency Grammars$dependency gramma$	0
The process of removing and pruning is based on the knowledge base and the  four axioms of DG (Robinson, J .	DG	Dependency Grammar$Dependency grammar$Dependency Grammars$dependency gramma$	0
They can be a very basic  filtering process as proposed by Constraint  Grammars (see [Karlsson90]) or can be part to  an actual theory as with HPSG (see [Sag03]), the  Optimality Theory (see [Prince03]) or Constraint  DGs (cf. [	DG	Dependency Grammar$Dependency grammar$Dependency Grammars$dependency gramma$	0
\[2\] van Zuljlcn, Job M.(1989) : "The Application of Simulated Annealing in  DG Pars ing' .	DG	Dependency Grammar$Dependency grammar$Dependency Grammars$dependency gramma$	0
DG induction via bitext pro- jection constraints.	DG	Dependency Grammar$Dependency grammar$Dependency Grammars$dependency gramma$	1
2.1 Latent Dependency Structure DG is a lexically-oriented syn- tactic formalism in which syntactic relationships are expressed as dependencies between individual words.	DG	Dependency Grammar$Dependency grammar$Dependency Grammars$dependency gramma$	1
DG induction via bitext projection constraints.	DG	Dependency Grammar$Dependency grammar$Dependency Grammars$dependency gramma$	1
2  DG is equivalent to an X-bar theory  with only one phrasal bar level (Figure 3)--the dependents  of a word are the heads of its sisters.	DG	Dependency Grammar$Dependency grammar$Dependency Grammars$dependency gramma$	1
DG and dependency parsing.	DG	Dependency Grammar$Dependency grammar$Dependency Grammars$dependency gramma$	1
of the Workshop on DG, pages 88?	DG	Dependency Grammar$Dependency grammar$Dependency Grammars$dependency gramma$	2
In Workshop on Recent Advances in DG (COLING), pages 90?97.	DG	Dependency Grammar$Dependency grammar$Dependency Grammars$dependency gramma$	2
They can be a very basic  filtering process as proposed by Constraint  Grammars (see [Karlsson90]) or can be part to  an actual theory as with HPSG (see [Sag03]), the  Optimality Theory (see [Prince03]) or Constraint  DG (cf. [	DG	Dependency Grammar$Dependency grammar$Dependency Grammars$dependency gramma$	2
TAL (Special Issue Grammaires de D?pendance /  DG), 41 (1): 47?66.	DG	Dependency Grammar$Dependency grammar$Dependency Grammars$dependency gramma$	2
Rambow O., Joshi A., A Formal Look at  DG and Phrase-Structure  Grammars, with Special Consideration of Word-  Order Phenomena, Proc.	DG	Dependency Grammar$Dependency grammar$Dependency Grammars$dependency gramma$	2
LTAG is a lexicalized mildly-context sensitive tree  rewriting system \[Joshi et al, 1975; Schabes, 1990\]  that is closely related to DG and  Categorial Grammars.	DG	Dependency Grammar$Dependency grammar$Dependency Grammars$dependency gramma$	2
As most DGrs, the PD-grammars are analyzing.	DG	Dependency Grammar$Dependency grammar$Dependency Grammars$dependency gramma$	3
One of such properties, projectivity, re- quires that any word occurring between a word 	 and a word   dependent on 	 be dominated by 	 In first DGrs (Gaifman, 1961) and in some more recent proposals: link gram- mars (Sleator and Temperly, 1993), projective DGrs (Lombardo and Lesmo, 1996) the projectivity is implied by definition.	DG	Dependency Grammar$Dependency grammar$Dependency Grammars$dependency gramma$	3
3 Polarized DGrs Polarized DGrs determine DV- structures in the bottom-up manner in the course of reduction of phrases to their types, just as the categorial grammars do.	DG	Dependency Grammar$Dependency grammar$Dependency Grammars$dependency gramma$	3
Towards a generic multilingual DGr for text generation.	DG	Dependency Grammar$Dependency grammar$Dependency Grammars$dependency gramma$	3
VERONIS,  J., IDE, N., M. (1990) Word Sense  Disambiguation with Very Large Neural Networks  Extracted frmn MRD.	MRD	Machine Readable Dictionaries$machine readable dictionary$	0
We intend to derive the entire core lexicon for the system from MRD and then to tun e it against appropriate corpora .	MRD	Machine Readable Dictionaries$machine readable dictionary$	0
Automatic Sense Disambigua- tion Using MRD: How to Tell a Pine Cone from an Ice Cream Cone.	MRD	Machine Readable Dictionaries$machine readable dictionary$	0
Development of a Computational  Methodology for Deriving Natural Language  Semantic Structures via Analysis of  MRD.	MRD	Machine Readable Dictionaries$machine readable dictionary$	0
Word Sense Dis-  ambiguation with Very Large Neural Networks  Extracted from MRD.	MRD	Machine Readable Dictionaries$machine readable dictionary$	0
Veronis, Jean and Nancy Ide, "Word Sense Disam-  biguation with Very Large Neural Networks Extracted  from MRD," in Proceedings,  COLING-90, pp 389-394, 1990.	MRD	Machine Readable Dictionaries$machine readable dictionary$	0
have also translated synset definitions (e.g. Span- ish and Korean), so we can hope to create a multi- lingual corpus here as well and (v) the definitions can be used as a MRD, and various information extracted from there (Barn- brook, 2002; Nichols et al, 2006) 4.3 Kyoto Text Corpus The Kyoto Text Corpus consists of newspaper text from the Mainichi Newspaper (1995), seg- mented and annotated with Japanese POS tags and dependency trees (Kurohashi and Nagao, 2003).	MRD	Machine Readable Dictionaries$machine readable dictionary$	1
9 Conclusion  In this paper we have argued that semantic tagging  can be carried out only relative to the senses in some  lexicon and that a MRD pro-  vides an appropriate set of senses.	MRD	Machine Readable Dictionaries$machine readable dictionary$	1
A more knowledge-free system would have used a MRD or a large nat- ural language sample to retrieve its synonyms (see, for example, Lin (1998)), but our system falls short of this, relying on Roget?s New Millennium The- saurus1 (henceforth RT) as a source of synonyms.	MRD	Machine Readable Dictionaries$machine readable dictionary$	1
Metrics can either employ a MRD, i.e. textual defi- nitions of words therein as an underlying knowledge base [1,11], or operate on the structure of a conceptual network, whereby textual definitions themselves are not available [9,7].	MRD	Machine Readable Dictionaries$machine readable dictionary$	1
2 MiniWordnet Ideally the lexicon we would like to extend is a broad coverage MRD like Wordnet (Miller et al, 1990; Fellbaum, 1998).	MRD	Machine Readable Dictionaries$machine readable dictionary$	1
Segmentation phase: Employing maximal  matching algorithm to segment a sentences  into some words, and setting a word?s POS  set in MRD as its  POS tagging.	MRD	Machine Readable Dictionaries$machine readable dictionary$	1
Variety of MNr of kinds of token-level (0-5) alignments Table 2: Features used for Machine Learning Features 1-7 reflect relative numbers of matches (rel- ative to length of either the target or learner re- sponse).	MN	Match Numbe$mixed ngrams$	0
Number MNr is determined as fol- lows: Phrases starting with the words a, an, or this are singular; those, these, or some indicate plural.	MN	Match Numbe$mixed ngrams$	0
Variety of MNr of kinds of (0-5) token-level alignments Table 2: Features used in CoMiC?s classification phase Current versions of CoMiC use the WEKA toolkit (Hall et al 2009), allowing us to experiment with different machine learning strategies.	MN	Match Numbe$mixed ngrams$	0
Variety of MNr of kinds of (0-5) token-level alignments Table 3: Features used in the CoMiC-EN system dependency graph alignment in connection with two different machine learning approaches.	MN	Match Numbe$mixed ngrams$	0
Variety of MNr of kinds of token-level (0-5) alignments Table 3: Features used for the memory-based classifier 3 4 Content Assessment Experiment 4.1 Setup We ran our content assessment experiment using the two data sets introduced in section 2, one from Kansas University and the other from The Ohio State University.	MN	Match Numbe$mixed ngrams$	0
Also, MN helped to capture long patterns like ?	MN	Match Numbe$mixed ngrams$	1
We use another fea- ture set LEX to capture word ngrams, POS (part of speech) ngrams and MN.	MN	Match Numbe$mixed ngrams$	1
In addition to the features described in Sec- tion 4.2, the power prediction system presented in (Prabhakaran and Rambow, 2014) uses a lexi- cal feature set (LEX) that captures word ngrams, POS (part of speech) ngrams and MN, since lexical features have been established to be very useful for power prediction.	MN	Match Numbe$mixed ngrams$	1
Bickel et al (Bickel et al, 2007) discriminatively learns a scaling factor for STrain, so as to adapt the source domain data distribution to resemble the target domain data distribution, under the [S+T-] setting.	STrain	source domain training data$source domain training$	0
6 Analysis Using only the STrain, a coreference resolution system achieves an F- measure of 39.8% on the GENIA test set (the col- umn of ?	STrain	source domain training data$source domain training$	0
In the domain adaptation, we do semantic association inference on the STrain using LaSA model at first, then the original source domain NER model is tuned on the STrain set by incorporating these generated semantic association features.	STrain	source domain training data$source domain training$	0
In LaSA-based domain adaptation, the semantic association features of each unit in the observation window {-2,2} are generated by LaSA model at first, then the basic source domain NER model is tuned on the original STrain set by incor- porating the semantic association features.	STrain	source domain training data$source domain training$	0
5.5 Domain Adaptation with Active Learning In the experiments on domain adaptation with ac- tive learning for coreference resolution, we as- sume that the STrain are an- notated.	STrain	source domain training data$source domain training$	0
the set of STrain instances D t ?	STrain	source domain training data$source domain training$	1
To calibrate the conditional probability P (y|x) from the source domain to the target domain, ide- ally each STrain instance (x i , y i ) should be given a weight Pt(y s i |x s i ) P s (y s i |x s i ) .	STrain	source domain training data$source domain training$	1
Bickel et al (Bickel et al, 2007) discriminatively learns a scaling factor for STrain data, so as to adapt the source domain data distribution to resemble the target domain data distribution, under the [S+T-] setting.	STrain	source domain training data$source domain training$	1
We first train an NER model on a large STrain corpus, and then learn the correlation between the source and tar- get NE types.	STrain	source domain training data$source domain training$	1
In domain adaptation, there are typi- cally many more STrain instances than target domain training instances.	STrain	source domain training data$source domain training$	1
few features in target domain test instances appear in STrain instances.	STrain	source domain training data$source domain training$	1
Each HITs (HIT) consists of a pair of top 20 ranked aspect lists for an entity, with one list from UDB-m and the other chosen from one of the baseline algorithms.	HITs	Human Intelligence Tasks$human intelligence tasks$	0
To collect human answers on the test ques- tions, we delivered them to human beings through Amazon Mechanical Turk (AMT), a crowd-sourcing Internet marketplace that allows people to partici- pate in HITs.	HITs	Human Intelligence Tasks$human intelligence tasks$	0
These include inference rule discovery for question-answering and information retrieval (Lin and Pantel, 2001), idiom or multiword ex- 2HITs 3often referred to as turkers 4http://crowdflower.com 205 pression acquisition (Fellbaum et al 2006) and identification (Boukobza and Rappoport, 2009), machine translation evaluation (Snover et al 2009), textual entailment recognition, and many more.	HITs	Human Intelligence Tasks$human intelligence tasks$	0
Based on this, we loaded each MTurk HITs (HIT) with five tweets, and paid workers five cents per HIT.	HITs	Human Intelligence Tasks$human intelligence tasks$	0
However, a less apparent advantage is the need for researchers to provide succinct and comprehensible descriptions of HITs, and the need to break complex annotation tasks down to simpler basic units of work for annotators.	HITs	Human Intelligence Tasks$human intelligence tasks$	0
Regardless of the cause, given these results, we re- stricted the availability of all following experiments to Turkers in the US.Ideally we would include other English-speaking countries, but there is no straight- 2HITs ?	HITs	Human Intelligence Tasks$human intelligence tasks$	0
purpose crowdsourcing marketplace, the Univer- sal Human Relevance System (UHRS).3 The mar- ketplace connects HITs with a large population of workers across the globe.	HITs	Human Intelligence Tasks$human intelligence tasks$	1
In the sec- ond blog, RWNs, which is a conservative blog, we see a different picture.	RWN	Right Wing New$Right Wing News$	0
We selected three blog sites from this dataset: the RWNs (right-ideology) ; the Carpetbagger, and Daily Kos as representatives 1144 palest inian is raeli peace year  polit ical  proces s   state  end  rig ht   g overnment  need  conflict way s ecurit y palest inian is raeli Peace polit ical  occupation  proces s end  s ecurit y   conflict   way  g overnment   people t ime year force  neg ot iation bush US pres ident  american sharon admi	RWN	Right Wing New$Right Wing News$	0
In the sec- ond blog, RWN, which is a conservative blog, we see a different picture.	RWN	Right Wing New$Right Wing News$	1
We selected three blog sites from this dataset: the RWN (right-ideology) ; the Carpetbagger, and Daily Kos as representatives 1144 palest inian is raeli peace year  polit ical  proces s   state  end  rig ht   g overnment  need  conflict way s ecurit y palest inian is raeli Peace polit ical  occupation  proces s end  s ecurit y   conflict   way  g overnment   people t ime year force  neg ot iation bush US pres ident  american sharon admi	RWN	Right Wing New$Right Wing News$	1
We also estimated the lower bound  of this evaluation, that is, we also conducted the same  trials using the BGH.	BGH	Bunruigoihyo thesaurus$Bunrui Goi Hyou$Bunruigoihy3$Bunruigoihy6$	0
Since resolution  of equations is time-consuming, we tentatively general-  ized 23,223 nouns into 303 semantic lasses (represented  by the first 4 digits of the semantic ode given in the  BGH), reducing the total number of  equations to 45,753.	BGH	Bunruigoihyo thesaurus$Bunrui Goi Hyou$Bunruigoihy3$Bunruigoihy6$	0
Further, tuples with  nouns appearing in the BGH were se-  lected.	BGH	Bunruigoihyo thesaurus$Bunrui Goi Hyou$Bunruigoihy3$Bunruigoihy6$	0
When the noun comprised a compound noun,  it was transformed into the maximal eftmost substring  contained in the BGH.	BGH	Bunruigoihyo thesaurus$Bunrui Goi Hyou$Bunruigoihy3$Bunruigoihy6$	0
We tentatively use the  BGH, inwhich each word corresponds  to a leaf in the tree structure.	BGH	Bunruigoihyo thesaurus$Bunrui Goi Hyou$Bunruigoihy3$Bunruigoihy6$	0
We use the BGH, which contains 96,000 Japanese words (The National In- stitute for Japanese Language, 2004).	BGH	Bunruigoihyo thesaurus$Bunrui Goi Hyou$Bunruigoihy3$Bunruigoihy6$	0
\[Utsuro et al, 1993\] categorise  words using the "BGH"  (Japanese) thesaurus.	BGH	Bunruigoihyo thesaurus$Bunrui Goi Hyou$Bunruigoihy3$Bunruigoihy6$	1
The National Language Research Institute: BGH,  (in Japanese), Shuuei Publishing, 1964.	BGH	Bunruigoihyo thesaurus$Bunrui Goi Hyou$Bunruigoihy3$Bunruigoihy6$	1
BGH.	BGH	Bunruigoihyo thesaurus$Bunrui Goi Hyou$Bunruigoihy3$Bunruigoihy6$	1
In this case, we use  the thesaurus dictionary "BGH" (NLRI,  1964) to learn the meanings of nouns.	BGH	Bunruigoihyo thesaurus$Bunrui Goi Hyou$Bunruigoihy3$Bunruigoihy6$	1
A Japanese thesaurus, the BGH (NLRI, 1964), was used to determine the category number of each word.	BGH	Bunruigoihyo thesaurus$Bunrui Goi Hyou$Bunruigoihy3$Bunruigoihy6$	1
The value S is the semantic  similarity between a possible antecedent and Noun  X of "Noun X no Noun Y." Semantic similarity is  shown by level in BGH (NLRI, 1964).	BGH	Bunruigoihyo thesaurus$Bunrui Goi Hyou$Bunruigoihy3$Bunruigoihy6$	1
Learning structure using ABL.	ABL	Alignment Based Learning$ablative$	0
Nouns are inflected based  on number (singular, plural), article and case [k?- raka] (nominative, accusative, instrumental, dative,  ABL, genitive, locative and vocative).	ABL	Alignment Based Learning$ablative$	1
Although the pure DG for-  malism proved to be particularly practical for integration of idioms and exceptions,  its lack of constituent symbols, i.e., non-terminals, would have lead to a grammar  of enormous ize and made it difficult to integrate special Latin constructs uch as  accusative cum infinitive or ABL absolute.	ABL	Alignment Based Learning$ablative$	1
At most one dative (locative, ABL, instrumental) adjunct can link to a verb.	ABL	Alignment Based Learning$ablative$	1
I then gave a short (over- simplified) tutorial on Latin and Japanese gram- mar, suggesting a connection between Latin cas- es (e.g., nominative, accusative, ABL, etc.)	ABL	Alignment Based Learning$ablative$	1
return + ABL case  (e) Vowel elimination ????	ABL	Alignment Based Learning$ablative$	1
work + ABL case   Figure 1: Inflection types of content words in  Mongolian phrases.	ABL	Alignment Based Learning$ablative$	1
health, care, insurance, public, pri- vate Kent Conrad, Paul Hsieh, PK, Ezra Klein, Jacob Hacker ?	PK	Paul Krugman$Prior Knowledge$Peking University$	0
User Input Wikipedia?s Suggestion Correct Spelling Suchifun Houkingu Suchin Housing Stephen Hawking Stefan Hoking Stefan Ho king Stephen Hawking Pol Crugman Poll Krugman PK Paal Kragaman PK PK Suburaamaniya Ba- haarachi Subramaniya Baracchi Subrahmaniya Bharati search in Wikipedia.	PK	Paul Krugman$Prior Knowledge$Peking University$	0
Consider the following sentences from a New York Times (NYT) column written by PK: ?	PK	Paul Krugman$Prior Knowledge$Peking University$	0
For each document 115 Columnist Cluster I Cluster II Dowd 294 4 Krugman 3 328 Table 2: Results when clustering 629 documents written by Maureen Dowd and PK into two clusters.	PK	Paul Krugman$Prior Knowledge$Peking University$	0
economist (Princeton, economist, Princeton economist PK 7 PK) was awarded the Nobel prize in 2008.	PK	Paul Krugman$Prior Knowledge$Peking University$	0
Aspect Extraction with Automated PK Learning.	PK	Paul Krugman$Prior Knowledge$Peking University$	1
PK Aspect_i Sentiment_i  ?	PK	Paul Krugman$Prior Knowledge$Peking University$	1
Leveraging  Multi-Domain PK in Topic Models.	PK	Paul Krugman$Prior Knowledge$Peking University$	1
c?2013 Association for Computational Linguistics AMI&ERIC: How to Learn with Naive Bayes and PK: an Application to Sentiment Analysis Mohamed Dermouche1,2, Leila Khouas1, 1AMI Software R&D 1475 av.	PK	Paul Krugman$Prior Knowledge$Peking University$	1
Reading both High- coherence and Low-coherence Texts: Effects of Text  Sequence and PK.	PK	Paul Krugman$Prior Knowledge$Peking University$	1
and Anderson, R.C. 1986 What They Don't Know Will  Hurt Them: The Role of PK in Comprehension.	PK	Paul Krugman$Prior Knowledge$Peking University$	1
Specification for Corpus Proc- essing at PK:Word Segmentation,  POS Tagging and Phonetic Notation.	PK	Paul Krugman$Prior Knowledge$Peking University$	2
4 Discussion and Future Work In our survey, only 33% of nouns and 44% of verbal nouns created by kanji/hanzi conversion method exist in the PK dictionary.	PK	Paul Krugman$Prior Knowledge$Peking University$	2
PK: (PK Dictionary) http://www.icl.pku.edu.cn/.  Proceedings of the Fifth SIGHAN Workshop on Chinese Language Processing, pages 122?125, Sydney, July 2006.	PK	Paul Krugman$Prior Knowledge$Peking University$	2
Then, we consult these words using a Chinese dictionary provided by PK [14].	PK	Paul Krugman$Prior Knowledge$Peking University$	2
Knowledge Sharing via Social Login: Exploiting Microblogging Service for Warming up Social Question Answering Websites Yang Xiao 1 , Wayne Xin Zhao 2 , Kun Wang 1 and Zhen Xiao 1 1 School of Electronics Engineering and Computer Science, PK, China 2 School of Information, Renmin University of China, China {xiaoyangpku, batmanfly}@gmail.com {wangkun, xiaozhen}@net.pku.edu.cn Abstract Community Question Answering (CQA) websites such as Quora are widely used for users to get high quality answers.	PK	Paul Krugman$Prior Knowledge$Peking University$	2
Contextual Corrector employs lan- guage modelling to rank a list of potential can- didates in the scope of whole sentence whereas WC chooses the best candidate for each syllable that has the highest weights.	WC	Weighting-based Corrector$Words Correct$	0
3.4 Corrector In VOSE, we propose two possible correctors: WC Given a ranked top-K list of potential can- didates from Non-syllable Detector and Real- syllable Detector, WC simply chooses the best candidates based on their weights (Equation 5) to produce the final output.	WC	Weighting-based Corrector$Words Correct$	0
Statistical analyses provide  evidence that machine WC scores  correlate well with scores provided by teachers  and expert scorers, with all (Pearson?s  correlation coefficient) r?s > 0.98 at the  individual response level, and all r?s > 0.99 at  the ?	WC	Weighting-based Corrector$Words Correct$	1
i pi log pi Window Width 2 4 6 8 10 Nu mb er  of  R ule s 0 1000 2000 3000 4000 5000 6000 LTS Rule Count vs Window Width Legend English 40k Dutch 40k Afrikaans 37k Italian 40k Legend Chars Correct WC WC Number of Rules 0 10 20 30 40 50 60 Pe rce nt  C or re ct 0 20 40 60 80 100 Spanish LTS Ruleset Performance W=3W=2W=1 235 Beyond a window width of 7, rule growth tapers off  considerably.	WC	Weighting-based Corrector$Words Correct$	1
Answering the  first part of the question involves comparing  machine WC scores to human scores  when teachers make ratings in the classroom  environment as the student reads into the phone.	WC	Weighting-based Corrector$Words Correct$	1
or median  WC value, from expert scorers.	WC	Weighting-based Corrector$Words Correct$	1
Answering the second  part of the question involves comparing machine  WC scores to a ?	WC	Weighting-based Corrector$Words Correct$	1
Dynamic help generation by estimating user?s mental model in SDSs.	SDSs	spoken dialogue systems$Spoken Dialogue Systems$	0
1 Introduction Studies on SDSs have recently proceeded from in-laboratory systems to ones de- ployed to the open public (Raux et al, 2006; Ko- matani et al, 2007; Nisimura et al, 2005).	SDSs	spoken dialogue systems$Spoken Dialogue Systems$	0
2 Related Work Early discriminative approaches to text generation were introduced in SDSs, and usually tackled content selection and surface re- alization separately.	SDSs	spoken dialogue systems$Spoken Dialogue Systems$	0
2 Related Work Various studies have been done on generating help messages in SDSs.	SDSs	spoken dialogue systems$Spoken Dialogue Systems$	0
Targeted help for SDSs: intelligent feed- back improves naive users?	SDSs	spoken dialogue systems$Spoken Dialogue Systems$	0
Adding intelligent help to mixed-initiative SDSs.	SDSs	spoken dialogue systems$Spoken Dialogue Systems$	0
of the AAAI Workshop on Statistical and Empirical Methods in SDSs.	SDSs	spoken dialogue systems$Spoken Dialogue Systems$	1
A Reinforce-ment Learning Approach to Evaluating State Rep-resentations in SDSs.	SDSs	spoken dialogue systems$Spoken Dialogue Systems$	1
c?2009 Association for Computational Linguistics Ranking Help Message Candidates Based on Robust Grammar Verification Results and Utterance History in SDSs Kazunori Komatani Satoshi Ikeda Yuichiro Fukubayashi Tetsuya Ogata Hiroshi G. Okuno Graduate School of Informatics Kyoto University Yoshida-Hommachi, Sakyo, Kyoto 606-8501, Japan {komatani,sikeda,fukubaya,ogata,okuno}@kuis.kyoto-u.ac.jp Abstract We address an issue of out-of-grammar (OOG) utterances in spoken dialogue sys- tems by generating help messages for novice users.	SDSs	spoken dialogue systems$Spoken Dialogue Systems$	1
SemEval-2014 Task 2: Grammar Induction for SDSs Ioannis Klasinas 1 , Elias Iosif 2,4 , Katerina Louka 3 , Alexandros Potamianos 2,4 1 School of ECE, Technical University of Crete, Chania 73100, Greece 2 School of ECE, National Technical University of Athens, Zografou 15780, Greece 3 Voiceweb S.A., Athens 15124, Greece 4 Athena Research Center, Marousi 15125, Greece iklasinas@isc.tuc.gr,{iosife,potam}@telecom.tuc.gr,klouk	SDSs	spoken dialogue systems$Spoken Dialogue Systems$	1
c?2009 Association for Computational Linguistics Natural Language Generation as Planning Under Uncertainty for SDSs Verena Rieser School of Informatics University of Edinburgh vrieser@inf.ed.ac.uk Oliver Lemon School of Informatics University of Edinburgh olemon@inf.ed.ac.uk Abstract We present and evaluate a new model for Natural Language Generation (NLG) in SDSs, based on statis- tical planning, given noisy feedback from the current generation context (e.g. a user and a surface realiser).	SDSs	spoken dialogue systems$Spoken Dialogue Systems$	1
In this paper we present and evaluate a new model for NLG in SDSs as planning under uncertainty.	SDSs	spoken dialogue systems$Spoken Dialogue Systems$	1
Deep CNNs for sentiment analysis of short texts.	CNN	convolutional neural network$Convolutional Neural Network$convolutional neural networks$	0
Recent ad- 1161 vances in CNNs.	CNN	convolutional neural network$Convolutional Neural Network$convolutional neural networks$	0
With the help of an advanced entity linking system and a deep CNN model that matches questions and predicate se- quences, our system outperforms previous meth- ods substantially on the WEBQUESTIONS dataset.	CNN	convolutional neural network$Convolutional Neural Network$convolutional neural networks$	0
Here we present a non-linear method based on a deep CNN.	CNN	convolutional neural network$Convolutional Neural Network$convolutional neural networks$	0
Learning semantic representations using CNNs for web search.	CNN	convolutional neural network$Convolutional Neural Network$convolutional neural networks$	0
A CNN for mod- elling sentences.	CNN	convolutional neural network$Convolutional Neural Network$convolutional neural networks$	0
Recur- rent CNNs for Discourse Compositionality.	CNN	convolutional neural network$Convolutional Neural Network$convolutional neural networks$	1
c?2014 Association for Computational Linguistics Learning Image Embeddings using CNNs for Improved Multi-Modal Semantics Douwe Kiela ?	CNN	convolutional neural network$Convolutional Neural Network$convolutional neural networks$	1
2014 Association for Computational Linguistics A CNN for Modelling Sentences Nal Kalchbrenner Edward Grefenstette {nal.kalchbrenner, edward.grefenstette, phil.blunsom}@cs.ox.ac.uk Department of Computer Science University of Oxford Phil Blunsom Abstract The ability to accurately represent sen- tences is central to language understand- ing.	CNN	convolutional neural network$Convolutional Neural Network$convolutional neural networks$	1
2015 Association for Computational Linguistics Classifying Relations by Ranking with CNNs C??cero Nogueira dos Santos IBM Research 138/146 Av.	CNN	convolutional neural network$Convolutional Neural Network$convolutional neural networks$	1
c?2015 Association for Computational Linguistics Non-Linear Text Regression with a Deep CNN Zsolt Bitvai University of Sheffield, UK z.bitvai@shef.ac.uk Trevor Cohn University of Melbourne, Australia t.cohn@unimelb.edu.au Abstract Text regression has traditionally been tackled using linear models.	CNN	convolutional neural network$Convolutional Neural Network$convolutional neural networks$	1
2016) use a CNN for implicit discourse relation classification.	CNN	convolutional neural network$Convolutional Neural Network$convolutional neural networks$	1
Deep CNN for sentiment analysis of short texts.	CNN	convolutional neural network$Convolutional Neural Network$convolutional neural networks$	2
Kiela and Bottou (2014) showed that transferring representations from deep CNN (ConvNets) yield much better performance than bag-of-visual-words in multi-modal semantics.	CNN	convolutional neural network$Convolutional Neural Network$convolutional neural networks$	2
Recent ad- 1161 vances in CNN.	CNN	convolutional neural network$Convolutional Neural Network$convolutional neural networks$	2
Learning semantic representations using CNN for web search.	CNN	convolutional neural network$Convolutional Neural Network$convolutional neural networks$	2
Our approach does not de- pend on the availability of large amounts of in-domain parallel data, but only re- lies on available large datasets of monolin- gually captioned images, and on state-of- the-art CNN to compute image similarities.	CNN	convolutional neural network$Convolutional Neural Network$convolutional neural networks$	2
Learning image embeddings using CNN for improved multi-modal semantics.	CNN	convolutional neural network$Convolutional Neural Network$convolutional neural networks$	2
This often  has the BNF effect of separating inflectional  and derivational ffixes from the roots.	BNF	beneficial$Backus Normal Form$	0
It is possible to use an alternative, heuristic search based on Viterbi n  best (we will not go into the PCFG-reduction technique presented in Goodman (1998) since that heuristic only works for Tree- DOP and is BNF only if all subtrees are taken into account and if the so-called "labeled recall parse" is computed).	BNF	beneficial$Backus Normal Form$	0
It would be BNF to compact such a network using FSRTs, and to inspect the time versus space tradeoff on such a comprehensive network.	BNF	beneficial$Backus Normal Form$	0
syntactic information would be BNF for the  machine to learn.	BNF	beneficial$Backus Normal Form$	0
Furthermore, the use of predicates can be BNF for describing natural language reduplication where the reduplication is not as bounded as the example we deal with in this work.	BNF	beneficial$Backus Normal Form$	0
For in- tersection, we believe that the use of predicates (van Noord and Gerdemann 2001b) can be BNF.	BNF	beneficial$Backus Normal Form$	0
The d e f i n i t i o n s  are w r i t t e n  i n  BNF, A n  example of  a s t r i n g  d e f i n i t i o n  is:  <ASSERTION> ::= <SA><SUBJECT><SA><TENSE><SA><VERB><SA><OBJECT><RV><SA>.	BNF	beneficial$Backus Normal Form$	1
Mathematical Model of Serbo- Croatian Morphology (Nominal INFL).	INFL	Inflection$in the transformational$	0
INFLs can change the vowel of the first  o-syllable of the stem.	INFL	Inflection$in the transformational$	0
3  Table 1: O-syllable Form Examples  3 Morphological Impact of INFLs  Like English, the inflections in Bengali work as a  suffix to the stem.	INFL	Inflection$in the transformational$	0
INFL We introduced tag suffixes for inflec- tion as clues to identify the attachment position of the verb and adjective phrases, because Japanese verbs and adjectives have inflections, which depends 110 (no label) base form cont continuative form attr attributive form neg negative form hyp hypothetical form imp imperative form stem stem Table 2: INFL tag suffixes on their modifying wor	INFL	Inflection$in the transformational$	0
INFL We introduced tag suffixes for inflec- tion as clues to identify the attachment position of the verb and adjective phrases, because Japanese verbs and adjectives have inflections, which depends 110 (no label) base form cont continuative form attr attributive form neg negative form hyp hypothetical form imp imperative form stem stem Table 2: INFL tag suffixes on their modifying words and phrases (e.g. noun and verb phrases).	INFL	Inflection$in the transformational$	0
INFLs can act as simple suffix and do not  make any change in the verb stem.	INFL	Inflection$in the transformational$	0
The binary tree?s structure has the follow- NP Noun phrase PP Postposition phrase VP Verb phrase ADJP Adjective phrase ADVP Adverbial phrase CONJP Conjunction phrase S Sentence (=root) IP INFLal phrase IP-MAT Matrix clause IP-ADV Adverb clause IP-REL Gapping relative clause IP-ADN Non-gapping adnominal clause CP Complementizer phrase CP-THT Sentential complement Function tags semantic role for mandatory argument (gap notation) -ARG0 ( arg0) -ARG1 ( arg1) -ARG2 ( arg2) grammatical role for mandatory argument (gap notation) -SBJ ( sbj) Subjective case -OBJ ( obj) Objective case	INFL	Inflection$in the transformational$	0
These labels, that are called phonological and seman- tic features INFL tradition, are computed from the proofs and consist of two parts that can be superimposed: a phonological label, denoted by ffi"!$#&%'(ffi , and a semantic label2 de- noted by )*!	INFL	Inflection$in the transformational$	1
Surface structure is  identical to S-structure, except for the fact that the  association between moved phrases and their traces  is not present; chain indices that reveal history of  movement INFL ccount are not  present.	INFL	Inflection$in the transformational$	1
TCMPDBLK exemplifies the interaction of syntactic and semantic  information INFL component of the REQUEST Sys-  tem, in that it filters out a variety of otherwise acceptable structures in  which a noun phrase or prepositional phrase with head noun marked  (+ TIME) is adjacent o a noun phrase with head noun marked (+  PERIODIC), but where the former is not analyzed as a modifier of  the latter.	INFL	Inflection$in the transformational$	1
This modification opera- tion allows the FST to encode information about the history of states INFL FST as part of the model structure.	INFL	Inflection$in the transformational$	1
predicate -raising "  INFL grammar).	INFL	Inflection$in the transformational$	1
We have tried to  show, however, that even the empir ical  approach felt the  need for mult id imensional  structuring, and certain rules  set up on empir ical  considerat ions - e.g. those treating  verb with mult ip le meaning - turn out to be mappings of  phrase markers into phrase markers INFL   sense.- . . . . . .	INFL	Inflection$in the transformational$	1
In our VJ-WIMP application, we use Deg ree  of C ons trict ion Front Central Back High Mid Low TB Position [iy ] [ix ] [uw ] [ey] [ax ] [ow ] [ae ] [a] [aa ] Figure 1: Vowel configurations as a function of their dominant articulatory configurations.	TB	Tongue Body$the base$	0
For the exact match scheme, the obtained performance is higher7 than TBline (random guess) that equals to 0.250.	TB	Tongue Body$the base$	1
Since the  Hobbs' algorithm serves as TB of our  scheme, we expect the accuracy to be much  higher with more accurately transformed trees.	TB	Tongue Body$the base$	1
We now list TB lexical features that were considered for this experiment.	TB	Tongue Body$the base$	1
This concludes TB features we considered.	TB	Tongue Body$the base$	1
Using random guesses, TBline precision is 0.010 and 0.333 for quote-to-speaker attribution and gender estimation, respectively.	TB	Tongue Body$the base$	1
Using ran- dom guesses, TBline accuracy is 0.33.	TB	Tongue Body$the base$	1
Semeval-2013 task 8: CLTE for content syn- chronization.	CLTE	Cross-lingual textual entailment$classifiers for detecting$	0
Fbk: CLTE with-out translation.	CLTE	Cross-lingual textual entailment$classifiers for detecting$	0
semeval-2012 task 8: CLTE for con- tent synchronization.	CLTE	Cross-lingual textual entailment$classifiers for detecting$	0
Semeval-2012 task 8: CLTE for content synchronization.	CLTE	Cross-lingual textual entailment$classifiers for detecting$	0
semeval- 2012 task 8: CLTE for content syn- chronization.	CLTE	Cross-lingual textual entailment$classifiers for detecting$	0
We got some improvement from training two separate CLTE anaphora and antecedent mentions.	CLTE	Cross-lingual textual entailment$classifiers for detecting$	1
We report significant absolute im- provements in performance in multi-class prediction, as well as significant improve- ment of binary CLTE the presence of implicit Temporal, Compari- son and Contingency relations.	CLTE	Cross-lingual textual entailment$classifiers for detecting$	1
We trained separate CLTE anaphor and antecedent mentions, and experimented with several clustering techniques to discover the most suitable algorithm for producing coreference chains in this domain.	CLTE	Cross-lingual textual entailment$classifiers for detecting$	1
We develop a series of supervised CLTE the expression of views on the legitimacy of income inequality.	CLTE	Cross-lingual textual entailment$classifiers for detecting$	1
We apply error generation methods and train CLTE and correcting arti- cle errors in essays written by non-native En- glish speakers; we show that training on data that contain errors produces higher accuracy when compared to a system that is trained on clean native data.	CLTE	Cross-lingual textual entailment$classifiers for detecting$	1
Han and Baldwin, 2011) de- veloped CLTE the ill-formed words and generated corrections based on the morpho- phonemic similarity. (	CLTE	Cross-lingual textual entailment$classifiers for detecting$	1
The MADCOW collection and evaluation procedures  have provided effective tools for assessing the current ca-  pabilities of interactive SLS.	SLS	spoken language systems$Spoken Language  Systems$Spoken Language System$	0
effective tools for assessing the current ca-  pabilities of interactive SLS.	SLS	spoken language systems$Spoken Language  Systems$Spoken Language System$	0
These reference answers  were used by the system developers and by NIST to eval-  uate the responses of the MADCOW natural language  and SLS.	SLS	spoken language systems$Spoken Language  Systems$Spoken Language System$	0
As our SLS evolve,  data collection and evaluation methods mu	SLS	spoken language systems$Spoken Language  Systems$Spoken Language System$	0
Our shared goal is  to build interactive SLS; however,  our evaluation methods rely on a canned corpus and  evaluate a system's recognition performance under static  conditions that are not representative of the interactive  environments in which these systems will eventually be  used.	SLS	spoken language systems$Spoken Language  Systems$Spoken Language System$	0
As our SLS evolve,  data collection and evaluation methods must evolve with  them.	SLS	spoken language systems$Spoken Language  Systems$Spoken Language System$	0
Conc lus ions   This paper has described several approaches to training  and evaluation of SLS.	SLS	spoken language systems$Spoken Language  Systems$Spoken Language System$	0
The ATIS SLS pilot  corpus.	SLS	spoken language systems$Spoken Language  Systems$Spoken Language System$	0
In Proceedings  of the ARPA SLSs Tech-  nology Workshop.	SLS	spoken language systems$Spoken Language  Systems$Spoken Language System$	2
INESC-ID Lisboa, SLSs Lab, R. Alves Redol 9, 1000-029 LISBOA, Portugal.	SLS	spoken language systems$Spoken Language  Systems$Spoken Language System$	2
Gauvain, J.L., et al, "LIMSI Nov92 Evaluation", Oral  Presentation at the SLSs Technology  Workshop, January 20-22, 1993, Cambridge, MA.	SLS	spoken language systems$Spoken Language  Systems$Spoken Language System$	2
Coling 2008: Proceedings of the workshop on Multi-source Multilingual Information Extraction and Summarization, pages 33?40 Manchester, August 2008 Mixed-Source Multi-Document Speech-to-Text Summarization Ricardo Ribeiro INESC ID Lisboa/ISCTE/IST SLSs Lab Rua Alves Redol, 9 1000-029 Lisboa, Portugal rdmr@l2f.inesc-id.pt David Martins de Matos INESC ID Lisboa/IST SLSs Lab Rua Alves Redol, 9 1000-029 Lisboa, Portugal david@l2f.inesc-id.pt Abstract Speech-to-text summarization systems usually take as input the output of an automatic speech recognition (ASR) system that is affected by issues like speech recognition errors, disfluencies, or difficulties in the accurate identification of sentence boundaries.	SLS	spoken language systems$Spoken Language  Systems$Spoken Language System$	2
Coling 2008: Proceedings of the workshop on Multi-source Multilingual Information Extraction and Summarization, pages 33?40 Manchester, August 2008 Mixed-Source Multi-Document Speech-to-Text Summarization Ricardo Ribeiro INESC ID Lisboa/ISCTE/IST SLSs Lab Rua Alves Redol, 9 1000-029 Lisboa, Portugal rdmr@l2f.inesc-id.pt David Martins de Matos INESC ID Lisboa/IST SLSs Lab Rua Alves Redol, 9 1000-029 Lisboa, Portugal david@l2f.inesc-id.pt Abstract Speech-to-text summarization systems usually take as input the output of an automatic speech recognition (ASR) system that is affected by issues like speech re	SLS	spoken language systems$Spoken Language  Systems$Spoken Language System$	2
Thanks to Matthew John- son, Ramesh Sridharan, Finale Doshi, S.R.K. Brana- van, the MIT SLSs group and the anonymous reviewers for helpful comments.	SLS	spoken language systems$Spoken Language  Systems$Spoken Language System$	2
In these chains of utterances, the speaker is not explicitly mentioned because the author relies on the shared understanding with the reader that adja- cent pieces of quoted speech are not inDD (Zhang et al.,	DD	Dear Daughter$dependent$dual decomposition$	1
salience(re/)  = -2  log  Making the unrealistic simplifying assumption  that references of one gender class are com-  pletely inDD of references for another  classes 1, the likelihood function in this case is  just the product over all classes of the probabil-  ities of each class of reference to the power of  the number of observations of this class.	DD	Dear Daughter$dependent$dual decomposition$	1
Given a particular choice of the antecedent  candidates, the distance is inDD of  distances of candidates other than the an-  tecedent (and the distance to non-referents  can be ignored):  P(so, d~a, 2~) o?	DD	Dear Daughter$dependent$dual decomposition$	1
It is reason-  able to assume that the antecedents in W are  inDD of each other; in other words,  P(wo+llwo, h,t , l ,a) = P(wo+llh, t,l,a}.	DD	Dear Daughter$dependent$dual decomposition$	1
We assume the selection of the  pronoun is inDD of the candidates  other than the antecedent.	DD	Dear Daughter$dependent$dual decomposition$	1
The syntnctic structure st, and the distance  from the pronoun da are inDD of the  number of times the referent is mentioned.	DD	Dear Daughter$dependent$dual decomposition$	1
The technique of DD has recently been shown to yield state-of-the-art perfor- mance in dependency parsing (Koo et al, 2010).	DD	Dear Daughter$dependent$dual decomposition$	2
3.2 Dual Problem Formulation To describe a DD inference proce- dure for our model, we first restate the inference problem under our graphical model in terms of the two overlapping subgraphs that admit tractable in- ference.	DD	Dear Daughter$dependent$dual decomposition$	2
Inference can be per- formed via DD, which reuses the efficient inference algorithms of the direc- tional models.	DD	Dear Daughter$dependent$dual decomposition$	2
The DD inference approach al- lows us to exploit this sub-graph structure (Rush et al.,	DD	Dear Daughter$dependent$dual decomposition$	2
On DD and linear programming relaxations for natural language processing.	DD	Dear Daughter$dependent$dual decomposition$	2
A comparison of loopy belief propagation and DD for integrated CCG supertagging and parsing.	DD	Dear Daughter$dependent$dual decomposition$	2
The technique of DD has recently been shown to yield state-of-the-art perfor- mance in dependency parsing (Koo et al, 20	DD	Dear Daughter$dependent$dual decomposition$	2
Figure 3 also includes three DIM (the 310 is _n ot (n n) is _n ot (n p) do es _n ot (n n) do es _n ot (n p) n o t(n n) n o t(n p) do _n ot (n n) do _n ot (n p) n o (n n) n o (n p) 0.15 0.20 0.25 0.30 Figure 4: The behavior of individual negators in negated negative (nn) and negated positive (np) context.	DIM	diminishers$diminutive$	0
no reason at all to believe is irrealis, for example); word sense (e.g., Environmental Trust versus He has won the peo- ple?s trust); the syntactic role of a word in the sen- tence (e.g., polluters are versus they are polluters); and DIM such as little (e.g., little truth, lit- tle threat). (	DIM	diminishers$diminutive$	0
DIM (the 310 is _n ot (n n) is _n ot (n p) do es _n ot (n n) do es _n ot (n p) n o t(n n) n o t(n p) do _n ot (n n) do _n ot (n p) n o (n n) n o (n p) 0.15 0.20 0.25 0.30 Figure 4: The behavior of individual negators in negated negative (nn) and negated positive (np) context.	DIM	diminishers$diminutive$	0
This shows that the boundary between negators and DIM can by fuzzy.	DIM	diminishers$diminutive$	0
By following (Kennedy and Inkpen, 2006), we ex- tracted 319 DIM (also called understate- ment or downtoners) from General Inquirer3.	DIM	diminishers$diminutive$	0
Based on a shallow error analysis, we believe that including additional classification features may also be promising: modifiers other than nega- tion cues (DIM, increasers, modal verbs, etc.)	DIM	diminishers$diminutive$	0
Bi- grams are used especially to spot the influence of modifiers (negations, intensifiers, DIM) on the polarity of the sentiment-bearing words.	DIM	diminishers$diminutive$	0
It can also be found in Hebrew as a DIM formation of nouns and adjectives: keleb klablab $apan $panpan zaqan zqanqan $axor $xarxar dog puppy rabbit bunny beard goatee black dark qatan qtantan little tiny Let ?	DIM	diminishers$diminutive$	1
The computational  grammar writer reads the description and tries to  implement it, but a question arises: is the  DIM qualifier used in all the environments  that the three allomorphs of the non-DIM  qualifier are used, or only one of those  environments?	DIM	diminishers$diminutive$	1
In agreement with this pattern, several polite verb forms (-masu, -mashi) and a polite honorific (o-) are among the k-top female words, as is a DIM honorific often used to refer to women (-chan).	DIM	diminishers$diminutive$	1
The language expert finds examples  showing the DIM in all environments,  enabling the computational grammar writer to  proceed.	DIM	diminishers$diminutive$	1
This list allows the system to consider the most frequent synonyms and DIMs.	DIM	diminishers$diminutive$	1
The instances are taken from a corpus of PDN.	PDN	People?s Daily News$Predictive Dialogue Network$	0
It was obtained from the  PDNpaper from 01/1991 to  12/1993 and from the Xinhua News Agency for  04/1994 to 09/1995 from the Linguistic Data  Consortium (http://www.ldc.upenn.edu).	PDN	People?s Daily News$Predictive Dialogue Network$	0
Based on the 20 words, we extracted 28,000  sentences from the 60 MB PDN  with segmentation information as our train- ing/test set which is then manually sense-tagged.	PDN	People?s Daily News$Predictive Dialogue Network$	0
The collocation list is constructed from a  combination of a digital collocation dictionary, a  return result from a collocation automatic ex- traction system [21], and a hand collection from  the PDN.	PDN	People?s Daily News$Predictive Dialogue Network$	0
These features are  extracted form the 60MB human sense-tagged  PDN with segmentation infor- mation.	PDN	People?s Daily News$Predictive Dialogue Network$	0
A useful result  from this work based on (about one million  words) the tagged PDN shows  that adding more features from richer levels of  linguistic information such as PoS tagging  yielded no significant improvement (less than  1%) over using only the bi-gram co-occurrences  information.	PDN	People?s Daily News$Predictive Dialogue Network$	0
At the core of the FERRET?s predictive dialogue module is the PDN (PQN), a large database of QUABs that were either generated off-line by human annotators or created on-line by FERRET (either during the current dialogue or dur- ing some previous dialogue)1.	PDN	People?s Daily News$Predictive Dialogue Network$	1
In Proceedings of 10th Workshop on ALRs, at 24th International Conference on Computational Linguis- tics (COLING 2012).	ALR	Asian Language Resource$Asian Language Resources$	0
300  Proceedings of the 7th Workshop on ALRs, ACL-IJCNLP 2009, pages 17?23, Suntec, Singapore, 6-7 August 2009.	ALR	Asian Language Resource$Asian Language Resources$	0
In Proceedings of the 3rd Work- shop on ALRs and International Standardization, COLING 19, Taipei, Taiwan.	ALR	Asian Language Resource$Asian Language Resources$	0
352  Proceedings of the 8th Workshop on ALRs, pages 169?177, Beijing, China, 21-22 August 2010.	ALR	Asian Language Resource$Asian Language Resources$	0
In In Proceedings of the Fourth Workshop on ALRs, Sanya, China, pp.	ALR	Asian Language Resource$Asian Language Resources$	0
In Proceedings of the 3rd Workshop on ALRs and In- ternational Standardization at the 19th International Conference on Computational Linguistics, Vol.	ALR	Asian Language Resource$Asian Language Resources$	0
In Proceedings of 10th Workshop on ALR, at 24th International Conference on Computational Linguis- tics (COLING 2012).	ALR	Asian Language Resource$Asian Language Resources$	1
300  Proceedings of the 7th Workshop on ALR, ACL-IJCNLP 2009, pages 17?23, Suntec, Singapore, 6-7 August 2009.	ALR	Asian Language Resource$Asian Language Resources$	1
In Proceedings of the 3rd Work- shop on ALR and International Standardization, COLING 19, Taipei, Taiwan.	ALR	Asian Language Resource$Asian Language Resources$	1
352  Proceedings of the 8th Workshop on ALR, pages 169?177, Beijing, China, 21-22 August 2010.	ALR	Asian Language Resource$Asian Language Resources$	1
In In Proceedings of the Fourth Workshop on ALR, Sanya, China, pp.	ALR	Asian Language Resource$Asian Language Resources$	1
In Proceedings of the 3rd Workshop on ALR and In- ternational Standardization at the 19th International Conference on Computational Linguistics, Vol.	ALR	Asian Language Resource$Asian Language Resources$	1
This document is organized as follows: sec- tion 2 briefly introduces the related work; section 3 presents a characterization of the STT summarization problem and how we propose to address it; section 4 explicits our use of phonetic domain information, given the previously defined context; the next section describes the case study, including the experimental set up and results; con- clusions close the document.	STT	speech-to-text$speech recognition outputs$	0
5.4 Experimental Results Our main objective was to understand if it is pos- sible to select relevant information from back- ground information that could improve the quality of STT summaries.	STT	speech-to-text$speech recognition outputs$	0
The main idea is the inclusion of related, solid background information to cope with the difficulties of summarizing spoken lan- guage and the use of multi-document summariza- tion techniques in single document STT summarization.	STT	speech-to-text$speech recognition outputs$	0
The fisher corpus: a resource for the next generations of STT.	STT	speech-to-text$speech recognition outputs$	0
To support this argument, we de- veloped a new approach to STT summa- rization that combines information from multiple information sources to produce a summary driven by the spoken language document to be summa- rized.	STT	speech-to-text$speech recognition outputs$	0
For the ma- jority of these events, physicians and nurses generate free text data either by typing the information them- selves or by using a local or remote STT engine.	STT	speech-to-text$speech recognition outputs$	0
Indeed standard Language Models (LMs) ap- plied to OOD utterances are likely to generate erro- neous STT and more gener- ally highly noisy word lattices from which it might not be relevant and probably harmful to apply SLU modules.	STT	speech-to-text$speech recognition outputs$	1
ation for Computational Linguistics Punctuation Prediction with Transition-based Parsing  Dongdong Zhang1, Shuangzhi Wu2, Nan Yang3, Mu Li1    1Microsoft Research Asia, Beijing, China  2Harbin Institute of Technology, Harbin, China  3University of Science and Technology of China, Hefei, China  {dozhang,v-shuawu,v-nayang,muli}@microsoft.com    Abstract  Punctuations are not available in automatic  STT, which could cre- ate barriers to many subsequent text pro- cessing tasks.	STT	speech-to-text$speech recognition outputs$	1
To test this explanation, we looked at the word error rates for the STT for the different systems.	STT	speech-to-text$speech recognition outputs$	1
The alignment of STT is fairly straightforward due to the strict constraint in word order.	STT	speech-to-text$speech recognition outputs$	1
The method (Izumi et al, 2003) aims to de- tect omission-type and replacement-type errors and transformation-based leaning is employed in (Shi and Zhou, 2005) to learn rules to detect errors for STT.	STT	speech-to-text$speech recognition outputs$	1
If this is so then it is possible that a relatively ac-  cepting natural anguage system might work wen with worse  STT (because ven a relatively accept-  ing natural language system can reject very had inputs), but  with better speech recognizer output one might get good per-  formance with a stricter natural language system.	STT	speech-to-text$speech recognition outputs$	1
Proceedings of the ACM International Conference on  WSDM.	WSDM	Web Search and Web Data Mining$Web Search and Data Mining$	0
In Proceedings of the International Conference on  WSDM, pages 231-240.	WSDM	Web Search and Web Data Mining$Web Search and Data Mining$	0
In Proceedings of the international conference on WSDM, pages 171?182, Palo Alto, CA, USA.	WSDM	Web Search and Web Data Mining$Web Search and Data Mining$	0
Proceedings of the ACM International Conference  on WSDM.	WSDM	Web Search and Web Data Mining$Web Search and Data Mining$	0
In Pro- ceedings of the Second ACM International Conference on WSDM, pages 54?63.	WSDM	Web Search and Web Data Mining$Web Search and Data Mining$	1
In Pro- ceedings of the Second ACM International Conference on WSDM, pages 94?103.	WSDM	Web Search and Web Data Mining$Web Search and Data Mining$	1
In Proceed- ings of the Second ACM International Conference  on WSDM, Barcelona, Spain,  ACM.	WSDM	Web Search and Web Data Mining$Web Search and Data Mining$	1
In Proceed- ings of WSDM.	WSDM	Web Search and Web Data Mining$Web Search and Data Mining$	1
In Proceedings of the Second ACM International Con- ference on WSDM, Barcelona, Spain, February.	WSDM	Web Search and Web Data Mining$Web Search and Data Mining$	1
In Proceedings of First ACM International Conference on WSDM.	WSDM	Web Search and Web Data Mining$Web Search and Data Mining$	1
Implicit argument identification for nominal predicates is complementary to VPE r	VPE	verb phrase ellipsis$Verb Phrase Ellipsis$	0
Implicit argument identification for nominal predicates is complementary to VPE resolution: Both work to make implicit information explicit.	VPE	verb phrase ellipsis$Verb Phrase Ellipsis$	0
Dynamic interpretation of VPE.	VPE	verb phrase ellipsis$Verb Phrase Ellipsis$	0
We also implement simple heuristics that allow us to capture simple cases of control and VPE in many cases.	VPE	verb phrase ellipsis$Verb Phrase Ellipsis$	0
Webber considers three types of antecedents  those for definite pronouns, those for  one-anaphora, 13 and those for VPE.	VPE	verb phrase ellipsis$Verb Phrase Ellipsis$	0
Exploring the steps of VPE.	VPE	verb phrase ellipsis$Verb Phrase Ellipsis$	0
If one resolved the VPE, then the implicit agent (Bill) would be recovered.4 Nielsen (2004) created a system able to detect the presence of ellipses, producing the bracketing in Example (17).	VPE	verb phrase ellipsis$Verb Phrase Ellipsis$	0
A corpus-based study of VPE Identification and Resolution.	VPE	verb phrase ellipsis$Verb Phrase Ellipsis$	1
887  Subdeletion in VPE  Paul G. Donecker  Villanova University  800 Lancaster Avenue  Villanova, PA 19085  donecker@monet.vill.edu  Abstract  This paper stems from an ongoing research  project ~on verb phrase llipsis.	VPE	verb phrase ellipsis$Verb Phrase Ellipsis$	1
VPE:  Form, Meaning, and Processing.	VPE	verb phrase ellipsis$Verb Phrase Ellipsis$	1
\[Stainton-Ellis 1988\] Stainton-Ellis, C.S.:1988, A  processing perspective on VPE,  MPhil dissertation, University of Edinburgh.	VPE	verb phrase ellipsis$Verb Phrase Ellipsis$	1
These include verbs of becoming or seeming (e.g., trans- form, appear), light verbs, AUX, and aspec- tual verbs.	AUX	auxiliaries$auxiliary$	0
The inlransitive verbs.are  claksified accor .ding to semantic criteria (verbs of motion~ state) or  by their syntactic usa~ (like predicativi~ AUX, urgpers6nal  verbs with dative ).W'e should-notice that he same verb maybe  transitiv 9 or intransitive, accor "ding to its m e.	AUX	auxiliaries$auxiliary$	0
The general left-branching structure of the tree is a result of the analysis of the second-position clitic cluster: The clitic clusters are treated as argument-composition AUX, which combine with a lexical verb and ?	AUX	auxiliaries$auxiliary$	0
For the gold input, apart from LEMMA that pro- vides around 0.7 points, the most useful feature is 4NORK2, NOR1 and NMG are AUX case markers.	AUX	auxiliaries$auxiliary$	0
The AUX first pick up all dependents to the right, and then combine with exactly one constituent to the left.	AUX	auxiliaries$auxiliary$	0
The  light verbs themselves do not diminish in form  over time in a manner similar to AUX (Butt,  2004), although the complements of common  LVCs can change over time such that it is no  longer clear that the complement is a predicating  element.	AUX	auxiliaries$auxiliary$	0
Other constraints require an AUX verb to be modified by a full verb, or prescribe morphosyntactical agreement between a determiner and its regent (the word modified by the determiner).	AUX	auxiliaries$auxiliary$	1
There are many reasons why a simple word-to-word (1-to-1) correspondence is not possible for every sentence pair: for instance, AUX verbs used in one lan- guage but not the other (e.g., English He walked and French Il est alle?),	AUX	auxiliaries$auxiliary$	1
Perhaps the most striking feature of Wambaya is its word order: it is a radically non-configurational language with a second position AUX/clitic clus- ter.	AUX	auxiliaries$auxiliary$	1
in the surface structure, as follows: The AUX ngiya is subject to the constraints in (2), meaning that it combines with a verb as its first complement and then the verb?s complements as its remaining complements.9 The AUX can combine with its complements in any order, thanks to a series of head- complement rules which realize the nth element of 6The grammar in fact finds 42 parses for this example.	AUX	auxiliaries$auxiliary$	1
For in- stance, the finite and the full verb must com- bine to form an AUX phrase, because this is the only way of accounting for all words while satisfying valence and category con- straints.	AUX	auxiliaries$auxiliary$	1
Technical Report RS-87-190, USC/ISI, 1987.	ISI	Information Sciences Institute$International Statistical Institute$	0
c?2006 Association for Computational Linguistics A Better -Best List: Practical Determinization of Weighted Finite Tree Automata Jonathan May ISI University of Southern California Marina del Rey, CA 90292 jonmay@isi.edu Kevin Knight ISI University of Southern California Marina del Rey, CA 90292 knight@isi.edu Abstract Ranked lists of output trees from syn- tactic statistical NLP applications fre- quently contain multiple repeated entries.	ISI	Information Sciences Institute$International Statistical Institute$	0
c?2010 Association for Computational Linguistics Efficient Incremental Decoding for Tree-to-String Translation Liang Huang 1 1ISI University of Southern California 4676 Admiralty Way, Suite 1001 Marina del Rey, CA 90292, USA {lhuang,haitaomi}@isi.edu Haitao Mi 2,1 2Key Lab.	ISI	Information Sciences Institute$International Statistical Institute$	0
Available from  USC/ISI, Marina del  Rey, CA.	ISI	Information Sciences Institute$International Statistical Institute$	0
In Defense  of  Syntax:   In fo rmat iona l ,  In tent iona l ,  and Rhetor i ca l  S t ruc tures   in D iscourse   Eduard H. Hovy  ISI  of the University of Southern California  4676 Admiralty Way  Marina del Rey, CA 90292-6695  U.S.A.  tel: 310-822-1511  fax: 310-823-6714  email: hovy@isi.edu  Introduction: The Point of this Paper  Much has been written on the nature and use of so-called rhetorical relations to govern the structure anti  coherence of discourse, and much has been written on the need	ISI	Information Sciences Institute$International Statistical Institute$	0
Bul- letin of the ISI:92?94,  1969  G. Savova, T. Pedersen, A. Kulkarni and A. Puran- dare.	ISI	Information Sciences Institute$International Statistical Institute$	1
We map 51 different relations to the corpus and result in about 50,000 entity tuples, 134,000 sentences for training and 30,000 entity tuples, 53,000 STe.	STe	sentences for testing$sentence testing$	0
Out of which, 14500 sentences were taken as training set, 500 for development set and remaining 1000 STe set.	STe	sentences for testing$sentence testing$	0
It means we used 1,800 and 7,200 STe and training the discriminative sequence learning models, respectively.	STe	sentences for testing$sentence testing$	0
For Syntag, we split the treebank into 3,524 sen- tences for training and 1,763 STe.	STe	sentences for testing$sentence testing$	0
The first is 700 STe and the  other is for training.	STe	sentences for testing$sentence testing$	0
Using five-fold validation (i.e., chose different 100  STe each time and repeating the ex-  periment five times), The program achieved an aver-  age success rate of 81.3%.	STe	sentences for testing$sentence testing$	0
The data used for empirical evaluation was taken from (Roth and Yih, 2004) and consists of 1436 sen- tences, which is split into a 1149 (80%) sentence training set and a 287 (20%) STe set such that all have at least one active relation.	STe	sentences for testing$sentence testing$	1
c?2016 Association for Computational Linguistics Generalizing and Hybridizing Count-based and NLMs Graham Neubig?	NLM	Neural Language Model$neural language model$	0
Decoding with Large- scale NLMs improves Transla- tion.	NLM	Neural Language Model$neural language model$	0
c?2006 Association for Computational Linguistics Factored NLMs Andrei Alexandrescu Department of Comp.	NLM	Neural Language Model$neural language model$	0
2.1 Softmax NLM Our feed-forward neural network implements an n-gram language model, i.e., it is a parametric function estimating the probability of the next word w t given n ?	NLM	Neural Language Model$neural language model$	0
Decoding with Large-scale NLMs improves Translation.	NLM	Neural Language Model$neural language model$	0
c?2016 Association for Computational Linguistics Strategies for Training Large Vocabulary NLMs Wenlin Chen David Grangier Michael Auli Facebook, Menlo Park, CA Abstract Training neural network language mod- els over large vocabularies is computa- tionally costly compared to count-based models such as Kneser-Ney.	NLM	Neural Language Model$neural language model$	0
Recent work (Lebret and Lebret, 2013) has shown that the Hellinger distance is an especially effective mea- sure in learning distributional embeddings, with Hellinger PCA being much more computationally inexpensive than NLMing ap- proaches, while performing much better than stan- dard PCA, and competitive with the state-of-the- art in downstream evaluations.	NLM	Neural Language Model$neural language model$	1
5 Experiments To evaluate PLRE, we compared its performance on English and Russian corpora with several vari- 2 for derivation see proof of Lemma 4 in the supplemen- tary material 1493 ants of KN smoothing, class-based models, and the log-bilinear NLM (Mnih and Hinton, 2007).	NLM	Neural Language Model$neural language model$	1
2.1 Matrix factorization view of SkipGram SkipGram can be categorized as one of the simplest NLMs (Mnih and Kavukcuoglu, 2013).	NLM	Neural Language Model$neural language model$	1
Decoding with large- scale NLMs improves translation.	NLM	Neural Language Model$neural language model$	1
In computing these probabilities, the state ht?1 represents the tar- get history, and h0 is typically set to be some func- tion of x. The complete model (including encoder) is trained, analogously to a NLM, to minimize the cross-entropy loss at each time-step while conditioning on the gold history in the train- ing data.	NLM	Neural Language Model$neural language model$	1
Unifying visual-semantic embeddings with multimodal NLMs.	NLM	Neural Language Model$neural language model$	1
c?2016 Association for Computational Linguistics CPM in ASL Syntactic Facial Expression Synthesis Hernisa Kacorri Carnegie Mellon University Human-Computer Interaction Institute 5000 Forbes Avenue Pittsburgh, PA 15213, USA hkacorri@andrew.cmu.edu Matt Huenerfauth Rochester Institute of Technology B. Thomas Golisano College of Computing and Information Sciences 152 Lomb Memorial Drive Rochester, NY 14623, USA matt.huenerfaut	CPM	Continuous Profile Models$Continuous Profile Model$	0
c?2016 Association for Computational Linguistics CPMs in ASL Syntactic Facial Expression Synthesis Hernisa Kacorri Carnegie Mellon University Human-Computer Interaction Institute 5000 Forbes Avenue Pittsburgh, PA 15213, USA hkacorri@andrew.cmu.edu Matt Huenerfauth Rochester Institute of Technology B. Thomas Golisano College of Computing and Information Sciences 152 Lomb Memorial Drive Rochester, NY 14623, USA matt.huenerfaut	CPM	Continuous Profile Models$Continuous Profile Model$	1
